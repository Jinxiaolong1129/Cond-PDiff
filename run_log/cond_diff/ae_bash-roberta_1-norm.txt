stty: 'standard input': Inappropriate ioctl for device
2024-11-05 00:20:34,898 - INFO - CUDA_VISIBLE_DEVICES: 0,1,2,3
2024-11-05 00:20:34,898 - INFO - layer_num: [[11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]]
2024-11-05 00:20:34,898 - INFO - datasets_para: [['sst2', 'rte', 'mrpc', 'cola', 'qnli', 'stsb']]
2024-11-05 00:20:34,898 - INFO - gpu_id: 0
2024-11-05 00:20:34,898 - INFO - batch_size: 256
2024-11-05 00:20:34,898 - INFO - epochs: 10000
2024-11-05 00:20:34,898 - INFO - ae_test: True
2024-11-05 00:20:34,898 - INFO - load_ae_checkpoint: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.10000/model/auto_encoder_model_9999.pth
2024-11-05 00:20:34,898 - INFO - lr: 0.001
2024-11-05 00:20:34,898 - INFO - patience: 5000
transformers.__file__: /home/jin509/para_diff/lora-cond-p-diff/src/transformers/__init__.py 
 peft.__file_: /home/jin509/para_diff/lora-cond-p-diff/src/peft/__init__.py
=====================================
layer number: [11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
datasets: ['sst2', 'rte', 'mrpc', 'cola', 'qnli', 'stsb']
=====================================
wandb: Tracking run with wandb version 0.12.21
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/jin509/para_diff/lora-cond-p-diff/dataset/weights_dataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  dataset_data = torch.tensor(dataset_matrix, dtype=torch.float32)
/home/jin509/para_diff/lora-cond-p-diff/dataset/weights_dataset.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return (torch.tensor(data_row, dtype=torch.float32) - mean) / std
2024-11-05 00:22:02,728 - INFO - Load model: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.10000/model/auto_encoder_model_9999.pth
2024-11-05 00:22:02,729 - INFO - Start epoch: 9999
2024-11-05 00:22:02,747 - INFO - Loaded checkpoint from /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.10000/model/auto_encoder_model_9999.pth with best metric -inf
2024-11-05 00:22:02,750 - INFO - Model save dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.9999_end_epoch.10000/model
2024-11-05 00:22:02,750 - INFO - Log dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.9999_end_epoch.10000/log
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 00:22:03,130 - INFO - =========== ddpm diffusion model | num_conditions: 6 ===========
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
2024-11-05 00:22:06,152 - INFO - [diffusion][Epoch 0] Load model: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.8000/clip_pdiff_model/clip_cond_diffusion_model_7999.pth
2024-11-05 00:22:06,169 - INFO - [diffusion][Epoch 0] Loaded checkpoint from /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.8000/clip_pdiff_model/clip_cond_diffusion_model_7999.pth
2024-11-05 00:22:06,171 - INFO - [diffusion][Epoch 0] ========================================
2024-11-05 00:22:06,202 - INFO - Model save dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.12000/clip_pdiff_model
2024-11-05 00:22:06,202 - INFO - Log dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.12000/log
2024-11-05 00:22:06,202 - INFO - [diffusion][Epoch 0] Training begin
2024-11-05 00:22:06,204 - INFO - [diffusion][Epoch 7999] Epoch 8000/12000
=====================================
Test the auto_encoder_model
training_parameters:
  ddpm_eval_epoch: 10
  ddpm_save_epoch: 1000
  ddpm_start_epoch: 1
  ddpm_end_epoch: 12000
  batch_size: 256
  lr: 0.001
  patience: 5000
  optimizer: Adam
  loss_function: MSE
  metrics:
  - accuracy
  weight_decay: 2.0e-06
  ddpm_test: false
  test_similarity: false
model_mean_type: eps
model_var_type: fixedlarge
loss_type: mse
model_parameters:
  in_channel: 1
  in_dim: 12
  num_conditions: 2
  cond_emb_size: 512
beta_schedule:
  start: 0.0001
  end: 0.02
  schedule: linear
  n_timestep: 1000
glue_test_parameters:
  config_path: config/multiple/glue.json

2024-11-05 00:22:09,153 - INFO - [diffusion][Epoch 7999] diffusion training Loss: 0.10705429501831532
2024-11-05 00:22:09,155 - INFO - [diffusion][Epoch 7999] diffusion learning rate: 0.001
2024-11-05 00:22:09,236 - INFO - [diffusion][Epoch 7999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:09,238 - INFO - [diffusion][Epoch 8000] Epoch 8001/12000
2024-11-05 00:22:11,573 - INFO - [diffusion][Epoch 8000] diffusion training Loss: 0.09865966066718102
2024-11-05 00:22:11,574 - INFO - [diffusion][Epoch 8000] diffusion learning rate: 0.001
2024-11-05 00:22:11,576 - INFO - [diffusion][Epoch 8000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:11,577 - INFO - [diffusion][Epoch 8001] Epoch 8002/12000
2024-11-05 00:22:13,721 - INFO - [diffusion][Epoch 8001] diffusion training Loss: 0.08212138898670673
2024-11-05 00:22:13,723 - INFO - [diffusion][Epoch 8001] diffusion learning rate: 0.001
2024-11-05 00:22:13,724 - INFO - [diffusion][Epoch 8001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:13,726 - INFO - [diffusion][Epoch 8002] Epoch 8003/12000
2024-11-05 00:22:15,958 - INFO - [diffusion][Epoch 8002] diffusion training Loss: 0.09484061598777771
2024-11-05 00:22:15,960 - INFO - [diffusion][Epoch 8002] diffusion learning rate: 0.001
2024-11-05 00:22:15,962 - INFO - [diffusion][Epoch 8002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:15,963 - INFO - [diffusion][Epoch 8003] Epoch 8004/12000
2024-11-05 00:22:18,257 - INFO - [diffusion][Epoch 8003] diffusion training Loss: 0.0725772324949503
2024-11-05 00:22:18,258 - INFO - [diffusion][Epoch 8003] diffusion learning rate: 0.001
2024-11-05 00:22:18,260 - INFO - [diffusion][Epoch 8003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:18,261 - INFO - [diffusion][Epoch 8004] Epoch 8005/12000
2024-11-05 00:22:20,454 - INFO - [diffusion][Epoch 8004] diffusion training Loss: 0.07704857923090458
2024-11-05 00:22:20,457 - INFO - [diffusion][Epoch 8004] diffusion learning rate: 0.001
2024-11-05 00:22:20,459 - INFO - [diffusion][Epoch 8004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:20,461 - INFO - [diffusion][Epoch 8005] Epoch 8006/12000
2024-11-05 00:22:22,718 - INFO - [diffusion][Epoch 8005] diffusion training Loss: 0.07497860305011272
2024-11-05 00:22:22,720 - INFO - [diffusion][Epoch 8005] diffusion learning rate: 0.001
2024-11-05 00:22:22,722 - INFO - [diffusion][Epoch 8005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:22,723 - INFO - [diffusion][Epoch 8006] Epoch 8007/12000
2024-11-05 00:22:25,026 - INFO - [diffusion][Epoch 8006] diffusion training Loss: 0.06832671351730824
2024-11-05 00:22:25,028 - INFO - [diffusion][Epoch 8006] diffusion learning rate: 0.001
2024-11-05 00:22:25,030 - INFO - [diffusion][Epoch 8006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:25,031 - INFO - [diffusion][Epoch 8007] Epoch 8008/12000
2024-11-05 00:22:27,247 - INFO - [diffusion][Epoch 8007] diffusion training Loss: 0.07188398577272892
2024-11-05 00:22:27,249 - INFO - [diffusion][Epoch 8007] diffusion learning rate: 0.001
2024-11-05 00:22:27,251 - INFO - [diffusion][Epoch 8007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:27,252 - INFO - [diffusion][Epoch 8008] Epoch 8009/12000
2024-11-05 00:22:29,487 - INFO - [diffusion][Epoch 8008] diffusion training Loss: 0.0708263348788023
2024-11-05 00:22:29,489 - INFO - [diffusion][Epoch 8008] diffusion learning rate: 0.001
2024-11-05 00:22:29,491 - INFO - [diffusion][Epoch 8008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:29,492 - INFO - [diffusion][Epoch 8009] Epoch 8010/12000
2024-11-05 00:22:31,767 - INFO - [diffusion][Epoch 8009] diffusion training Loss: 0.07414091564714909
2024-11-05 00:22:31,769 - INFO - [diffusion][Epoch 8009] diffusion learning rate: 0.001
2024-11-05 00:22:31,771 - INFO - [diffusion][Epoch 8009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:31,772 - INFO - [diffusion][Epoch 8010] Epoch 8011/12000
2024-11-05 00:22:34,059 - INFO - [diffusion][Epoch 8010] diffusion training Loss: 0.07265067286789417
2024-11-05 00:22:34,061 - INFO - [diffusion][Epoch 8010] diffusion learning rate: 0.001
2024-11-05 00:22:34,063 - INFO - [diffusion][Epoch 8010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:34,064 - INFO - [diffusion][Epoch 8011] Epoch 8012/12000
2024-11-05 00:22:36,289 - INFO - [diffusion][Epoch 8011] diffusion training Loss: 0.07605722546577454
2024-11-05 00:22:36,290 - INFO - [diffusion][Epoch 8011] diffusion learning rate: 0.001
2024-11-05 00:22:36,292 - INFO - [diffusion][Epoch 8011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:36,293 - INFO - [diffusion][Epoch 8012] Epoch 8013/12000
2024-11-05 00:22:38,522 - INFO - [diffusion][Epoch 8012] diffusion training Loss: 0.07446286082267761
2024-11-05 00:22:38,524 - INFO - [diffusion][Epoch 8012] diffusion learning rate: 0.001
2024-11-05 00:22:38,526 - INFO - [diffusion][Epoch 8012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:38,527 - INFO - [diffusion][Epoch 8013] Epoch 8014/12000
2024-11-05 00:22:40,777 - INFO - [diffusion][Epoch 8013] diffusion training Loss: 0.07617359422147274
2024-11-05 00:22:40,779 - INFO - [diffusion][Epoch 8013] diffusion learning rate: 0.001
2024-11-05 00:22:40,781 - INFO - [diffusion][Epoch 8013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:40,782 - INFO - [diffusion][Epoch 8014] Epoch 8015/12000
2024-11-05 00:22:43,036 - INFO - [diffusion][Epoch 8014] diffusion training Loss: 0.0707135908305645
2024-11-05 00:22:43,037 - INFO - [diffusion][Epoch 8014] diffusion learning rate: 0.001
2024-11-05 00:22:43,041 - INFO - [diffusion][Epoch 8014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:43,043 - INFO - [diffusion][Epoch 8015] Epoch 8016/12000
2024-11-05 00:22:45,310 - INFO - [diffusion][Epoch 8015] diffusion training Loss: 0.07162963971495628
2024-11-05 00:22:45,312 - INFO - [diffusion][Epoch 8015] diffusion learning rate: 0.001
2024-11-05 00:22:45,314 - INFO - [diffusion][Epoch 8015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:45,315 - INFO - [diffusion][Epoch 8016] Epoch 8017/12000
2024-11-05 00:22:47,642 - INFO - [diffusion][Epoch 8016] diffusion training Loss: 0.0720862876623869
2024-11-05 00:22:47,644 - INFO - [diffusion][Epoch 8016] diffusion learning rate: 0.001
2024-11-05 00:22:47,645 - INFO - [diffusion][Epoch 8016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:47,647 - INFO - [diffusion][Epoch 8017] Epoch 8018/12000
2024-11-05 00:22:49,937 - INFO - [diffusion][Epoch 8017] diffusion training Loss: 0.07988349162042141
2024-11-05 00:22:49,939 - INFO - [diffusion][Epoch 8017] diffusion learning rate: 0.001
2024-11-05 00:22:49,979 - INFO - [diffusion][Epoch 8017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:49,980 - INFO - [diffusion][Epoch 8018] Epoch 8019/12000
2024-11-05 00:22:52,495 - INFO - [diffusion][Epoch 8018] diffusion training Loss: 0.06750327162444592
2024-11-05 00:22:52,496 - INFO - [diffusion][Epoch 8018] diffusion learning rate: 0.001
2024-11-05 00:22:52,498 - INFO - [diffusion][Epoch 8018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:52,499 - INFO - [diffusion][Epoch 8019] Epoch 8020/12000
2024-11-05 00:22:54,757 - INFO - [diffusion][Epoch 8019] diffusion training Loss: 0.06898715533316135
2024-11-05 00:22:54,759 - INFO - [diffusion][Epoch 8019] diffusion learning rate: 0.001
2024-11-05 00:22:54,762 - INFO - [diffusion][Epoch 8019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:54,763 - INFO - [diffusion][Epoch 8020] Epoch 8021/12000
2024-11-05 00:22:57,006 - INFO - [diffusion][Epoch 8020] diffusion training Loss: 0.06788433529436588
2024-11-05 00:22:57,008 - INFO - [diffusion][Epoch 8020] diffusion learning rate: 0.001
2024-11-05 00:22:57,010 - INFO - [diffusion][Epoch 8020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:57,011 - INFO - [diffusion][Epoch 8021] Epoch 8022/12000
2024-11-05 00:22:59,250 - INFO - [diffusion][Epoch 8021] diffusion training Loss: 0.0670560970902443
2024-11-05 00:22:59,252 - INFO - [diffusion][Epoch 8021] diffusion learning rate: 0.001
2024-11-05 00:22:59,254 - INFO - [diffusion][Epoch 8021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:59,255 - INFO - [diffusion][Epoch 8022] Epoch 8023/12000
2024-11-05 00:23:01,349 - INFO - [diffusion][Epoch 8022] diffusion training Loss: 0.06816500797867775
2024-11-05 00:23:01,352 - INFO - [diffusion][Epoch 8022] diffusion learning rate: 0.001
2024-11-05 00:23:01,354 - INFO - [diffusion][Epoch 8022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:01,356 - INFO - [diffusion][Epoch 8023] Epoch 8024/12000
2024-11-05 00:23:03,611 - INFO - [diffusion][Epoch 8023] diffusion training Loss: 0.07195567712187767
2024-11-05 00:23:03,614 - INFO - [diffusion][Epoch 8023] diffusion learning rate: 0.001
2024-11-05 00:23:03,616 - INFO - [diffusion][Epoch 8023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:03,617 - INFO - [diffusion][Epoch 8024] Epoch 8025/12000
2024-11-05 00:23:05,866 - INFO - [diffusion][Epoch 8024] diffusion training Loss: 0.07357652485370636
2024-11-05 00:23:05,868 - INFO - [diffusion][Epoch 8024] diffusion learning rate: 0.001
2024-11-05 00:23:05,870 - INFO - [diffusion][Epoch 8024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:05,871 - INFO - [diffusion][Epoch 8025] Epoch 8026/12000
2024-11-05 00:23:08,112 - INFO - [diffusion][Epoch 8025] diffusion training Loss: 0.07708279229700565
2024-11-05 00:23:08,114 - INFO - [diffusion][Epoch 8025] diffusion learning rate: 0.001
2024-11-05 00:23:08,116 - INFO - [diffusion][Epoch 8025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:08,117 - INFO - [diffusion][Epoch 8026] Epoch 8027/12000
2024-11-05 00:23:10,408 - INFO - [diffusion][Epoch 8026] diffusion training Loss: 0.07127372734248638
2024-11-05 00:23:10,409 - INFO - [diffusion][Epoch 8026] diffusion learning rate: 0.001
2024-11-05 00:23:10,411 - INFO - [diffusion][Epoch 8026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:10,412 - INFO - [diffusion][Epoch 8027] Epoch 8028/12000
2024-11-05 00:23:12,668 - INFO - [diffusion][Epoch 8027] diffusion training Loss: 0.06636240053921938
2024-11-05 00:23:12,670 - INFO - [diffusion][Epoch 8027] diffusion learning rate: 0.001
2024-11-05 00:23:12,672 - INFO - [diffusion][Epoch 8027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:12,674 - INFO - [diffusion][Epoch 8028] Epoch 8029/12000
2024-11-05 00:23:14,895 - INFO - [diffusion][Epoch 8028] diffusion training Loss: 0.07210040651261806
2024-11-05 00:23:14,897 - INFO - [diffusion][Epoch 8028] diffusion learning rate: 0.001
2024-11-05 00:23:14,898 - INFO - [diffusion][Epoch 8028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:14,900 - INFO - [diffusion][Epoch 8029] Epoch 8030/12000
2024-11-05 00:23:17,179 - INFO - [diffusion][Epoch 8029] diffusion training Loss: 0.07453407533466816
2024-11-05 00:23:17,181 - INFO - [diffusion][Epoch 8029] diffusion learning rate: 0.001
2024-11-05 00:23:17,183 - INFO - [diffusion][Epoch 8029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:17,184 - INFO - [diffusion][Epoch 8030] Epoch 8031/12000
2024-11-05 00:23:19,476 - INFO - [diffusion][Epoch 8030] diffusion training Loss: 0.0762416236102581
2024-11-05 00:23:19,478 - INFO - [diffusion][Epoch 8030] diffusion learning rate: 0.001
2024-11-05 00:23:19,480 - INFO - [diffusion][Epoch 8030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:19,481 - INFO - [diffusion][Epoch 8031] Epoch 8032/12000
2024-11-05 00:23:21,689 - INFO - [diffusion][Epoch 8031] diffusion training Loss: 0.07288318313658237
2024-11-05 00:23:21,691 - INFO - [diffusion][Epoch 8031] diffusion learning rate: 0.001
2024-11-05 00:23:21,692 - INFO - [diffusion][Epoch 8031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:21,694 - INFO - [diffusion][Epoch 8032] Epoch 8033/12000
2024-11-05 00:23:23,940 - INFO - [diffusion][Epoch 8032] diffusion training Loss: 0.07107261940836906
2024-11-05 00:23:23,942 - INFO - [diffusion][Epoch 8032] diffusion learning rate: 0.001
2024-11-05 00:23:23,944 - INFO - [diffusion][Epoch 8032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:23,945 - INFO - [diffusion][Epoch 8033] Epoch 8034/12000
2024-11-05 00:23:26,219 - INFO - [diffusion][Epoch 8033] diffusion training Loss: 0.06940057966858149
2024-11-05 00:23:26,221 - INFO - [diffusion][Epoch 8033] diffusion learning rate: 0.001
2024-11-05 00:23:26,223 - INFO - [diffusion][Epoch 8033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:26,226 - INFO - [diffusion][Epoch 8034] Epoch 8035/12000
2024-11-05 00:23:28,508 - INFO - [diffusion][Epoch 8034] diffusion training Loss: 0.06997493095695972
2024-11-05 00:23:28,511 - INFO - [diffusion][Epoch 8034] diffusion learning rate: 0.001
2024-11-05 00:23:28,513 - INFO - [diffusion][Epoch 8034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:28,516 - INFO - [diffusion][Epoch 8035] Epoch 8036/12000
2024-11-05 00:23:30,773 - INFO - [diffusion][Epoch 8035] diffusion training Loss: 0.07389407977461815
2024-11-05 00:23:30,774 - INFO - [diffusion][Epoch 8035] diffusion learning rate: 0.001
2024-11-05 00:23:30,776 - INFO - [diffusion][Epoch 8035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:30,777 - INFO - [diffusion][Epoch 8036] Epoch 8037/12000
2024-11-05 00:23:33,028 - INFO - [diffusion][Epoch 8036] diffusion training Loss: 0.07370783761143684
2024-11-05 00:23:33,030 - INFO - [diffusion][Epoch 8036] diffusion learning rate: 0.001
2024-11-05 00:23:33,032 - INFO - [diffusion][Epoch 8036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:33,033 - INFO - [diffusion][Epoch 8037] Epoch 8038/12000
2024-11-05 00:23:35,368 - INFO - [diffusion][Epoch 8037] diffusion training Loss: 0.07843305356800556
2024-11-05 00:23:35,370 - INFO - [diffusion][Epoch 8037] diffusion learning rate: 0.001
2024-11-05 00:23:35,372 - INFO - [diffusion][Epoch 8037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:35,373 - INFO - [diffusion][Epoch 8038] Epoch 8039/12000
2024-11-05 00:23:38,266 - INFO - [diffusion][Epoch 8038] diffusion training Loss: 0.0748674813657999
2024-11-05 00:23:38,268 - INFO - [diffusion][Epoch 8038] diffusion learning rate: 0.001
2024-11-05 00:23:38,269 - INFO - [diffusion][Epoch 8038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:38,271 - INFO - [diffusion][Epoch 8039] Epoch 8040/12000
2024-11-05 00:23:40,486 - INFO - [diffusion][Epoch 8039] diffusion training Loss: 0.07538732327520847
2024-11-05 00:23:40,488 - INFO - [diffusion][Epoch 8039] diffusion learning rate: 0.001
2024-11-05 00:23:40,492 - INFO - [diffusion][Epoch 8039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:40,493 - INFO - [diffusion][Epoch 8040] Epoch 8041/12000
2024-11-05 00:23:42,739 - INFO - [diffusion][Epoch 8040] diffusion training Loss: 0.0725899338722229
2024-11-05 00:23:42,742 - INFO - [diffusion][Epoch 8040] diffusion learning rate: 0.001
2024-11-05 00:23:42,744 - INFO - [diffusion][Epoch 8040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:42,746 - INFO - [diffusion][Epoch 8041] Epoch 8042/12000
2024-11-05 00:23:44,986 - INFO - [diffusion][Epoch 8041] diffusion training Loss: 0.07204283028841019
2024-11-05 00:23:44,988 - INFO - [diffusion][Epoch 8041] diffusion learning rate: 0.001
2024-11-05 00:23:44,990 - INFO - [diffusion][Epoch 8041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:44,991 - INFO - [diffusion][Epoch 8042] Epoch 8043/12000
2024-11-05 00:23:47,222 - INFO - [diffusion][Epoch 8042] diffusion training Loss: 0.07325763441622257
2024-11-05 00:23:47,224 - INFO - [diffusion][Epoch 8042] diffusion learning rate: 0.001
2024-11-05 00:23:47,226 - INFO - [diffusion][Epoch 8042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:47,227 - INFO - [diffusion][Epoch 8043] Epoch 8044/12000
2024-11-05 00:23:49,452 - INFO - [diffusion][Epoch 8043] diffusion training Loss: 0.0715298242866993
2024-11-05 00:23:49,454 - INFO - [diffusion][Epoch 8043] diffusion learning rate: 0.001
2024-11-05 00:23:49,456 - INFO - [diffusion][Epoch 8043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:49,457 - INFO - [diffusion][Epoch 8044] Epoch 8045/12000
2024-11-05 00:23:51,795 - INFO - [diffusion][Epoch 8044] diffusion training Loss: 0.07834602892398834
2024-11-05 00:23:51,797 - INFO - [diffusion][Epoch 8044] diffusion learning rate: 0.001
2024-11-05 00:23:51,799 - INFO - [diffusion][Epoch 8044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:51,800 - INFO - [diffusion][Epoch 8045] Epoch 8046/12000
2024-11-05 00:23:54,015 - INFO - [diffusion][Epoch 8045] diffusion training Loss: 0.0703588780015707
2024-11-05 00:23:54,018 - INFO - [diffusion][Epoch 8045] diffusion learning rate: 0.001
2024-11-05 00:23:54,020 - INFO - [diffusion][Epoch 8045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:54,021 - INFO - [diffusion][Epoch 8046] Epoch 8047/12000
2024-11-05 00:23:56,363 - INFO - [diffusion][Epoch 8046] diffusion training Loss: 0.07024548389017582
2024-11-05 00:23:56,365 - INFO - [diffusion][Epoch 8046] diffusion learning rate: 0.001
2024-11-05 00:23:56,367 - INFO - [diffusion][Epoch 8046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:56,368 - INFO - [diffusion][Epoch 8047] Epoch 8048/12000
2024-11-05 00:23:58,666 - INFO - [diffusion][Epoch 8047] diffusion training Loss: 0.08053156919777393
2024-11-05 00:23:58,668 - INFO - [diffusion][Epoch 8047] diffusion learning rate: 0.001
2024-11-05 00:23:58,670 - INFO - [diffusion][Epoch 8047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:58,671 - INFO - [diffusion][Epoch 8048] Epoch 8049/12000
2024-11-05 00:24:00,947 - INFO - [diffusion][Epoch 8048] diffusion training Loss: 0.07341919653117657
2024-11-05 00:24:00,949 - INFO - [diffusion][Epoch 8048] diffusion learning rate: 0.001
2024-11-05 00:24:00,951 - INFO - [diffusion][Epoch 8048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:00,952 - INFO - [diffusion][Epoch 8049] Epoch 8050/12000
2024-11-05 00:24:03,201 - INFO - [diffusion][Epoch 8049] diffusion training Loss: 0.07812819257378578
2024-11-05 00:24:03,203 - INFO - [diffusion][Epoch 8049] diffusion learning rate: 0.001
2024-11-05 00:24:03,205 - INFO - [diffusion][Epoch 8049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:03,206 - INFO - [diffusion][Epoch 8050] Epoch 8051/12000
2024-11-05 00:24:05,433 - INFO - [diffusion][Epoch 8050] diffusion training Loss: 0.06977858673781157
2024-11-05 00:24:05,437 - INFO - [diffusion][Epoch 8050] diffusion learning rate: 0.001
2024-11-05 00:24:05,439 - INFO - [diffusion][Epoch 8050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:05,440 - INFO - [diffusion][Epoch 8051] Epoch 8052/12000
2024-11-05 00:24:07,680 - INFO - [diffusion][Epoch 8051] diffusion training Loss: 0.07252059318125248
2024-11-05 00:24:07,681 - INFO - [diffusion][Epoch 8051] diffusion learning rate: 0.001
2024-11-05 00:24:07,683 - INFO - [diffusion][Epoch 8051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:07,684 - INFO - [diffusion][Epoch 8052] Epoch 8053/12000
2024-11-05 00:24:09,973 - INFO - [diffusion][Epoch 8052] diffusion training Loss: 0.0775025226175785
2024-11-05 00:24:09,976 - INFO - [diffusion][Epoch 8052] diffusion learning rate: 0.001
2024-11-05 00:24:09,978 - INFO - [diffusion][Epoch 8052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:09,980 - INFO - [diffusion][Epoch 8053] Epoch 8054/12000
2024-11-05 00:24:12,225 - INFO - [diffusion][Epoch 8053] diffusion training Loss: 0.07320088520646095
2024-11-05 00:24:12,227 - INFO - [diffusion][Epoch 8053] diffusion learning rate: 0.001
2024-11-05 00:24:12,229 - INFO - [diffusion][Epoch 8053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:12,230 - INFO - [diffusion][Epoch 8054] Epoch 8055/12000
2024-11-05 00:24:14,708 - INFO - [diffusion][Epoch 8054] diffusion training Loss: 0.0713801197707653
2024-11-05 00:24:14,710 - INFO - [diffusion][Epoch 8054] diffusion learning rate: 0.001
2024-11-05 00:24:14,713 - INFO - [diffusion][Epoch 8054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:14,715 - INFO - [diffusion][Epoch 8055] Epoch 8056/12000
2024-11-05 00:24:17,159 - INFO - [diffusion][Epoch 8055] diffusion training Loss: 0.06771265901625156
2024-11-05 00:24:17,163 - INFO - [diffusion][Epoch 8055] diffusion learning rate: 0.001
2024-11-05 00:24:17,166 - INFO - [diffusion][Epoch 8055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:17,168 - INFO - [diffusion][Epoch 8056] Epoch 8057/12000
2024-11-05 00:24:19,536 - INFO - [diffusion][Epoch 8056] diffusion training Loss: 0.07769240997731686
2024-11-05 00:24:19,538 - INFO - [diffusion][Epoch 8056] diffusion learning rate: 0.001
2024-11-05 00:24:19,539 - INFO - [diffusion][Epoch 8056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:19,541 - INFO - [diffusion][Epoch 8057] Epoch 8058/12000
2024-11-05 00:24:22,273 - INFO - [diffusion][Epoch 8057] diffusion training Loss: 0.07677488215267658
2024-11-05 00:24:22,275 - INFO - [diffusion][Epoch 8057] diffusion learning rate: 0.001
2024-11-05 00:24:22,277 - INFO - [diffusion][Epoch 8057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:22,279 - INFO - [diffusion][Epoch 8058] Epoch 8059/12000
2024-11-05 00:24:24,663 - INFO - [diffusion][Epoch 8058] diffusion training Loss: 0.07539288513362408
2024-11-05 00:24:24,666 - INFO - [diffusion][Epoch 8058] diffusion learning rate: 0.001
2024-11-05 00:24:24,668 - INFO - [diffusion][Epoch 8058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:24,669 - INFO - [diffusion][Epoch 8059] Epoch 8060/12000
2024-11-05 00:24:27,049 - INFO - [diffusion][Epoch 8059] diffusion training Loss: 0.06979629769921303
2024-11-05 00:24:27,053 - INFO - [diffusion][Epoch 8059] diffusion learning rate: 0.001
2024-11-05 00:24:27,055 - INFO - [diffusion][Epoch 8059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:27,057 - INFO - [diffusion][Epoch 8060] Epoch 8061/12000
2024-11-05 00:24:29,457 - INFO - [diffusion][Epoch 8060] diffusion training Loss: 0.07378198951482773
2024-11-05 00:24:29,459 - INFO - [diffusion][Epoch 8060] diffusion learning rate: 0.001
2024-11-05 00:24:29,461 - INFO - [diffusion][Epoch 8060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:29,465 - INFO - [diffusion][Epoch 8061] Epoch 8062/12000
2024-11-05 00:24:32,002 - INFO - [diffusion][Epoch 8061] diffusion training Loss: 0.07163715362548828
2024-11-05 00:24:32,005 - INFO - [diffusion][Epoch 8061] diffusion learning rate: 0.001
2024-11-05 00:24:32,007 - INFO - [diffusion][Epoch 8061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:32,008 - INFO - [diffusion][Epoch 8062] Epoch 8063/12000
2024-11-05 00:24:34,364 - INFO - [diffusion][Epoch 8062] diffusion training Loss: 0.07477328553795815
2024-11-05 00:24:34,400 - INFO - [diffusion][Epoch 8062] diffusion learning rate: 0.001
2024-11-05 00:24:34,405 - INFO - [diffusion][Epoch 8062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:34,406 - INFO - [diffusion][Epoch 8063] Epoch 8064/12000
2024-11-05 00:24:36,690 - INFO - [diffusion][Epoch 8063] diffusion training Loss: 0.07798895239830017
2024-11-05 00:24:36,692 - INFO - [diffusion][Epoch 8063] diffusion learning rate: 0.001
2024-11-05 00:24:36,694 - INFO - [diffusion][Epoch 8063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:36,696 - INFO - [diffusion][Epoch 8064] Epoch 8065/12000
2024-11-05 00:24:39,080 - INFO - [diffusion][Epoch 8064] diffusion training Loss: 0.07103190757334232
2024-11-05 00:24:39,082 - INFO - [diffusion][Epoch 8064] diffusion learning rate: 0.001
2024-11-05 00:24:39,083 - INFO - [diffusion][Epoch 8064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:39,085 - INFO - [diffusion][Epoch 8065] Epoch 8066/12000
2024-11-05 00:24:41,432 - INFO - [diffusion][Epoch 8065] diffusion training Loss: 0.07190589606761932
2024-11-05 00:24:41,434 - INFO - [diffusion][Epoch 8065] diffusion learning rate: 0.001
2024-11-05 00:24:41,436 - INFO - [diffusion][Epoch 8065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:41,437 - INFO - [diffusion][Epoch 8066] Epoch 8067/12000
2024-11-05 00:24:43,828 - INFO - [diffusion][Epoch 8066] diffusion training Loss: 0.0734672024846077
2024-11-05 00:24:43,830 - INFO - [diffusion][Epoch 8066] diffusion learning rate: 0.001
2024-11-05 00:24:43,832 - INFO - [diffusion][Epoch 8066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:43,833 - INFO - [diffusion][Epoch 8067] Epoch 8068/12000
2024-11-05 00:24:46,163 - INFO - [diffusion][Epoch 8067] diffusion training Loss: 0.0733902994543314
2024-11-05 00:24:46,165 - INFO - [diffusion][Epoch 8067] diffusion learning rate: 0.001
2024-11-05 00:24:46,167 - INFO - [diffusion][Epoch 8067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:46,168 - INFO - [diffusion][Epoch 8068] Epoch 8069/12000
2024-11-05 00:24:48,549 - INFO - [diffusion][Epoch 8068] diffusion training Loss: 0.0706787146627903
2024-11-05 00:24:48,550 - INFO - [diffusion][Epoch 8068] diffusion learning rate: 0.001
2024-11-05 00:24:48,552 - INFO - [diffusion][Epoch 8068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:48,554 - INFO - [diffusion][Epoch 8069] Epoch 8070/12000
2024-11-05 00:24:50,966 - INFO - [diffusion][Epoch 8069] diffusion training Loss: 0.06824187189340591
2024-11-05 00:24:50,969 - INFO - [diffusion][Epoch 8069] diffusion learning rate: 0.001
2024-11-05 00:24:50,971 - INFO - [diffusion][Epoch 8069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:50,973 - INFO - [diffusion][Epoch 8070] Epoch 8071/12000
2024-11-05 00:24:53,361 - INFO - [diffusion][Epoch 8070] diffusion training Loss: 0.07051024958491325
2024-11-05 00:24:53,363 - INFO - [diffusion][Epoch 8070] diffusion learning rate: 0.001
2024-11-05 00:24:53,365 - INFO - [diffusion][Epoch 8070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:53,366 - INFO - [diffusion][Epoch 8071] Epoch 8072/12000
2024-11-05 00:24:55,815 - INFO - [diffusion][Epoch 8071] diffusion training Loss: 0.07026712037622929
2024-11-05 00:24:55,818 - INFO - [diffusion][Epoch 8071] diffusion learning rate: 0.001
2024-11-05 00:24:55,820 - INFO - [diffusion][Epoch 8071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:55,821 - INFO - [diffusion][Epoch 8072] Epoch 8073/12000
2024-11-05 00:24:58,037 - INFO - [diffusion][Epoch 8072] diffusion training Loss: 0.07335190288722515
2024-11-05 00:24:58,041 - INFO - [diffusion][Epoch 8072] diffusion learning rate: 0.001
2024-11-05 00:24:58,044 - INFO - [diffusion][Epoch 8072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:58,046 - INFO - [diffusion][Epoch 8073] Epoch 8074/12000
2024-11-05 00:25:00,233 - INFO - [diffusion][Epoch 8073] diffusion training Loss: 0.0738174170255661
2024-11-05 00:25:00,236 - INFO - [diffusion][Epoch 8073] diffusion learning rate: 0.001
2024-11-05 00:25:00,238 - INFO - [diffusion][Epoch 8073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:00,239 - INFO - [diffusion][Epoch 8074] Epoch 8075/12000
2024-11-05 00:25:02,564 - INFO - [diffusion][Epoch 8074] diffusion training Loss: 0.0692348126322031
2024-11-05 00:25:02,566 - INFO - [diffusion][Epoch 8074] diffusion learning rate: 0.001
2024-11-05 00:25:02,568 - INFO - [diffusion][Epoch 8074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:02,569 - INFO - [diffusion][Epoch 8075] Epoch 8076/12000
2024-11-05 00:25:04,834 - INFO - [diffusion][Epoch 8075] diffusion training Loss: 0.06713112443685532
2024-11-05 00:25:04,836 - INFO - [diffusion][Epoch 8075] diffusion learning rate: 0.001
2024-11-05 00:25:04,838 - INFO - [diffusion][Epoch 8075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:04,839 - INFO - [diffusion][Epoch 8076] Epoch 8077/12000
2024-11-05 00:25:08,305 - INFO - [diffusion][Epoch 8076] diffusion training Loss: 0.0675344280898571
2024-11-05 00:25:08,309 - INFO - [diffusion][Epoch 8076] diffusion learning rate: 0.001
2024-11-05 00:25:08,312 - INFO - [diffusion][Epoch 8076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:08,314 - INFO - [diffusion][Epoch 8077] Epoch 8078/12000
2024-11-05 00:25:11,507 - INFO - [diffusion][Epoch 8077] diffusion training Loss: 0.06579519994556904
2024-11-05 00:25:11,509 - INFO - [diffusion][Epoch 8077] diffusion learning rate: 0.001
2024-11-05 00:25:11,510 - INFO - [diffusion][Epoch 8077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:11,511 - INFO - [diffusion][Epoch 8078] Epoch 8079/12000
2024-11-05 00:25:14,568 - INFO - [diffusion][Epoch 8078] diffusion training Loss: 0.07748826034367085
2024-11-05 00:25:14,571 - INFO - [diffusion][Epoch 8078] diffusion learning rate: 0.001
2024-11-05 00:25:14,574 - INFO - [diffusion][Epoch 8078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:14,575 - INFO - [diffusion][Epoch 8079] Epoch 8080/12000
2024-11-05 00:25:17,699 - INFO - [diffusion][Epoch 8079] diffusion training Loss: 0.07256294414401054
2024-11-05 00:25:17,701 - INFO - [diffusion][Epoch 8079] diffusion learning rate: 0.001
2024-11-05 00:25:17,703 - INFO - [diffusion][Epoch 8079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:17,704 - INFO - [diffusion][Epoch 8080] Epoch 8081/12000
2024-11-05 00:25:21,239 - INFO - [diffusion][Epoch 8080] diffusion training Loss: 0.07534060068428516
2024-11-05 00:25:21,241 - INFO - [diffusion][Epoch 8080] diffusion learning rate: 0.001
2024-11-05 00:25:21,243 - INFO - [diffusion][Epoch 8080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:21,244 - INFO - [diffusion][Epoch 8081] Epoch 8082/12000
2024-11-05 00:25:24,732 - INFO - [diffusion][Epoch 8081] diffusion training Loss: 0.07249870896339417
2024-11-05 00:25:24,756 - INFO - [diffusion][Epoch 8081] diffusion learning rate: 0.001
2024-11-05 00:25:24,758 - INFO - [diffusion][Epoch 8081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:24,759 - INFO - [diffusion][Epoch 8082] Epoch 8083/12000
2024-11-05 00:25:27,899 - INFO - [diffusion][Epoch 8082] diffusion training Loss: 0.07333688251674175
2024-11-05 00:25:27,901 - INFO - [diffusion][Epoch 8082] diffusion learning rate: 0.001
2024-11-05 00:25:27,903 - INFO - [diffusion][Epoch 8082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:27,904 - INFO - [diffusion][Epoch 8083] Epoch 8084/12000
2024-11-05 00:25:31,051 - INFO - [diffusion][Epoch 8083] diffusion training Loss: 0.07125422917306423
2024-11-05 00:25:31,053 - INFO - [diffusion][Epoch 8083] diffusion learning rate: 0.001
2024-11-05 00:25:31,055 - INFO - [diffusion][Epoch 8083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:31,056 - INFO - [diffusion][Epoch 8084] Epoch 8085/12000
2024-11-05 00:25:34,265 - INFO - [diffusion][Epoch 8084] diffusion training Loss: 0.07351776584982872
2024-11-05 00:25:34,267 - INFO - [diffusion][Epoch 8084] diffusion learning rate: 0.001
2024-11-05 00:25:34,269 - INFO - [diffusion][Epoch 8084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:34,270 - INFO - [diffusion][Epoch 8085] Epoch 8086/12000
2024-11-05 00:25:38,008 - INFO - [diffusion][Epoch 8085] diffusion training Loss: 0.07286391966044903
2024-11-05 00:25:38,010 - INFO - [diffusion][Epoch 8085] diffusion learning rate: 0.001
2024-11-05 00:25:38,012 - INFO - [diffusion][Epoch 8085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:38,013 - INFO - [diffusion][Epoch 8086] Epoch 8087/12000
2024-11-05 00:25:41,350 - INFO - [diffusion][Epoch 8086] diffusion training Loss: 0.07438644021749496
2024-11-05 00:25:41,353 - INFO - [diffusion][Epoch 8086] diffusion learning rate: 0.001
2024-11-05 00:25:41,355 - INFO - [diffusion][Epoch 8086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:41,357 - INFO - [diffusion][Epoch 8087] Epoch 8088/12000
2024-11-05 00:25:44,286 - INFO - [diffusion][Epoch 8087] diffusion training Loss: 0.07043531537055969
2024-11-05 00:25:44,288 - INFO - [diffusion][Epoch 8087] diffusion learning rate: 0.001
2024-11-05 00:25:44,325 - INFO - [diffusion][Epoch 8087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:44,327 - INFO - [diffusion][Epoch 8088] Epoch 8089/12000
2024-11-05 00:25:47,471 - INFO - [diffusion][Epoch 8088] diffusion training Loss: 0.06959590315818787
2024-11-05 00:25:47,474 - INFO - [diffusion][Epoch 8088] diffusion learning rate: 0.001
2024-11-05 00:25:47,476 - INFO - [diffusion][Epoch 8088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:47,477 - INFO - [diffusion][Epoch 8089] Epoch 8090/12000
2024-11-05 00:25:51,066 - INFO - [diffusion][Epoch 8089] diffusion training Loss: 0.07450933940708637
2024-11-05 00:25:51,068 - INFO - [diffusion][Epoch 8089] diffusion learning rate: 0.001
2024-11-05 00:25:51,070 - INFO - [diffusion][Epoch 8089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:51,071 - INFO - [diffusion][Epoch 8090] Epoch 8091/12000
2024-11-05 00:25:54,615 - INFO - [diffusion][Epoch 8090] diffusion training Loss: 0.07362900115549564
2024-11-05 00:25:54,939 - INFO - [diffusion][Epoch 8090] diffusion learning rate: 0.001
2024-11-05 00:25:54,941 - INFO - [diffusion][Epoch 8090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:54,942 - INFO - [diffusion][Epoch 8091] Epoch 8092/12000
2024-11-05 00:25:58,503 - INFO - [diffusion][Epoch 8091] diffusion training Loss: 0.07425772771239281
2024-11-05 00:25:58,505 - INFO - [diffusion][Epoch 8091] diffusion learning rate: 0.001
2024-11-05 00:25:58,507 - INFO - [diffusion][Epoch 8091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:58,508 - INFO - [diffusion][Epoch 8092] Epoch 8093/12000
2024-11-05 00:26:01,641 - INFO - [diffusion][Epoch 8092] diffusion training Loss: 0.07328417710959911
2024-11-05 00:26:01,643 - INFO - [diffusion][Epoch 8092] diffusion learning rate: 0.001
2024-11-05 00:26:01,645 - INFO - [diffusion][Epoch 8092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:01,646 - INFO - [diffusion][Epoch 8093] Epoch 8094/12000
2024-11-05 00:26:04,761 - INFO - [diffusion][Epoch 8093] diffusion training Loss: 0.07520995289087296
2024-11-05 00:26:04,789 - INFO - [diffusion][Epoch 8093] diffusion learning rate: 0.001
2024-11-05 00:26:04,791 - INFO - [diffusion][Epoch 8093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:04,792 - INFO - [diffusion][Epoch 8094] Epoch 8095/12000
2024-11-05 00:26:07,897 - INFO - [diffusion][Epoch 8094] diffusion training Loss: 0.07003067061305046
2024-11-05 00:26:07,899 - INFO - [diffusion][Epoch 8094] diffusion learning rate: 0.001
2024-11-05 00:26:07,900 - INFO - [diffusion][Epoch 8094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:07,902 - INFO - [diffusion][Epoch 8095] Epoch 8096/12000
2024-11-05 00:26:11,464 - INFO - [diffusion][Epoch 8095] diffusion training Loss: 0.06621268671005964
2024-11-05 00:26:11,467 - INFO - [diffusion][Epoch 8095] diffusion learning rate: 0.001
2024-11-05 00:26:11,469 - INFO - [diffusion][Epoch 8095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:11,470 - INFO - [diffusion][Epoch 8096] Epoch 8097/12000
2024-11-05 00:26:15,023 - INFO - [diffusion][Epoch 8096] diffusion training Loss: 0.07322202436625957
2024-11-05 00:26:15,025 - INFO - [diffusion][Epoch 8096] diffusion learning rate: 0.001
2024-11-05 00:26:15,027 - INFO - [diffusion][Epoch 8096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:15,029 - INFO - [diffusion][Epoch 8097] Epoch 8098/12000
2024-11-05 00:26:18,099 - INFO - [diffusion][Epoch 8097] diffusion training Loss: 0.06476955488324165
2024-11-05 00:26:18,101 - INFO - [diffusion][Epoch 8097] diffusion learning rate: 0.001
2024-11-05 00:26:18,103 - INFO - [diffusion][Epoch 8097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:18,104 - INFO - [diffusion][Epoch 8098] Epoch 8099/12000
2024-11-05 00:26:21,289 - INFO - [diffusion][Epoch 8098] diffusion training Loss: 0.0702868327498436
2024-11-05 00:26:21,291 - INFO - [diffusion][Epoch 8098] diffusion learning rate: 0.001
2024-11-05 00:26:21,292 - INFO - [diffusion][Epoch 8098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:21,294 - INFO - [diffusion][Epoch 8099] Epoch 8100/12000
2024-11-05 00:26:24,387 - INFO - [diffusion][Epoch 8099] diffusion training Loss: 0.07839665561914444
2024-11-05 00:26:24,389 - INFO - [diffusion][Epoch 8099] diffusion learning rate: 0.001
2024-11-05 00:26:24,391 - INFO - [diffusion][Epoch 8099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:24,392 - INFO - [diffusion][Epoch 8100] Epoch 8101/12000
2024-11-05 00:26:27,986 - INFO - [diffusion][Epoch 8100] diffusion training Loss: 0.06943554803729057
2024-11-05 00:26:27,989 - INFO - [diffusion][Epoch 8100] diffusion learning rate: 0.001
2024-11-05 00:26:27,991 - INFO - [diffusion][Epoch 8100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:27,992 - INFO - [diffusion][Epoch 8101] Epoch 8102/12000
2024-11-05 00:26:31,529 - INFO - [diffusion][Epoch 8101] diffusion training Loss: 0.07246167026460171
2024-11-05 00:26:31,532 - INFO - [diffusion][Epoch 8101] diffusion learning rate: 0.001
2024-11-05 00:26:31,533 - INFO - [diffusion][Epoch 8101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:31,535 - INFO - [diffusion][Epoch 8102] Epoch 8103/12000
2024-11-05 00:26:34,208 - INFO - [diffusion][Epoch 8102] diffusion training Loss: 0.07249417528510094
2024-11-05 00:26:34,210 - INFO - [diffusion][Epoch 8102] diffusion learning rate: 0.001
2024-11-05 00:26:34,212 - INFO - [diffusion][Epoch 8102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:34,213 - INFO - [diffusion][Epoch 8103] Epoch 8104/12000
2024-11-05 00:26:37,296 - INFO - [diffusion][Epoch 8103] diffusion training Loss: 0.07177329622209072
2024-11-05 00:26:37,298 - INFO - [diffusion][Epoch 8103] diffusion learning rate: 0.001
2024-11-05 00:26:37,300 - INFO - [diffusion][Epoch 8103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:37,301 - INFO - [diffusion][Epoch 8104] Epoch 8105/12000
2024-11-05 00:26:40,848 - INFO - [diffusion][Epoch 8104] diffusion training Loss: 0.07733048684895039
2024-11-05 00:26:40,850 - INFO - [diffusion][Epoch 8104] diffusion learning rate: 0.001
2024-11-05 00:26:40,852 - INFO - [diffusion][Epoch 8104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:40,853 - INFO - [diffusion][Epoch 8105] Epoch 8106/12000
2024-11-05 00:26:44,363 - INFO - [diffusion][Epoch 8105] diffusion training Loss: 0.06725577637553215
2024-11-05 00:26:44,365 - INFO - [diffusion][Epoch 8105] diffusion learning rate: 0.001
2024-11-05 00:26:44,367 - INFO - [diffusion][Epoch 8105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:44,368 - INFO - [diffusion][Epoch 8106] Epoch 8107/12000
2024-11-05 00:26:47,335 - INFO - [diffusion][Epoch 8106] diffusion training Loss: 0.07358581386506557
2024-11-05 00:26:47,338 - INFO - [diffusion][Epoch 8106] diffusion learning rate: 0.001
2024-11-05 00:26:47,340 - INFO - [diffusion][Epoch 8106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:47,341 - INFO - [diffusion][Epoch 8107] Epoch 8108/12000
2024-11-05 00:26:50,467 - INFO - [diffusion][Epoch 8107] diffusion training Loss: 0.07187492772936821
2024-11-05 00:26:50,495 - INFO - [diffusion][Epoch 8107] diffusion learning rate: 0.001
2024-11-05 00:26:50,497 - INFO - [diffusion][Epoch 8107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:50,499 - INFO - [diffusion][Epoch 8108] Epoch 8109/12000
2024-11-05 00:26:53,825 - INFO - [diffusion][Epoch 8108] diffusion training Loss: 0.0704503282904625
2024-11-05 00:26:53,827 - INFO - [diffusion][Epoch 8108] diffusion learning rate: 0.001
2024-11-05 00:26:53,829 - INFO - [diffusion][Epoch 8108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:53,830 - INFO - [diffusion][Epoch 8109] Epoch 8110/12000
2024-11-05 00:26:57,569 - INFO - [diffusion][Epoch 8109] diffusion training Loss: 0.0698613841086626
2024-11-05 00:26:57,571 - INFO - [diffusion][Epoch 8109] diffusion learning rate: 0.001
2024-11-05 00:26:57,573 - INFO - [diffusion][Epoch 8109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:57,574 - INFO - [diffusion][Epoch 8110] Epoch 8111/12000
2024-11-05 00:27:00,960 - INFO - [diffusion][Epoch 8110] diffusion training Loss: 0.06699258834123611
2024-11-05 00:27:00,961 - INFO - [diffusion][Epoch 8110] diffusion learning rate: 0.001
2024-11-05 00:27:00,963 - INFO - [diffusion][Epoch 8110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:00,964 - INFO - [diffusion][Epoch 8111] Epoch 8112/12000
2024-11-05 00:27:03,856 - INFO - [diffusion][Epoch 8111] diffusion training Loss: 0.07357440236955881
2024-11-05 00:27:03,858 - INFO - [diffusion][Epoch 8111] diffusion learning rate: 0.001
2024-11-05 00:27:03,859 - INFO - [diffusion][Epoch 8111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:03,861 - INFO - [diffusion][Epoch 8112] Epoch 8113/12000
2024-11-05 00:27:06,958 - INFO - [diffusion][Epoch 8112] diffusion training Loss: 0.07089457102119923
2024-11-05 00:27:06,960 - INFO - [diffusion][Epoch 8112] diffusion learning rate: 0.001
2024-11-05 00:27:06,962 - INFO - [diffusion][Epoch 8112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:06,963 - INFO - [diffusion][Epoch 8113] Epoch 8114/12000
2024-11-05 00:27:10,543 - INFO - [diffusion][Epoch 8113] diffusion training Loss: 0.07570347934961319
2024-11-05 00:27:10,546 - INFO - [diffusion][Epoch 8113] diffusion learning rate: 0.001
2024-11-05 00:27:10,547 - INFO - [diffusion][Epoch 8113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:10,549 - INFO - [diffusion][Epoch 8114] Epoch 8115/12000
2024-11-05 00:27:14,109 - INFO - [diffusion][Epoch 8114] diffusion training Loss: 0.07483257725834846
2024-11-05 00:27:14,112 - INFO - [diffusion][Epoch 8114] diffusion learning rate: 0.001
2024-11-05 00:27:14,114 - INFO - [diffusion][Epoch 8114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:14,115 - INFO - [diffusion][Epoch 8115] Epoch 8116/12000
2024-11-05 00:27:17,224 - INFO - [diffusion][Epoch 8115] diffusion training Loss: 0.07020112778991461
2024-11-05 00:27:17,226 - INFO - [diffusion][Epoch 8115] diffusion learning rate: 0.001
2024-11-05 00:27:17,228 - INFO - [diffusion][Epoch 8115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:17,229 - INFO - [diffusion][Epoch 8116] Epoch 8117/12000
2024-11-05 00:27:20,318 - INFO - [diffusion][Epoch 8116] diffusion training Loss: 0.06791848130524158
2024-11-05 00:27:20,319 - INFO - [diffusion][Epoch 8116] diffusion learning rate: 0.001
2024-11-05 00:27:20,321 - INFO - [diffusion][Epoch 8116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:20,322 - INFO - [diffusion][Epoch 8117] Epoch 8118/12000
2024-11-05 00:27:23,511 - INFO - [diffusion][Epoch 8117] diffusion training Loss: 0.07052123174071312
2024-11-05 00:27:23,513 - INFO - [diffusion][Epoch 8117] diffusion learning rate: 0.001
2024-11-05 00:27:23,515 - INFO - [diffusion][Epoch 8117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:23,516 - INFO - [diffusion][Epoch 8118] Epoch 8119/12000
2024-11-05 00:27:27,320 - INFO - [diffusion][Epoch 8118] diffusion training Loss: 0.07004374265670776
2024-11-05 00:27:27,322 - INFO - [diffusion][Epoch 8118] diffusion learning rate: 0.001
2024-11-05 00:27:27,324 - INFO - [diffusion][Epoch 8118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:27,325 - INFO - [diffusion][Epoch 8119] Epoch 8120/12000
2024-11-05 00:27:30,989 - INFO - [diffusion][Epoch 8119] diffusion training Loss: 0.06997977569699287
2024-11-05 00:27:30,991 - INFO - [diffusion][Epoch 8119] diffusion learning rate: 0.001
2024-11-05 00:27:30,993 - INFO - [diffusion][Epoch 8119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:30,994 - INFO - [diffusion][Epoch 8120] Epoch 8121/12000
2024-11-05 00:27:34,520 - INFO - [diffusion][Epoch 8120] diffusion training Loss: 0.06845683418214321
2024-11-05 00:27:34,522 - INFO - [diffusion][Epoch 8120] diffusion learning rate: 0.001
2024-11-05 00:27:34,524 - INFO - [diffusion][Epoch 8120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:34,525 - INFO - [diffusion][Epoch 8121] Epoch 8122/12000
2024-11-05 00:27:37,642 - INFO - [diffusion][Epoch 8121] diffusion training Loss: 0.07529506459832191
2024-11-05 00:27:37,644 - INFO - [diffusion][Epoch 8121] diffusion learning rate: 0.001
2024-11-05 00:27:37,645 - INFO - [diffusion][Epoch 8121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:37,647 - INFO - [diffusion][Epoch 8122] Epoch 8123/12000
2024-11-05 00:27:40,768 - INFO - [diffusion][Epoch 8122] diffusion training Loss: 0.07122965343296528
2024-11-05 00:27:40,770 - INFO - [diffusion][Epoch 8122] diffusion learning rate: 0.001
2024-11-05 00:27:40,772 - INFO - [diffusion][Epoch 8122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:40,773 - INFO - [diffusion][Epoch 8123] Epoch 8124/12000
2024-11-05 00:27:44,106 - INFO - [diffusion][Epoch 8123] diffusion training Loss: 0.07444393634796143
2024-11-05 00:27:44,107 - INFO - [diffusion][Epoch 8123] diffusion learning rate: 0.001
2024-11-05 00:27:44,109 - INFO - [diffusion][Epoch 8123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:44,110 - INFO - [diffusion][Epoch 8124] Epoch 8125/12000
2024-11-05 00:27:47,804 - INFO - [diffusion][Epoch 8124] diffusion training Loss: 0.07040127459913492
2024-11-05 00:27:47,806 - INFO - [diffusion][Epoch 8124] diffusion learning rate: 0.001
2024-11-05 00:27:47,807 - INFO - [diffusion][Epoch 8124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:47,809 - INFO - [diffusion][Epoch 8125] Epoch 8126/12000
2024-11-05 00:27:51,229 - INFO - [diffusion][Epoch 8125] diffusion training Loss: 0.07087497413158417
2024-11-05 00:27:51,232 - INFO - [diffusion][Epoch 8125] diffusion learning rate: 0.001
2024-11-05 00:27:51,234 - INFO - [diffusion][Epoch 8125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:51,235 - INFO - [diffusion][Epoch 8126] Epoch 8127/12000
2024-11-05 00:27:54,451 - INFO - [diffusion][Epoch 8126] diffusion training Loss: 0.07043062336742878
2024-11-05 00:27:54,492 - INFO - [diffusion][Epoch 8126] diffusion learning rate: 0.001
2024-11-05 00:27:54,494 - INFO - [diffusion][Epoch 8126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:54,495 - INFO - [diffusion][Epoch 8127] Epoch 8128/12000
2024-11-05 00:27:57,562 - INFO - [diffusion][Epoch 8127] diffusion training Loss: 0.06876779347658157
2024-11-05 00:27:57,564 - INFO - [diffusion][Epoch 8127] diffusion learning rate: 0.001
2024-11-05 00:27:57,566 - INFO - [diffusion][Epoch 8127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:57,567 - INFO - [diffusion][Epoch 8128] Epoch 8129/12000
2024-11-05 00:28:00,939 - INFO - [diffusion][Epoch 8128] diffusion training Loss: 0.07011869177222252
2024-11-05 00:28:00,942 - INFO - [diffusion][Epoch 8128] diffusion learning rate: 0.001
2024-11-05 00:28:00,944 - INFO - [diffusion][Epoch 8128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:00,945 - INFO - [diffusion][Epoch 8129] Epoch 8130/12000
2024-11-05 00:28:04,365 - INFO - [diffusion][Epoch 8129] diffusion training Loss: 0.07319149002432823
2024-11-05 00:28:04,367 - INFO - [diffusion][Epoch 8129] diffusion learning rate: 0.001
2024-11-05 00:28:04,369 - INFO - [diffusion][Epoch 8129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:04,370 - INFO - [diffusion][Epoch 8130] Epoch 8131/12000
2024-11-05 00:28:07,520 - INFO - [diffusion][Epoch 8130] diffusion training Loss: 0.07543458230793476
2024-11-05 00:28:07,522 - INFO - [diffusion][Epoch 8130] diffusion learning rate: 0.001
2024-11-05 00:28:07,523 - INFO - [diffusion][Epoch 8130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:07,525 - INFO - [diffusion][Epoch 8131] Epoch 8132/12000
2024-11-05 00:28:10,621 - INFO - [diffusion][Epoch 8131] diffusion training Loss: 0.07124661654233932
2024-11-05 00:28:10,623 - INFO - [diffusion][Epoch 8131] diffusion learning rate: 0.001
2024-11-05 00:28:10,625 - INFO - [diffusion][Epoch 8131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:10,626 - INFO - [diffusion][Epoch 8132] Epoch 8133/12000
2024-11-05 00:28:13,771 - INFO - [diffusion][Epoch 8132] diffusion training Loss: 0.07431553117930889
2024-11-05 00:28:13,773 - INFO - [diffusion][Epoch 8132] diffusion learning rate: 0.001
2024-11-05 00:28:13,775 - INFO - [diffusion][Epoch 8132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:13,776 - INFO - [diffusion][Epoch 8133] Epoch 8134/12000
2024-11-05 00:28:17,370 - INFO - [diffusion][Epoch 8133] diffusion training Loss: 0.06680142693221569
2024-11-05 00:28:17,372 - INFO - [diffusion][Epoch 8133] diffusion learning rate: 0.001
2024-11-05 00:28:17,374 - INFO - [diffusion][Epoch 8133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:17,375 - INFO - [diffusion][Epoch 8134] Epoch 8135/12000
2024-11-05 00:28:21,004 - INFO - [diffusion][Epoch 8134] diffusion training Loss: 0.0677198302000761
2024-11-05 00:28:21,006 - INFO - [diffusion][Epoch 8134] diffusion learning rate: 0.001
2024-11-05 00:28:21,007 - INFO - [diffusion][Epoch 8134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:21,009 - INFO - [diffusion][Epoch 8135] Epoch 8136/12000
2024-11-05 00:28:24,191 - INFO - [diffusion][Epoch 8135] diffusion training Loss: 0.06631367094814777
2024-11-05 00:28:24,193 - INFO - [diffusion][Epoch 8135] diffusion learning rate: 0.001
2024-11-05 00:28:24,194 - INFO - [diffusion][Epoch 8135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:24,196 - INFO - [diffusion][Epoch 8136] Epoch 8137/12000
2024-11-05 00:28:27,347 - INFO - [diffusion][Epoch 8136] diffusion training Loss: 0.07899347506463528
2024-11-05 00:28:27,349 - INFO - [diffusion][Epoch 8136] diffusion learning rate: 0.001
2024-11-05 00:28:27,350 - INFO - [diffusion][Epoch 8136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:27,352 - INFO - [diffusion][Epoch 8137] Epoch 8138/12000
2024-11-05 00:28:31,052 - INFO - [diffusion][Epoch 8137] diffusion training Loss: 0.07090813107788563
2024-11-05 00:28:31,054 - INFO - [diffusion][Epoch 8137] diffusion learning rate: 0.001
2024-11-05 00:28:31,093 - INFO - [diffusion][Epoch 8137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:31,095 - INFO - [diffusion][Epoch 8138] Epoch 8139/12000
2024-11-05 00:28:34,148 - INFO - [diffusion][Epoch 8138] diffusion training Loss: 0.07207298651337624
2024-11-05 00:28:34,150 - INFO - [diffusion][Epoch 8138] diffusion learning rate: 0.001
2024-11-05 00:28:34,152 - INFO - [diffusion][Epoch 8138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:34,154 - INFO - [diffusion][Epoch 8139] Epoch 8140/12000
2024-11-05 00:28:37,699 - INFO - [diffusion][Epoch 8139] diffusion training Loss: 0.07267136313021183
2024-11-05 00:28:37,701 - INFO - [diffusion][Epoch 8139] diffusion learning rate: 0.001
2024-11-05 00:28:37,702 - INFO - [diffusion][Epoch 8139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:37,704 - INFO - [diffusion][Epoch 8140] Epoch 8141/12000
2024-11-05 00:28:41,245 - INFO - [diffusion][Epoch 8140] diffusion training Loss: 0.06848311796784401
2024-11-05 00:28:41,247 - INFO - [diffusion][Epoch 8140] diffusion learning rate: 0.001
2024-11-05 00:28:41,248 - INFO - [diffusion][Epoch 8140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:41,249 - INFO - [diffusion][Epoch 8141] Epoch 8142/12000
2024-11-05 00:28:44,372 - INFO - [diffusion][Epoch 8141] diffusion training Loss: 0.06611723825335503
2024-11-05 00:28:44,374 - INFO - [diffusion][Epoch 8141] diffusion learning rate: 0.001
2024-11-05 00:28:44,405 - INFO - [diffusion][Epoch 8141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:44,406 - INFO - [diffusion][Epoch 8142] Epoch 8143/12000
2024-11-05 00:28:47,485 - INFO - [diffusion][Epoch 8142] diffusion training Loss: 0.06852675788104534
2024-11-05 00:28:47,487 - INFO - [diffusion][Epoch 8142] diffusion learning rate: 0.001
2024-11-05 00:28:47,489 - INFO - [diffusion][Epoch 8142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:47,490 - INFO - [diffusion][Epoch 8143] Epoch 8144/12000
2024-11-05 00:28:50,585 - INFO - [diffusion][Epoch 8143] diffusion training Loss: 0.06756320595741272
2024-11-05 00:28:50,587 - INFO - [diffusion][Epoch 8143] diffusion learning rate: 0.001
2024-11-05 00:28:50,589 - INFO - [diffusion][Epoch 8143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:50,590 - INFO - [diffusion][Epoch 8144] Epoch 8145/12000
2024-11-05 00:28:54,021 - INFO - [diffusion][Epoch 8144] diffusion training Loss: 0.07602141797542572
2024-11-05 00:28:54,023 - INFO - [diffusion][Epoch 8144] diffusion learning rate: 0.001
2024-11-05 00:28:54,024 - INFO - [diffusion][Epoch 8144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:54,026 - INFO - [diffusion][Epoch 8145] Epoch 8146/12000
2024-11-05 00:28:57,333 - INFO - [diffusion][Epoch 8145] diffusion training Loss: 0.07599917985498905
2024-11-05 00:28:57,335 - INFO - [diffusion][Epoch 8145] diffusion learning rate: 0.001
2024-11-05 00:28:57,337 - INFO - [diffusion][Epoch 8145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:57,338 - INFO - [diffusion][Epoch 8146] Epoch 8147/12000
2024-11-05 00:29:00,536 - INFO - [diffusion][Epoch 8146] diffusion training Loss: 0.07184928841888905
2024-11-05 00:29:00,538 - INFO - [diffusion][Epoch 8146] diffusion learning rate: 0.001
2024-11-05 00:29:00,540 - INFO - [diffusion][Epoch 8146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:00,541 - INFO - [diffusion][Epoch 8147] Epoch 8148/12000
2024-11-05 00:29:03,659 - INFO - [diffusion][Epoch 8147] diffusion training Loss: 0.06853251997381449
2024-11-05 00:29:03,661 - INFO - [diffusion][Epoch 8147] diffusion learning rate: 0.001
2024-11-05 00:29:03,663 - INFO - [diffusion][Epoch 8147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:03,664 - INFO - [diffusion][Epoch 8148] Epoch 8149/12000
2024-11-05 00:29:06,969 - INFO - [diffusion][Epoch 8148] diffusion training Loss: 0.07030640915036201
2024-11-05 00:29:06,970 - INFO - [diffusion][Epoch 8148] diffusion learning rate: 0.001
2024-11-05 00:29:06,972 - INFO - [diffusion][Epoch 8148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:06,974 - INFO - [diffusion][Epoch 8149] Epoch 8150/12000
2024-11-05 00:29:10,470 - INFO - [diffusion][Epoch 8149] diffusion training Loss: 0.06703861523419619
2024-11-05 00:29:10,471 - INFO - [diffusion][Epoch 8149] diffusion learning rate: 0.001
2024-11-05 00:29:10,473 - INFO - [diffusion][Epoch 8149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:10,474 - INFO - [diffusion][Epoch 8150] Epoch 8151/12000
2024-11-05 00:29:13,788 - INFO - [diffusion][Epoch 8150] diffusion training Loss: 0.06900626793503761
2024-11-05 00:29:13,790 - INFO - [diffusion][Epoch 8150] diffusion learning rate: 0.001
2024-11-05 00:29:13,791 - INFO - [diffusion][Epoch 8150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:13,792 - INFO - [diffusion][Epoch 8151] Epoch 8152/12000
2024-11-05 00:29:16,923 - INFO - [diffusion][Epoch 8151] diffusion training Loss: 0.07186987530440092
2024-11-05 00:29:16,926 - INFO - [diffusion][Epoch 8151] diffusion learning rate: 0.001
2024-11-05 00:29:16,928 - INFO - [diffusion][Epoch 8151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:16,930 - INFO - [diffusion][Epoch 8152] Epoch 8153/12000
2024-11-05 00:29:20,010 - INFO - [diffusion][Epoch 8152] diffusion training Loss: 0.07642146572470665
2024-11-05 00:29:20,013 - INFO - [diffusion][Epoch 8152] diffusion learning rate: 0.001
2024-11-05 00:29:20,015 - INFO - [diffusion][Epoch 8152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:20,016 - INFO - [diffusion][Epoch 8153] Epoch 8154/12000
2024-11-05 00:29:23,481 - INFO - [diffusion][Epoch 8153] diffusion training Loss: 0.06745305098593235
2024-11-05 00:29:23,483 - INFO - [diffusion][Epoch 8153] diffusion learning rate: 0.001
2024-11-05 00:29:23,485 - INFO - [diffusion][Epoch 8153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:23,486 - INFO - [diffusion][Epoch 8154] Epoch 8155/12000
2024-11-05 00:29:26,911 - INFO - [diffusion][Epoch 8154] diffusion training Loss: 0.06805926561355591
2024-11-05 00:29:26,913 - INFO - [diffusion][Epoch 8154] diffusion learning rate: 0.001
2024-11-05 00:29:26,915 - INFO - [diffusion][Epoch 8154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:26,916 - INFO - [diffusion][Epoch 8155] Epoch 8156/12000
2024-11-05 00:29:29,633 - INFO - [diffusion][Epoch 8155] diffusion training Loss: 0.063326021656394
2024-11-05 00:29:29,635 - INFO - [diffusion][Epoch 8155] diffusion learning rate: 0.001
2024-11-05 00:29:29,637 - INFO - [diffusion][Epoch 8155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:29,638 - INFO - [diffusion][Epoch 8156] Epoch 8157/12000
2024-11-05 00:29:32,929 - INFO - [diffusion][Epoch 8156] diffusion training Loss: 0.06972994282841682
2024-11-05 00:29:32,931 - INFO - [diffusion][Epoch 8156] diffusion learning rate: 0.001
2024-11-05 00:29:32,932 - INFO - [diffusion][Epoch 8156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:32,934 - INFO - [diffusion][Epoch 8157] Epoch 8158/12000
2024-11-05 00:29:36,433 - INFO - [diffusion][Epoch 8157] diffusion training Loss: 0.06955540180206299
2024-11-05 00:29:36,435 - INFO - [diffusion][Epoch 8157] diffusion learning rate: 0.001
2024-11-05 00:29:36,437 - INFO - [diffusion][Epoch 8157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:36,439 - INFO - [diffusion][Epoch 8158] Epoch 8159/12000
2024-11-05 00:29:40,300 - INFO - [diffusion][Epoch 8158] diffusion training Loss: 0.07511435449123383
2024-11-05 00:29:40,302 - INFO - [diffusion][Epoch 8158] diffusion learning rate: 0.001
2024-11-05 00:29:40,304 - INFO - [diffusion][Epoch 8158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:40,305 - INFO - [diffusion][Epoch 8159] Epoch 8160/12000
2024-11-05 00:29:43,469 - INFO - [diffusion][Epoch 8159] diffusion training Loss: 0.07274382188916206
2024-11-05 00:29:43,471 - INFO - [diffusion][Epoch 8159] diffusion learning rate: 0.001
2024-11-05 00:29:43,473 - INFO - [diffusion][Epoch 8159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:43,475 - INFO - [diffusion][Epoch 8160] Epoch 8161/12000
2024-11-05 00:29:46,509 - INFO - [diffusion][Epoch 8160] diffusion training Loss: 0.0676959976553917
2024-11-05 00:29:46,511 - INFO - [diffusion][Epoch 8160] diffusion learning rate: 0.001
2024-11-05 00:29:46,513 - INFO - [diffusion][Epoch 8160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:46,514 - INFO - [diffusion][Epoch 8161] Epoch 8162/12000
2024-11-05 00:29:49,777 - INFO - [diffusion][Epoch 8161] diffusion training Loss: 0.07364979293197393
2024-11-05 00:29:49,778 - INFO - [diffusion][Epoch 8161] diffusion learning rate: 0.001
2024-11-05 00:29:49,780 - INFO - [diffusion][Epoch 8161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:49,781 - INFO - [diffusion][Epoch 8162] Epoch 8163/12000
2024-11-05 00:29:53,447 - INFO - [diffusion][Epoch 8162] diffusion training Loss: 0.06983014196157455
2024-11-05 00:29:53,448 - INFO - [diffusion][Epoch 8162] diffusion learning rate: 0.001
2024-11-05 00:29:53,450 - INFO - [diffusion][Epoch 8162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:53,451 - INFO - [diffusion][Epoch 8163] Epoch 8164/12000
2024-11-05 00:29:56,853 - INFO - [diffusion][Epoch 8163] diffusion training Loss: 0.07096799276769161
2024-11-05 00:29:56,855 - INFO - [diffusion][Epoch 8163] diffusion learning rate: 0.001
2024-11-05 00:29:56,857 - INFO - [diffusion][Epoch 8163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:56,858 - INFO - [diffusion][Epoch 8164] Epoch 8165/12000
2024-11-05 00:30:00,091 - INFO - [diffusion][Epoch 8164] diffusion training Loss: 0.07150430791079998
2024-11-05 00:30:00,093 - INFO - [diffusion][Epoch 8164] diffusion learning rate: 0.001
2024-11-05 00:30:00,094 - INFO - [diffusion][Epoch 8164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:00,095 - INFO - [diffusion][Epoch 8165] Epoch 8166/12000
2024-11-05 00:30:03,263 - INFO - [diffusion][Epoch 8165] diffusion training Loss: 0.06913595274090767
2024-11-05 00:30:03,265 - INFO - [diffusion][Epoch 8165] diffusion learning rate: 0.001
2024-11-05 00:30:03,267 - INFO - [diffusion][Epoch 8165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:03,268 - INFO - [diffusion][Epoch 8166] Epoch 8167/12000
2024-11-05 00:30:06,555 - INFO - [diffusion][Epoch 8166] diffusion training Loss: 0.07239753007888794
2024-11-05 00:30:06,556 - INFO - [diffusion][Epoch 8166] diffusion learning rate: 0.001
2024-11-05 00:30:06,558 - INFO - [diffusion][Epoch 8166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:06,559 - INFO - [diffusion][Epoch 8167] Epoch 8168/12000
2024-11-05 00:30:10,143 - INFO - [diffusion][Epoch 8167] diffusion training Loss: 0.0728578120470047
2024-11-05 00:30:10,145 - INFO - [diffusion][Epoch 8167] diffusion learning rate: 0.001
2024-11-05 00:30:10,147 - INFO - [diffusion][Epoch 8167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:10,148 - INFO - [diffusion][Epoch 8168] Epoch 8169/12000
2024-11-05 00:30:13,303 - INFO - [diffusion][Epoch 8168] diffusion training Loss: 0.07338784635066986
2024-11-05 00:30:13,305 - INFO - [diffusion][Epoch 8168] diffusion learning rate: 0.001
2024-11-05 00:30:13,307 - INFO - [diffusion][Epoch 8168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:13,309 - INFO - [diffusion][Epoch 8169] Epoch 8170/12000
2024-11-05 00:30:16,508 - INFO - [diffusion][Epoch 8169] diffusion training Loss: 0.07442282512784004
2024-11-05 00:30:16,510 - INFO - [diffusion][Epoch 8169] diffusion learning rate: 0.001
2024-11-05 00:30:16,512 - INFO - [diffusion][Epoch 8169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:16,513 - INFO - [diffusion][Epoch 8170] Epoch 8171/12000
2024-11-05 00:30:19,603 - INFO - [diffusion][Epoch 8170] diffusion training Loss: 0.06795480288565159
2024-11-05 00:30:19,605 - INFO - [diffusion][Epoch 8170] diffusion learning rate: 0.001
2024-11-05 00:30:19,606 - INFO - [diffusion][Epoch 8170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:19,608 - INFO - [diffusion][Epoch 8171] Epoch 8172/12000
2024-11-05 00:30:23,137 - INFO - [diffusion][Epoch 8171] diffusion training Loss: 0.07288590352982283
2024-11-05 00:30:23,139 - INFO - [diffusion][Epoch 8171] diffusion learning rate: 0.001
2024-11-05 00:30:23,141 - INFO - [diffusion][Epoch 8171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:23,143 - INFO - [diffusion][Epoch 8172] Epoch 8173/12000
2024-11-05 00:30:26,620 - INFO - [diffusion][Epoch 8172] diffusion training Loss: 0.06942855007946491
2024-11-05 00:30:26,622 - INFO - [diffusion][Epoch 8172] diffusion learning rate: 0.001
2024-11-05 00:30:26,623 - INFO - [diffusion][Epoch 8172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:26,624 - INFO - [diffusion][Epoch 8173] Epoch 8174/12000
2024-11-05 00:30:29,732 - INFO - [diffusion][Epoch 8173] diffusion training Loss: 0.07027638889849186
2024-11-05 00:30:29,734 - INFO - [diffusion][Epoch 8173] diffusion learning rate: 0.001
2024-11-05 00:30:29,736 - INFO - [diffusion][Epoch 8173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:29,737 - INFO - [diffusion][Epoch 8174] Epoch 8175/12000
2024-11-05 00:30:32,828 - INFO - [diffusion][Epoch 8174] diffusion training Loss: 0.07337703835219145
2024-11-05 00:30:32,830 - INFO - [diffusion][Epoch 8174] diffusion learning rate: 0.001
2024-11-05 00:30:32,832 - INFO - [diffusion][Epoch 8174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:32,833 - INFO - [diffusion][Epoch 8175] Epoch 8176/12000
2024-11-05 00:30:36,141 - INFO - [diffusion][Epoch 8175] diffusion training Loss: 0.07088184542953968
2024-11-05 00:30:36,143 - INFO - [diffusion][Epoch 8175] diffusion learning rate: 0.001
2024-11-05 00:30:36,145 - INFO - [diffusion][Epoch 8175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:36,146 - INFO - [diffusion][Epoch 8176] Epoch 8177/12000
2024-11-05 00:30:39,855 - INFO - [diffusion][Epoch 8176] diffusion training Loss: 0.07771510258316994
2024-11-05 00:30:39,857 - INFO - [diffusion][Epoch 8176] diffusion learning rate: 0.001
2024-11-05 00:30:39,859 - INFO - [diffusion][Epoch 8176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:39,860 - INFO - [diffusion][Epoch 8177] Epoch 8178/12000
2024-11-05 00:30:43,397 - INFO - [diffusion][Epoch 8177] diffusion training Loss: 0.06201594974845648
2024-11-05 00:30:43,399 - INFO - [diffusion][Epoch 8177] diffusion learning rate: 0.001
2024-11-05 00:30:43,401 - INFO - [diffusion][Epoch 8177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:43,402 - INFO - [diffusion][Epoch 8178] Epoch 8179/12000
2024-11-05 00:30:46,453 - INFO - [diffusion][Epoch 8178] diffusion training Loss: 0.06994477938860655
2024-11-05 00:30:46,454 - INFO - [diffusion][Epoch 8178] diffusion learning rate: 0.001
2024-11-05 00:30:46,456 - INFO - [diffusion][Epoch 8178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:46,457 - INFO - [diffusion][Epoch 8179] Epoch 8180/12000
2024-11-05 00:30:49,520 - INFO - [diffusion][Epoch 8179] diffusion training Loss: 0.06702623330056667
2024-11-05 00:30:49,579 - INFO - [diffusion][Epoch 8179] diffusion learning rate: 0.001
2024-11-05 00:30:49,581 - INFO - [diffusion][Epoch 8179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:49,582 - INFO - [diffusion][Epoch 8180] Epoch 8181/12000
2024-11-05 00:30:52,941 - INFO - [diffusion][Epoch 8180] diffusion training Loss: 0.07152893021702766
2024-11-05 00:30:52,943 - INFO - [diffusion][Epoch 8180] diffusion learning rate: 0.001
2024-11-05 00:30:52,945 - INFO - [diffusion][Epoch 8180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:52,946 - INFO - [diffusion][Epoch 8181] Epoch 8182/12000
2024-11-05 00:30:56,475 - INFO - [diffusion][Epoch 8181] diffusion training Loss: 0.0732171218842268
2024-11-05 00:30:56,476 - INFO - [diffusion][Epoch 8181] diffusion learning rate: 0.001
2024-11-05 00:30:56,478 - INFO - [diffusion][Epoch 8181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:56,479 - INFO - [diffusion][Epoch 8182] Epoch 8183/12000
2024-11-05 00:30:59,589 - INFO - [diffusion][Epoch 8182] diffusion training Loss: 0.07617932930588722
2024-11-05 00:30:59,591 - INFO - [diffusion][Epoch 8182] diffusion learning rate: 0.001
2024-11-05 00:30:59,593 - INFO - [diffusion][Epoch 8182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:59,594 - INFO - [diffusion][Epoch 8183] Epoch 8184/12000
2024-11-05 00:31:02,677 - INFO - [diffusion][Epoch 8183] diffusion training Loss: 0.06833192706108093
2024-11-05 00:31:02,679 - INFO - [diffusion][Epoch 8183] diffusion learning rate: 0.001
2024-11-05 00:31:02,681 - INFO - [diffusion][Epoch 8183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:02,682 - INFO - [diffusion][Epoch 8184] Epoch 8185/12000
2024-11-05 00:31:06,252 - INFO - [diffusion][Epoch 8184] diffusion training Loss: 0.06993614137172699
2024-11-05 00:31:06,254 - INFO - [diffusion][Epoch 8184] diffusion learning rate: 0.001
2024-11-05 00:31:06,256 - INFO - [diffusion][Epoch 8184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:06,257 - INFO - [diffusion][Epoch 8185] Epoch 8186/12000
2024-11-05 00:31:09,762 - INFO - [diffusion][Epoch 8185] diffusion training Loss: 0.07830464839935303
2024-11-05 00:31:09,764 - INFO - [diffusion][Epoch 8185] diffusion learning rate: 0.001
2024-11-05 00:31:09,766 - INFO - [diffusion][Epoch 8185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:09,767 - INFO - [diffusion][Epoch 8186] Epoch 8187/12000
2024-11-05 00:31:12,869 - INFO - [diffusion][Epoch 8186] diffusion training Loss: 0.07231234945356846
2024-11-05 00:31:12,872 - INFO - [diffusion][Epoch 8186] diffusion learning rate: 0.001
2024-11-05 00:31:12,874 - INFO - [diffusion][Epoch 8186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:12,875 - INFO - [diffusion][Epoch 8187] Epoch 8188/12000
2024-11-05 00:31:15,961 - INFO - [diffusion][Epoch 8187] diffusion training Loss: 0.06756063923239708
2024-11-05 00:31:15,963 - INFO - [diffusion][Epoch 8187] diffusion learning rate: 0.001
2024-11-05 00:31:15,965 - INFO - [diffusion][Epoch 8187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:15,966 - INFO - [diffusion][Epoch 8188] Epoch 8189/12000
2024-11-05 00:31:19,152 - INFO - [diffusion][Epoch 8188] diffusion training Loss: 0.06519145052880049
2024-11-05 00:31:19,154 - INFO - [diffusion][Epoch 8188] diffusion learning rate: 0.001
2024-11-05 00:31:19,156 - INFO - [diffusion][Epoch 8188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:19,157 - INFO - [diffusion][Epoch 8189] Epoch 8190/12000
2024-11-05 00:31:22,804 - INFO - [diffusion][Epoch 8189] diffusion training Loss: 0.069119225256145
2024-11-05 00:31:22,806 - INFO - [diffusion][Epoch 8189] diffusion learning rate: 0.001
2024-11-05 00:31:22,807 - INFO - [diffusion][Epoch 8189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:22,808 - INFO - [diffusion][Epoch 8190] Epoch 8191/12000
2024-11-05 00:31:26,283 - INFO - [diffusion][Epoch 8190] diffusion training Loss: 0.06425304990261793
2024-11-05 00:31:26,285 - INFO - [diffusion][Epoch 8190] diffusion learning rate: 0.001
2024-11-05 00:31:26,287 - INFO - [diffusion][Epoch 8190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:26,288 - INFO - [diffusion][Epoch 8191] Epoch 8192/12000
2024-11-05 00:31:29,399 - INFO - [diffusion][Epoch 8191] diffusion training Loss: 0.07004178315401077
2024-11-05 00:31:29,401 - INFO - [diffusion][Epoch 8191] diffusion learning rate: 0.001
2024-11-05 00:31:29,442 - INFO - [diffusion][Epoch 8191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:29,443 - INFO - [diffusion][Epoch 8192] Epoch 8193/12000
2024-11-05 00:31:32,260 - INFO - [diffusion][Epoch 8192] diffusion training Loss: 0.07044573500752449
2024-11-05 00:31:32,262 - INFO - [diffusion][Epoch 8192] diffusion learning rate: 0.001
2024-11-05 00:31:32,263 - INFO - [diffusion][Epoch 8192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:32,264 - INFO - [diffusion][Epoch 8193] Epoch 8194/12000
2024-11-05 00:31:35,742 - INFO - [diffusion][Epoch 8193] diffusion training Loss: 0.07000777497887611
2024-11-05 00:31:35,744 - INFO - [diffusion][Epoch 8193] diffusion learning rate: 0.001
2024-11-05 00:31:35,747 - INFO - [diffusion][Epoch 8193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:35,748 - INFO - [diffusion][Epoch 8194] Epoch 8195/12000
2024-11-05 00:31:39,335 - INFO - [diffusion][Epoch 8194] diffusion training Loss: 0.06811020523309708
2024-11-05 00:31:39,337 - INFO - [diffusion][Epoch 8194] diffusion learning rate: 0.001
2024-11-05 00:31:39,338 - INFO - [diffusion][Epoch 8194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:39,340 - INFO - [diffusion][Epoch 8195] Epoch 8196/12000
2024-11-05 00:31:42,467 - INFO - [diffusion][Epoch 8195] diffusion training Loss: 0.0705022718757391
2024-11-05 00:31:42,469 - INFO - [diffusion][Epoch 8195] diffusion learning rate: 0.001
2024-11-05 00:31:42,471 - INFO - [diffusion][Epoch 8195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:42,472 - INFO - [diffusion][Epoch 8196] Epoch 8197/12000
2024-11-05 00:31:45,594 - INFO - [diffusion][Epoch 8196] diffusion training Loss: 0.06898138858377934
2024-11-05 00:31:45,596 - INFO - [diffusion][Epoch 8196] diffusion learning rate: 0.001
2024-11-05 00:31:45,682 - INFO - [diffusion][Epoch 8196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:45,683 - INFO - [diffusion][Epoch 8197] Epoch 8198/12000
2024-11-05 00:31:49,316 - INFO - [diffusion][Epoch 8197] diffusion training Loss: 0.0698139313608408
2024-11-05 00:31:49,318 - INFO - [diffusion][Epoch 8197] diffusion learning rate: 0.001
2024-11-05 00:31:49,320 - INFO - [diffusion][Epoch 8197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:49,321 - INFO - [diffusion][Epoch 8198] Epoch 8199/12000
2024-11-05 00:31:52,465 - INFO - [diffusion][Epoch 8198] diffusion training Loss: 0.07329536601901054
2024-11-05 00:31:52,466 - INFO - [diffusion][Epoch 8198] diffusion learning rate: 0.001
2024-11-05 00:31:52,468 - INFO - [diffusion][Epoch 8198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:52,469 - INFO - [diffusion][Epoch 8199] Epoch 8200/12000
2024-11-05 00:31:56,195 - INFO - [diffusion][Epoch 8199] diffusion training Loss: 0.06593638006597757
2024-11-05 00:31:56,197 - INFO - [diffusion][Epoch 8199] diffusion learning rate: 0.001
2024-11-05 00:31:56,199 - INFO - [diffusion][Epoch 8199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:56,200 - INFO - [diffusion][Epoch 8200] Epoch 8201/12000
2024-11-05 00:31:59,657 - INFO - [diffusion][Epoch 8200] diffusion training Loss: 0.06488881912082434
2024-11-05 00:31:59,659 - INFO - [diffusion][Epoch 8200] diffusion learning rate: 0.001
2024-11-05 00:31:59,660 - INFO - [diffusion][Epoch 8200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:59,662 - INFO - [diffusion][Epoch 8201] Epoch 8202/12000
2024-11-05 00:32:02,832 - INFO - [diffusion][Epoch 8201] diffusion training Loss: 0.06851761043071747
2024-11-05 00:32:02,834 - INFO - [diffusion][Epoch 8201] diffusion learning rate: 0.001
2024-11-05 00:32:02,835 - INFO - [diffusion][Epoch 8201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:02,836 - INFO - [diffusion][Epoch 8202] Epoch 8203/12000
2024-11-05 00:32:05,758 - INFO - [diffusion][Epoch 8202] diffusion training Loss: 0.07068377919495106
2024-11-05 00:32:05,760 - INFO - [diffusion][Epoch 8202] diffusion learning rate: 0.001
2024-11-05 00:32:05,762 - INFO - [diffusion][Epoch 8202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:05,763 - INFO - [diffusion][Epoch 8203] Epoch 8204/12000
2024-11-05 00:32:09,083 - INFO - [diffusion][Epoch 8203] diffusion training Loss: 0.06978217326104641
2024-11-05 00:32:09,085 - INFO - [diffusion][Epoch 8203] diffusion learning rate: 0.001
2024-11-05 00:32:09,087 - INFO - [diffusion][Epoch 8203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:09,088 - INFO - [diffusion][Epoch 8204] Epoch 8205/12000
2024-11-05 00:32:12,785 - INFO - [diffusion][Epoch 8204] diffusion training Loss: 0.0684724971652031
2024-11-05 00:32:12,787 - INFO - [diffusion][Epoch 8204] diffusion learning rate: 0.001
2024-11-05 00:32:12,788 - INFO - [diffusion][Epoch 8204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:12,790 - INFO - [diffusion][Epoch 8205] Epoch 8206/12000
2024-11-05 00:32:16,103 - INFO - [diffusion][Epoch 8205] diffusion training Loss: 0.06992442160844803
2024-11-05 00:32:16,104 - INFO - [diffusion][Epoch 8205] diffusion learning rate: 0.001
2024-11-05 00:32:16,106 - INFO - [diffusion][Epoch 8205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:16,107 - INFO - [diffusion][Epoch 8206] Epoch 8207/12000
2024-11-05 00:32:19,249 - INFO - [diffusion][Epoch 8206] diffusion training Loss: 0.06459975801408291
2024-11-05 00:32:19,252 - INFO - [diffusion][Epoch 8206] diffusion learning rate: 0.001
2024-11-05 00:32:19,253 - INFO - [diffusion][Epoch 8206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:19,255 - INFO - [diffusion][Epoch 8207] Epoch 8208/12000
2024-11-05 00:32:22,153 - INFO - [diffusion][Epoch 8207] diffusion training Loss: 0.06894266977906227
2024-11-05 00:32:22,155 - INFO - [diffusion][Epoch 8207] diffusion learning rate: 0.001
2024-11-05 00:32:22,185 - INFO - [diffusion][Epoch 8207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:22,187 - INFO - [diffusion][Epoch 8208] Epoch 8209/12000
2024-11-05 00:32:25,759 - INFO - [diffusion][Epoch 8208] diffusion training Loss: 0.07481377758085728
2024-11-05 00:32:25,761 - INFO - [diffusion][Epoch 8208] diffusion learning rate: 0.001
2024-11-05 00:32:25,763 - INFO - [diffusion][Epoch 8208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:25,764 - INFO - [diffusion][Epoch 8209] Epoch 8210/12000
2024-11-05 00:32:28,840 - INFO - [diffusion][Epoch 8209] diffusion training Loss: 0.06455488316714764
2024-11-05 00:32:28,843 - INFO - [diffusion][Epoch 8209] diffusion learning rate: 0.001
2024-11-05 00:32:28,845 - INFO - [diffusion][Epoch 8209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:28,846 - INFO - [diffusion][Epoch 8210] Epoch 8211/12000
2024-11-05 00:32:31,964 - INFO - [diffusion][Epoch 8210] diffusion training Loss: 0.06747970171272755
2024-11-05 00:32:31,966 - INFO - [diffusion][Epoch 8210] diffusion learning rate: 0.001
2024-11-05 00:32:31,968 - INFO - [diffusion][Epoch 8210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:31,969 - INFO - [diffusion][Epoch 8211] Epoch 8212/12000
2024-11-05 00:32:35,218 - INFO - [diffusion][Epoch 8211] diffusion training Loss: 0.07193720899522305
2024-11-05 00:32:35,220 - INFO - [diffusion][Epoch 8211] diffusion learning rate: 0.001
2024-11-05 00:32:35,222 - INFO - [diffusion][Epoch 8211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:35,224 - INFO - [diffusion][Epoch 8212] Epoch 8213/12000
2024-11-05 00:32:38,767 - INFO - [diffusion][Epoch 8212] diffusion training Loss: 0.06777161546051502
2024-11-05 00:32:38,768 - INFO - [diffusion][Epoch 8212] diffusion learning rate: 0.001
2024-11-05 00:32:38,770 - INFO - [diffusion][Epoch 8212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:38,772 - INFO - [diffusion][Epoch 8213] Epoch 8214/12000
2024-11-05 00:32:42,380 - INFO - [diffusion][Epoch 8213] diffusion training Loss: 0.0726261306554079
2024-11-05 00:32:42,382 - INFO - [diffusion][Epoch 8213] diffusion learning rate: 0.001
2024-11-05 00:32:42,383 - INFO - [diffusion][Epoch 8213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:42,385 - INFO - [diffusion][Epoch 8214] Epoch 8215/12000
2024-11-05 00:32:46,387 - INFO - [diffusion][Epoch 8214] diffusion training Loss: 0.07270376570522785
2024-11-05 00:32:46,389 - INFO - [diffusion][Epoch 8214] diffusion learning rate: 0.001
2024-11-05 00:32:46,391 - INFO - [diffusion][Epoch 8214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:46,392 - INFO - [diffusion][Epoch 8215] Epoch 8216/12000
2024-11-05 00:32:50,223 - INFO - [diffusion][Epoch 8215] diffusion training Loss: 0.06975619681179523
2024-11-05 00:32:50,225 - INFO - [diffusion][Epoch 8215] diffusion learning rate: 0.001
2024-11-05 00:32:50,227 - INFO - [diffusion][Epoch 8215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:50,228 - INFO - [diffusion][Epoch 8216] Epoch 8217/12000
2024-11-05 00:32:53,418 - INFO - [diffusion][Epoch 8216] diffusion training Loss: 0.0627280781045556
2024-11-05 00:32:53,420 - INFO - [diffusion][Epoch 8216] diffusion learning rate: 0.001
2024-11-05 00:32:53,422 - INFO - [diffusion][Epoch 8216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:53,423 - INFO - [diffusion][Epoch 8217] Epoch 8218/12000
2024-11-05 00:32:56,510 - INFO - [diffusion][Epoch 8217] diffusion training Loss: 0.06667960062623024
2024-11-05 00:32:56,512 - INFO - [diffusion][Epoch 8217] diffusion learning rate: 0.001
2024-11-05 00:32:56,514 - INFO - [diffusion][Epoch 8217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:56,515 - INFO - [diffusion][Epoch 8218] Epoch 8219/12000
2024-11-05 00:32:59,607 - INFO - [diffusion][Epoch 8218] diffusion training Loss: 0.07085254229605198
2024-11-05 00:32:59,609 - INFO - [diffusion][Epoch 8218] diffusion learning rate: 0.001
2024-11-05 00:32:59,611 - INFO - [diffusion][Epoch 8218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:59,612 - INFO - [diffusion][Epoch 8219] Epoch 8220/12000
2024-11-05 00:33:03,203 - INFO - [diffusion][Epoch 8219] diffusion training Loss: 0.07041132263839245
2024-11-05 00:33:03,205 - INFO - [diffusion][Epoch 8219] diffusion learning rate: 0.001
2024-11-05 00:33:03,206 - INFO - [diffusion][Epoch 8219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:03,208 - INFO - [diffusion][Epoch 8220] Epoch 8221/12000
2024-11-05 00:33:06,751 - INFO - [diffusion][Epoch 8220] diffusion training Loss: 0.06918119918555021
2024-11-05 00:33:06,752 - INFO - [diffusion][Epoch 8220] diffusion learning rate: 0.001
2024-11-05 00:33:06,754 - INFO - [diffusion][Epoch 8220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:06,755 - INFO - [diffusion][Epoch 8221] Epoch 8222/12000
2024-11-05 00:33:09,877 - INFO - [diffusion][Epoch 8221] diffusion training Loss: 0.07099741697311401
2024-11-05 00:33:09,879 - INFO - [diffusion][Epoch 8221] diffusion learning rate: 0.001
2024-11-05 00:33:09,881 - INFO - [diffusion][Epoch 8221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:09,882 - INFO - [diffusion][Epoch 8222] Epoch 8223/12000
2024-11-05 00:33:13,001 - INFO - [diffusion][Epoch 8222] diffusion training Loss: 0.06675412878394127
2024-11-05 00:33:13,003 - INFO - [diffusion][Epoch 8222] diffusion learning rate: 0.001
2024-11-05 00:33:13,005 - INFO - [diffusion][Epoch 8222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:13,007 - INFO - [diffusion][Epoch 8223] Epoch 8224/12000
2024-11-05 00:33:16,220 - INFO - [diffusion][Epoch 8223] diffusion training Loss: 0.06916516087949276
2024-11-05 00:33:16,222 - INFO - [diffusion][Epoch 8223] diffusion learning rate: 0.001
2024-11-05 00:33:16,223 - INFO - [diffusion][Epoch 8223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:16,225 - INFO - [diffusion][Epoch 8224] Epoch 8225/12000
2024-11-05 00:33:19,878 - INFO - [diffusion][Epoch 8224] diffusion training Loss: 0.06996682286262512
2024-11-05 00:33:19,880 - INFO - [diffusion][Epoch 8224] diffusion learning rate: 0.001
2024-11-05 00:33:19,882 - INFO - [diffusion][Epoch 8224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:19,883 - INFO - [diffusion][Epoch 8225] Epoch 8226/12000
2024-11-05 00:33:23,444 - INFO - [diffusion][Epoch 8225] diffusion training Loss: 0.06823085714131594
2024-11-05 00:33:23,446 - INFO - [diffusion][Epoch 8225] diffusion learning rate: 0.001
2024-11-05 00:33:23,448 - INFO - [diffusion][Epoch 8225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:23,449 - INFO - [diffusion][Epoch 8226] Epoch 8227/12000
2024-11-05 00:33:26,539 - INFO - [diffusion][Epoch 8226] diffusion training Loss: 0.06871277745813131
2024-11-05 00:33:26,541 - INFO - [diffusion][Epoch 8226] diffusion learning rate: 0.001
2024-11-05 00:33:26,543 - INFO - [diffusion][Epoch 8226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:26,544 - INFO - [diffusion][Epoch 8227] Epoch 8228/12000
2024-11-05 00:33:29,676 - INFO - [diffusion][Epoch 8227] diffusion training Loss: 0.07061014883220196
2024-11-05 00:33:29,679 - INFO - [diffusion][Epoch 8227] diffusion learning rate: 0.001
2024-11-05 00:33:29,681 - INFO - [diffusion][Epoch 8227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:29,682 - INFO - [diffusion][Epoch 8228] Epoch 8229/12000
2024-11-05 00:33:32,822 - INFO - [diffusion][Epoch 8228] diffusion training Loss: 0.07160613127052784
2024-11-05 00:33:32,824 - INFO - [diffusion][Epoch 8228] diffusion learning rate: 0.001
2024-11-05 00:33:32,826 - INFO - [diffusion][Epoch 8228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:32,827 - INFO - [diffusion][Epoch 8229] Epoch 8230/12000
2024-11-05 00:33:36,369 - INFO - [diffusion][Epoch 8229] diffusion training Loss: 0.06320272851735353
2024-11-05 00:33:36,371 - INFO - [diffusion][Epoch 8229] diffusion learning rate: 0.001
2024-11-05 00:33:36,373 - INFO - [diffusion][Epoch 8229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:36,374 - INFO - [diffusion][Epoch 8230] Epoch 8231/12000
2024-11-05 00:33:40,074 - INFO - [diffusion][Epoch 8230] diffusion training Loss: 0.06668069399893284
2024-11-05 00:33:40,076 - INFO - [diffusion][Epoch 8230] diffusion learning rate: 0.001
2024-11-05 00:33:40,078 - INFO - [diffusion][Epoch 8230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:40,079 - INFO - [diffusion][Epoch 8231] Epoch 8232/12000
2024-11-05 00:33:43,206 - INFO - [diffusion][Epoch 8231] diffusion training Loss: 0.07090882584452629
2024-11-05 00:33:43,208 - INFO - [diffusion][Epoch 8231] diffusion learning rate: 0.001
2024-11-05 00:33:43,209 - INFO - [diffusion][Epoch 8231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:43,211 - INFO - [diffusion][Epoch 8232] Epoch 8233/12000
2024-11-05 00:33:46,357 - INFO - [diffusion][Epoch 8232] diffusion training Loss: 0.06625925935804844
2024-11-05 00:33:46,359 - INFO - [diffusion][Epoch 8232] diffusion learning rate: 0.001
2024-11-05 00:33:46,360 - INFO - [diffusion][Epoch 8232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:46,362 - INFO - [diffusion][Epoch 8233] Epoch 8234/12000
2024-11-05 00:33:49,498 - INFO - [diffusion][Epoch 8233] diffusion training Loss: 0.06996143981814384
2024-11-05 00:33:49,500 - INFO - [diffusion][Epoch 8233] diffusion learning rate: 0.001
2024-11-05 00:33:49,502 - INFO - [diffusion][Epoch 8233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:49,503 - INFO - [diffusion][Epoch 8234] Epoch 8235/12000
2024-11-05 00:33:53,047 - INFO - [diffusion][Epoch 8234] diffusion training Loss: 0.07173215039074421
2024-11-05 00:33:53,049 - INFO - [diffusion][Epoch 8234] diffusion learning rate: 0.001
2024-11-05 00:33:53,051 - INFO - [diffusion][Epoch 8234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:53,052 - INFO - [diffusion][Epoch 8235] Epoch 8236/12000
2024-11-05 00:33:56,488 - INFO - [diffusion][Epoch 8235] diffusion training Loss: 0.06545612495392561
2024-11-05 00:33:56,491 - INFO - [diffusion][Epoch 8235] diffusion learning rate: 0.001
2024-11-05 00:33:56,493 - INFO - [diffusion][Epoch 8235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:56,495 - INFO - [diffusion][Epoch 8236] Epoch 8237/12000
2024-11-05 00:34:00,519 - INFO - [diffusion][Epoch 8236] diffusion training Loss: 0.07097525708377361
2024-11-05 00:34:00,521 - INFO - [diffusion][Epoch 8236] diffusion learning rate: 0.001
2024-11-05 00:34:00,523 - INFO - [diffusion][Epoch 8236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:00,524 - INFO - [diffusion][Epoch 8237] Epoch 8238/12000
2024-11-05 00:34:03,640 - INFO - [diffusion][Epoch 8237] diffusion training Loss: 0.06571773625910282
2024-11-05 00:34:03,643 - INFO - [diffusion][Epoch 8237] diffusion learning rate: 0.001
2024-11-05 00:34:03,645 - INFO - [diffusion][Epoch 8237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:03,646 - INFO - [diffusion][Epoch 8238] Epoch 8239/12000
2024-11-05 00:34:06,805 - INFO - [diffusion][Epoch 8238] diffusion training Loss: 0.063725502230227
2024-11-05 00:34:06,807 - INFO - [diffusion][Epoch 8238] diffusion learning rate: 0.001
2024-11-05 00:34:06,808 - INFO - [diffusion][Epoch 8238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:06,810 - INFO - [diffusion][Epoch 8239] Epoch 8240/12000
2024-11-05 00:34:09,986 - INFO - [diffusion][Epoch 8239] diffusion training Loss: 0.0672847293317318
2024-11-05 00:34:09,988 - INFO - [diffusion][Epoch 8239] diffusion learning rate: 0.001
2024-11-05 00:34:09,990 - INFO - [diffusion][Epoch 8239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:09,991 - INFO - [diffusion][Epoch 8240] Epoch 8241/12000
2024-11-05 00:34:13,505 - INFO - [diffusion][Epoch 8240] diffusion training Loss: 0.06866362318396568
2024-11-05 00:34:13,506 - INFO - [diffusion][Epoch 8240] diffusion learning rate: 0.001
2024-11-05 00:34:13,508 - INFO - [diffusion][Epoch 8240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:13,509 - INFO - [diffusion][Epoch 8241] Epoch 8242/12000
2024-11-05 00:34:17,060 - INFO - [diffusion][Epoch 8241] diffusion training Loss: 0.06954293325543404
2024-11-05 00:34:17,062 - INFO - [diffusion][Epoch 8241] diffusion learning rate: 0.001
2024-11-05 00:34:17,063 - INFO - [diffusion][Epoch 8241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:17,065 - INFO - [diffusion][Epoch 8242] Epoch 8243/12000
2024-11-05 00:34:20,164 - INFO - [diffusion][Epoch 8242] diffusion training Loss: 0.07368601113557816
2024-11-05 00:34:20,385 - INFO - [diffusion][Epoch 8242] diffusion learning rate: 0.001
2024-11-05 00:34:20,387 - INFO - [diffusion][Epoch 8242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:20,388 - INFO - [diffusion][Epoch 8243] Epoch 8244/12000
2024-11-05 00:34:23,552 - INFO - [diffusion][Epoch 8243] diffusion training Loss: 0.07132812775671482
2024-11-05 00:34:23,554 - INFO - [diffusion][Epoch 8243] diffusion learning rate: 0.001
2024-11-05 00:34:23,556 - INFO - [diffusion][Epoch 8243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:23,557 - INFO - [diffusion][Epoch 8244] Epoch 8245/12000
2024-11-05 00:34:26,725 - INFO - [diffusion][Epoch 8244] diffusion training Loss: 0.07191919721662998
2024-11-05 00:34:26,727 - INFO - [diffusion][Epoch 8244] diffusion learning rate: 0.001
2024-11-05 00:34:26,729 - INFO - [diffusion][Epoch 8244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:26,730 - INFO - [diffusion][Epoch 8245] Epoch 8246/12000
2024-11-05 00:34:30,103 - INFO - [diffusion][Epoch 8245] diffusion training Loss: 0.06983601860702038
2024-11-05 00:34:30,106 - INFO - [diffusion][Epoch 8245] diffusion learning rate: 0.001
2024-11-05 00:34:30,108 - INFO - [diffusion][Epoch 8245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:30,109 - INFO - [diffusion][Epoch 8246] Epoch 8247/12000
2024-11-05 00:34:33,750 - INFO - [diffusion][Epoch 8246] diffusion training Loss: 0.0720273107290268
2024-11-05 00:34:33,751 - INFO - [diffusion][Epoch 8246] diffusion learning rate: 0.001
2024-11-05 00:34:33,753 - INFO - [diffusion][Epoch 8246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:33,754 - INFO - [diffusion][Epoch 8247] Epoch 8248/12000
2024-11-05 00:34:37,262 - INFO - [diffusion][Epoch 8247] diffusion training Loss: 0.06875472795218229
2024-11-05 00:34:37,263 - INFO - [diffusion][Epoch 8247] diffusion learning rate: 0.001
2024-11-05 00:34:37,265 - INFO - [diffusion][Epoch 8247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:37,267 - INFO - [diffusion][Epoch 8248] Epoch 8249/12000
2024-11-05 00:34:40,382 - INFO - [diffusion][Epoch 8248] diffusion training Loss: 0.06511490140110254
2024-11-05 00:34:40,384 - INFO - [diffusion][Epoch 8248] diffusion learning rate: 0.001
2024-11-05 00:34:40,386 - INFO - [diffusion][Epoch 8248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:40,387 - INFO - [diffusion][Epoch 8249] Epoch 8250/12000
2024-11-05 00:34:43,612 - INFO - [diffusion][Epoch 8249] diffusion training Loss: 0.07012241706252098
2024-11-05 00:34:43,615 - INFO - [diffusion][Epoch 8249] diffusion learning rate: 0.001
2024-11-05 00:34:43,617 - INFO - [diffusion][Epoch 8249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:43,618 - INFO - [diffusion][Epoch 8250] Epoch 8251/12000
2024-11-05 00:34:46,866 - INFO - [diffusion][Epoch 8250] diffusion training Loss: 0.07112514227628708
2024-11-05 00:34:46,868 - INFO - [diffusion][Epoch 8250] diffusion learning rate: 0.001
2024-11-05 00:34:46,910 - INFO - [diffusion][Epoch 8250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:46,911 - INFO - [diffusion][Epoch 8251] Epoch 8252/12000
2024-11-05 00:34:50,186 - INFO - [diffusion][Epoch 8251] diffusion training Loss: 0.07715646177530289
2024-11-05 00:34:50,188 - INFO - [diffusion][Epoch 8251] diffusion learning rate: 0.001
2024-11-05 00:34:50,190 - INFO - [diffusion][Epoch 8251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:50,191 - INFO - [diffusion][Epoch 8252] Epoch 8253/12000
2024-11-05 00:34:53,780 - INFO - [diffusion][Epoch 8252] diffusion training Loss: 0.06870031170547009
2024-11-05 00:34:53,782 - INFO - [diffusion][Epoch 8252] diffusion learning rate: 0.001
2024-11-05 00:34:53,783 - INFO - [diffusion][Epoch 8252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:53,785 - INFO - [diffusion][Epoch 8253] Epoch 8254/12000
2024-11-05 00:34:57,099 - INFO - [diffusion][Epoch 8253] diffusion training Loss: 0.0706600621342659
2024-11-05 00:34:57,101 - INFO - [diffusion][Epoch 8253] diffusion learning rate: 0.001
2024-11-05 00:34:57,203 - INFO - [diffusion][Epoch 8253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:57,204 - INFO - [diffusion][Epoch 8254] Epoch 8255/12000
2024-11-05 00:35:00,283 - INFO - [diffusion][Epoch 8254] diffusion training Loss: 0.07404549047350883
2024-11-05 00:35:00,288 - INFO - [diffusion][Epoch 8254] diffusion learning rate: 0.001
2024-11-05 00:35:00,289 - INFO - [diffusion][Epoch 8254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:00,291 - INFO - [diffusion][Epoch 8255] Epoch 8256/12000
2024-11-05 00:35:03,387 - INFO - [diffusion][Epoch 8255] diffusion training Loss: 0.07063422352075577
2024-11-05 00:35:03,389 - INFO - [diffusion][Epoch 8255] diffusion learning rate: 0.001
2024-11-05 00:35:03,391 - INFO - [diffusion][Epoch 8255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:03,392 - INFO - [diffusion][Epoch 8256] Epoch 8257/12000
2024-11-05 00:35:06,916 - INFO - [diffusion][Epoch 8256] diffusion training Loss: 0.06743538193404675
2024-11-05 00:35:06,918 - INFO - [diffusion][Epoch 8256] diffusion learning rate: 0.001
2024-11-05 00:35:06,919 - INFO - [diffusion][Epoch 8256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:06,921 - INFO - [diffusion][Epoch 8257] Epoch 8258/12000
2024-11-05 00:35:11,075 - INFO - [diffusion][Epoch 8257] diffusion training Loss: 0.07050518691539764
2024-11-05 00:35:11,076 - INFO - [diffusion][Epoch 8257] diffusion learning rate: 0.001
2024-11-05 00:35:11,078 - INFO - [diffusion][Epoch 8257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:11,079 - INFO - [diffusion][Epoch 8258] Epoch 8259/12000
2024-11-05 00:35:14,484 - INFO - [diffusion][Epoch 8258] diffusion training Loss: 0.07494894601404667
2024-11-05 00:35:14,486 - INFO - [diffusion][Epoch 8258] diffusion learning rate: 0.001
2024-11-05 00:35:14,488 - INFO - [diffusion][Epoch 8258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:14,489 - INFO - [diffusion][Epoch 8259] Epoch 8260/12000
2024-11-05 00:35:17,352 - INFO - [diffusion][Epoch 8259] diffusion training Loss: 0.06775848008692265
2024-11-05 00:35:17,353 - INFO - [diffusion][Epoch 8259] diffusion learning rate: 0.001
2024-11-05 00:35:17,355 - INFO - [diffusion][Epoch 8259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:17,356 - INFO - [diffusion][Epoch 8260] Epoch 8261/12000
2024-11-05 00:35:20,469 - INFO - [diffusion][Epoch 8260] diffusion training Loss: 0.06508235726505518
2024-11-05 00:35:20,471 - INFO - [diffusion][Epoch 8260] diffusion learning rate: 0.001
2024-11-05 00:35:20,473 - INFO - [diffusion][Epoch 8260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:20,474 - INFO - [diffusion][Epoch 8261] Epoch 8262/12000
2024-11-05 00:35:23,990 - INFO - [diffusion][Epoch 8261] diffusion training Loss: 0.07152482494711876
2024-11-05 00:35:23,992 - INFO - [diffusion][Epoch 8261] diffusion learning rate: 0.001
2024-11-05 00:35:23,993 - INFO - [diffusion][Epoch 8261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:23,995 - INFO - [diffusion][Epoch 8262] Epoch 8263/12000
2024-11-05 00:35:27,548 - INFO - [diffusion][Epoch 8262] diffusion training Loss: 0.07181774638593197
2024-11-05 00:35:27,550 - INFO - [diffusion][Epoch 8262] diffusion learning rate: 0.001
2024-11-05 00:35:27,551 - INFO - [diffusion][Epoch 8262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:27,553 - INFO - [diffusion][Epoch 8263] Epoch 8264/12000
2024-11-05 00:35:30,712 - INFO - [diffusion][Epoch 8263] diffusion training Loss: 0.07130214758217335
2024-11-05 00:35:30,714 - INFO - [diffusion][Epoch 8263] diffusion learning rate: 0.001
2024-11-05 00:35:30,716 - INFO - [diffusion][Epoch 8263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:30,717 - INFO - [diffusion][Epoch 8264] Epoch 8265/12000
2024-11-05 00:35:33,800 - INFO - [diffusion][Epoch 8264] diffusion training Loss: 0.065749267116189
2024-11-05 00:35:33,802 - INFO - [diffusion][Epoch 8264] diffusion learning rate: 0.001
2024-11-05 00:35:33,804 - INFO - [diffusion][Epoch 8264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:33,805 - INFO - [diffusion][Epoch 8265] Epoch 8266/12000
2024-11-05 00:35:36,900 - INFO - [diffusion][Epoch 8265] diffusion training Loss: 0.07614080794155598
2024-11-05 00:35:36,903 - INFO - [diffusion][Epoch 8265] diffusion learning rate: 0.001
2024-11-05 00:35:36,905 - INFO - [diffusion][Epoch 8265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:36,906 - INFO - [diffusion][Epoch 8266] Epoch 8267/12000
2024-11-05 00:35:40,421 - INFO - [diffusion][Epoch 8266] diffusion training Loss: 0.07681562565267086
2024-11-05 00:35:40,423 - INFO - [diffusion][Epoch 8266] diffusion learning rate: 0.001
2024-11-05 00:35:40,424 - INFO - [diffusion][Epoch 8266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:40,426 - INFO - [diffusion][Epoch 8267] Epoch 8268/12000
2024-11-05 00:35:44,132 - INFO - [diffusion][Epoch 8267] diffusion training Loss: 0.0747134480625391
2024-11-05 00:35:44,134 - INFO - [diffusion][Epoch 8267] diffusion learning rate: 0.001
2024-11-05 00:35:44,136 - INFO - [diffusion][Epoch 8267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:44,137 - INFO - [diffusion][Epoch 8268] Epoch 8269/12000
2024-11-05 00:35:47,381 - INFO - [diffusion][Epoch 8268] diffusion training Loss: 0.0668114461004734
2024-11-05 00:35:47,383 - INFO - [diffusion][Epoch 8268] diffusion learning rate: 0.001
2024-11-05 00:35:47,439 - INFO - [diffusion][Epoch 8268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:47,441 - INFO - [diffusion][Epoch 8269] Epoch 8270/12000
2024-11-05 00:35:50,551 - INFO - [diffusion][Epoch 8269] diffusion training Loss: 0.07126221805810928
2024-11-05 00:35:50,553 - INFO - [diffusion][Epoch 8269] diffusion learning rate: 0.001
2024-11-05 00:35:50,554 - INFO - [diffusion][Epoch 8269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:50,556 - INFO - [diffusion][Epoch 8270] Epoch 8271/12000
2024-11-05 00:35:53,756 - INFO - [diffusion][Epoch 8270] diffusion training Loss: 0.0742037296295166
2024-11-05 00:35:53,758 - INFO - [diffusion][Epoch 8270] diffusion learning rate: 0.001
2024-11-05 00:35:53,760 - INFO - [diffusion][Epoch 8270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:53,761 - INFO - [diffusion][Epoch 8271] Epoch 8272/12000
2024-11-05 00:35:57,211 - INFO - [diffusion][Epoch 8271] diffusion training Loss: 0.07236645929515362
2024-11-05 00:35:57,213 - INFO - [diffusion][Epoch 8271] diffusion learning rate: 0.001
2024-11-05 00:35:57,215 - INFO - [diffusion][Epoch 8271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:57,216 - INFO - [diffusion][Epoch 8272] Epoch 8273/12000
2024-11-05 00:36:00,919 - INFO - [diffusion][Epoch 8272] diffusion training Loss: 0.07104392349720001
2024-11-05 00:36:00,921 - INFO - [diffusion][Epoch 8272] diffusion learning rate: 0.001
2024-11-05 00:36:00,923 - INFO - [diffusion][Epoch 8272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:00,924 - INFO - [diffusion][Epoch 8273] Epoch 8274/12000
2024-11-05 00:36:04,254 - INFO - [diffusion][Epoch 8273] diffusion training Loss: 0.06640385277569294
2024-11-05 00:36:04,255 - INFO - [diffusion][Epoch 8273] diffusion learning rate: 0.001
2024-11-05 00:36:04,257 - INFO - [diffusion][Epoch 8273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:04,259 - INFO - [diffusion][Epoch 8274] Epoch 8275/12000
2024-11-05 00:36:07,380 - INFO - [diffusion][Epoch 8274] diffusion training Loss: 0.0694885179400444
2024-11-05 00:36:07,382 - INFO - [diffusion][Epoch 8274] diffusion learning rate: 0.001
2024-11-05 00:36:07,383 - INFO - [diffusion][Epoch 8274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:07,385 - INFO - [diffusion][Epoch 8275] Epoch 8276/12000
2024-11-05 00:36:10,463 - INFO - [diffusion][Epoch 8275] diffusion training Loss: 0.07500061579048634
2024-11-05 00:36:10,465 - INFO - [diffusion][Epoch 8275] diffusion learning rate: 0.001
2024-11-05 00:36:10,467 - INFO - [diffusion][Epoch 8275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:10,468 - INFO - [diffusion][Epoch 8276] Epoch 8277/12000
2024-11-05 00:36:14,019 - INFO - [diffusion][Epoch 8276] diffusion training Loss: 0.07101347297430038
2024-11-05 00:36:14,020 - INFO - [diffusion][Epoch 8276] diffusion learning rate: 0.001
2024-11-05 00:36:14,022 - INFO - [diffusion][Epoch 8276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:14,023 - INFO - [diffusion][Epoch 8277] Epoch 8278/12000
2024-11-05 00:36:18,137 - INFO - [diffusion][Epoch 8277] diffusion training Loss: 0.07230184692889452
2024-11-05 00:36:18,139 - INFO - [diffusion][Epoch 8277] diffusion learning rate: 0.001
2024-11-05 00:36:18,141 - INFO - [diffusion][Epoch 8277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:18,142 - INFO - [diffusion][Epoch 8278] Epoch 8279/12000
2024-11-05 00:36:21,682 - INFO - [diffusion][Epoch 8278] diffusion training Loss: 0.06591522507369518
2024-11-05 00:36:21,684 - INFO - [diffusion][Epoch 8278] diffusion learning rate: 0.001
2024-11-05 00:36:21,686 - INFO - [diffusion][Epoch 8278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:21,688 - INFO - [diffusion][Epoch 8279] Epoch 8280/12000
2024-11-05 00:36:24,822 - INFO - [diffusion][Epoch 8279] diffusion training Loss: 0.06976509094238281
2024-11-05 00:36:24,824 - INFO - [diffusion][Epoch 8279] diffusion learning rate: 0.001
2024-11-05 00:36:24,826 - INFO - [diffusion][Epoch 8279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:24,827 - INFO - [diffusion][Epoch 8280] Epoch 8281/12000
2024-11-05 00:36:27,825 - INFO - [diffusion][Epoch 8280] diffusion training Loss: 0.0699352566152811
2024-11-05 00:36:27,827 - INFO - [diffusion][Epoch 8280] diffusion learning rate: 0.001
2024-11-05 00:36:27,829 - INFO - [diffusion][Epoch 8280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:27,830 - INFO - [diffusion][Epoch 8281] Epoch 8282/12000
2024-11-05 00:36:31,122 - INFO - [diffusion][Epoch 8281] diffusion training Loss: 0.07267928868532181
2024-11-05 00:36:31,124 - INFO - [diffusion][Epoch 8281] diffusion learning rate: 0.001
2024-11-05 00:36:31,126 - INFO - [diffusion][Epoch 8281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:31,127 - INFO - [diffusion][Epoch 8282] Epoch 8283/12000
2024-11-05 00:36:34,720 - INFO - [diffusion][Epoch 8282] diffusion training Loss: 0.07724130153656006
2024-11-05 00:36:34,723 - INFO - [diffusion][Epoch 8282] diffusion learning rate: 0.001
2024-11-05 00:36:34,725 - INFO - [diffusion][Epoch 8282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:34,726 - INFO - [diffusion][Epoch 8283] Epoch 8284/12000
2024-11-05 00:36:37,908 - INFO - [diffusion][Epoch 8283] diffusion training Loss: 0.06878145411610603
2024-11-05 00:36:37,910 - INFO - [diffusion][Epoch 8283] diffusion learning rate: 0.001
2024-11-05 00:36:37,912 - INFO - [diffusion][Epoch 8283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:37,914 - INFO - [diffusion][Epoch 8284] Epoch 8285/12000
2024-11-05 00:36:41,226 - INFO - [diffusion][Epoch 8284] diffusion training Loss: 0.07122483570128679
2024-11-05 00:36:41,228 - INFO - [diffusion][Epoch 8284] diffusion learning rate: 0.001
2024-11-05 00:36:41,230 - INFO - [diffusion][Epoch 8284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:41,231 - INFO - [diffusion][Epoch 8285] Epoch 8286/12000
2024-11-05 00:36:44,341 - INFO - [diffusion][Epoch 8285] diffusion training Loss: 0.071389589458704
2024-11-05 00:36:44,343 - INFO - [diffusion][Epoch 8285] diffusion learning rate: 0.001
2024-11-05 00:36:44,345 - INFO - [diffusion][Epoch 8285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:44,346 - INFO - [diffusion][Epoch 8286] Epoch 8287/12000
2024-11-05 00:36:47,836 - INFO - [diffusion][Epoch 8286] diffusion training Loss: 0.0703110508620739
2024-11-05 00:36:47,839 - INFO - [diffusion][Epoch 8286] diffusion learning rate: 0.001
2024-11-05 00:36:47,842 - INFO - [diffusion][Epoch 8286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:47,844 - INFO - [diffusion][Epoch 8287] Epoch 8288/12000
2024-11-05 00:36:51,377 - INFO - [diffusion][Epoch 8287] diffusion training Loss: 0.0748435240238905
2024-11-05 00:36:51,379 - INFO - [diffusion][Epoch 8287] diffusion learning rate: 0.001
2024-11-05 00:36:51,381 - INFO - [diffusion][Epoch 8287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:51,382 - INFO - [diffusion][Epoch 8288] Epoch 8289/12000
2024-11-05 00:36:54,466 - INFO - [diffusion][Epoch 8288] diffusion training Loss: 0.07393874600529671
2024-11-05 00:36:54,468 - INFO - [diffusion][Epoch 8288] diffusion learning rate: 0.001
2024-11-05 00:36:54,470 - INFO - [diffusion][Epoch 8288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:54,471 - INFO - [diffusion][Epoch 8289] Epoch 8290/12000
2024-11-05 00:36:57,675 - INFO - [diffusion][Epoch 8289] diffusion training Loss: 0.07404723949730396
2024-11-05 00:36:57,677 - INFO - [diffusion][Epoch 8289] diffusion learning rate: 0.001
2024-11-05 00:36:57,679 - INFO - [diffusion][Epoch 8289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:57,680 - INFO - [diffusion][Epoch 8290] Epoch 8291/12000
2024-11-05 00:37:00,915 - INFO - [diffusion][Epoch 8290] diffusion training Loss: 0.07132810354232788
2024-11-05 00:37:00,917 - INFO - [diffusion][Epoch 8290] diffusion learning rate: 0.001
2024-11-05 00:37:00,919 - INFO - [diffusion][Epoch 8290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:00,920 - INFO - [diffusion][Epoch 8291] Epoch 8292/12000
2024-11-05 00:37:04,544 - INFO - [diffusion][Epoch 8291] diffusion training Loss: 0.06989185884594917
2024-11-05 00:37:04,579 - INFO - [diffusion][Epoch 8291] diffusion learning rate: 0.001
2024-11-05 00:37:04,581 - INFO - [diffusion][Epoch 8291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:04,583 - INFO - [diffusion][Epoch 8292] Epoch 8293/12000
2024-11-05 00:37:08,072 - INFO - [diffusion][Epoch 8292] diffusion training Loss: 0.0740469191223383
2024-11-05 00:37:08,074 - INFO - [diffusion][Epoch 8292] diffusion learning rate: 0.001
2024-11-05 00:37:08,076 - INFO - [diffusion][Epoch 8292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:08,077 - INFO - [diffusion][Epoch 8293] Epoch 8294/12000
2024-11-05 00:37:11,153 - INFO - [diffusion][Epoch 8293] diffusion training Loss: 0.06773993000388145
2024-11-05 00:37:11,507 - INFO - [diffusion][Epoch 8293] diffusion learning rate: 0.001
2024-11-05 00:37:11,509 - INFO - [diffusion][Epoch 8293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:11,511 - INFO - [diffusion][Epoch 8294] Epoch 8295/12000
2024-11-05 00:37:14,614 - INFO - [diffusion][Epoch 8294] diffusion training Loss: 0.07756834477186203
2024-11-05 00:37:14,616 - INFO - [diffusion][Epoch 8294] diffusion learning rate: 0.001
2024-11-05 00:37:14,618 - INFO - [diffusion][Epoch 8294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:14,619 - INFO - [diffusion][Epoch 8295] Epoch 8296/12000
2024-11-05 00:37:17,720 - INFO - [diffusion][Epoch 8295] diffusion training Loss: 0.0714540146291256
2024-11-05 00:37:17,722 - INFO - [diffusion][Epoch 8295] diffusion learning rate: 0.001
2024-11-05 00:37:17,724 - INFO - [diffusion][Epoch 8295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:17,725 - INFO - [diffusion][Epoch 8296] Epoch 8297/12000
2024-11-05 00:37:21,323 - INFO - [diffusion][Epoch 8296] diffusion training Loss: 0.06769614852964878
2024-11-05 00:37:21,325 - INFO - [diffusion][Epoch 8296] diffusion learning rate: 0.001
2024-11-05 00:37:21,326 - INFO - [diffusion][Epoch 8296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:21,327 - INFO - [diffusion][Epoch 8297] Epoch 8298/12000
2024-11-05 00:37:25,441 - INFO - [diffusion][Epoch 8297] diffusion training Loss: 0.07196730934083462
2024-11-05 00:37:25,443 - INFO - [diffusion][Epoch 8297] diffusion learning rate: 0.001
2024-11-05 00:37:25,445 - INFO - [diffusion][Epoch 8297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:25,447 - INFO - [diffusion][Epoch 8298] Epoch 8299/12000
2024-11-05 00:37:28,683 - INFO - [diffusion][Epoch 8298] diffusion training Loss: 0.0734919011592865
2024-11-05 00:37:28,685 - INFO - [diffusion][Epoch 8298] diffusion learning rate: 0.001
2024-11-05 00:37:28,687 - INFO - [diffusion][Epoch 8298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:28,689 - INFO - [diffusion][Epoch 8299] Epoch 8300/12000
2024-11-05 00:37:31,864 - INFO - [diffusion][Epoch 8299] diffusion training Loss: 0.07340112328529358
2024-11-05 00:37:31,867 - INFO - [diffusion][Epoch 8299] diffusion learning rate: 0.001
2024-11-05 00:37:31,868 - INFO - [diffusion][Epoch 8299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:31,870 - INFO - [diffusion][Epoch 8300] Epoch 8301/12000
2024-11-05 00:37:34,931 - INFO - [diffusion][Epoch 8300] diffusion training Loss: 0.0722171775996685
2024-11-05 00:37:34,934 - INFO - [diffusion][Epoch 8300] diffusion learning rate: 0.001
2024-11-05 00:37:34,935 - INFO - [diffusion][Epoch 8300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:34,937 - INFO - [diffusion][Epoch 8301] Epoch 8302/12000
2024-11-05 00:37:38,473 - INFO - [diffusion][Epoch 8301] diffusion training Loss: 0.07399869803339243
2024-11-05 00:37:38,475 - INFO - [diffusion][Epoch 8301] diffusion learning rate: 0.001
2024-11-05 00:37:38,477 - INFO - [diffusion][Epoch 8301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:38,478 - INFO - [diffusion][Epoch 8302] Epoch 8303/12000
2024-11-05 00:37:42,154 - INFO - [diffusion][Epoch 8302] diffusion training Loss: 0.06964937783777714
2024-11-05 00:37:42,156 - INFO - [diffusion][Epoch 8302] diffusion learning rate: 0.001
2024-11-05 00:37:42,157 - INFO - [diffusion][Epoch 8302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:42,158 - INFO - [diffusion][Epoch 8303] Epoch 8304/12000
2024-11-05 00:37:45,367 - INFO - [diffusion][Epoch 8303] diffusion training Loss: 0.07362517528235912
2024-11-05 00:37:45,369 - INFO - [diffusion][Epoch 8303] diffusion learning rate: 0.001
2024-11-05 00:37:45,371 - INFO - [diffusion][Epoch 8303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:45,372 - INFO - [diffusion][Epoch 8304] Epoch 8305/12000
2024-11-05 00:37:48,489 - INFO - [diffusion][Epoch 8304] diffusion training Loss: 0.06666082330048084
2024-11-05 00:37:48,491 - INFO - [diffusion][Epoch 8304] diffusion learning rate: 0.001
2024-11-05 00:37:48,492 - INFO - [diffusion][Epoch 8304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:48,494 - INFO - [diffusion][Epoch 8305] Epoch 8306/12000
2024-11-05 00:37:51,595 - INFO - [diffusion][Epoch 8305] diffusion training Loss: 0.07059063762426376
2024-11-05 00:37:51,596 - INFO - [diffusion][Epoch 8305] diffusion learning rate: 0.001
2024-11-05 00:37:51,598 - INFO - [diffusion][Epoch 8305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:51,599 - INFO - [diffusion][Epoch 8306] Epoch 8307/12000
2024-11-05 00:37:55,157 - INFO - [diffusion][Epoch 8306] diffusion training Loss: 0.0740930549800396
2024-11-05 00:37:55,159 - INFO - [diffusion][Epoch 8306] diffusion learning rate: 0.001
2024-11-05 00:37:55,161 - INFO - [diffusion][Epoch 8306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:55,162 - INFO - [diffusion][Epoch 8307] Epoch 8308/12000
2024-11-05 00:37:58,698 - INFO - [diffusion][Epoch 8307] diffusion training Loss: 0.06579236499965191
2024-11-05 00:37:58,700 - INFO - [diffusion][Epoch 8307] diffusion learning rate: 0.001
2024-11-05 00:37:58,702 - INFO - [diffusion][Epoch 8307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:58,703 - INFO - [diffusion][Epoch 8308] Epoch 8309/12000
2024-11-05 00:38:01,824 - INFO - [diffusion][Epoch 8308] diffusion training Loss: 0.07356247119605541
2024-11-05 00:38:01,826 - INFO - [diffusion][Epoch 8308] diffusion learning rate: 0.001
2024-11-05 00:38:01,827 - INFO - [diffusion][Epoch 8308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:01,828 - INFO - [diffusion][Epoch 8309] Epoch 8310/12000
2024-11-05 00:38:04,906 - INFO - [diffusion][Epoch 8309] diffusion training Loss: 0.06902719847857952
2024-11-05 00:38:04,908 - INFO - [diffusion][Epoch 8309] diffusion learning rate: 0.001
2024-11-05 00:38:04,910 - INFO - [diffusion][Epoch 8309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:04,911 - INFO - [diffusion][Epoch 8310] Epoch 8311/12000
2024-11-05 00:38:08,073 - INFO - [diffusion][Epoch 8310] diffusion training Loss: 0.06933928653597832
2024-11-05 00:38:08,075 - INFO - [diffusion][Epoch 8310] diffusion learning rate: 0.001
2024-11-05 00:38:08,077 - INFO - [diffusion][Epoch 8310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:08,078 - INFO - [diffusion][Epoch 8311] Epoch 8312/12000
2024-11-05 00:38:11,700 - INFO - [diffusion][Epoch 8311] diffusion training Loss: 0.06959820911288261
2024-11-05 00:38:11,701 - INFO - [diffusion][Epoch 8311] diffusion learning rate: 0.001
2024-11-05 00:38:11,703 - INFO - [diffusion][Epoch 8311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:11,704 - INFO - [diffusion][Epoch 8312] Epoch 8313/12000
2024-11-05 00:38:14,851 - INFO - [diffusion][Epoch 8312] diffusion training Loss: 0.07584169134497643
2024-11-05 00:38:14,853 - INFO - [diffusion][Epoch 8312] diffusion learning rate: 0.001
2024-11-05 00:38:14,907 - INFO - [diffusion][Epoch 8312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:14,909 - INFO - [diffusion][Epoch 8313] Epoch 8314/12000
2024-11-05 00:38:18,026 - INFO - [diffusion][Epoch 8313] diffusion training Loss: 0.07075153104960918
2024-11-05 00:38:18,029 - INFO - [diffusion][Epoch 8313] diffusion learning rate: 0.001
2024-11-05 00:38:18,030 - INFO - [diffusion][Epoch 8313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:18,032 - INFO - [diffusion][Epoch 8314] Epoch 8315/12000
2024-11-05 00:38:21,184 - INFO - [diffusion][Epoch 8314] diffusion training Loss: 0.0741574726998806
2024-11-05 00:38:21,187 - INFO - [diffusion][Epoch 8314] diffusion learning rate: 0.001
2024-11-05 00:38:21,188 - INFO - [diffusion][Epoch 8314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:21,190 - INFO - [diffusion][Epoch 8315] Epoch 8316/12000
2024-11-05 00:38:24,740 - INFO - [diffusion][Epoch 8315] diffusion training Loss: 0.06661222036927938
2024-11-05 00:38:24,742 - INFO - [diffusion][Epoch 8315] diffusion learning rate: 0.001
2024-11-05 00:38:24,744 - INFO - [diffusion][Epoch 8315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:24,745 - INFO - [diffusion][Epoch 8316] Epoch 8317/12000
2024-11-05 00:38:28,254 - INFO - [diffusion][Epoch 8316] diffusion training Loss: 0.07561255246400833
2024-11-05 00:38:28,256 - INFO - [diffusion][Epoch 8316] diffusion learning rate: 0.001
2024-11-05 00:38:28,258 - INFO - [diffusion][Epoch 8316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:28,259 - INFO - [diffusion][Epoch 8317] Epoch 8318/12000
2024-11-05 00:38:31,937 - INFO - [diffusion][Epoch 8317] diffusion training Loss: 0.07012375816702843
2024-11-05 00:38:31,940 - INFO - [diffusion][Epoch 8317] diffusion learning rate: 0.001
2024-11-05 00:38:31,941 - INFO - [diffusion][Epoch 8317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:31,943 - INFO - [diffusion][Epoch 8318] Epoch 8319/12000
2024-11-05 00:38:35,027 - INFO - [diffusion][Epoch 8318] diffusion training Loss: 0.06579829659312963
2024-11-05 00:38:35,029 - INFO - [diffusion][Epoch 8318] diffusion learning rate: 0.001
2024-11-05 00:38:35,031 - INFO - [diffusion][Epoch 8318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:35,033 - INFO - [diffusion][Epoch 8319] Epoch 8320/12000
2024-11-05 00:38:38,130 - INFO - [diffusion][Epoch 8319] diffusion training Loss: 0.07119392976164818
2024-11-05 00:38:38,132 - INFO - [diffusion][Epoch 8319] diffusion learning rate: 0.001
2024-11-05 00:38:38,133 - INFO - [diffusion][Epoch 8319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:38,135 - INFO - [diffusion][Epoch 8320] Epoch 8321/12000
2024-11-05 00:38:41,670 - INFO - [diffusion][Epoch 8320] diffusion training Loss: 0.0767242144793272
2024-11-05 00:38:41,672 - INFO - [diffusion][Epoch 8320] diffusion learning rate: 0.001
2024-11-05 00:38:41,673 - INFO - [diffusion][Epoch 8320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:41,675 - INFO - [diffusion][Epoch 8321] Epoch 8322/12000
2024-11-05 00:38:45,224 - INFO - [diffusion][Epoch 8321] diffusion training Loss: 0.07171004638075829
2024-11-05 00:38:45,226 - INFO - [diffusion][Epoch 8321] diffusion learning rate: 0.001
2024-11-05 00:38:45,228 - INFO - [diffusion][Epoch 8321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:45,229 - INFO - [diffusion][Epoch 8322] Epoch 8323/12000
2024-11-05 00:38:48,324 - INFO - [diffusion][Epoch 8322] diffusion training Loss: 0.076279666274786
2024-11-05 00:38:48,326 - INFO - [diffusion][Epoch 8322] diffusion learning rate: 0.001
2024-11-05 00:38:48,327 - INFO - [diffusion][Epoch 8322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:48,329 - INFO - [diffusion][Epoch 8323] Epoch 8324/12000
2024-11-05 00:38:51,404 - INFO - [diffusion][Epoch 8323] diffusion training Loss: 0.07494188100099564
2024-11-05 00:38:51,406 - INFO - [diffusion][Epoch 8323] diffusion learning rate: 0.001
2024-11-05 00:38:51,408 - INFO - [diffusion][Epoch 8323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:51,409 - INFO - [diffusion][Epoch 8324] Epoch 8325/12000
2024-11-05 00:38:54,643 - INFO - [diffusion][Epoch 8324] diffusion training Loss: 0.07677867077291012
2024-11-05 00:38:54,645 - INFO - [diffusion][Epoch 8324] diffusion learning rate: 0.001
2024-11-05 00:38:54,647 - INFO - [diffusion][Epoch 8324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:54,648 - INFO - [diffusion][Epoch 8325] Epoch 8326/12000
2024-11-05 00:38:58,102 - INFO - [diffusion][Epoch 8325] diffusion training Loss: 0.06820065900683403
2024-11-05 00:38:58,104 - INFO - [diffusion][Epoch 8325] diffusion learning rate: 0.001
2024-11-05 00:38:58,106 - INFO - [diffusion][Epoch 8325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:58,107 - INFO - [diffusion][Epoch 8326] Epoch 8327/12000
2024-11-05 00:39:01,796 - INFO - [diffusion][Epoch 8326] diffusion training Loss: 0.07347015663981438
2024-11-05 00:39:01,798 - INFO - [diffusion][Epoch 8326] diffusion learning rate: 0.001
2024-11-05 00:39:01,800 - INFO - [diffusion][Epoch 8326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:01,801 - INFO - [diffusion][Epoch 8327] Epoch 8328/12000
2024-11-05 00:39:05,106 - INFO - [diffusion][Epoch 8327] diffusion training Loss: 0.0702034356072545
2024-11-05 00:39:05,108 - INFO - [diffusion][Epoch 8327] diffusion learning rate: 0.001
2024-11-05 00:39:05,110 - INFO - [diffusion][Epoch 8327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:05,111 - INFO - [diffusion][Epoch 8328] Epoch 8329/12000
2024-11-05 00:39:08,249 - INFO - [diffusion][Epoch 8328] diffusion training Loss: 0.07524986378848553
2024-11-05 00:39:08,251 - INFO - [diffusion][Epoch 8328] diffusion learning rate: 0.001
2024-11-05 00:39:08,253 - INFO - [diffusion][Epoch 8328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:08,254 - INFO - [diffusion][Epoch 8329] Epoch 8330/12000
2024-11-05 00:39:11,481 - INFO - [diffusion][Epoch 8329] diffusion training Loss: 0.06755033507943153
2024-11-05 00:39:11,601 - INFO - [diffusion][Epoch 8329] diffusion learning rate: 0.001
2024-11-05 00:39:11,602 - INFO - [diffusion][Epoch 8329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:11,604 - INFO - [diffusion][Epoch 8330] Epoch 8331/12000
2024-11-05 00:39:14,890 - INFO - [diffusion][Epoch 8330] diffusion training Loss: 0.06615927256643772
2024-11-05 00:39:14,892 - INFO - [diffusion][Epoch 8330] diffusion learning rate: 0.001
2024-11-05 00:39:14,894 - INFO - [diffusion][Epoch 8330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:14,895 - INFO - [diffusion][Epoch 8331] Epoch 8332/12000
2024-11-05 00:39:18,477 - INFO - [diffusion][Epoch 8331] diffusion training Loss: 0.06915266532450914
2024-11-05 00:39:18,479 - INFO - [diffusion][Epoch 8331] diffusion learning rate: 0.001
2024-11-05 00:39:18,481 - INFO - [diffusion][Epoch 8331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:18,482 - INFO - [diffusion][Epoch 8332] Epoch 8333/12000
2024-11-05 00:39:22,013 - INFO - [diffusion][Epoch 8332] diffusion training Loss: 0.07192685641348362
2024-11-05 00:39:22,015 - INFO - [diffusion][Epoch 8332] diffusion learning rate: 0.001
2024-11-05 00:39:22,016 - INFO - [diffusion][Epoch 8332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:22,018 - INFO - [diffusion][Epoch 8333] Epoch 8334/12000
2024-11-05 00:39:25,135 - INFO - [diffusion][Epoch 8333] diffusion training Loss: 0.06582315173000097
2024-11-05 00:39:25,138 - INFO - [diffusion][Epoch 8333] diffusion learning rate: 0.001
2024-11-05 00:39:25,197 - INFO - [diffusion][Epoch 8333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:25,198 - INFO - [diffusion][Epoch 8334] Epoch 8335/12000
2024-11-05 00:39:28,387 - INFO - [diffusion][Epoch 8334] diffusion training Loss: 0.06602956354618073
2024-11-05 00:39:28,388 - INFO - [diffusion][Epoch 8334] diffusion learning rate: 0.001
2024-11-05 00:39:28,390 - INFO - [diffusion][Epoch 8334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:28,391 - INFO - [diffusion][Epoch 8335] Epoch 8336/12000
2024-11-05 00:39:31,480 - INFO - [diffusion][Epoch 8335] diffusion training Loss: 0.06571037322282791
2024-11-05 00:39:31,482 - INFO - [diffusion][Epoch 8335] diffusion learning rate: 0.001
2024-11-05 00:39:31,483 - INFO - [diffusion][Epoch 8335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:31,484 - INFO - [diffusion][Epoch 8336] Epoch 8337/12000
2024-11-05 00:39:35,032 - INFO - [diffusion][Epoch 8336] diffusion training Loss: 0.0704096294939518
2024-11-05 00:39:35,035 - INFO - [diffusion][Epoch 8336] diffusion learning rate: 0.001
2024-11-05 00:39:35,036 - INFO - [diffusion][Epoch 8336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:35,037 - INFO - [diffusion][Epoch 8337] Epoch 8338/12000
2024-11-05 00:39:38,836 - INFO - [diffusion][Epoch 8337] diffusion training Loss: 0.07560831867158413
2024-11-05 00:39:38,838 - INFO - [diffusion][Epoch 8337] diffusion learning rate: 0.001
2024-11-05 00:39:38,839 - INFO - [diffusion][Epoch 8337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:38,841 - INFO - [diffusion][Epoch 8338] Epoch 8339/12000
2024-11-05 00:39:42,341 - INFO - [diffusion][Epoch 8338] diffusion training Loss: 0.06892327219247818
2024-11-05 00:39:42,342 - INFO - [diffusion][Epoch 8338] diffusion learning rate: 0.001
2024-11-05 00:39:42,344 - INFO - [diffusion][Epoch 8338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:42,345 - INFO - [diffusion][Epoch 8339] Epoch 8340/12000
2024-11-05 00:39:45,449 - INFO - [diffusion][Epoch 8339] diffusion training Loss: 0.06959250569343567
2024-11-05 00:39:45,451 - INFO - [diffusion][Epoch 8339] diffusion learning rate: 0.001
2024-11-05 00:39:45,453 - INFO - [diffusion][Epoch 8339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:45,454 - INFO - [diffusion][Epoch 8340] Epoch 8341/12000
2024-11-05 00:39:48,567 - INFO - [diffusion][Epoch 8340] diffusion training Loss: 0.06959318555891514
2024-11-05 00:39:48,571 - INFO - [diffusion][Epoch 8340] diffusion learning rate: 0.001
2024-11-05 00:39:48,573 - INFO - [diffusion][Epoch 8340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:48,574 - INFO - [diffusion][Epoch 8341] Epoch 8342/12000
2024-11-05 00:39:52,112 - INFO - [diffusion][Epoch 8341] diffusion training Loss: 0.0723187979310751
2024-11-05 00:39:52,114 - INFO - [diffusion][Epoch 8341] diffusion learning rate: 0.001
2024-11-05 00:39:52,115 - INFO - [diffusion][Epoch 8341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:52,117 - INFO - [diffusion][Epoch 8342] Epoch 8343/12000
2024-11-05 00:39:55,628 - INFO - [diffusion][Epoch 8342] diffusion training Loss: 0.06929651275277138
2024-11-05 00:39:55,629 - INFO - [diffusion][Epoch 8342] diffusion learning rate: 0.001
2024-11-05 00:39:55,631 - INFO - [diffusion][Epoch 8342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:55,633 - INFO - [diffusion][Epoch 8343] Epoch 8344/12000
2024-11-05 00:39:58,757 - INFO - [diffusion][Epoch 8343] diffusion training Loss: 0.06837852112948895
2024-11-05 00:39:58,759 - INFO - [diffusion][Epoch 8343] diffusion learning rate: 0.001
2024-11-05 00:39:58,761 - INFO - [diffusion][Epoch 8343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:58,762 - INFO - [diffusion][Epoch 8344] Epoch 8345/12000
2024-11-05 00:40:01,885 - INFO - [diffusion][Epoch 8344] diffusion training Loss: 0.06545394100248814
2024-11-05 00:40:02,159 - INFO - [diffusion][Epoch 8344] diffusion learning rate: 0.001
2024-11-05 00:40:02,161 - INFO - [diffusion][Epoch 8344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:02,163 - INFO - [diffusion][Epoch 8345] Epoch 8346/12000
2024-11-05 00:40:05,320 - INFO - [diffusion][Epoch 8345] diffusion training Loss: 0.06941616907715797
2024-11-05 00:40:05,322 - INFO - [diffusion][Epoch 8345] diffusion learning rate: 0.001
2024-11-05 00:40:05,324 - INFO - [diffusion][Epoch 8345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:05,325 - INFO - [diffusion][Epoch 8346] Epoch 8347/12000
2024-11-05 00:40:08,611 - INFO - [diffusion][Epoch 8346] diffusion training Loss: 0.07443460077047348
2024-11-05 00:40:08,613 - INFO - [diffusion][Epoch 8346] diffusion learning rate: 0.001
2024-11-05 00:40:08,614 - INFO - [diffusion][Epoch 8346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:08,616 - INFO - [diffusion][Epoch 8347] Epoch 8348/12000
2024-11-05 00:40:12,186 - INFO - [diffusion][Epoch 8347] diffusion training Loss: 0.07713751494884491
2024-11-05 00:40:12,188 - INFO - [diffusion][Epoch 8347] diffusion learning rate: 0.001
2024-11-05 00:40:12,190 - INFO - [diffusion][Epoch 8347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:12,192 - INFO - [diffusion][Epoch 8348] Epoch 8349/12000
2024-11-05 00:40:15,819 - INFO - [diffusion][Epoch 8348] diffusion training Loss: 0.06952882464975119
2024-11-05 00:40:15,821 - INFO - [diffusion][Epoch 8348] diffusion learning rate: 0.001
2024-11-05 00:40:15,823 - INFO - [diffusion][Epoch 8348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:15,824 - INFO - [diffusion][Epoch 8349] Epoch 8350/12000
2024-11-05 00:40:19,055 - INFO - [diffusion][Epoch 8349] diffusion training Loss: 0.06772652827203274
2024-11-05 00:40:19,057 - INFO - [diffusion][Epoch 8349] diffusion learning rate: 0.001
2024-11-05 00:40:19,059 - INFO - [diffusion][Epoch 8349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:19,060 - INFO - [diffusion][Epoch 8350] Epoch 8351/12000
2024-11-05 00:40:22,255 - INFO - [diffusion][Epoch 8350] diffusion training Loss: 0.07375168427824974
2024-11-05 00:40:22,258 - INFO - [diffusion][Epoch 8350] diffusion learning rate: 0.001
2024-11-05 00:40:22,260 - INFO - [diffusion][Epoch 8350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:22,261 - INFO - [diffusion][Epoch 8351] Epoch 8352/12000
2024-11-05 00:40:25,212 - INFO - [diffusion][Epoch 8351] diffusion training Loss: 0.07336024940013885
2024-11-05 00:40:25,215 - INFO - [diffusion][Epoch 8351] diffusion learning rate: 0.001
2024-11-05 00:40:25,216 - INFO - [diffusion][Epoch 8351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:25,218 - INFO - [diffusion][Epoch 8352] Epoch 8353/12000
2024-11-05 00:40:28,771 - INFO - [diffusion][Epoch 8352] diffusion training Loss: 0.06909165438264608
2024-11-05 00:40:28,775 - INFO - [diffusion][Epoch 8352] diffusion learning rate: 0.001
2024-11-05 00:40:28,777 - INFO - [diffusion][Epoch 8352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:28,779 - INFO - [diffusion][Epoch 8353] Epoch 8354/12000
2024-11-05 00:40:32,331 - INFO - [diffusion][Epoch 8353] diffusion training Loss: 0.07456174213439226
2024-11-05 00:40:32,333 - INFO - [diffusion][Epoch 8353] diffusion learning rate: 0.001
2024-11-05 00:40:32,335 - INFO - [diffusion][Epoch 8353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:32,337 - INFO - [diffusion][Epoch 8354] Epoch 8355/12000
2024-11-05 00:40:35,431 - INFO - [diffusion][Epoch 8354] diffusion training Loss: 0.07081740349531174
2024-11-05 00:40:35,433 - INFO - [diffusion][Epoch 8354] diffusion learning rate: 0.001
2024-11-05 00:40:35,435 - INFO - [diffusion][Epoch 8354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:35,436 - INFO - [diffusion][Epoch 8355] Epoch 8356/12000
2024-11-05 00:40:38,512 - INFO - [diffusion][Epoch 8355] diffusion training Loss: 0.07582485862076283
2024-11-05 00:40:38,514 - INFO - [diffusion][Epoch 8355] diffusion learning rate: 0.001
2024-11-05 00:40:38,516 - INFO - [diffusion][Epoch 8355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:38,517 - INFO - [diffusion][Epoch 8356] Epoch 8357/12000
2024-11-05 00:40:41,617 - INFO - [diffusion][Epoch 8356] diffusion training Loss: 0.07325839251279831
2024-11-05 00:40:41,620 - INFO - [diffusion][Epoch 8356] diffusion learning rate: 0.001
2024-11-05 00:40:41,621 - INFO - [diffusion][Epoch 8356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:41,622 - INFO - [diffusion][Epoch 8357] Epoch 8358/12000
2024-11-05 00:40:45,107 - INFO - [diffusion][Epoch 8357] diffusion training Loss: 0.06828907877206802
2024-11-05 00:40:45,110 - INFO - [diffusion][Epoch 8357] diffusion learning rate: 0.001
2024-11-05 00:40:45,114 - INFO - [diffusion][Epoch 8357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:45,117 - INFO - [diffusion][Epoch 8358] Epoch 8359/12000
2024-11-05 00:40:49,325 - INFO - [diffusion][Epoch 8358] diffusion training Loss: 0.07275266759097576
2024-11-05 00:40:49,327 - INFO - [diffusion][Epoch 8358] diffusion learning rate: 0.001
2024-11-05 00:40:49,328 - INFO - [diffusion][Epoch 8358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:49,330 - INFO - [diffusion][Epoch 8359] Epoch 8360/12000
2024-11-05 00:40:52,913 - INFO - [diffusion][Epoch 8359] diffusion training Loss: 0.07244710437953472
2024-11-05 00:40:52,915 - INFO - [diffusion][Epoch 8359] diffusion learning rate: 0.001
2024-11-05 00:40:52,916 - INFO - [diffusion][Epoch 8359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:52,918 - INFO - [diffusion][Epoch 8360] Epoch 8361/12000
2024-11-05 00:40:56,007 - INFO - [diffusion][Epoch 8360] diffusion training Loss: 0.07159286178648472
2024-11-05 00:40:56,009 - INFO - [diffusion][Epoch 8360] diffusion learning rate: 0.001
2024-11-05 00:40:56,011 - INFO - [diffusion][Epoch 8360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:56,012 - INFO - [diffusion][Epoch 8361] Epoch 8362/12000
2024-11-05 00:40:58,713 - INFO - [diffusion][Epoch 8361] diffusion training Loss: 0.06721284054219723
2024-11-05 00:40:58,715 - INFO - [diffusion][Epoch 8361] diffusion learning rate: 0.001
2024-11-05 00:40:58,717 - INFO - [diffusion][Epoch 8361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:58,718 - INFO - [diffusion][Epoch 8362] Epoch 8363/12000
2024-11-05 00:41:02,233 - INFO - [diffusion][Epoch 8362] diffusion training Loss: 0.06751636043190956
2024-11-05 00:41:02,235 - INFO - [diffusion][Epoch 8362] diffusion learning rate: 0.001
2024-11-05 00:41:02,236 - INFO - [diffusion][Epoch 8362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:02,238 - INFO - [diffusion][Epoch 8363] Epoch 8364/12000
2024-11-05 00:41:05,744 - INFO - [diffusion][Epoch 8363] diffusion training Loss: 0.07785340026021004
2024-11-05 00:41:05,746 - INFO - [diffusion][Epoch 8363] diffusion learning rate: 0.001
2024-11-05 00:41:05,748 - INFO - [diffusion][Epoch 8363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:05,749 - INFO - [diffusion][Epoch 8364] Epoch 8365/12000
2024-11-05 00:41:08,881 - INFO - [diffusion][Epoch 8364] diffusion training Loss: 0.07281178049743176
2024-11-05 00:41:08,883 - INFO - [diffusion][Epoch 8364] diffusion learning rate: 0.001
2024-11-05 00:41:08,885 - INFO - [diffusion][Epoch 8364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:08,886 - INFO - [diffusion][Epoch 8365] Epoch 8366/12000
2024-11-05 00:41:11,959 - INFO - [diffusion][Epoch 8365] diffusion training Loss: 0.06829117238521576
2024-11-05 00:41:11,961 - INFO - [diffusion][Epoch 8365] diffusion learning rate: 0.001
2024-11-05 00:41:11,963 - INFO - [diffusion][Epoch 8365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:11,964 - INFO - [diffusion][Epoch 8366] Epoch 8367/12000
2024-11-05 00:41:15,257 - INFO - [diffusion][Epoch 8366] diffusion training Loss: 0.06620543729513884
2024-11-05 00:41:15,259 - INFO - [diffusion][Epoch 8366] diffusion learning rate: 0.001
2024-11-05 00:41:15,261 - INFO - [diffusion][Epoch 8366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:15,262 - INFO - [diffusion][Epoch 8367] Epoch 8368/12000
2024-11-05 00:41:18,959 - INFO - [diffusion][Epoch 8367] diffusion training Loss: 0.0700903395190835
2024-11-05 00:41:18,962 - INFO - [diffusion][Epoch 8367] diffusion learning rate: 0.001
2024-11-05 00:41:18,975 - INFO - [diffusion][Epoch 8367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:18,976 - INFO - [diffusion][Epoch 8368] Epoch 8369/12000
2024-11-05 00:41:22,465 - INFO - [diffusion][Epoch 8368] diffusion training Loss: 0.0728966724127531
2024-11-05 00:41:22,468 - INFO - [diffusion][Epoch 8368] diffusion learning rate: 0.001
2024-11-05 00:41:22,470 - INFO - [diffusion][Epoch 8368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:22,471 - INFO - [diffusion][Epoch 8369] Epoch 8370/12000
2024-11-05 00:41:25,586 - INFO - [diffusion][Epoch 8369] diffusion training Loss: 0.06571158487349749
2024-11-05 00:41:25,587 - INFO - [diffusion][Epoch 8369] diffusion learning rate: 0.001
2024-11-05 00:41:25,589 - INFO - [diffusion][Epoch 8369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:25,591 - INFO - [diffusion][Epoch 8370] Epoch 8371/12000
2024-11-05 00:41:28,727 - INFO - [diffusion][Epoch 8370] diffusion training Loss: 0.06596488878130913
2024-11-05 00:41:28,729 - INFO - [diffusion][Epoch 8370] diffusion learning rate: 0.001
2024-11-05 00:41:28,731 - INFO - [diffusion][Epoch 8370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:28,732 - INFO - [diffusion][Epoch 8371] Epoch 8372/12000
2024-11-05 00:41:31,872 - INFO - [diffusion][Epoch 8371] diffusion training Loss: 0.06995571777224541
2024-11-05 00:41:31,873 - INFO - [diffusion][Epoch 8371] diffusion learning rate: 0.001
2024-11-05 00:41:31,875 - INFO - [diffusion][Epoch 8371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:31,876 - INFO - [diffusion][Epoch 8372] Epoch 8373/12000
2024-11-05 00:41:35,430 - INFO - [diffusion][Epoch 8372] diffusion training Loss: 0.07340700738132
2024-11-05 00:41:35,433 - INFO - [diffusion][Epoch 8372] diffusion learning rate: 0.001
2024-11-05 00:41:35,439 - INFO - [diffusion][Epoch 8372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:35,441 - INFO - [diffusion][Epoch 8373] Epoch 8374/12000
2024-11-05 00:41:38,964 - INFO - [diffusion][Epoch 8373] diffusion training Loss: 0.06765094958245754
2024-11-05 00:41:38,967 - INFO - [diffusion][Epoch 8373] diffusion learning rate: 0.001
2024-11-05 00:41:38,969 - INFO - [diffusion][Epoch 8373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:38,970 - INFO - [diffusion][Epoch 8374] Epoch 8375/12000
2024-11-05 00:41:42,067 - INFO - [diffusion][Epoch 8374] diffusion training Loss: 0.07173970900475979
2024-11-05 00:41:42,068 - INFO - [diffusion][Epoch 8374] diffusion learning rate: 0.001
2024-11-05 00:41:42,070 - INFO - [diffusion][Epoch 8374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:42,071 - INFO - [diffusion][Epoch 8375] Epoch 8376/12000
2024-11-05 00:41:45,184 - INFO - [diffusion][Epoch 8375] diffusion training Loss: 0.07116304710507393
2024-11-05 00:41:45,186 - INFO - [diffusion][Epoch 8375] diffusion learning rate: 0.001
2024-11-05 00:41:45,188 - INFO - [diffusion][Epoch 8375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:45,189 - INFO - [diffusion][Epoch 8376] Epoch 8377/12000
2024-11-05 00:41:48,297 - INFO - [diffusion][Epoch 8376] diffusion training Loss: 0.07647401094436646
2024-11-05 00:41:48,299 - INFO - [diffusion][Epoch 8376] diffusion learning rate: 0.001
2024-11-05 00:41:48,301 - INFO - [diffusion][Epoch 8376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:48,302 - INFO - [diffusion][Epoch 8377] Epoch 8378/12000
2024-11-05 00:41:51,892 - INFO - [diffusion][Epoch 8377] diffusion training Loss: 0.07227504439651966
2024-11-05 00:41:51,894 - INFO - [diffusion][Epoch 8377] diffusion learning rate: 0.001
2024-11-05 00:41:51,896 - INFO - [diffusion][Epoch 8377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:51,897 - INFO - [diffusion][Epoch 8378] Epoch 8379/12000
2024-11-05 00:41:55,448 - INFO - [diffusion][Epoch 8378] diffusion training Loss: 0.0696408860385418
2024-11-05 00:41:55,450 - INFO - [diffusion][Epoch 8378] diffusion learning rate: 0.001
2024-11-05 00:41:55,452 - INFO - [diffusion][Epoch 8378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:55,453 - INFO - [diffusion][Epoch 8379] Epoch 8380/12000
2024-11-05 00:41:58,810 - INFO - [diffusion][Epoch 8379] diffusion training Loss: 0.06831817142665386
2024-11-05 00:41:58,812 - INFO - [diffusion][Epoch 8379] diffusion learning rate: 0.001
2024-11-05 00:41:58,813 - INFO - [diffusion][Epoch 8379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:58,815 - INFO - [diffusion][Epoch 8380] Epoch 8381/12000
2024-11-05 00:42:01,913 - INFO - [diffusion][Epoch 8380] diffusion training Loss: 0.07154200039803982
2024-11-05 00:42:01,915 - INFO - [diffusion][Epoch 8380] diffusion learning rate: 0.001
2024-11-05 00:42:01,917 - INFO - [diffusion][Epoch 8380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:01,918 - INFO - [diffusion][Epoch 8381] Epoch 8382/12000
2024-11-05 00:42:05,051 - INFO - [diffusion][Epoch 8381] diffusion training Loss: 0.07050737552344799
2024-11-05 00:42:05,053 - INFO - [diffusion][Epoch 8381] diffusion learning rate: 0.001
2024-11-05 00:42:05,055 - INFO - [diffusion][Epoch 8381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:05,057 - INFO - [diffusion][Epoch 8382] Epoch 8383/12000
2024-11-05 00:42:08,593 - INFO - [diffusion][Epoch 8382] diffusion training Loss: 0.07051429711282253
2024-11-05 00:42:08,595 - INFO - [diffusion][Epoch 8382] diffusion learning rate: 0.001
2024-11-05 00:42:08,597 - INFO - [diffusion][Epoch 8382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:08,598 - INFO - [diffusion][Epoch 8383] Epoch 8384/12000
2024-11-05 00:42:12,187 - INFO - [diffusion][Epoch 8383] diffusion training Loss: 0.07124442793428898
2024-11-05 00:42:12,189 - INFO - [diffusion][Epoch 8383] diffusion learning rate: 0.001
2024-11-05 00:42:12,190 - INFO - [diffusion][Epoch 8383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:12,192 - INFO - [diffusion][Epoch 8384] Epoch 8385/12000
2024-11-05 00:42:15,298 - INFO - [diffusion][Epoch 8384] diffusion training Loss: 0.07168340496718884
2024-11-05 00:42:15,300 - INFO - [diffusion][Epoch 8384] diffusion learning rate: 0.001
2024-11-05 00:42:15,302 - INFO - [diffusion][Epoch 8384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:15,303 - INFO - [diffusion][Epoch 8385] Epoch 8386/12000
2024-11-05 00:42:18,386 - INFO - [diffusion][Epoch 8385] diffusion training Loss: 0.07250460796058178
2024-11-05 00:42:18,388 - INFO - [diffusion][Epoch 8385] diffusion learning rate: 0.001
2024-11-05 00:42:18,390 - INFO - [diffusion][Epoch 8385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:18,391 - INFO - [diffusion][Epoch 8386] Epoch 8387/12000
2024-11-05 00:42:21,510 - INFO - [diffusion][Epoch 8386] diffusion training Loss: 0.0744995791465044
2024-11-05 00:42:21,512 - INFO - [diffusion][Epoch 8386] diffusion learning rate: 0.001
2024-11-05 00:42:21,513 - INFO - [diffusion][Epoch 8386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:21,515 - INFO - [diffusion][Epoch 8387] Epoch 8388/12000
2024-11-05 00:42:25,027 - INFO - [diffusion][Epoch 8387] diffusion training Loss: 0.07390755601227283
2024-11-05 00:42:25,029 - INFO - [diffusion][Epoch 8387] diffusion learning rate: 0.001
2024-11-05 00:42:25,031 - INFO - [diffusion][Epoch 8387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:25,032 - INFO - [diffusion][Epoch 8388] Epoch 8389/12000
2024-11-05 00:42:27,899 - INFO - [diffusion][Epoch 8388] diffusion training Loss: 0.07017292268574238
2024-11-05 00:42:27,901 - INFO - [diffusion][Epoch 8388] diffusion learning rate: 0.001
2024-11-05 00:42:27,902 - INFO - [diffusion][Epoch 8388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:27,904 - INFO - [diffusion][Epoch 8389] Epoch 8390/12000
2024-11-05 00:42:31,106 - INFO - [diffusion][Epoch 8389] diffusion training Loss: 0.07274884916841984
2024-11-05 00:42:31,107 - INFO - [diffusion][Epoch 8389] diffusion learning rate: 0.001
2024-11-05 00:42:31,109 - INFO - [diffusion][Epoch 8389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:31,110 - INFO - [diffusion][Epoch 8390] Epoch 8391/12000
2024-11-05 00:42:34,405 - INFO - [diffusion][Epoch 8390] diffusion training Loss: 0.0674381610006094
2024-11-05 00:42:34,407 - INFO - [diffusion][Epoch 8390] diffusion learning rate: 0.001
2024-11-05 00:42:34,409 - INFO - [diffusion][Epoch 8390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:34,410 - INFO - [diffusion][Epoch 8391] Epoch 8392/12000
2024-11-05 00:42:38,042 - INFO - [diffusion][Epoch 8391] diffusion training Loss: 0.07296479493379593
2024-11-05 00:42:38,045 - INFO - [diffusion][Epoch 8391] diffusion learning rate: 0.001
2024-11-05 00:42:38,047 - INFO - [diffusion][Epoch 8391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:38,048 - INFO - [diffusion][Epoch 8392] Epoch 8393/12000
2024-11-05 00:42:41,570 - INFO - [diffusion][Epoch 8392] diffusion training Loss: 0.06607725843787193
2024-11-05 00:42:41,573 - INFO - [diffusion][Epoch 8392] diffusion learning rate: 0.001
2024-11-05 00:42:41,595 - INFO - [diffusion][Epoch 8392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:41,596 - INFO - [diffusion][Epoch 8393] Epoch 8394/12000
2024-11-05 00:42:44,687 - INFO - [diffusion][Epoch 8393] diffusion training Loss: 0.0713100153952837
2024-11-05 00:42:44,689 - INFO - [diffusion][Epoch 8393] diffusion learning rate: 0.001
2024-11-05 00:42:44,690 - INFO - [diffusion][Epoch 8393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:44,692 - INFO - [diffusion][Epoch 8394] Epoch 8395/12000
2024-11-05 00:42:47,839 - INFO - [diffusion][Epoch 8394] diffusion training Loss: 0.06806012801826
2024-11-05 00:42:47,841 - INFO - [diffusion][Epoch 8394] diffusion learning rate: 0.001
2024-11-05 00:42:47,843 - INFO - [diffusion][Epoch 8394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:47,845 - INFO - [diffusion][Epoch 8395] Epoch 8396/12000
2024-11-05 00:42:50,959 - INFO - [diffusion][Epoch 8395] diffusion training Loss: 0.07136388681828976
2024-11-05 00:42:50,961 - INFO - [diffusion][Epoch 8395] diffusion learning rate: 0.001
2024-11-05 00:42:50,963 - INFO - [diffusion][Epoch 8395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:50,964 - INFO - [diffusion][Epoch 8396] Epoch 8397/12000
2024-11-05 00:42:54,488 - INFO - [diffusion][Epoch 8396] diffusion training Loss: 0.06946117617189884
2024-11-05 00:42:54,490 - INFO - [diffusion][Epoch 8396] diffusion learning rate: 0.001
2024-11-05 00:42:54,492 - INFO - [diffusion][Epoch 8396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:54,493 - INFO - [diffusion][Epoch 8397] Epoch 8398/12000
2024-11-05 00:42:58,289 - INFO - [diffusion][Epoch 8397] diffusion training Loss: 0.07440883293747902
2024-11-05 00:42:58,291 - INFO - [diffusion][Epoch 8397] diffusion learning rate: 0.001
2024-11-05 00:42:58,293 - INFO - [diffusion][Epoch 8397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:58,294 - INFO - [diffusion][Epoch 8398] Epoch 8399/12000
2024-11-05 00:43:01,737 - INFO - [diffusion][Epoch 8398] diffusion training Loss: 0.0712746512144804
2024-11-05 00:43:01,739 - INFO - [diffusion][Epoch 8398] diffusion learning rate: 0.001
2024-11-05 00:43:01,741 - INFO - [diffusion][Epoch 8398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:01,742 - INFO - [diffusion][Epoch 8399] Epoch 8400/12000
2024-11-05 00:43:05,473 - INFO - [diffusion][Epoch 8399] diffusion training Loss: 0.06823472678661346
2024-11-05 00:43:05,475 - INFO - [diffusion][Epoch 8399] diffusion learning rate: 0.001
2024-11-05 00:43:05,477 - INFO - [diffusion][Epoch 8399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:05,478 - INFO - [diffusion][Epoch 8400] Epoch 8401/12000
2024-11-05 00:43:08,720 - INFO - [diffusion][Epoch 8400] diffusion training Loss: 0.06850961595773697
2024-11-05 00:43:08,722 - INFO - [diffusion][Epoch 8400] diffusion learning rate: 0.001
2024-11-05 00:43:08,770 - INFO - [diffusion][Epoch 8400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:08,772 - INFO - [diffusion][Epoch 8401] Epoch 8402/12000
2024-11-05 00:43:11,974 - INFO - [diffusion][Epoch 8401] diffusion training Loss: 0.07272970676422119
2024-11-05 00:43:11,976 - INFO - [diffusion][Epoch 8401] diffusion learning rate: 0.001
2024-11-05 00:43:11,978 - INFO - [diffusion][Epoch 8401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:11,979 - INFO - [diffusion][Epoch 8402] Epoch 8403/12000
2024-11-05 00:43:15,145 - INFO - [diffusion][Epoch 8402] diffusion training Loss: 0.07052689418196678
2024-11-05 00:43:15,147 - INFO - [diffusion][Epoch 8402] diffusion learning rate: 0.001
2024-11-05 00:43:15,149 - INFO - [diffusion][Epoch 8402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:15,150 - INFO - [diffusion][Epoch 8403] Epoch 8404/12000
2024-11-05 00:43:18,652 - INFO - [diffusion][Epoch 8403] diffusion training Loss: 0.07534187659621239
2024-11-05 00:43:18,654 - INFO - [diffusion][Epoch 8403] diffusion learning rate: 0.001
2024-11-05 00:43:18,655 - INFO - [diffusion][Epoch 8403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:18,657 - INFO - [diffusion][Epoch 8404] Epoch 8405/12000
2024-11-05 00:43:22,285 - INFO - [diffusion][Epoch 8404] diffusion training Loss: 0.06799641344696283
2024-11-05 00:43:22,287 - INFO - [diffusion][Epoch 8404] diffusion learning rate: 0.001
2024-11-05 00:43:22,289 - INFO - [diffusion][Epoch 8404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:22,290 - INFO - [diffusion][Epoch 8405] Epoch 8406/12000
2024-11-05 00:43:25,419 - INFO - [diffusion][Epoch 8405] diffusion training Loss: 0.07000103034079075
2024-11-05 00:43:25,421 - INFO - [diffusion][Epoch 8405] diffusion learning rate: 0.001
2024-11-05 00:43:25,423 - INFO - [diffusion][Epoch 8405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:25,424 - INFO - [diffusion][Epoch 8406] Epoch 8407/12000
2024-11-05 00:43:28,560 - INFO - [diffusion][Epoch 8406] diffusion training Loss: 0.06849735602736473
2024-11-05 00:43:28,562 - INFO - [diffusion][Epoch 8406] diffusion learning rate: 0.001
2024-11-05 00:43:28,564 - INFO - [diffusion][Epoch 8406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:28,565 - INFO - [diffusion][Epoch 8407] Epoch 8408/12000
2024-11-05 00:43:31,810 - INFO - [diffusion][Epoch 8407] diffusion training Loss: 0.07100566662847996
2024-11-05 00:43:31,812 - INFO - [diffusion][Epoch 8407] diffusion learning rate: 0.001
2024-11-05 00:43:31,814 - INFO - [diffusion][Epoch 8407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:31,815 - INFO - [diffusion][Epoch 8408] Epoch 8409/12000
2024-11-05 00:43:35,316 - INFO - [diffusion][Epoch 8408] diffusion training Loss: 0.0698170131072402
2024-11-05 00:43:35,317 - INFO - [diffusion][Epoch 8408] diffusion learning rate: 0.001
2024-11-05 00:43:35,319 - INFO - [diffusion][Epoch 8408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:35,320 - INFO - [diffusion][Epoch 8409] Epoch 8410/12000
2024-11-05 00:43:39,060 - INFO - [diffusion][Epoch 8409] diffusion training Loss: 0.07143963873386383
2024-11-05 00:43:39,062 - INFO - [diffusion][Epoch 8409] diffusion learning rate: 0.001
2024-11-05 00:43:39,064 - INFO - [diffusion][Epoch 8409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:39,065 - INFO - [diffusion][Epoch 8410] Epoch 8411/12000
2024-11-05 00:43:42,336 - INFO - [diffusion][Epoch 8410] diffusion training Loss: 0.06903011165559292
2024-11-05 00:43:42,338 - INFO - [diffusion][Epoch 8410] diffusion learning rate: 0.001
2024-11-05 00:43:42,340 - INFO - [diffusion][Epoch 8410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:42,341 - INFO - [diffusion][Epoch 8411] Epoch 8412/12000
2024-11-05 00:43:45,666 - INFO - [diffusion][Epoch 8411] diffusion training Loss: 0.07101044803857803
2024-11-05 00:43:45,668 - INFO - [diffusion][Epoch 8411] diffusion learning rate: 0.001
2024-11-05 00:43:45,670 - INFO - [diffusion][Epoch 8411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:45,672 - INFO - [diffusion][Epoch 8412] Epoch 8413/12000
2024-11-05 00:43:48,499 - INFO - [diffusion][Epoch 8412] diffusion training Loss: 0.06948532536625862
2024-11-05 00:43:48,501 - INFO - [diffusion][Epoch 8412] diffusion learning rate: 0.001
2024-11-05 00:43:48,503 - INFO - [diffusion][Epoch 8412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:48,505 - INFO - [diffusion][Epoch 8413] Epoch 8414/12000
2024-11-05 00:43:52,032 - INFO - [diffusion][Epoch 8413] diffusion training Loss: 0.07081410475075245
2024-11-05 00:43:52,034 - INFO - [diffusion][Epoch 8413] diffusion learning rate: 0.001
2024-11-05 00:43:52,036 - INFO - [diffusion][Epoch 8413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:52,037 - INFO - [diffusion][Epoch 8414] Epoch 8415/12000
2024-11-05 00:43:55,574 - INFO - [diffusion][Epoch 8414] diffusion training Loss: 0.06732356920838356
2024-11-05 00:43:55,576 - INFO - [diffusion][Epoch 8414] diffusion learning rate: 0.001
2024-11-05 00:43:55,577 - INFO - [diffusion][Epoch 8414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:55,579 - INFO - [diffusion][Epoch 8415] Epoch 8416/12000
2024-11-05 00:43:58,768 - INFO - [diffusion][Epoch 8415] diffusion training Loss: 0.0751277394592762
2024-11-05 00:43:58,770 - INFO - [diffusion][Epoch 8415] diffusion learning rate: 0.001
2024-11-05 00:43:58,772 - INFO - [diffusion][Epoch 8415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:58,773 - INFO - [diffusion][Epoch 8416] Epoch 8417/12000
2024-11-05 00:44:01,879 - INFO - [diffusion][Epoch 8416] diffusion training Loss: 0.07436497509479523
2024-11-05 00:44:01,881 - INFO - [diffusion][Epoch 8416] diffusion learning rate: 0.001
2024-11-05 00:44:01,883 - INFO - [diffusion][Epoch 8416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:01,884 - INFO - [diffusion][Epoch 8417] Epoch 8418/12000
2024-11-05 00:44:05,018 - INFO - [diffusion][Epoch 8417] diffusion training Loss: 0.06923339981585741
2024-11-05 00:44:05,020 - INFO - [diffusion][Epoch 8417] diffusion learning rate: 0.001
2024-11-05 00:44:05,021 - INFO - [diffusion][Epoch 8417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:05,023 - INFO - [diffusion][Epoch 8418] Epoch 8419/12000
2024-11-05 00:44:08,662 - INFO - [diffusion][Epoch 8418] diffusion training Loss: 0.0707106925547123
2024-11-05 00:44:08,664 - INFO - [diffusion][Epoch 8418] diffusion learning rate: 0.001
2024-11-05 00:44:08,665 - INFO - [diffusion][Epoch 8418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:08,667 - INFO - [diffusion][Epoch 8419] Epoch 8420/12000
2024-11-05 00:44:12,450 - INFO - [diffusion][Epoch 8419] diffusion training Loss: 0.06775457225739956
2024-11-05 00:44:12,452 - INFO - [diffusion][Epoch 8419] diffusion learning rate: 0.001
2024-11-05 00:44:12,454 - INFO - [diffusion][Epoch 8419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:12,455 - INFO - [diffusion][Epoch 8420] Epoch 8421/12000
2024-11-05 00:44:15,832 - INFO - [diffusion][Epoch 8420] diffusion training Loss: 0.07164466008543968
2024-11-05 00:44:15,834 - INFO - [diffusion][Epoch 8420] diffusion learning rate: 0.001
2024-11-05 00:44:15,837 - INFO - [diffusion][Epoch 8420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:15,838 - INFO - [diffusion][Epoch 8421] Epoch 8422/12000
2024-11-05 00:44:19,017 - INFO - [diffusion][Epoch 8421] diffusion training Loss: 0.07075277157127857
2024-11-05 00:44:19,019 - INFO - [diffusion][Epoch 8421] diffusion learning rate: 0.001
2024-11-05 00:44:19,021 - INFO - [diffusion][Epoch 8421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:19,022 - INFO - [diffusion][Epoch 8422] Epoch 8423/12000
2024-11-05 00:44:22,123 - INFO - [diffusion][Epoch 8422] diffusion training Loss: 0.0658987807109952
2024-11-05 00:44:22,125 - INFO - [diffusion][Epoch 8422] diffusion learning rate: 0.001
2024-11-05 00:44:22,127 - INFO - [diffusion][Epoch 8422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:22,128 - INFO - [diffusion][Epoch 8423] Epoch 8424/12000
2024-11-05 00:44:25,499 - INFO - [diffusion][Epoch 8423] diffusion training Loss: 0.0726263802498579
2024-11-05 00:44:25,501 - INFO - [diffusion][Epoch 8423] diffusion learning rate: 0.001
2024-11-05 00:44:25,503 - INFO - [diffusion][Epoch 8423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:25,504 - INFO - [diffusion][Epoch 8424] Epoch 8425/12000
2024-11-05 00:44:29,256 - INFO - [diffusion][Epoch 8424] diffusion training Loss: 0.0693676806986332
2024-11-05 00:44:29,259 - INFO - [diffusion][Epoch 8424] diffusion learning rate: 0.001
2024-11-05 00:44:29,261 - INFO - [diffusion][Epoch 8424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:29,262 - INFO - [diffusion][Epoch 8425] Epoch 8426/12000
2024-11-05 00:44:32,729 - INFO - [diffusion][Epoch 8425] diffusion training Loss: 0.07623717933893204
2024-11-05 00:44:32,731 - INFO - [diffusion][Epoch 8425] diffusion learning rate: 0.001
2024-11-05 00:44:32,733 - INFO - [diffusion][Epoch 8425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:32,739 - INFO - [diffusion][Epoch 8426] Epoch 8427/12000
2024-11-05 00:44:35,878 - INFO - [diffusion][Epoch 8426] diffusion training Loss: 0.06972064822912216
2024-11-05 00:44:35,880 - INFO - [diffusion][Epoch 8426] diffusion learning rate: 0.001
2024-11-05 00:44:35,882 - INFO - [diffusion][Epoch 8426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:35,883 - INFO - [diffusion][Epoch 8427] Epoch 8428/12000
2024-11-05 00:44:39,037 - INFO - [diffusion][Epoch 8427] diffusion training Loss: 0.0714946985244751
2024-11-05 00:44:39,038 - INFO - [diffusion][Epoch 8427] diffusion learning rate: 0.001
2024-11-05 00:44:39,040 - INFO - [diffusion][Epoch 8427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:39,041 - INFO - [diffusion][Epoch 8428] Epoch 8429/12000
2024-11-05 00:44:42,326 - INFO - [diffusion][Epoch 8428] diffusion training Loss: 0.0662850420922041
2024-11-05 00:44:42,329 - INFO - [diffusion][Epoch 8428] diffusion learning rate: 0.001
2024-11-05 00:44:42,331 - INFO - [diffusion][Epoch 8428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:42,332 - INFO - [diffusion][Epoch 8429] Epoch 8430/12000
2024-11-05 00:44:45,969 - INFO - [diffusion][Epoch 8429] diffusion training Loss: 0.06843594834208488
2024-11-05 00:44:45,971 - INFO - [diffusion][Epoch 8429] diffusion learning rate: 0.001
2024-11-05 00:44:45,973 - INFO - [diffusion][Epoch 8429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:45,974 - INFO - [diffusion][Epoch 8430] Epoch 8431/12000
2024-11-05 00:44:49,387 - INFO - [diffusion][Epoch 8430] diffusion training Loss: 0.06791992485523224
2024-11-05 00:44:49,389 - INFO - [diffusion][Epoch 8430] diffusion learning rate: 0.001
2024-11-05 00:44:49,391 - INFO - [diffusion][Epoch 8430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:49,393 - INFO - [diffusion][Epoch 8431] Epoch 8432/12000
2024-11-05 00:44:52,485 - INFO - [diffusion][Epoch 8431] diffusion training Loss: 0.06593573279678822
2024-11-05 00:44:52,487 - INFO - [diffusion][Epoch 8431] diffusion learning rate: 0.001
2024-11-05 00:44:52,489 - INFO - [diffusion][Epoch 8431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:52,490 - INFO - [diffusion][Epoch 8432] Epoch 8433/12000
2024-11-05 00:44:55,387 - INFO - [diffusion][Epoch 8432] diffusion training Loss: 0.07411127537488937
2024-11-05 00:44:55,389 - INFO - [diffusion][Epoch 8432] diffusion learning rate: 0.001
2024-11-05 00:44:55,391 - INFO - [diffusion][Epoch 8432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:55,392 - INFO - [diffusion][Epoch 8433] Epoch 8434/12000
2024-11-05 00:44:58,702 - INFO - [diffusion][Epoch 8433] diffusion training Loss: 0.07552441395819187
2024-11-05 00:44:58,704 - INFO - [diffusion][Epoch 8433] diffusion learning rate: 0.001
2024-11-05 00:44:58,706 - INFO - [diffusion][Epoch 8433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:58,707 - INFO - [diffusion][Epoch 8434] Epoch 8435/12000
2024-11-05 00:45:01,962 - INFO - [diffusion][Epoch 8434] diffusion training Loss: 0.07041704654693604
2024-11-05 00:45:01,964 - INFO - [diffusion][Epoch 8434] diffusion learning rate: 0.001
2024-11-05 00:45:01,966 - INFO - [diffusion][Epoch 8434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:01,967 - INFO - [diffusion][Epoch 8435] Epoch 8436/12000
2024-11-05 00:45:05,276 - INFO - [diffusion][Epoch 8435] diffusion training Loss: 0.069308340549469
2024-11-05 00:45:05,278 - INFO - [diffusion][Epoch 8435] diffusion learning rate: 0.001
2024-11-05 00:45:05,280 - INFO - [diffusion][Epoch 8435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:05,281 - INFO - [diffusion][Epoch 8436] Epoch 8437/12000
2024-11-05 00:45:08,339 - INFO - [diffusion][Epoch 8436] diffusion training Loss: 0.068258848041296
2024-11-05 00:45:08,341 - INFO - [diffusion][Epoch 8436] diffusion learning rate: 0.001
2024-11-05 00:45:08,343 - INFO - [diffusion][Epoch 8436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:08,344 - INFO - [diffusion][Epoch 8437] Epoch 8438/12000
2024-11-05 00:45:11,810 - INFO - [diffusion][Epoch 8437] diffusion training Loss: 0.06441949959844351
2024-11-05 00:45:11,812 - INFO - [diffusion][Epoch 8437] diffusion learning rate: 0.001
2024-11-05 00:45:11,813 - INFO - [diffusion][Epoch 8437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:11,814 - INFO - [diffusion][Epoch 8438] Epoch 8439/12000
2024-11-05 00:45:15,496 - INFO - [diffusion][Epoch 8438] diffusion training Loss: 0.07050615921616554
2024-11-05 00:45:15,498 - INFO - [diffusion][Epoch 8438] diffusion learning rate: 0.001
2024-11-05 00:45:15,500 - INFO - [diffusion][Epoch 8438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:15,501 - INFO - [diffusion][Epoch 8439] Epoch 8440/12000
2024-11-05 00:45:19,803 - INFO - [diffusion][Epoch 8439] diffusion training Loss: 0.0678572878241539
2024-11-05 00:45:19,804 - INFO - [diffusion][Epoch 8439] diffusion learning rate: 0.001
2024-11-05 00:45:19,882 - INFO - [diffusion][Epoch 8439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:19,884 - INFO - [diffusion][Epoch 8440] Epoch 8441/12000
2024-11-05 00:45:23,002 - INFO - [diffusion][Epoch 8440] diffusion training Loss: 0.06720725260674953
2024-11-05 00:45:23,004 - INFO - [diffusion][Epoch 8440] diffusion learning rate: 0.001
2024-11-05 00:45:23,005 - INFO - [diffusion][Epoch 8440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:23,007 - INFO - [diffusion][Epoch 8441] Epoch 8442/12000
2024-11-05 00:45:26,090 - INFO - [diffusion][Epoch 8441] diffusion training Loss: 0.07042138651013374
2024-11-05 00:45:26,092 - INFO - [diffusion][Epoch 8441] diffusion learning rate: 0.001
2024-11-05 00:45:26,093 - INFO - [diffusion][Epoch 8441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:26,095 - INFO - [diffusion][Epoch 8442] Epoch 8443/12000
2024-11-05 00:45:29,292 - INFO - [diffusion][Epoch 8442] diffusion training Loss: 0.07556007988750935
2024-11-05 00:45:29,295 - INFO - [diffusion][Epoch 8442] diffusion learning rate: 0.001
2024-11-05 00:45:29,297 - INFO - [diffusion][Epoch 8442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:29,298 - INFO - [diffusion][Epoch 8443] Epoch 8444/12000
2024-11-05 00:45:32,879 - INFO - [diffusion][Epoch 8443] diffusion training Loss: 0.06813539285212755
2024-11-05 00:45:32,881 - INFO - [diffusion][Epoch 8443] diffusion learning rate: 0.001
2024-11-05 00:45:32,882 - INFO - [diffusion][Epoch 8443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:32,884 - INFO - [diffusion][Epoch 8444] Epoch 8445/12000
2024-11-05 00:45:36,030 - INFO - [diffusion][Epoch 8444] diffusion training Loss: 0.06547220516949892
2024-11-05 00:45:36,039 - INFO - [diffusion][Epoch 8444] diffusion learning rate: 0.001
2024-11-05 00:45:36,041 - INFO - [diffusion][Epoch 8444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:36,042 - INFO - [diffusion][Epoch 8445] Epoch 8446/12000
2024-11-05 00:45:39,156 - INFO - [diffusion][Epoch 8445] diffusion training Loss: 0.0678285751491785
2024-11-05 00:45:39,158 - INFO - [diffusion][Epoch 8445] diffusion learning rate: 0.001
2024-11-05 00:45:39,159 - INFO - [diffusion][Epoch 8445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:39,161 - INFO - [diffusion][Epoch 8446] Epoch 8447/12000
2024-11-05 00:45:42,234 - INFO - [diffusion][Epoch 8446] diffusion training Loss: 0.06410778127610683
2024-11-05 00:45:42,236 - INFO - [diffusion][Epoch 8446] diffusion learning rate: 0.001
2024-11-05 00:45:42,237 - INFO - [diffusion][Epoch 8446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:42,239 - INFO - [diffusion][Epoch 8447] Epoch 8448/12000
2024-11-05 00:45:45,818 - INFO - [diffusion][Epoch 8447] diffusion training Loss: 0.07054462470114231
2024-11-05 00:45:45,821 - INFO - [diffusion][Epoch 8447] diffusion learning rate: 0.001
2024-11-05 00:45:45,823 - INFO - [diffusion][Epoch 8447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:45,824 - INFO - [diffusion][Epoch 8448] Epoch 8449/12000
2024-11-05 00:45:49,414 - INFO - [diffusion][Epoch 8448] diffusion training Loss: 0.06630144082009792
2024-11-05 00:45:49,415 - INFO - [diffusion][Epoch 8448] diffusion learning rate: 0.001
2024-11-05 00:45:49,417 - INFO - [diffusion][Epoch 8448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:49,419 - INFO - [diffusion][Epoch 8449] Epoch 8450/12000
2024-11-05 00:45:52,440 - INFO - [diffusion][Epoch 8449] diffusion training Loss: 0.06375093758106232
2024-11-05 00:45:52,442 - INFO - [diffusion][Epoch 8449] diffusion learning rate: 0.001
2024-11-05 00:45:52,444 - INFO - [diffusion][Epoch 8449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:52,445 - INFO - [diffusion][Epoch 8450] Epoch 8451/12000
2024-11-05 00:45:55,551 - INFO - [diffusion][Epoch 8450] diffusion training Loss: 0.07098030857741833
2024-11-05 00:45:55,553 - INFO - [diffusion][Epoch 8450] diffusion learning rate: 0.001
2024-11-05 00:45:55,555 - INFO - [diffusion][Epoch 8450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:55,557 - INFO - [diffusion][Epoch 8451] Epoch 8452/12000
2024-11-05 00:45:58,827 - INFO - [diffusion][Epoch 8451] diffusion training Loss: 0.07154174521565437
2024-11-05 00:45:58,828 - INFO - [diffusion][Epoch 8451] diffusion learning rate: 0.001
2024-11-05 00:45:58,831 - INFO - [diffusion][Epoch 8451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:58,832 - INFO - [diffusion][Epoch 8452] Epoch 8453/12000
2024-11-05 00:46:02,396 - INFO - [diffusion][Epoch 8452] diffusion training Loss: 0.07083838153630495
2024-11-05 00:46:02,398 - INFO - [diffusion][Epoch 8452] diffusion learning rate: 0.001
2024-11-05 00:46:02,443 - INFO - [diffusion][Epoch 8452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:02,445 - INFO - [diffusion][Epoch 8453] Epoch 8454/12000
2024-11-05 00:46:05,764 - INFO - [diffusion][Epoch 8453] diffusion training Loss: 0.07023114524781704
2024-11-05 00:46:05,766 - INFO - [diffusion][Epoch 8453] diffusion learning rate: 0.001
2024-11-05 00:46:05,768 - INFO - [diffusion][Epoch 8453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:05,769 - INFO - [diffusion][Epoch 8454] Epoch 8455/12000
2024-11-05 00:46:08,920 - INFO - [diffusion][Epoch 8454] diffusion training Loss: 0.06818819046020508
2024-11-05 00:46:08,922 - INFO - [diffusion][Epoch 8454] diffusion learning rate: 0.001
2024-11-05 00:46:08,924 - INFO - [diffusion][Epoch 8454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:08,925 - INFO - [diffusion][Epoch 8455] Epoch 8456/12000
2024-11-05 00:46:12,086 - INFO - [diffusion][Epoch 8455] diffusion training Loss: 0.06523718032985926
2024-11-05 00:46:12,088 - INFO - [diffusion][Epoch 8455] diffusion learning rate: 0.001
2024-11-05 00:46:12,089 - INFO - [diffusion][Epoch 8455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:12,090 - INFO - [diffusion][Epoch 8456] Epoch 8457/12000
2024-11-05 00:46:15,537 - INFO - [diffusion][Epoch 8456] diffusion training Loss: 0.06564806960523129
2024-11-05 00:46:15,539 - INFO - [diffusion][Epoch 8456] diffusion learning rate: 0.001
2024-11-05 00:46:15,541 - INFO - [diffusion][Epoch 8456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:15,542 - INFO - [diffusion][Epoch 8457] Epoch 8458/12000
2024-11-05 00:46:19,036 - INFO - [diffusion][Epoch 8457] diffusion training Loss: 0.07295282185077667
2024-11-05 00:46:19,038 - INFO - [diffusion][Epoch 8457] diffusion learning rate: 0.001
2024-11-05 00:46:19,041 - INFO - [diffusion][Epoch 8457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:19,043 - INFO - [diffusion][Epoch 8458] Epoch 8459/12000
2024-11-05 00:46:22,547 - INFO - [diffusion][Epoch 8458] diffusion training Loss: 0.07469850406050682
2024-11-05 00:46:22,548 - INFO - [diffusion][Epoch 8458] diffusion learning rate: 0.001
2024-11-05 00:46:22,550 - INFO - [diffusion][Epoch 8458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:22,552 - INFO - [diffusion][Epoch 8459] Epoch 8460/12000
2024-11-05 00:46:25,688 - INFO - [diffusion][Epoch 8459] diffusion training Loss: 0.0714327022433281
2024-11-05 00:46:25,690 - INFO - [diffusion][Epoch 8459] diffusion learning rate: 0.001
2024-11-05 00:46:25,692 - INFO - [diffusion][Epoch 8459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:25,693 - INFO - [diffusion][Epoch 8460] Epoch 8461/12000
2024-11-05 00:46:28,772 - INFO - [diffusion][Epoch 8460] diffusion training Loss: 0.0702755730599165
2024-11-05 00:46:28,775 - INFO - [diffusion][Epoch 8460] diffusion learning rate: 0.001
2024-11-05 00:46:28,777 - INFO - [diffusion][Epoch 8460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:28,780 - INFO - [diffusion][Epoch 8461] Epoch 8462/12000
2024-11-05 00:46:32,309 - INFO - [diffusion][Epoch 8461] diffusion training Loss: 0.06833200994879007
2024-11-05 00:46:32,311 - INFO - [diffusion][Epoch 8461] diffusion learning rate: 0.001
2024-11-05 00:46:32,313 - INFO - [diffusion][Epoch 8461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:32,314 - INFO - [diffusion][Epoch 8462] Epoch 8463/12000
2024-11-05 00:46:36,099 - INFO - [diffusion][Epoch 8462] diffusion training Loss: 0.0712005216628313
2024-11-05 00:46:36,101 - INFO - [diffusion][Epoch 8462] diffusion learning rate: 0.001
2024-11-05 00:46:36,151 - INFO - [diffusion][Epoch 8462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:36,153 - INFO - [diffusion][Epoch 8463] Epoch 8464/12000
2024-11-05 00:46:39,705 - INFO - [diffusion][Epoch 8463] diffusion training Loss: 0.06832650490105152
2024-11-05 00:46:39,707 - INFO - [diffusion][Epoch 8463] diffusion learning rate: 0.001
2024-11-05 00:46:39,709 - INFO - [diffusion][Epoch 8463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:39,711 - INFO - [diffusion][Epoch 8464] Epoch 8465/12000
2024-11-05 00:46:42,853 - INFO - [diffusion][Epoch 8464] diffusion training Loss: 0.06459005549550056
2024-11-05 00:46:42,855 - INFO - [diffusion][Epoch 8464] diffusion learning rate: 0.001
2024-11-05 00:46:42,856 - INFO - [diffusion][Epoch 8464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:42,858 - INFO - [diffusion][Epoch 8465] Epoch 8466/12000
2024-11-05 00:46:45,639 - INFO - [diffusion][Epoch 8465] diffusion training Loss: 0.06671592406928539
2024-11-05 00:46:45,641 - INFO - [diffusion][Epoch 8465] diffusion learning rate: 0.001
2024-11-05 00:46:45,643 - INFO - [diffusion][Epoch 8465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:45,644 - INFO - [diffusion][Epoch 8466] Epoch 8467/12000
2024-11-05 00:46:49,290 - INFO - [diffusion][Epoch 8466] diffusion training Loss: 0.06944423541426659
2024-11-05 00:46:49,293 - INFO - [diffusion][Epoch 8466] diffusion learning rate: 0.001
2024-11-05 00:46:49,295 - INFO - [diffusion][Epoch 8466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:49,296 - INFO - [diffusion][Epoch 8467] Epoch 8468/12000
2024-11-05 00:46:52,809 - INFO - [diffusion][Epoch 8467] diffusion training Loss: 0.0715465284883976
2024-11-05 00:46:52,811 - INFO - [diffusion][Epoch 8467] diffusion learning rate: 0.001
2024-11-05 00:46:52,813 - INFO - [diffusion][Epoch 8467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:52,814 - INFO - [diffusion][Epoch 8468] Epoch 8469/12000
2024-11-05 00:46:55,955 - INFO - [diffusion][Epoch 8468] diffusion training Loss: 0.07362854853272438
2024-11-05 00:46:55,957 - INFO - [diffusion][Epoch 8468] diffusion learning rate: 0.001
2024-11-05 00:46:55,959 - INFO - [diffusion][Epoch 8468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:55,960 - INFO - [diffusion][Epoch 8469] Epoch 8470/12000
2024-11-05 00:46:59,176 - INFO - [diffusion][Epoch 8469] diffusion training Loss: 0.06840640865266323
2024-11-05 00:46:59,179 - INFO - [diffusion][Epoch 8469] diffusion learning rate: 0.001
2024-11-05 00:46:59,181 - INFO - [diffusion][Epoch 8469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:59,182 - INFO - [diffusion][Epoch 8470] Epoch 8471/12000
2024-11-05 00:47:02,395 - INFO - [diffusion][Epoch 8470] diffusion training Loss: 0.06918937154114246
2024-11-05 00:47:02,397 - INFO - [diffusion][Epoch 8470] diffusion learning rate: 0.001
2024-11-05 00:47:02,437 - INFO - [diffusion][Epoch 8470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:02,438 - INFO - [diffusion][Epoch 8471] Epoch 8472/12000
2024-11-05 00:47:05,841 - INFO - [diffusion][Epoch 8471] diffusion training Loss: 0.0696762427687645
2024-11-05 00:47:05,843 - INFO - [diffusion][Epoch 8471] diffusion learning rate: 0.001
2024-11-05 00:47:05,845 - INFO - [diffusion][Epoch 8471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:05,846 - INFO - [diffusion][Epoch 8472] Epoch 8473/12000
2024-11-05 00:47:08,915 - INFO - [diffusion][Epoch 8472] diffusion training Loss: 0.07266060821712017
2024-11-05 00:47:08,917 - INFO - [diffusion][Epoch 8472] diffusion learning rate: 0.001
2024-11-05 00:47:08,918 - INFO - [diffusion][Epoch 8472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:08,919 - INFO - [diffusion][Epoch 8473] Epoch 8474/12000
2024-11-05 00:47:11,960 - INFO - [diffusion][Epoch 8473] diffusion training Loss: 0.06945425644516945
2024-11-05 00:47:11,962 - INFO - [diffusion][Epoch 8473] diffusion learning rate: 0.001
2024-11-05 00:47:11,963 - INFO - [diffusion][Epoch 8473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:11,965 - INFO - [diffusion][Epoch 8474] Epoch 8475/12000
2024-11-05 00:47:15,053 - INFO - [diffusion][Epoch 8474] diffusion training Loss: 0.07426103390753269
2024-11-05 00:47:15,056 - INFO - [diffusion][Epoch 8474] diffusion learning rate: 0.001
2024-11-05 00:47:15,058 - INFO - [diffusion][Epoch 8474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:15,059 - INFO - [diffusion][Epoch 8475] Epoch 8476/12000
2024-11-05 00:47:18,475 - INFO - [diffusion][Epoch 8475] diffusion training Loss: 0.06649499759078026
2024-11-05 00:47:18,477 - INFO - [diffusion][Epoch 8475] diffusion learning rate: 0.001
2024-11-05 00:47:18,479 - INFO - [diffusion][Epoch 8475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:18,481 - INFO - [diffusion][Epoch 8476] Epoch 8477/12000
2024-11-05 00:47:21,540 - INFO - [diffusion][Epoch 8476] diffusion training Loss: 0.06795520894229412
2024-11-05 00:47:21,542 - INFO - [diffusion][Epoch 8476] diffusion learning rate: 0.001
2024-11-05 00:47:21,543 - INFO - [diffusion][Epoch 8476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:21,545 - INFO - [diffusion][Epoch 8477] Epoch 8478/12000
2024-11-05 00:47:24,551 - INFO - [diffusion][Epoch 8477] diffusion training Loss: 0.0699264146387577
2024-11-05 00:47:24,553 - INFO - [diffusion][Epoch 8477] diffusion learning rate: 0.001
2024-11-05 00:47:24,555 - INFO - [diffusion][Epoch 8477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:24,556 - INFO - [diffusion][Epoch 8478] Epoch 8479/12000
2024-11-05 00:47:27,718 - INFO - [diffusion][Epoch 8478] diffusion training Loss: 0.06928499788045883
2024-11-05 00:47:27,720 - INFO - [diffusion][Epoch 8478] diffusion learning rate: 0.001
2024-11-05 00:47:27,722 - INFO - [diffusion][Epoch 8478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:27,723 - INFO - [diffusion][Epoch 8479] Epoch 8480/12000
2024-11-05 00:47:31,246 - INFO - [diffusion][Epoch 8479] diffusion training Loss: 0.07279680483043194
2024-11-05 00:47:31,247 - INFO - [diffusion][Epoch 8479] diffusion learning rate: 0.001
2024-11-05 00:47:31,249 - INFO - [diffusion][Epoch 8479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:31,251 - INFO - [diffusion][Epoch 8480] Epoch 8481/12000
2024-11-05 00:47:34,790 - INFO - [diffusion][Epoch 8480] diffusion training Loss: 0.07022149860858917
2024-11-05 00:47:34,792 - INFO - [diffusion][Epoch 8480] diffusion learning rate: 0.001
2024-11-05 00:47:34,794 - INFO - [diffusion][Epoch 8480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:34,796 - INFO - [diffusion][Epoch 8481] Epoch 8482/12000
2024-11-05 00:47:37,874 - INFO - [diffusion][Epoch 8481] diffusion training Loss: 0.07232141681015491
2024-11-05 00:47:37,876 - INFO - [diffusion][Epoch 8481] diffusion learning rate: 0.001
2024-11-05 00:47:37,878 - INFO - [diffusion][Epoch 8481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:37,879 - INFO - [diffusion][Epoch 8482] Epoch 8483/12000
2024-11-05 00:47:40,979 - INFO - [diffusion][Epoch 8482] diffusion training Loss: 0.07343967445194721
2024-11-05 00:47:40,981 - INFO - [diffusion][Epoch 8482] diffusion learning rate: 0.001
2024-11-05 00:47:40,983 - INFO - [diffusion][Epoch 8482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:40,984 - INFO - [diffusion][Epoch 8483] Epoch 8484/12000
2024-11-05 00:47:44,204 - INFO - [diffusion][Epoch 8483] diffusion training Loss: 0.07043718360364437
2024-11-05 00:47:44,206 - INFO - [diffusion][Epoch 8483] diffusion learning rate: 0.001
2024-11-05 00:47:44,208 - INFO - [diffusion][Epoch 8483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:44,209 - INFO - [diffusion][Epoch 8484] Epoch 8485/12000
2024-11-05 00:47:47,894 - INFO - [diffusion][Epoch 8484] diffusion training Loss: 0.06942167319357395
2024-11-05 00:47:47,896 - INFO - [diffusion][Epoch 8484] diffusion learning rate: 0.001
2024-11-05 00:47:47,898 - INFO - [diffusion][Epoch 8484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:47,899 - INFO - [diffusion][Epoch 8485] Epoch 8486/12000
2024-11-05 00:47:51,245 - INFO - [diffusion][Epoch 8485] diffusion training Loss: 0.07478399388492107
2024-11-05 00:47:51,248 - INFO - [diffusion][Epoch 8485] diffusion learning rate: 0.001
2024-11-05 00:47:51,250 - INFO - [diffusion][Epoch 8485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:51,251 - INFO - [diffusion][Epoch 8486] Epoch 8487/12000
2024-11-05 00:47:54,393 - INFO - [diffusion][Epoch 8486] diffusion training Loss: 0.06656027771532536
2024-11-05 00:47:54,395 - INFO - [diffusion][Epoch 8486] diffusion learning rate: 0.001
2024-11-05 00:47:54,396 - INFO - [diffusion][Epoch 8486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:54,398 - INFO - [diffusion][Epoch 8487] Epoch 8488/12000
2024-11-05 00:47:57,481 - INFO - [diffusion][Epoch 8487] diffusion training Loss: 0.07773000746965408
2024-11-05 00:47:57,483 - INFO - [diffusion][Epoch 8487] diffusion learning rate: 0.001
2024-11-05 00:47:57,489 - INFO - [diffusion][Epoch 8487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:57,491 - INFO - [diffusion][Epoch 8488] Epoch 8489/12000
2024-11-05 00:48:00,945 - INFO - [diffusion][Epoch 8488] diffusion training Loss: 0.06957695074379444
2024-11-05 00:48:00,946 - INFO - [diffusion][Epoch 8488] diffusion learning rate: 0.001
2024-11-05 00:48:00,948 - INFO - [diffusion][Epoch 8488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:00,949 - INFO - [diffusion][Epoch 8489] Epoch 8490/12000
2024-11-05 00:48:04,484 - INFO - [diffusion][Epoch 8489] diffusion training Loss: 0.07243315782397985
2024-11-05 00:48:04,486 - INFO - [diffusion][Epoch 8489] diffusion learning rate: 0.001
2024-11-05 00:48:04,488 - INFO - [diffusion][Epoch 8489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:04,490 - INFO - [diffusion][Epoch 8490] Epoch 8491/12000
2024-11-05 00:48:07,580 - INFO - [diffusion][Epoch 8490] diffusion training Loss: 0.06985507905483246
2024-11-05 00:48:07,582 - INFO - [diffusion][Epoch 8490] diffusion learning rate: 0.001
2024-11-05 00:48:07,584 - INFO - [diffusion][Epoch 8490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:07,585 - INFO - [diffusion][Epoch 8491] Epoch 8492/12000
2024-11-05 00:48:10,441 - INFO - [diffusion][Epoch 8491] diffusion training Loss: 0.07344971969723701
2024-11-05 00:48:10,443 - INFO - [diffusion][Epoch 8491] diffusion learning rate: 0.001
2024-11-05 00:48:10,445 - INFO - [diffusion][Epoch 8491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:10,446 - INFO - [diffusion][Epoch 8492] Epoch 8493/12000
2024-11-05 00:48:13,855 - INFO - [diffusion][Epoch 8492] diffusion training Loss: 0.07578089274466038
2024-11-05 00:48:13,857 - INFO - [diffusion][Epoch 8492] diffusion learning rate: 0.001
2024-11-05 00:48:13,859 - INFO - [diffusion][Epoch 8492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:13,860 - INFO - [diffusion][Epoch 8493] Epoch 8494/12000
2024-11-05 00:48:17,627 - INFO - [diffusion][Epoch 8493] diffusion training Loss: 0.0671169450506568
2024-11-05 00:48:17,629 - INFO - [diffusion][Epoch 8493] diffusion learning rate: 0.001
2024-11-05 00:48:17,631 - INFO - [diffusion][Epoch 8493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:17,632 - INFO - [diffusion][Epoch 8494] Epoch 8495/12000
2024-11-05 00:48:21,030 - INFO - [diffusion][Epoch 8494] diffusion training Loss: 0.071900125592947
2024-11-05 00:48:21,032 - INFO - [diffusion][Epoch 8494] diffusion learning rate: 0.001
2024-11-05 00:48:21,034 - INFO - [diffusion][Epoch 8494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:21,035 - INFO - [diffusion][Epoch 8495] Epoch 8496/12000
2024-11-05 00:48:24,233 - INFO - [diffusion][Epoch 8495] diffusion training Loss: 0.07019100338220596
2024-11-05 00:48:24,236 - INFO - [diffusion][Epoch 8495] diffusion learning rate: 0.001
2024-11-05 00:48:24,237 - INFO - [diffusion][Epoch 8495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:24,239 - INFO - [diffusion][Epoch 8496] Epoch 8497/12000
2024-11-05 00:48:27,459 - INFO - [diffusion][Epoch 8496] diffusion training Loss: 0.06718787085264921
2024-11-05 00:48:27,461 - INFO - [diffusion][Epoch 8496] diffusion learning rate: 0.001
2024-11-05 00:48:27,462 - INFO - [diffusion][Epoch 8496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:27,464 - INFO - [diffusion][Epoch 8497] Epoch 8498/12000
2024-11-05 00:48:30,617 - INFO - [diffusion][Epoch 8497] diffusion training Loss: 0.0699143884703517
2024-11-05 00:48:30,619 - INFO - [diffusion][Epoch 8497] diffusion learning rate: 0.001
2024-11-05 00:48:30,621 - INFO - [diffusion][Epoch 8497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:30,622 - INFO - [diffusion][Epoch 8498] Epoch 8499/12000
2024-11-05 00:48:34,340 - INFO - [diffusion][Epoch 8498] diffusion training Loss: 0.0703018456697464
2024-11-05 00:48:34,343 - INFO - [diffusion][Epoch 8498] diffusion learning rate: 0.001
2024-11-05 00:48:34,345 - INFO - [diffusion][Epoch 8498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:34,346 - INFO - [diffusion][Epoch 8499] Epoch 8500/12000
2024-11-05 00:48:37,884 - INFO - [diffusion][Epoch 8499] diffusion training Loss: 0.07540448196232319
2024-11-05 00:48:37,886 - INFO - [diffusion][Epoch 8499] diffusion learning rate: 0.001
2024-11-05 00:48:37,889 - INFO - [diffusion][Epoch 8499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:37,890 - INFO - [diffusion][Epoch 8500] Epoch 8501/12000
2024-11-05 00:48:40,999 - INFO - [diffusion][Epoch 8500] diffusion training Loss: 0.0722544975578785
2024-11-05 00:48:41,001 - INFO - [diffusion][Epoch 8500] diffusion learning rate: 0.001
2024-11-05 00:48:41,063 - INFO - [diffusion][Epoch 8500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:41,065 - INFO - [diffusion][Epoch 8501] Epoch 8502/12000
2024-11-05 00:48:44,180 - INFO - [diffusion][Epoch 8501] diffusion training Loss: 0.07038611173629761
2024-11-05 00:48:44,182 - INFO - [diffusion][Epoch 8501] diffusion learning rate: 0.001
2024-11-05 00:48:44,184 - INFO - [diffusion][Epoch 8501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:44,185 - INFO - [diffusion][Epoch 8502] Epoch 8503/12000
2024-11-05 00:48:47,316 - INFO - [diffusion][Epoch 8502] diffusion training Loss: 0.07519201375544071
2024-11-05 00:48:47,318 - INFO - [diffusion][Epoch 8502] diffusion learning rate: 0.001
2024-11-05 00:48:47,320 - INFO - [diffusion][Epoch 8502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:47,321 - INFO - [diffusion][Epoch 8503] Epoch 8504/12000
2024-11-05 00:48:50,848 - INFO - [diffusion][Epoch 8503] diffusion training Loss: 0.07038409635424614
2024-11-05 00:48:50,850 - INFO - [diffusion][Epoch 8503] diffusion learning rate: 0.001
2024-11-05 00:48:50,851 - INFO - [diffusion][Epoch 8503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:50,852 - INFO - [diffusion][Epoch 8504] Epoch 8505/12000
2024-11-05 00:48:54,511 - INFO - [diffusion][Epoch 8504] diffusion training Loss: 0.0736624076962471
2024-11-05 00:48:54,513 - INFO - [diffusion][Epoch 8504] diffusion learning rate: 0.001
2024-11-05 00:48:54,515 - INFO - [diffusion][Epoch 8504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:54,516 - INFO - [diffusion][Epoch 8505] Epoch 8506/12000
2024-11-05 00:48:57,742 - INFO - [diffusion][Epoch 8505] diffusion training Loss: 0.08151458576321602
2024-11-05 00:48:57,744 - INFO - [diffusion][Epoch 8505] diffusion learning rate: 0.001
2024-11-05 00:48:57,745 - INFO - [diffusion][Epoch 8505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:57,747 - INFO - [diffusion][Epoch 8506] Epoch 8507/12000
2024-11-05 00:49:00,892 - INFO - [diffusion][Epoch 8506] diffusion training Loss: 0.07121001742780209
2024-11-05 00:49:00,894 - INFO - [diffusion][Epoch 8506] diffusion learning rate: 0.001
2024-11-05 00:49:00,896 - INFO - [diffusion][Epoch 8506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:00,897 - INFO - [diffusion][Epoch 8507] Epoch 8508/12000
2024-11-05 00:49:04,058 - INFO - [diffusion][Epoch 8507] diffusion training Loss: 0.06833438202738762
2024-11-05 00:49:04,060 - INFO - [diffusion][Epoch 8507] diffusion learning rate: 0.001
2024-11-05 00:49:04,062 - INFO - [diffusion][Epoch 8507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:04,063 - INFO - [diffusion][Epoch 8508] Epoch 8509/12000
2024-11-05 00:49:07,584 - INFO - [diffusion][Epoch 8508] diffusion training Loss: 0.074845464900136
2024-11-05 00:49:07,586 - INFO - [diffusion][Epoch 8508] diffusion learning rate: 0.001
2024-11-05 00:49:07,587 - INFO - [diffusion][Epoch 8508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:07,589 - INFO - [diffusion][Epoch 8509] Epoch 8510/12000
2024-11-05 00:49:11,213 - INFO - [diffusion][Epoch 8509] diffusion training Loss: 0.07064865902066231
2024-11-05 00:49:11,214 - INFO - [diffusion][Epoch 8509] diffusion learning rate: 0.001
2024-11-05 00:49:11,216 - INFO - [diffusion][Epoch 8509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:11,217 - INFO - [diffusion][Epoch 8510] Epoch 8511/12000
2024-11-05 00:49:14,595 - INFO - [diffusion][Epoch 8510] diffusion training Loss: 0.07255111262202263
2024-11-05 00:49:14,597 - INFO - [diffusion][Epoch 8510] diffusion learning rate: 0.001
2024-11-05 00:49:14,599 - INFO - [diffusion][Epoch 8510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:14,600 - INFO - [diffusion][Epoch 8511] Epoch 8512/12000
2024-11-05 00:49:17,856 - INFO - [diffusion][Epoch 8511] diffusion training Loss: 0.07092630118131638
2024-11-05 00:49:17,859 - INFO - [diffusion][Epoch 8511] diffusion learning rate: 0.001
2024-11-05 00:49:17,861 - INFO - [diffusion][Epoch 8511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:17,862 - INFO - [diffusion][Epoch 8512] Epoch 8513/12000
2024-11-05 00:49:20,861 - INFO - [diffusion][Epoch 8512] diffusion training Loss: 0.0760035589337349
2024-11-05 00:49:20,863 - INFO - [diffusion][Epoch 8512] diffusion learning rate: 0.001
2024-11-05 00:49:20,864 - INFO - [diffusion][Epoch 8512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:20,866 - INFO - [diffusion][Epoch 8513] Epoch 8514/12000
2024-11-05 00:49:23,950 - INFO - [diffusion][Epoch 8513] diffusion training Loss: 0.07624430488795042
2024-11-05 00:49:23,953 - INFO - [diffusion][Epoch 8513] diffusion learning rate: 0.001
2024-11-05 00:49:23,957 - INFO - [diffusion][Epoch 8513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:23,959 - INFO - [diffusion][Epoch 8514] Epoch 8515/12000
2024-11-05 00:49:27,674 - INFO - [diffusion][Epoch 8514] diffusion training Loss: 0.06971392594277859
2024-11-05 00:49:27,676 - INFO - [diffusion][Epoch 8514] diffusion learning rate: 0.001
2024-11-05 00:49:27,678 - INFO - [diffusion][Epoch 8514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:27,679 - INFO - [diffusion][Epoch 8515] Epoch 8516/12000
2024-11-05 00:49:30,993 - INFO - [diffusion][Epoch 8515] diffusion training Loss: 0.06810971349477768
2024-11-05 00:49:30,995 - INFO - [diffusion][Epoch 8515] diffusion learning rate: 0.001
2024-11-05 00:49:30,996 - INFO - [diffusion][Epoch 8515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:30,998 - INFO - [diffusion][Epoch 8516] Epoch 8517/12000
2024-11-05 00:49:33,793 - INFO - [diffusion][Epoch 8516] diffusion training Loss: 0.07336491718888283
2024-11-05 00:49:33,795 - INFO - [diffusion][Epoch 8516] diffusion learning rate: 0.001
2024-11-05 00:49:33,797 - INFO - [diffusion][Epoch 8516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:33,798 - INFO - [diffusion][Epoch 8517] Epoch 8518/12000
2024-11-05 00:49:37,046 - INFO - [diffusion][Epoch 8517] diffusion training Loss: 0.06588517315685749
2024-11-05 00:49:37,048 - INFO - [diffusion][Epoch 8517] diffusion learning rate: 0.001
2024-11-05 00:49:37,049 - INFO - [diffusion][Epoch 8517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:37,051 - INFO - [diffusion][Epoch 8518] Epoch 8519/12000
2024-11-05 00:49:40,653 - INFO - [diffusion][Epoch 8518] diffusion training Loss: 0.07437011040747166
2024-11-05 00:49:40,655 - INFO - [diffusion][Epoch 8518] diffusion learning rate: 0.001
2024-11-05 00:49:40,656 - INFO - [diffusion][Epoch 8518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:40,658 - INFO - [diffusion][Epoch 8519] Epoch 8520/12000
2024-11-05 00:49:44,340 - INFO - [diffusion][Epoch 8519] diffusion training Loss: 0.0678136870265007
2024-11-05 00:49:44,342 - INFO - [diffusion][Epoch 8519] diffusion learning rate: 0.001
2024-11-05 00:49:44,343 - INFO - [diffusion][Epoch 8519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:44,344 - INFO - [diffusion][Epoch 8520] Epoch 8521/12000
2024-11-05 00:49:47,430 - INFO - [diffusion][Epoch 8520] diffusion training Loss: 0.07348533906042576
2024-11-05 00:49:47,432 - INFO - [diffusion][Epoch 8520] diffusion learning rate: 0.001
2024-11-05 00:49:47,434 - INFO - [diffusion][Epoch 8520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:47,435 - INFO - [diffusion][Epoch 8521] Epoch 8522/12000
2024-11-05 00:49:50,304 - INFO - [diffusion][Epoch 8521] diffusion training Loss: 0.07053693290799856
2024-11-05 00:49:50,307 - INFO - [diffusion][Epoch 8521] diffusion learning rate: 0.001
2024-11-05 00:49:50,309 - INFO - [diffusion][Epoch 8521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:50,311 - INFO - [diffusion][Epoch 8522] Epoch 8523/12000
2024-11-05 00:49:53,626 - INFO - [diffusion][Epoch 8522] diffusion training Loss: 0.07421571761369705
2024-11-05 00:49:53,628 - INFO - [diffusion][Epoch 8522] diffusion learning rate: 0.001
2024-11-05 00:49:53,629 - INFO - [diffusion][Epoch 8522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:53,631 - INFO - [diffusion][Epoch 8523] Epoch 8524/12000
2024-11-05 00:49:57,268 - INFO - [diffusion][Epoch 8523] diffusion training Loss: 0.07277792133390903
2024-11-05 00:49:57,270 - INFO - [diffusion][Epoch 8523] diffusion learning rate: 0.001
2024-11-05 00:49:57,272 - INFO - [diffusion][Epoch 8523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:57,273 - INFO - [diffusion][Epoch 8524] Epoch 8525/12000
2024-11-05 00:50:00,411 - INFO - [diffusion][Epoch 8524] diffusion training Loss: 0.0624837651848793
2024-11-05 00:50:00,414 - INFO - [diffusion][Epoch 8524] diffusion learning rate: 0.001
2024-11-05 00:50:00,415 - INFO - [diffusion][Epoch 8524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:00,417 - INFO - [diffusion][Epoch 8525] Epoch 8526/12000
2024-11-05 00:50:03,623 - INFO - [diffusion][Epoch 8525] diffusion training Loss: 0.07533550821244717
2024-11-05 00:50:03,625 - INFO - [diffusion][Epoch 8525] diffusion learning rate: 0.001
2024-11-05 00:50:03,627 - INFO - [diffusion][Epoch 8525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:03,629 - INFO - [diffusion][Epoch 8526] Epoch 8527/12000
2024-11-05 00:50:06,721 - INFO - [diffusion][Epoch 8526] diffusion training Loss: 0.07310999743640423
2024-11-05 00:50:06,723 - INFO - [diffusion][Epoch 8526] diffusion learning rate: 0.001
2024-11-05 00:50:06,724 - INFO - [diffusion][Epoch 8526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:06,726 - INFO - [diffusion][Epoch 8527] Epoch 8528/12000
2024-11-05 00:50:10,275 - INFO - [diffusion][Epoch 8527] diffusion training Loss: 0.0701714027673006
2024-11-05 00:50:10,277 - INFO - [diffusion][Epoch 8527] diffusion learning rate: 0.001
2024-11-05 00:50:10,279 - INFO - [diffusion][Epoch 8527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:10,280 - INFO - [diffusion][Epoch 8528] Epoch 8529/12000
2024-11-05 00:50:13,847 - INFO - [diffusion][Epoch 8528] diffusion training Loss: 0.07372242491692305
2024-11-05 00:50:13,862 - INFO - [diffusion][Epoch 8528] diffusion learning rate: 0.001
2024-11-05 00:50:13,864 - INFO - [diffusion][Epoch 8528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:13,865 - INFO - [diffusion][Epoch 8529] Epoch 8530/12000
2024-11-05 00:50:17,346 - INFO - [diffusion][Epoch 8529] diffusion training Loss: 0.06811411119997501
2024-11-05 00:50:17,348 - INFO - [diffusion][Epoch 8529] diffusion learning rate: 0.001
2024-11-05 00:50:17,350 - INFO - [diffusion][Epoch 8529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:17,351 - INFO - [diffusion][Epoch 8530] Epoch 8531/12000
2024-11-05 00:50:20,509 - INFO - [diffusion][Epoch 8530] diffusion training Loss: 0.06997463293373585
2024-11-05 00:50:20,511 - INFO - [diffusion][Epoch 8530] diffusion learning rate: 0.001
2024-11-05 00:50:20,513 - INFO - [diffusion][Epoch 8530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:20,514 - INFO - [diffusion][Epoch 8531] Epoch 8532/12000
2024-11-05 00:50:23,633 - INFO - [diffusion][Epoch 8531] diffusion training Loss: 0.07048793509602547
2024-11-05 00:50:23,635 - INFO - [diffusion][Epoch 8531] diffusion learning rate: 0.001
2024-11-05 00:50:23,637 - INFO - [diffusion][Epoch 8531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:23,638 - INFO - [diffusion][Epoch 8532] Epoch 8533/12000
2024-11-05 00:50:26,763 - INFO - [diffusion][Epoch 8532] diffusion training Loss: 0.06965318135917187
2024-11-05 00:50:26,765 - INFO - [diffusion][Epoch 8532] diffusion learning rate: 0.001
2024-11-05 00:50:26,767 - INFO - [diffusion][Epoch 8532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:26,768 - INFO - [diffusion][Epoch 8533] Epoch 8534/12000
2024-11-05 00:50:30,274 - INFO - [diffusion][Epoch 8533] diffusion training Loss: 0.07150558196008205
2024-11-05 00:50:30,276 - INFO - [diffusion][Epoch 8533] diffusion learning rate: 0.001
2024-11-05 00:50:30,279 - INFO - [diffusion][Epoch 8533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:30,280 - INFO - [diffusion][Epoch 8534] Epoch 8535/12000
2024-11-05 00:50:33,565 - INFO - [diffusion][Epoch 8534] diffusion training Loss: 0.07282464019954205
2024-11-05 00:50:33,567 - INFO - [diffusion][Epoch 8534] diffusion learning rate: 0.001
2024-11-05 00:50:33,569 - INFO - [diffusion][Epoch 8534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:33,570 - INFO - [diffusion][Epoch 8535] Epoch 8536/12000
2024-11-05 00:50:36,651 - INFO - [diffusion][Epoch 8535] diffusion training Loss: 0.0700626764446497
2024-11-05 00:50:36,653 - INFO - [diffusion][Epoch 8535] diffusion learning rate: 0.001
2024-11-05 00:50:36,655 - INFO - [diffusion][Epoch 8535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:36,656 - INFO - [diffusion][Epoch 8536] Epoch 8537/12000
2024-11-05 00:50:39,723 - INFO - [diffusion][Epoch 8536] diffusion training Loss: 0.06580561585724354
2024-11-05 00:50:39,725 - INFO - [diffusion][Epoch 8536] diffusion learning rate: 0.001
2024-11-05 00:50:39,727 - INFO - [diffusion][Epoch 8536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:39,728 - INFO - [diffusion][Epoch 8537] Epoch 8538/12000
2024-11-05 00:50:43,152 - INFO - [diffusion][Epoch 8537] diffusion training Loss: 0.06902385223656893
2024-11-05 00:50:43,154 - INFO - [diffusion][Epoch 8537] diffusion learning rate: 0.001
2024-11-05 00:50:43,156 - INFO - [diffusion][Epoch 8537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:43,157 - INFO - [diffusion][Epoch 8538] Epoch 8539/12000
2024-11-05 00:50:46,786 - INFO - [diffusion][Epoch 8538] diffusion training Loss: 0.07054907083511353
2024-11-05 00:50:46,788 - INFO - [diffusion][Epoch 8538] diffusion learning rate: 0.001
2024-11-05 00:50:46,790 - INFO - [diffusion][Epoch 8538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:46,792 - INFO - [diffusion][Epoch 8539] Epoch 8540/12000
2024-11-05 00:50:50,185 - INFO - [diffusion][Epoch 8539] diffusion training Loss: 0.07791729643940926
2024-11-05 00:50:50,187 - INFO - [diffusion][Epoch 8539] diffusion learning rate: 0.001
2024-11-05 00:50:50,188 - INFO - [diffusion][Epoch 8539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:50,190 - INFO - [diffusion][Epoch 8540] Epoch 8541/12000
2024-11-05 00:50:53,379 - INFO - [diffusion][Epoch 8540] diffusion training Loss: 0.07293030433356762
2024-11-05 00:50:53,381 - INFO - [diffusion][Epoch 8540] diffusion learning rate: 0.001
2024-11-05 00:50:53,383 - INFO - [diffusion][Epoch 8540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:53,386 - INFO - [diffusion][Epoch 8541] Epoch 8542/12000
2024-11-05 00:50:56,379 - INFO - [diffusion][Epoch 8541] diffusion training Loss: 0.0739909466356039
2024-11-05 00:50:56,381 - INFO - [diffusion][Epoch 8541] diffusion learning rate: 0.001
2024-11-05 00:50:56,383 - INFO - [diffusion][Epoch 8541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:56,385 - INFO - [diffusion][Epoch 8542] Epoch 8543/12000
2024-11-05 00:50:59,905 - INFO - [diffusion][Epoch 8542] diffusion training Loss: 0.06689854711294174
2024-11-05 00:50:59,907 - INFO - [diffusion][Epoch 8542] diffusion learning rate: 0.001
2024-11-05 00:50:59,909 - INFO - [diffusion][Epoch 8542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:59,910 - INFO - [diffusion][Epoch 8543] Epoch 8544/12000
2024-11-05 00:51:03,542 - INFO - [diffusion][Epoch 8543] diffusion training Loss: 0.07028809748589993
2024-11-05 00:51:03,543 - INFO - [diffusion][Epoch 8543] diffusion learning rate: 0.001
2024-11-05 00:51:03,545 - INFO - [diffusion][Epoch 8543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:03,546 - INFO - [diffusion][Epoch 8544] Epoch 8545/12000
2024-11-05 00:51:06,643 - INFO - [diffusion][Epoch 8544] diffusion training Loss: 0.0662002433091402
2024-11-05 00:51:06,645 - INFO - [diffusion][Epoch 8544] diffusion learning rate: 0.001
2024-11-05 00:51:06,647 - INFO - [diffusion][Epoch 8544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:06,648 - INFO - [diffusion][Epoch 8545] Epoch 8546/12000
2024-11-05 00:51:09,812 - INFO - [diffusion][Epoch 8545] diffusion training Loss: 0.0694904550909996
2024-11-05 00:51:09,813 - INFO - [diffusion][Epoch 8545] diffusion learning rate: 0.001
2024-11-05 00:51:09,815 - INFO - [diffusion][Epoch 8545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:09,816 - INFO - [diffusion][Epoch 8546] Epoch 8547/12000
2024-11-05 00:51:13,080 - INFO - [diffusion][Epoch 8546] diffusion training Loss: 0.0675730612128973
2024-11-05 00:51:13,082 - INFO - [diffusion][Epoch 8546] diffusion learning rate: 0.001
2024-11-05 00:51:13,084 - INFO - [diffusion][Epoch 8546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:13,085 - INFO - [diffusion][Epoch 8547] Epoch 8548/12000
2024-11-05 00:51:16,510 - INFO - [diffusion][Epoch 8547] diffusion training Loss: 0.0742432288825512
2024-11-05 00:51:16,512 - INFO - [diffusion][Epoch 8547] diffusion learning rate: 0.001
2024-11-05 00:51:16,514 - INFO - [diffusion][Epoch 8547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:16,516 - INFO - [diffusion][Epoch 8548] Epoch 8549/12000
2024-11-05 00:51:20,170 - INFO - [diffusion][Epoch 8548] diffusion training Loss: 0.06998572684824467
2024-11-05 00:51:20,192 - INFO - [diffusion][Epoch 8548] diffusion learning rate: 0.001
2024-11-05 00:51:20,193 - INFO - [diffusion][Epoch 8548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:20,195 - INFO - [diffusion][Epoch 8549] Epoch 8550/12000
2024-11-05 00:51:23,539 - INFO - [diffusion][Epoch 8549] diffusion training Loss: 0.06640562508255243
2024-11-05 00:51:23,541 - INFO - [diffusion][Epoch 8549] diffusion learning rate: 0.001
2024-11-05 00:51:23,543 - INFO - [diffusion][Epoch 8549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:23,544 - INFO - [diffusion][Epoch 8550] Epoch 8551/12000
2024-11-05 00:51:26,697 - INFO - [diffusion][Epoch 8550] diffusion training Loss: 0.07021609507501125
2024-11-05 00:51:26,699 - INFO - [diffusion][Epoch 8550] diffusion learning rate: 0.001
2024-11-05 00:51:26,701 - INFO - [diffusion][Epoch 8550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:26,702 - INFO - [diffusion][Epoch 8551] Epoch 8552/12000
2024-11-05 00:51:29,932 - INFO - [diffusion][Epoch 8551] diffusion training Loss: 0.07373940572142601
2024-11-05 00:51:29,935 - INFO - [diffusion][Epoch 8551] diffusion learning rate: 0.001
2024-11-05 00:51:29,936 - INFO - [diffusion][Epoch 8551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:29,937 - INFO - [diffusion][Epoch 8552] Epoch 8553/12000
2024-11-05 00:51:33,305 - INFO - [diffusion][Epoch 8552] diffusion training Loss: 0.06588161550462246
2024-11-05 00:51:33,307 - INFO - [diffusion][Epoch 8552] diffusion learning rate: 0.001
2024-11-05 00:51:33,309 - INFO - [diffusion][Epoch 8552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:33,310 - INFO - [diffusion][Epoch 8553] Epoch 8554/12000
2024-11-05 00:51:36,966 - INFO - [diffusion][Epoch 8553] diffusion training Loss: 0.06767937913537025
2024-11-05 00:51:36,969 - INFO - [diffusion][Epoch 8553] diffusion learning rate: 0.001
2024-11-05 00:51:36,970 - INFO - [diffusion][Epoch 8553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:36,972 - INFO - [diffusion][Epoch 8554] Epoch 8555/12000
2024-11-05 00:51:40,235 - INFO - [diffusion][Epoch 8554] diffusion training Loss: 0.06394448131322861
2024-11-05 00:51:40,237 - INFO - [diffusion][Epoch 8554] diffusion learning rate: 0.001
2024-11-05 00:51:40,238 - INFO - [diffusion][Epoch 8554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:40,240 - INFO - [diffusion][Epoch 8555] Epoch 8556/12000
2024-11-05 00:51:43,361 - INFO - [diffusion][Epoch 8555] diffusion training Loss: 0.06859623081982136
2024-11-05 00:51:43,362 - INFO - [diffusion][Epoch 8555] diffusion learning rate: 0.001
2024-11-05 00:51:43,364 - INFO - [diffusion][Epoch 8555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:43,366 - INFO - [diffusion][Epoch 8556] Epoch 8557/12000
2024-11-05 00:51:46,462 - INFO - [diffusion][Epoch 8556] diffusion training Loss: 0.06975739356130362
2024-11-05 00:51:46,464 - INFO - [diffusion][Epoch 8556] diffusion learning rate: 0.001
2024-11-05 00:51:46,465 - INFO - [diffusion][Epoch 8556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:46,466 - INFO - [diffusion][Epoch 8557] Epoch 8558/12000
2024-11-05 00:51:50,249 - INFO - [diffusion][Epoch 8557] diffusion training Loss: 0.07657731510698795
2024-11-05 00:51:50,251 - INFO - [diffusion][Epoch 8557] diffusion learning rate: 0.001
2024-11-05 00:51:50,253 - INFO - [diffusion][Epoch 8557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:50,254 - INFO - [diffusion][Epoch 8558] Epoch 8559/12000
2024-11-05 00:51:53,828 - INFO - [diffusion][Epoch 8558] diffusion training Loss: 0.06225316505879164
2024-11-05 00:51:53,830 - INFO - [diffusion][Epoch 8558] diffusion learning rate: 0.001
2024-11-05 00:51:53,832 - INFO - [diffusion][Epoch 8558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:53,833 - INFO - [diffusion][Epoch 8559] Epoch 8560/12000
2024-11-05 00:51:57,381 - INFO - [diffusion][Epoch 8559] diffusion training Loss: 0.07270443253219128
2024-11-05 00:51:57,383 - INFO - [diffusion][Epoch 8559] diffusion learning rate: 0.001
2024-11-05 00:51:57,385 - INFO - [diffusion][Epoch 8559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:57,389 - INFO - [diffusion][Epoch 8560] Epoch 8561/12000
2024-11-05 00:52:00,467 - INFO - [diffusion][Epoch 8560] diffusion training Loss: 0.06971613503992558
2024-11-05 00:52:00,469 - INFO - [diffusion][Epoch 8560] diffusion learning rate: 0.001
2024-11-05 00:52:00,471 - INFO - [diffusion][Epoch 8560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:00,472 - INFO - [diffusion][Epoch 8561] Epoch 8562/12000
2024-11-05 00:52:03,579 - INFO - [diffusion][Epoch 8561] diffusion training Loss: 0.06867462582886219
2024-11-05 00:52:03,581 - INFO - [diffusion][Epoch 8561] diffusion learning rate: 0.001
2024-11-05 00:52:03,582 - INFO - [diffusion][Epoch 8561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:03,584 - INFO - [diffusion][Epoch 8562] Epoch 8563/12000
2024-11-05 00:52:06,732 - INFO - [diffusion][Epoch 8562] diffusion training Loss: 0.07139089703559875
2024-11-05 00:52:06,734 - INFO - [diffusion][Epoch 8562] diffusion learning rate: 0.001
2024-11-05 00:52:06,736 - INFO - [diffusion][Epoch 8562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:06,737 - INFO - [diffusion][Epoch 8563] Epoch 8564/12000
2024-11-05 00:52:10,249 - INFO - [diffusion][Epoch 8563] diffusion training Loss: 0.06795929372310638
2024-11-05 00:52:10,251 - INFO - [diffusion][Epoch 8563] diffusion learning rate: 0.001
2024-11-05 00:52:10,253 - INFO - [diffusion][Epoch 8563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:10,254 - INFO - [diffusion][Epoch 8564] Epoch 8565/12000
2024-11-05 00:52:13,814 - INFO - [diffusion][Epoch 8564] diffusion training Loss: 0.07293634116649628
2024-11-05 00:52:13,816 - INFO - [diffusion][Epoch 8564] diffusion learning rate: 0.001
2024-11-05 00:52:13,818 - INFO - [diffusion][Epoch 8564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:13,819 - INFO - [diffusion][Epoch 8565] Epoch 8566/12000
2024-11-05 00:52:16,912 - INFO - [diffusion][Epoch 8565] diffusion training Loss: 0.06686975713819265
2024-11-05 00:52:16,913 - INFO - [diffusion][Epoch 8565] diffusion learning rate: 0.001
2024-11-05 00:52:16,915 - INFO - [diffusion][Epoch 8565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:16,916 - INFO - [diffusion][Epoch 8566] Epoch 8567/12000
2024-11-05 00:52:20,010 - INFO - [diffusion][Epoch 8566] diffusion training Loss: 0.07348843291401863
2024-11-05 00:52:20,011 - INFO - [diffusion][Epoch 8566] diffusion learning rate: 0.001
2024-11-05 00:52:20,013 - INFO - [diffusion][Epoch 8566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:20,015 - INFO - [diffusion][Epoch 8567] Epoch 8568/12000
2024-11-05 00:52:23,109 - INFO - [diffusion][Epoch 8567] diffusion training Loss: 0.0756426490843296
2024-11-05 00:52:23,111 - INFO - [diffusion][Epoch 8567] diffusion learning rate: 0.001
2024-11-05 00:52:23,113 - INFO - [diffusion][Epoch 8567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:23,114 - INFO - [diffusion][Epoch 8568] Epoch 8569/12000
2024-11-05 00:52:26,831 - INFO - [diffusion][Epoch 8568] diffusion training Loss: 0.07897910103201866
2024-11-05 00:52:26,833 - INFO - [diffusion][Epoch 8568] diffusion learning rate: 0.001
2024-11-05 00:52:26,835 - INFO - [diffusion][Epoch 8568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:26,837 - INFO - [diffusion][Epoch 8569] Epoch 8570/12000
2024-11-05 00:52:30,298 - INFO - [diffusion][Epoch 8569] diffusion training Loss: 0.07138106226921082
2024-11-05 00:52:30,300 - INFO - [diffusion][Epoch 8569] diffusion learning rate: 0.001
2024-11-05 00:52:30,302 - INFO - [diffusion][Epoch 8569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:30,303 - INFO - [diffusion][Epoch 8570] Epoch 8571/12000
2024-11-05 00:52:33,374 - INFO - [diffusion][Epoch 8570] diffusion training Loss: 0.07422534003853798
2024-11-05 00:52:33,375 - INFO - [diffusion][Epoch 8570] diffusion learning rate: 0.001
2024-11-05 00:52:33,377 - INFO - [diffusion][Epoch 8570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:33,378 - INFO - [diffusion][Epoch 8571] Epoch 8572/12000
2024-11-05 00:52:36,583 - INFO - [diffusion][Epoch 8571] diffusion training Loss: 0.06982918083667755
2024-11-05 00:52:36,585 - INFO - [diffusion][Epoch 8571] diffusion learning rate: 0.001
2024-11-05 00:52:36,587 - INFO - [diffusion][Epoch 8571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:36,588 - INFO - [diffusion][Epoch 8572] Epoch 8573/12000
2024-11-05 00:52:39,886 - INFO - [diffusion][Epoch 8572] diffusion training Loss: 0.07115873135626316
2024-11-05 00:52:39,888 - INFO - [diffusion][Epoch 8572] diffusion learning rate: 0.001
2024-11-05 00:52:39,889 - INFO - [diffusion][Epoch 8572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:39,891 - INFO - [diffusion][Epoch 8573] Epoch 8574/12000
2024-11-05 00:52:43,452 - INFO - [diffusion][Epoch 8573] diffusion training Loss: 0.06669498514384031
2024-11-05 00:52:43,454 - INFO - [diffusion][Epoch 8573] diffusion learning rate: 0.001
2024-11-05 00:52:43,456 - INFO - [diffusion][Epoch 8573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:43,457 - INFO - [diffusion][Epoch 8574] Epoch 8575/12000
2024-11-05 00:52:47,038 - INFO - [diffusion][Epoch 8574] diffusion training Loss: 0.0710955560207367
2024-11-05 00:52:47,040 - INFO - [diffusion][Epoch 8574] diffusion learning rate: 0.001
2024-11-05 00:52:47,041 - INFO - [diffusion][Epoch 8574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:47,043 - INFO - [diffusion][Epoch 8575] Epoch 8576/12000
2024-11-05 00:52:50,137 - INFO - [diffusion][Epoch 8575] diffusion training Loss: 0.072966443374753
2024-11-05 00:52:50,139 - INFO - [diffusion][Epoch 8575] diffusion learning rate: 0.001
2024-11-05 00:52:50,140 - INFO - [diffusion][Epoch 8575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:50,141 - INFO - [diffusion][Epoch 8576] Epoch 8577/12000
2024-11-05 00:52:53,240 - INFO - [diffusion][Epoch 8576] diffusion training Loss: 0.0656566470861435
2024-11-05 00:52:53,241 - INFO - [diffusion][Epoch 8576] diffusion learning rate: 0.001
2024-11-05 00:52:53,243 - INFO - [diffusion][Epoch 8576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:53,245 - INFO - [diffusion][Epoch 8577] Epoch 8578/12000
2024-11-05 00:52:57,121 - INFO - [diffusion][Epoch 8577] diffusion training Loss: 0.07165191322565079
2024-11-05 00:52:57,123 - INFO - [diffusion][Epoch 8577] diffusion learning rate: 0.001
2024-11-05 00:52:57,124 - INFO - [diffusion][Epoch 8577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:57,126 - INFO - [diffusion][Epoch 8578] Epoch 8579/12000
2024-11-05 00:53:00,371 - INFO - [diffusion][Epoch 8578] diffusion training Loss: 0.0697037111967802
2024-11-05 00:53:00,373 - INFO - [diffusion][Epoch 8578] diffusion learning rate: 0.001
2024-11-05 00:53:00,375 - INFO - [diffusion][Epoch 8578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:00,376 - INFO - [diffusion][Epoch 8579] Epoch 8580/12000
2024-11-05 00:53:04,064 - INFO - [diffusion][Epoch 8579] diffusion training Loss: 0.06963538005948067
2024-11-05 00:53:04,067 - INFO - [diffusion][Epoch 8579] diffusion learning rate: 0.001
2024-11-05 00:53:04,069 - INFO - [diffusion][Epoch 8579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:04,070 - INFO - [diffusion][Epoch 8580] Epoch 8581/12000
2024-11-05 00:53:07,532 - INFO - [diffusion][Epoch 8580] diffusion training Loss: 0.0720942821353674
2024-11-05 00:53:07,534 - INFO - [diffusion][Epoch 8580] diffusion learning rate: 0.001
2024-11-05 00:53:07,536 - INFO - [diffusion][Epoch 8580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:07,537 - INFO - [diffusion][Epoch 8581] Epoch 8582/12000
2024-11-05 00:53:10,577 - INFO - [diffusion][Epoch 8581] diffusion training Loss: 0.06866101548075676
2024-11-05 00:53:10,579 - INFO - [diffusion][Epoch 8581] diffusion learning rate: 0.001
2024-11-05 00:53:10,621 - INFO - [diffusion][Epoch 8581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:10,622 - INFO - [diffusion][Epoch 8582] Epoch 8583/12000
2024-11-05 00:53:13,672 - INFO - [diffusion][Epoch 8582] diffusion training Loss: 0.07291744090616703
2024-11-05 00:53:13,674 - INFO - [diffusion][Epoch 8582] diffusion learning rate: 0.001
2024-11-05 00:53:13,676 - INFO - [diffusion][Epoch 8582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:13,677 - INFO - [diffusion][Epoch 8583] Epoch 8584/12000
2024-11-05 00:53:17,188 - INFO - [diffusion][Epoch 8583] diffusion training Loss: 0.06618887186050415
2024-11-05 00:53:17,190 - INFO - [diffusion][Epoch 8583] diffusion learning rate: 0.001
2024-11-05 00:53:17,192 - INFO - [diffusion][Epoch 8583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:17,193 - INFO - [diffusion][Epoch 8584] Epoch 8585/12000
2024-11-05 00:53:20,960 - INFO - [diffusion][Epoch 8584] diffusion training Loss: 0.07511410489678383
2024-11-05 00:53:20,961 - INFO - [diffusion][Epoch 8584] diffusion learning rate: 0.001
2024-11-05 00:53:20,963 - INFO - [diffusion][Epoch 8584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:20,965 - INFO - [diffusion][Epoch 8585] Epoch 8586/12000
2024-11-05 00:53:24,469 - INFO - [diffusion][Epoch 8585] diffusion training Loss: 0.06833742931485176
2024-11-05 00:53:24,471 - INFO - [diffusion][Epoch 8585] diffusion learning rate: 0.001
2024-11-05 00:53:24,473 - INFO - [diffusion][Epoch 8585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:24,474 - INFO - [diffusion][Epoch 8586] Epoch 8587/12000
2024-11-05 00:53:27,548 - INFO - [diffusion][Epoch 8586] diffusion training Loss: 0.06352250929921865
2024-11-05 00:53:27,550 - INFO - [diffusion][Epoch 8586] diffusion learning rate: 0.001
2024-11-05 00:53:27,552 - INFO - [diffusion][Epoch 8586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:27,553 - INFO - [diffusion][Epoch 8587] Epoch 8588/12000
2024-11-05 00:53:30,650 - INFO - [diffusion][Epoch 8587] diffusion training Loss: 0.07361990585923195
2024-11-05 00:53:30,652 - INFO - [diffusion][Epoch 8587] diffusion learning rate: 0.001
2024-11-05 00:53:30,653 - INFO - [diffusion][Epoch 8587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:30,654 - INFO - [diffusion][Epoch 8588] Epoch 8589/12000
2024-11-05 00:53:33,982 - INFO - [diffusion][Epoch 8588] diffusion training Loss: 0.06771834753453732
2024-11-05 00:53:33,984 - INFO - [diffusion][Epoch 8588] diffusion learning rate: 0.001
2024-11-05 00:53:33,986 - INFO - [diffusion][Epoch 8588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:33,987 - INFO - [diffusion][Epoch 8589] Epoch 8590/12000
2024-11-05 00:53:37,665 - INFO - [diffusion][Epoch 8589] diffusion training Loss: 0.06893764063715935
2024-11-05 00:53:37,667 - INFO - [diffusion][Epoch 8589] diffusion learning rate: 0.001
2024-11-05 00:53:37,669 - INFO - [diffusion][Epoch 8589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:37,670 - INFO - [diffusion][Epoch 8590] Epoch 8591/12000
2024-11-05 00:53:41,086 - INFO - [diffusion][Epoch 8590] diffusion training Loss: 0.07119994796812534
2024-11-05 00:53:41,089 - INFO - [diffusion][Epoch 8590] diffusion learning rate: 0.001
2024-11-05 00:53:41,091 - INFO - [diffusion][Epoch 8590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:41,092 - INFO - [diffusion][Epoch 8591] Epoch 8592/12000
2024-11-05 00:53:44,201 - INFO - [diffusion][Epoch 8591] diffusion training Loss: 0.0746659878641367
2024-11-05 00:53:44,203 - INFO - [diffusion][Epoch 8591] diffusion learning rate: 0.001
2024-11-05 00:53:44,205 - INFO - [diffusion][Epoch 8591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:44,206 - INFO - [diffusion][Epoch 8592] Epoch 8593/12000
2024-11-05 00:53:47,037 - INFO - [diffusion][Epoch 8592] diffusion training Loss: 0.07092927955091
2024-11-05 00:53:47,039 - INFO - [diffusion][Epoch 8592] diffusion learning rate: 0.001
2024-11-05 00:53:47,041 - INFO - [diffusion][Epoch 8592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:47,042 - INFO - [diffusion][Epoch 8593] Epoch 8594/12000
2024-11-05 00:53:50,655 - INFO - [diffusion][Epoch 8593] diffusion training Loss: 0.07075139321386814
2024-11-05 00:53:50,657 - INFO - [diffusion][Epoch 8593] diffusion learning rate: 0.001
2024-11-05 00:53:50,659 - INFO - [diffusion][Epoch 8593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:50,660 - INFO - [diffusion][Epoch 8594] Epoch 8595/12000
2024-11-05 00:53:54,132 - INFO - [diffusion][Epoch 8594] diffusion training Loss: 0.06550873816013336
2024-11-05 00:53:54,134 - INFO - [diffusion][Epoch 8594] diffusion learning rate: 0.001
2024-11-05 00:53:54,136 - INFO - [diffusion][Epoch 8594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:54,137 - INFO - [diffusion][Epoch 8595] Epoch 8596/12000
2024-11-05 00:53:57,327 - INFO - [diffusion][Epoch 8595] diffusion training Loss: 0.06879109237343073
2024-11-05 00:53:57,328 - INFO - [diffusion][Epoch 8595] diffusion learning rate: 0.001
2024-11-05 00:53:57,330 - INFO - [diffusion][Epoch 8595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:57,331 - INFO - [diffusion][Epoch 8596] Epoch 8597/12000
2024-11-05 00:54:00,333 - INFO - [diffusion][Epoch 8596] diffusion training Loss: 0.07097919285297394
2024-11-05 00:54:00,334 - INFO - [diffusion][Epoch 8596] diffusion learning rate: 0.001
2024-11-05 00:54:00,336 - INFO - [diffusion][Epoch 8596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:00,337 - INFO - [diffusion][Epoch 8597] Epoch 8598/12000
2024-11-05 00:54:03,643 - INFO - [diffusion][Epoch 8597] diffusion training Loss: 0.06480812933295965
2024-11-05 00:54:03,645 - INFO - [diffusion][Epoch 8597] diffusion learning rate: 0.001
2024-11-05 00:54:03,647 - INFO - [diffusion][Epoch 8597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:03,648 - INFO - [diffusion][Epoch 8598] Epoch 8599/12000
2024-11-05 00:54:07,402 - INFO - [diffusion][Epoch 8598] diffusion training Loss: 0.07644755579531193
2024-11-05 00:54:07,406 - INFO - [diffusion][Epoch 8598] diffusion learning rate: 0.001
2024-11-05 00:54:07,440 - INFO - [diffusion][Epoch 8598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:07,441 - INFO - [diffusion][Epoch 8599] Epoch 8600/12000
2024-11-05 00:54:10,974 - INFO - [diffusion][Epoch 8599] diffusion training Loss: 0.06889297533780336
2024-11-05 00:54:10,977 - INFO - [diffusion][Epoch 8599] diffusion learning rate: 0.001
2024-11-05 00:54:10,979 - INFO - [diffusion][Epoch 8599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:10,980 - INFO - [diffusion][Epoch 8600] Epoch 8601/12000
2024-11-05 00:54:14,078 - INFO - [diffusion][Epoch 8600] diffusion training Loss: 0.07607006281614304
2024-11-05 00:54:14,080 - INFO - [diffusion][Epoch 8600] diffusion learning rate: 0.001
2024-11-05 00:54:14,082 - INFO - [diffusion][Epoch 8600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:14,084 - INFO - [diffusion][Epoch 8601] Epoch 8602/12000
2024-11-05 00:54:17,151 - INFO - [diffusion][Epoch 8601] diffusion training Loss: 0.07210548687726259
2024-11-05 00:54:17,153 - INFO - [diffusion][Epoch 8601] diffusion learning rate: 0.001
2024-11-05 00:54:17,154 - INFO - [diffusion][Epoch 8601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:17,156 - INFO - [diffusion][Epoch 8602] Epoch 8603/12000
2024-11-05 00:54:20,286 - INFO - [diffusion][Epoch 8602] diffusion training Loss: 0.07599492371082306
2024-11-05 00:54:20,288 - INFO - [diffusion][Epoch 8602] diffusion learning rate: 0.001
2024-11-05 00:54:20,290 - INFO - [diffusion][Epoch 8602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:20,291 - INFO - [diffusion][Epoch 8603] Epoch 8604/12000
2024-11-05 00:54:23,897 - INFO - [diffusion][Epoch 8603] diffusion training Loss: 0.06905541196465492
2024-11-05 00:54:23,899 - INFO - [diffusion][Epoch 8603] diffusion learning rate: 0.001
2024-11-05 00:54:23,934 - INFO - [diffusion][Epoch 8603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:23,935 - INFO - [diffusion][Epoch 8604] Epoch 8605/12000
2024-11-05 00:54:27,434 - INFO - [diffusion][Epoch 8604] diffusion training Loss: 0.07801091112196445
2024-11-05 00:54:27,436 - INFO - [diffusion][Epoch 8604] diffusion learning rate: 0.001
2024-11-05 00:54:27,438 - INFO - [diffusion][Epoch 8604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:27,439 - INFO - [diffusion][Epoch 8605] Epoch 8606/12000
2024-11-05 00:54:30,624 - INFO - [diffusion][Epoch 8605] diffusion training Loss: 0.06731687113642693
2024-11-05 00:54:30,626 - INFO - [diffusion][Epoch 8605] diffusion learning rate: 0.001
2024-11-05 00:54:30,627 - INFO - [diffusion][Epoch 8605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:30,628 - INFO - [diffusion][Epoch 8606] Epoch 8607/12000
2024-11-05 00:54:33,659 - INFO - [diffusion][Epoch 8606] diffusion training Loss: 0.07308002188801765
2024-11-05 00:54:33,661 - INFO - [diffusion][Epoch 8606] diffusion learning rate: 0.001
2024-11-05 00:54:33,662 - INFO - [diffusion][Epoch 8606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:33,664 - INFO - [diffusion][Epoch 8607] Epoch 8608/12000
2024-11-05 00:54:36,961 - INFO - [diffusion][Epoch 8607] diffusion training Loss: 0.07108051516115665
2024-11-05 00:54:36,963 - INFO - [diffusion][Epoch 8607] diffusion learning rate: 0.001
2024-11-05 00:54:36,965 - INFO - [diffusion][Epoch 8607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:36,966 - INFO - [diffusion][Epoch 8608] Epoch 8609/12000
2024-11-05 00:54:40,603 - INFO - [diffusion][Epoch 8608] diffusion training Loss: 0.06830001808702946
2024-11-05 00:54:40,605 - INFO - [diffusion][Epoch 8608] diffusion learning rate: 0.001
2024-11-05 00:54:40,607 - INFO - [diffusion][Epoch 8608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:40,608 - INFO - [diffusion][Epoch 8609] Epoch 8610/12000
2024-11-05 00:54:43,925 - INFO - [diffusion][Epoch 8609] diffusion training Loss: 0.08068432472646236
2024-11-05 00:54:43,927 - INFO - [diffusion][Epoch 8609] diffusion learning rate: 0.001
2024-11-05 00:54:43,929 - INFO - [diffusion][Epoch 8609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:43,930 - INFO - [diffusion][Epoch 8610] Epoch 8611/12000
2024-11-05 00:54:47,144 - INFO - [diffusion][Epoch 8610] diffusion training Loss: 0.06845071166753769
2024-11-05 00:54:47,146 - INFO - [diffusion][Epoch 8610] diffusion learning rate: 0.001
2024-11-05 00:54:47,147 - INFO - [diffusion][Epoch 8610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:47,149 - INFO - [diffusion][Epoch 8611] Epoch 8612/12000
2024-11-05 00:54:50,335 - INFO - [diffusion][Epoch 8611] diffusion training Loss: 0.0745574701577425
2024-11-05 00:54:50,338 - INFO - [diffusion][Epoch 8611] diffusion learning rate: 0.001
2024-11-05 00:54:50,340 - INFO - [diffusion][Epoch 8611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:50,341 - INFO - [diffusion][Epoch 8612] Epoch 8613/12000
2024-11-05 00:54:53,557 - INFO - [diffusion][Epoch 8612] diffusion training Loss: 0.07440532557666302
2024-11-05 00:54:53,559 - INFO - [diffusion][Epoch 8612] diffusion learning rate: 0.001
2024-11-05 00:54:53,561 - INFO - [diffusion][Epoch 8612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:53,563 - INFO - [diffusion][Epoch 8613] Epoch 8614/12000
2024-11-05 00:54:57,228 - INFO - [diffusion][Epoch 8613] diffusion training Loss: 0.0694303372874856
2024-11-05 00:54:57,230 - INFO - [diffusion][Epoch 8613] diffusion learning rate: 0.001
2024-11-05 00:54:57,232 - INFO - [diffusion][Epoch 8613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:57,233 - INFO - [diffusion][Epoch 8614] Epoch 8615/12000
2024-11-05 00:55:00,681 - INFO - [diffusion][Epoch 8614] diffusion training Loss: 0.07143992092460394
2024-11-05 00:55:00,683 - INFO - [diffusion][Epoch 8614] diffusion learning rate: 0.001
2024-11-05 00:55:00,685 - INFO - [diffusion][Epoch 8614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:00,686 - INFO - [diffusion][Epoch 8615] Epoch 8616/12000
2024-11-05 00:55:03,862 - INFO - [diffusion][Epoch 8615] diffusion training Loss: 0.0683344230055809
2024-11-05 00:55:03,864 - INFO - [diffusion][Epoch 8615] diffusion learning rate: 0.001
2024-11-05 00:55:03,866 - INFO - [diffusion][Epoch 8615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:03,867 - INFO - [diffusion][Epoch 8616] Epoch 8617/12000
2024-11-05 00:55:07,012 - INFO - [diffusion][Epoch 8616] diffusion training Loss: 0.07223737798631191
2024-11-05 00:55:07,015 - INFO - [diffusion][Epoch 8616] diffusion learning rate: 0.001
2024-11-05 00:55:07,016 - INFO - [diffusion][Epoch 8616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:07,018 - INFO - [diffusion][Epoch 8617] Epoch 8618/12000
2024-11-05 00:55:10,412 - INFO - [diffusion][Epoch 8617] diffusion training Loss: 0.06904935836791992
2024-11-05 00:55:10,415 - INFO - [diffusion][Epoch 8617] diffusion learning rate: 0.001
2024-11-05 00:55:10,417 - INFO - [diffusion][Epoch 8617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:10,418 - INFO - [diffusion][Epoch 8618] Epoch 8619/12000
2024-11-05 00:55:14,093 - INFO - [diffusion][Epoch 8618] diffusion training Loss: 0.07077713124454021
2024-11-05 00:55:14,095 - INFO - [diffusion][Epoch 8618] diffusion learning rate: 0.001
2024-11-05 00:55:14,097 - INFO - [diffusion][Epoch 8618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:14,099 - INFO - [diffusion][Epoch 8619] Epoch 8620/12000
2024-11-05 00:55:17,530 - INFO - [diffusion][Epoch 8619] diffusion training Loss: 0.07251361571252346
2024-11-05 00:55:17,532 - INFO - [diffusion][Epoch 8619] diffusion learning rate: 0.001
2024-11-05 00:55:17,534 - INFO - [diffusion][Epoch 8619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:17,535 - INFO - [diffusion][Epoch 8620] Epoch 8621/12000
2024-11-05 00:55:20,686 - INFO - [diffusion][Epoch 8620] diffusion training Loss: 0.0669865608215332
2024-11-05 00:55:20,688 - INFO - [diffusion][Epoch 8620] diffusion learning rate: 0.001
2024-11-05 00:55:20,689 - INFO - [diffusion][Epoch 8620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:20,690 - INFO - [diffusion][Epoch 8621] Epoch 8622/12000
2024-11-05 00:55:23,881 - INFO - [diffusion][Epoch 8621] diffusion training Loss: 0.06595725379884243
2024-11-05 00:55:23,883 - INFO - [diffusion][Epoch 8621] diffusion learning rate: 0.001
2024-11-05 00:55:23,884 - INFO - [diffusion][Epoch 8621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:23,886 - INFO - [diffusion][Epoch 8622] Epoch 8623/12000
2024-11-05 00:55:27,140 - INFO - [diffusion][Epoch 8622] diffusion training Loss: 0.06516936514526606
2024-11-05 00:55:27,142 - INFO - [diffusion][Epoch 8622] diffusion learning rate: 0.001
2024-11-05 00:55:27,143 - INFO - [diffusion][Epoch 8622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:27,145 - INFO - [diffusion][Epoch 8623] Epoch 8624/12000
2024-11-05 00:55:30,562 - INFO - [diffusion][Epoch 8623] diffusion training Loss: 0.06540844403207302
2024-11-05 00:55:30,565 - INFO - [diffusion][Epoch 8623] diffusion learning rate: 0.001
2024-11-05 00:55:30,566 - INFO - [diffusion][Epoch 8623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:30,567 - INFO - [diffusion][Epoch 8624] Epoch 8625/12000
2024-11-05 00:55:34,102 - INFO - [diffusion][Epoch 8624] diffusion training Loss: 0.06621958501636982
2024-11-05 00:55:34,113 - INFO - [diffusion][Epoch 8624] diffusion learning rate: 0.001
2024-11-05 00:55:34,114 - INFO - [diffusion][Epoch 8624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:34,116 - INFO - [diffusion][Epoch 8625] Epoch 8626/12000
2024-11-05 00:55:37,256 - INFO - [diffusion][Epoch 8625] diffusion training Loss: 0.06812233570963144
2024-11-05 00:55:37,258 - INFO - [diffusion][Epoch 8625] diffusion learning rate: 0.001
2024-11-05 00:55:37,260 - INFO - [diffusion][Epoch 8625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:37,261 - INFO - [diffusion][Epoch 8626] Epoch 8627/12000
2024-11-05 00:55:40,399 - INFO - [diffusion][Epoch 8626] diffusion training Loss: 0.07149461098015308
2024-11-05 00:55:40,401 - INFO - [diffusion][Epoch 8626] diffusion learning rate: 0.001
2024-11-05 00:55:40,403 - INFO - [diffusion][Epoch 8626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:40,404 - INFO - [diffusion][Epoch 8627] Epoch 8628/12000
2024-11-05 00:55:43,471 - INFO - [diffusion][Epoch 8627] diffusion training Loss: 0.06890886649489403
2024-11-05 00:55:43,473 - INFO - [diffusion][Epoch 8627] diffusion learning rate: 0.001
2024-11-05 00:55:43,474 - INFO - [diffusion][Epoch 8627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:43,476 - INFO - [diffusion][Epoch 8628] Epoch 8629/12000
2024-11-05 00:55:46,975 - INFO - [diffusion][Epoch 8628] diffusion training Loss: 0.06623672228306532
2024-11-05 00:55:46,977 - INFO - [diffusion][Epoch 8628] diffusion learning rate: 0.001
2024-11-05 00:55:46,978 - INFO - [diffusion][Epoch 8628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:46,979 - INFO - [diffusion][Epoch 8629] Epoch 8630/12000
2024-11-05 00:55:50,730 - INFO - [diffusion][Epoch 8629] diffusion training Loss: 0.0683060847222805
2024-11-05 00:55:50,732 - INFO - [diffusion][Epoch 8629] diffusion learning rate: 0.001
2024-11-05 00:55:50,734 - INFO - [diffusion][Epoch 8629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:50,735 - INFO - [diffusion][Epoch 8630] Epoch 8631/12000
2024-11-05 00:55:54,061 - INFO - [diffusion][Epoch 8630] diffusion training Loss: 0.06933963671326637
2024-11-05 00:55:54,063 - INFO - [diffusion][Epoch 8630] diffusion learning rate: 0.001
2024-11-05 00:55:54,065 - INFO - [diffusion][Epoch 8630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:54,066 - INFO - [diffusion][Epoch 8631] Epoch 8632/12000
2024-11-05 00:55:57,204 - INFO - [diffusion][Epoch 8631] diffusion training Loss: 0.0629902882501483
2024-11-05 00:55:57,206 - INFO - [diffusion][Epoch 8631] diffusion learning rate: 0.001
2024-11-05 00:55:57,208 - INFO - [diffusion][Epoch 8631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:57,209 - INFO - [diffusion][Epoch 8632] Epoch 8633/12000
2024-11-05 00:56:00,369 - INFO - [diffusion][Epoch 8632] diffusion training Loss: 0.06803745217621326
2024-11-05 00:56:00,370 - INFO - [diffusion][Epoch 8632] diffusion learning rate: 0.001
2024-11-05 00:56:00,372 - INFO - [diffusion][Epoch 8632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:00,373 - INFO - [diffusion][Epoch 8633] Epoch 8634/12000
2024-11-05 00:56:03,755 - INFO - [diffusion][Epoch 8633] diffusion training Loss: 0.06890209577977657
2024-11-05 00:56:03,757 - INFO - [diffusion][Epoch 8633] diffusion learning rate: 0.001
2024-11-05 00:56:03,759 - INFO - [diffusion][Epoch 8633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:03,760 - INFO - [diffusion][Epoch 8634] Epoch 8635/12000
2024-11-05 00:56:07,412 - INFO - [diffusion][Epoch 8634] diffusion training Loss: 0.0726219154894352
2024-11-05 00:56:07,414 - INFO - [diffusion][Epoch 8634] diffusion learning rate: 0.001
2024-11-05 00:56:07,415 - INFO - [diffusion][Epoch 8634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:07,416 - INFO - [diffusion][Epoch 8635] Epoch 8636/12000
2024-11-05 00:56:10,670 - INFO - [diffusion][Epoch 8635] diffusion training Loss: 0.06663307547569275
2024-11-05 00:56:10,672 - INFO - [diffusion][Epoch 8635] diffusion learning rate: 0.001
2024-11-05 00:56:10,674 - INFO - [diffusion][Epoch 8635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:10,675 - INFO - [diffusion][Epoch 8636] Epoch 8637/12000
2024-11-05 00:56:13,811 - INFO - [diffusion][Epoch 8636] diffusion training Loss: 0.06985638663172722
2024-11-05 00:56:13,813 - INFO - [diffusion][Epoch 8636] diffusion learning rate: 0.001
2024-11-05 00:56:13,815 - INFO - [diffusion][Epoch 8636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:13,816 - INFO - [diffusion][Epoch 8637] Epoch 8638/12000
2024-11-05 00:56:16,957 - INFO - [diffusion][Epoch 8637] diffusion training Loss: 0.06645397190004587
2024-11-05 00:56:16,959 - INFO - [diffusion][Epoch 8637] diffusion learning rate: 0.001
2024-11-05 00:56:16,961 - INFO - [diffusion][Epoch 8637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:16,962 - INFO - [diffusion][Epoch 8638] Epoch 8639/12000
2024-11-05 00:56:20,467 - INFO - [diffusion][Epoch 8638] diffusion training Loss: 0.06723612360656261
2024-11-05 00:56:20,469 - INFO - [diffusion][Epoch 8638] diffusion learning rate: 0.001
2024-11-05 00:56:20,470 - INFO - [diffusion][Epoch 8638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:20,472 - INFO - [diffusion][Epoch 8639] Epoch 8640/12000
2024-11-05 00:56:24,038 - INFO - [diffusion][Epoch 8639] diffusion training Loss: 0.07860066741704941
2024-11-05 00:56:24,040 - INFO - [diffusion][Epoch 8639] diffusion learning rate: 0.001
2024-11-05 00:56:24,042 - INFO - [diffusion][Epoch 8639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:24,043 - INFO - [diffusion][Epoch 8640] Epoch 8641/12000
2024-11-05 00:56:27,192 - INFO - [diffusion][Epoch 8640] diffusion training Loss: 0.07287392020225525
2024-11-05 00:56:27,198 - INFO - [diffusion][Epoch 8640] diffusion learning rate: 0.001
2024-11-05 00:56:27,200 - INFO - [diffusion][Epoch 8640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:27,202 - INFO - [diffusion][Epoch 8641] Epoch 8642/12000
2024-11-05 00:56:30,981 - INFO - [diffusion][Epoch 8641] diffusion training Loss: 0.07066481374204159
2024-11-05 00:56:30,983 - INFO - [diffusion][Epoch 8641] diffusion learning rate: 0.001
2024-11-05 00:56:30,985 - INFO - [diffusion][Epoch 8641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:30,987 - INFO - [diffusion][Epoch 8642] Epoch 8643/12000
2024-11-05 00:56:34,128 - INFO - [diffusion][Epoch 8642] diffusion training Loss: 0.07029790058732033
2024-11-05 00:56:34,130 - INFO - [diffusion][Epoch 8642] diffusion learning rate: 0.001
2024-11-05 00:56:34,132 - INFO - [diffusion][Epoch 8642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:34,133 - INFO - [diffusion][Epoch 8643] Epoch 8644/12000
2024-11-05 00:56:37,353 - INFO - [diffusion][Epoch 8643] diffusion training Loss: 0.07105863094329834
2024-11-05 00:56:37,355 - INFO - [diffusion][Epoch 8643] diffusion learning rate: 0.001
2024-11-05 00:56:37,357 - INFO - [diffusion][Epoch 8643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:37,358 - INFO - [diffusion][Epoch 8644] Epoch 8645/12000
2024-11-05 00:56:40,988 - INFO - [diffusion][Epoch 8644] diffusion training Loss: 0.07037943787872791
2024-11-05 00:56:40,990 - INFO - [diffusion][Epoch 8644] diffusion learning rate: 0.001
2024-11-05 00:56:40,992 - INFO - [diffusion][Epoch 8644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:40,993 - INFO - [diffusion][Epoch 8645] Epoch 8646/12000
2024-11-05 00:56:44,481 - INFO - [diffusion][Epoch 8645] diffusion training Loss: 0.07064573094248772
2024-11-05 00:56:44,483 - INFO - [diffusion][Epoch 8645] diffusion learning rate: 0.001
2024-11-05 00:56:44,485 - INFO - [diffusion][Epoch 8645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:44,487 - INFO - [diffusion][Epoch 8646] Epoch 8647/12000
2024-11-05 00:56:47,629 - INFO - [diffusion][Epoch 8646] diffusion training Loss: 0.06612127088010311
2024-11-05 00:56:47,631 - INFO - [diffusion][Epoch 8646] diffusion learning rate: 0.001
2024-11-05 00:56:47,633 - INFO - [diffusion][Epoch 8646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:47,634 - INFO - [diffusion][Epoch 8647] Epoch 8648/12000
2024-11-05 00:56:50,764 - INFO - [diffusion][Epoch 8647] diffusion training Loss: 0.06967799924314022
2024-11-05 00:56:50,766 - INFO - [diffusion][Epoch 8647] diffusion learning rate: 0.001
2024-11-05 00:56:50,768 - INFO - [diffusion][Epoch 8647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:50,769 - INFO - [diffusion][Epoch 8648] Epoch 8649/12000
2024-11-05 00:56:53,996 - INFO - [diffusion][Epoch 8648] diffusion training Loss: 0.06990247964859009
2024-11-05 00:56:53,998 - INFO - [diffusion][Epoch 8648] diffusion learning rate: 0.001
2024-11-05 00:56:54,000 - INFO - [diffusion][Epoch 8648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:54,001 - INFO - [diffusion][Epoch 8649] Epoch 8650/12000
2024-11-05 00:56:57,654 - INFO - [diffusion][Epoch 8649] diffusion training Loss: 0.07102437317371368
2024-11-05 00:56:57,656 - INFO - [diffusion][Epoch 8649] diffusion learning rate: 0.001
2024-11-05 00:56:57,658 - INFO - [diffusion][Epoch 8649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:57,659 - INFO - [diffusion][Epoch 8650] Epoch 8651/12000
2024-11-05 00:57:01,156 - INFO - [diffusion][Epoch 8650] diffusion training Loss: 0.06974432989954948
2024-11-05 00:57:01,158 - INFO - [diffusion][Epoch 8650] diffusion learning rate: 0.001
2024-11-05 00:57:01,160 - INFO - [diffusion][Epoch 8650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:01,161 - INFO - [diffusion][Epoch 8651] Epoch 8652/12000
2024-11-05 00:57:04,260 - INFO - [diffusion][Epoch 8651] diffusion training Loss: 0.07072813808917999
2024-11-05 00:57:04,261 - INFO - [diffusion][Epoch 8651] diffusion learning rate: 0.001
2024-11-05 00:57:04,263 - INFO - [diffusion][Epoch 8651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:04,264 - INFO - [diffusion][Epoch 8652] Epoch 8653/12000
2024-11-05 00:57:07,348 - INFO - [diffusion][Epoch 8652] diffusion training Loss: 0.06775284558534622
2024-11-05 00:57:07,349 - INFO - [diffusion][Epoch 8652] diffusion learning rate: 0.001
2024-11-05 00:57:07,351 - INFO - [diffusion][Epoch 8652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:07,353 - INFO - [diffusion][Epoch 8653] Epoch 8654/12000
2024-11-05 00:57:10,470 - INFO - [diffusion][Epoch 8653] diffusion training Loss: 0.06848592683672905
2024-11-05 00:57:10,472 - INFO - [diffusion][Epoch 8653] diffusion learning rate: 0.001
2024-11-05 00:57:10,474 - INFO - [diffusion][Epoch 8653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:10,476 - INFO - [diffusion][Epoch 8654] Epoch 8655/12000
2024-11-05 00:57:14,074 - INFO - [diffusion][Epoch 8654] diffusion training Loss: 0.0676119364798069
2024-11-05 00:57:14,076 - INFO - [diffusion][Epoch 8654] diffusion learning rate: 0.001
2024-11-05 00:57:14,078 - INFO - [diffusion][Epoch 8654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:14,079 - INFO - [diffusion][Epoch 8655] Epoch 8656/12000
2024-11-05 00:57:17,524 - INFO - [diffusion][Epoch 8655] diffusion training Loss: 0.069683987647295
2024-11-05 00:57:17,526 - INFO - [diffusion][Epoch 8655] diffusion learning rate: 0.001
2024-11-05 00:57:17,546 - INFO - [diffusion][Epoch 8655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:17,548 - INFO - [diffusion][Epoch 8656] Epoch 8657/12000
2024-11-05 00:57:20,650 - INFO - [diffusion][Epoch 8656] diffusion training Loss: 0.06924453843384981
2024-11-05 00:57:20,651 - INFO - [diffusion][Epoch 8656] diffusion learning rate: 0.001
2024-11-05 00:57:20,653 - INFO - [diffusion][Epoch 8656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:20,654 - INFO - [diffusion][Epoch 8657] Epoch 8658/12000
2024-11-05 00:57:23,724 - INFO - [diffusion][Epoch 8657] diffusion training Loss: 0.06602403149008751
2024-11-05 00:57:23,726 - INFO - [diffusion][Epoch 8657] diffusion learning rate: 0.001
2024-11-05 00:57:23,728 - INFO - [diffusion][Epoch 8657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:23,729 - INFO - [diffusion][Epoch 8658] Epoch 8659/12000
2024-11-05 00:57:27,181 - INFO - [diffusion][Epoch 8658] diffusion training Loss: 0.06885453872382641
2024-11-05 00:57:27,183 - INFO - [diffusion][Epoch 8658] diffusion learning rate: 0.001
2024-11-05 00:57:27,184 - INFO - [diffusion][Epoch 8658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:27,187 - INFO - [diffusion][Epoch 8659] Epoch 8660/12000
2024-11-05 00:57:30,907 - INFO - [diffusion][Epoch 8659] diffusion training Loss: 0.06702552363276482
2024-11-05 00:57:30,909 - INFO - [diffusion][Epoch 8659] diffusion learning rate: 0.001
2024-11-05 00:57:30,911 - INFO - [diffusion][Epoch 8659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:30,913 - INFO - [diffusion][Epoch 8660] Epoch 8661/12000
2024-11-05 00:57:33,992 - INFO - [diffusion][Epoch 8660] diffusion training Loss: 0.0781499594449997
2024-11-05 00:57:33,994 - INFO - [diffusion][Epoch 8660] diffusion learning rate: 0.001
2024-11-05 00:57:34,014 - INFO - [diffusion][Epoch 8660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:34,016 - INFO - [diffusion][Epoch 8661] Epoch 8662/12000
2024-11-05 00:57:37,382 - INFO - [diffusion][Epoch 8661] diffusion training Loss: 0.06651575211435556
2024-11-05 00:57:37,384 - INFO - [diffusion][Epoch 8661] diffusion learning rate: 0.001
2024-11-05 00:57:37,386 - INFO - [diffusion][Epoch 8661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:37,387 - INFO - [diffusion][Epoch 8662] Epoch 8663/12000
2024-11-05 00:57:40,371 - INFO - [diffusion][Epoch 8662] diffusion training Loss: 0.07249936833977699
2024-11-05 00:57:40,374 - INFO - [diffusion][Epoch 8662] diffusion learning rate: 0.001
2024-11-05 00:57:40,377 - INFO - [diffusion][Epoch 8662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:40,378 - INFO - [diffusion][Epoch 8663] Epoch 8664/12000
2024-11-05 00:57:43,782 - INFO - [diffusion][Epoch 8663] diffusion training Loss: 0.06818614155054092
2024-11-05 00:57:43,784 - INFO - [diffusion][Epoch 8663] diffusion learning rate: 0.001
2024-11-05 00:57:43,786 - INFO - [diffusion][Epoch 8663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:43,787 - INFO - [diffusion][Epoch 8664] Epoch 8665/12000
2024-11-05 00:57:47,348 - INFO - [diffusion][Epoch 8664] diffusion training Loss: 0.07395201921463013
2024-11-05 00:57:47,350 - INFO - [diffusion][Epoch 8664] diffusion learning rate: 0.001
2024-11-05 00:57:47,352 - INFO - [diffusion][Epoch 8664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:47,353 - INFO - [diffusion][Epoch 8665] Epoch 8666/12000
2024-11-05 00:57:50,450 - INFO - [diffusion][Epoch 8665] diffusion training Loss: 0.06446048151701689
2024-11-05 00:57:50,452 - INFO - [diffusion][Epoch 8665] diffusion learning rate: 0.001
2024-11-05 00:57:50,503 - INFO - [diffusion][Epoch 8665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:50,505 - INFO - [diffusion][Epoch 8666] Epoch 8667/12000
2024-11-05 00:57:53,642 - INFO - [diffusion][Epoch 8666] diffusion training Loss: 0.0775358434766531
2024-11-05 00:57:53,644 - INFO - [diffusion][Epoch 8666] diffusion learning rate: 0.001
2024-11-05 00:57:53,645 - INFO - [diffusion][Epoch 8666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:53,647 - INFO - [diffusion][Epoch 8667] Epoch 8668/12000
2024-11-05 00:57:56,775 - INFO - [diffusion][Epoch 8667] diffusion training Loss: 0.06706229597330093
2024-11-05 00:57:56,777 - INFO - [diffusion][Epoch 8667] diffusion learning rate: 0.001
2024-11-05 00:57:56,779 - INFO - [diffusion][Epoch 8667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:56,780 - INFO - [diffusion][Epoch 8668] Epoch 8669/12000
2024-11-05 00:58:00,477 - INFO - [diffusion][Epoch 8668] diffusion training Loss: 0.06966822780668736
2024-11-05 00:58:00,479 - INFO - [diffusion][Epoch 8668] diffusion learning rate: 0.001
2024-11-05 00:58:00,481 - INFO - [diffusion][Epoch 8668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:00,482 - INFO - [diffusion][Epoch 8669] Epoch 8670/12000
2024-11-05 00:58:03,977 - INFO - [diffusion][Epoch 8669] diffusion training Loss: 0.07125064171850681
2024-11-05 00:58:03,979 - INFO - [diffusion][Epoch 8669] diffusion learning rate: 0.001
2024-11-05 00:58:03,981 - INFO - [diffusion][Epoch 8669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:03,982 - INFO - [diffusion][Epoch 8670] Epoch 8671/12000
2024-11-05 00:58:07,069 - INFO - [diffusion][Epoch 8670] diffusion training Loss: 0.06689101178199053
2024-11-05 00:58:07,071 - INFO - [diffusion][Epoch 8670] diffusion learning rate: 0.001
2024-11-05 00:58:07,073 - INFO - [diffusion][Epoch 8670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:07,074 - INFO - [diffusion][Epoch 8671] Epoch 8672/12000
2024-11-05 00:58:10,176 - INFO - [diffusion][Epoch 8671] diffusion training Loss: 0.06997622456401587
2024-11-05 00:58:10,178 - INFO - [diffusion][Epoch 8671] diffusion learning rate: 0.001
2024-11-05 00:58:10,181 - INFO - [diffusion][Epoch 8671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:10,182 - INFO - [diffusion][Epoch 8672] Epoch 8673/12000
2024-11-05 00:58:13,239 - INFO - [diffusion][Epoch 8672] diffusion training Loss: 0.07128035835921764
2024-11-05 00:58:13,241 - INFO - [diffusion][Epoch 8672] diffusion learning rate: 0.001
2024-11-05 00:58:13,243 - INFO - [diffusion][Epoch 8672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:13,244 - INFO - [diffusion][Epoch 8673] Epoch 8674/12000
2024-11-05 00:58:16,761 - INFO - [diffusion][Epoch 8673] diffusion training Loss: 0.0701081994920969
2024-11-05 00:58:16,763 - INFO - [diffusion][Epoch 8673] diffusion learning rate: 0.001
2024-11-05 00:58:16,765 - INFO - [diffusion][Epoch 8673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:16,766 - INFO - [diffusion][Epoch 8674] Epoch 8675/12000
2024-11-05 00:58:20,375 - INFO - [diffusion][Epoch 8674] diffusion training Loss: 0.06688342615962029
2024-11-05 00:58:20,377 - INFO - [diffusion][Epoch 8674] diffusion learning rate: 0.001
2024-11-05 00:58:20,379 - INFO - [diffusion][Epoch 8674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:20,380 - INFO - [diffusion][Epoch 8675] Epoch 8676/12000
2024-11-05 00:58:23,577 - INFO - [diffusion][Epoch 8675] diffusion training Loss: 0.07476116716861725
2024-11-05 00:58:23,580 - INFO - [diffusion][Epoch 8675] diffusion learning rate: 0.001
2024-11-05 00:58:23,581 - INFO - [diffusion][Epoch 8675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:23,583 - INFO - [diffusion][Epoch 8676] Epoch 8677/12000
2024-11-05 00:58:26,891 - INFO - [diffusion][Epoch 8676] diffusion training Loss: 0.07071176543831825
2024-11-05 00:58:26,893 - INFO - [diffusion][Epoch 8676] diffusion learning rate: 0.001
2024-11-05 00:58:26,895 - INFO - [diffusion][Epoch 8676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:26,896 - INFO - [diffusion][Epoch 8677] Epoch 8678/12000
2024-11-05 00:58:30,140 - INFO - [diffusion][Epoch 8677] diffusion training Loss: 0.07411203160881996
2024-11-05 00:58:30,142 - INFO - [diffusion][Epoch 8677] diffusion learning rate: 0.001
2024-11-05 00:58:30,143 - INFO - [diffusion][Epoch 8677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:30,144 - INFO - [diffusion][Epoch 8678] Epoch 8679/12000
2024-11-05 00:58:33,390 - INFO - [diffusion][Epoch 8678] diffusion training Loss: 0.06969498097896576
2024-11-05 00:58:33,392 - INFO - [diffusion][Epoch 8678] diffusion learning rate: 0.001
2024-11-05 00:58:33,394 - INFO - [diffusion][Epoch 8678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:33,395 - INFO - [diffusion][Epoch 8679] Epoch 8680/12000
2024-11-05 00:58:37,107 - INFO - [diffusion][Epoch 8679] diffusion training Loss: 0.07456808537244797
2024-11-05 00:58:37,109 - INFO - [diffusion][Epoch 8679] diffusion learning rate: 0.001
2024-11-05 00:58:37,111 - INFO - [diffusion][Epoch 8679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:37,112 - INFO - [diffusion][Epoch 8680] Epoch 8681/12000
2024-11-05 00:58:40,599 - INFO - [diffusion][Epoch 8680] diffusion training Loss: 0.07751370780169964
2024-11-05 00:58:40,601 - INFO - [diffusion][Epoch 8680] diffusion learning rate: 0.001
2024-11-05 00:58:40,603 - INFO - [diffusion][Epoch 8680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:40,604 - INFO - [diffusion][Epoch 8681] Epoch 8682/12000
2024-11-05 00:58:43,938 - INFO - [diffusion][Epoch 8681] diffusion training Loss: 0.06781512126326561
2024-11-05 00:58:43,940 - INFO - [diffusion][Epoch 8681] diffusion learning rate: 0.001
2024-11-05 00:58:43,988 - INFO - [diffusion][Epoch 8681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:43,990 - INFO - [diffusion][Epoch 8682] Epoch 8683/12000
2024-11-05 00:58:47,049 - INFO - [diffusion][Epoch 8682] diffusion training Loss: 0.07344718836247921
2024-11-05 00:58:47,051 - INFO - [diffusion][Epoch 8682] diffusion learning rate: 0.001
2024-11-05 00:58:47,052 - INFO - [diffusion][Epoch 8682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:47,053 - INFO - [diffusion][Epoch 8683] Epoch 8684/12000
2024-11-05 00:58:50,085 - INFO - [diffusion][Epoch 8683] diffusion training Loss: 0.07005793135613203
2024-11-05 00:58:50,087 - INFO - [diffusion][Epoch 8683] diffusion learning rate: 0.001
2024-11-05 00:58:50,090 - INFO - [diffusion][Epoch 8683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:50,091 - INFO - [diffusion][Epoch 8684] Epoch 8685/12000
2024-11-05 00:58:53,626 - INFO - [diffusion][Epoch 8684] diffusion training Loss: 0.07117778807878494
2024-11-05 00:58:53,629 - INFO - [diffusion][Epoch 8684] diffusion learning rate: 0.001
2024-11-05 00:58:53,631 - INFO - [diffusion][Epoch 8684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:53,632 - INFO - [diffusion][Epoch 8685] Epoch 8686/12000
2024-11-05 00:58:57,093 - INFO - [diffusion][Epoch 8685] diffusion training Loss: 0.07268870901316404
2024-11-05 00:58:57,095 - INFO - [diffusion][Epoch 8685] diffusion learning rate: 0.001
2024-11-05 00:58:57,097 - INFO - [diffusion][Epoch 8685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:57,099 - INFO - [diffusion][Epoch 8686] Epoch 8687/12000
2024-11-05 00:59:00,297 - INFO - [diffusion][Epoch 8686] diffusion training Loss: 0.06966698914766312
2024-11-05 00:59:00,299 - INFO - [diffusion][Epoch 8686] diffusion learning rate: 0.001
2024-11-05 00:59:00,301 - INFO - [diffusion][Epoch 8686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:00,302 - INFO - [diffusion][Epoch 8687] Epoch 8688/12000
2024-11-05 00:59:03,394 - INFO - [diffusion][Epoch 8687] diffusion training Loss: 0.06984562426805496
2024-11-05 00:59:03,396 - INFO - [diffusion][Epoch 8687] diffusion learning rate: 0.001
2024-11-05 00:59:03,397 - INFO - [diffusion][Epoch 8687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:03,399 - INFO - [diffusion][Epoch 8688] Epoch 8689/12000
2024-11-05 00:59:06,633 - INFO - [diffusion][Epoch 8688] diffusion training Loss: 0.06850804388523102
2024-11-05 00:59:06,635 - INFO - [diffusion][Epoch 8688] diffusion learning rate: 0.001
2024-11-05 00:59:06,637 - INFO - [diffusion][Epoch 8688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:06,638 - INFO - [diffusion][Epoch 8689] Epoch 8690/12000
2024-11-05 00:59:10,285 - INFO - [diffusion][Epoch 8689] diffusion training Loss: 0.07124137319624424
2024-11-05 00:59:10,287 - INFO - [diffusion][Epoch 8689] diffusion learning rate: 0.001
2024-11-05 00:59:10,288 - INFO - [diffusion][Epoch 8689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:10,289 - INFO - [diffusion][Epoch 8690] Epoch 8691/12000
2024-11-05 00:59:13,771 - INFO - [diffusion][Epoch 8690] diffusion training Loss: 0.0679432712495327
2024-11-05 00:59:13,773 - INFO - [diffusion][Epoch 8690] diffusion learning rate: 0.001
2024-11-05 00:59:13,775 - INFO - [diffusion][Epoch 8690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:13,777 - INFO - [diffusion][Epoch 8691] Epoch 8692/12000
2024-11-05 00:59:16,928 - INFO - [diffusion][Epoch 8691] diffusion training Loss: 0.06560105364769697
2024-11-05 00:59:16,930 - INFO - [diffusion][Epoch 8691] diffusion learning rate: 0.001
2024-11-05 00:59:16,932 - INFO - [diffusion][Epoch 8691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:16,933 - INFO - [diffusion][Epoch 8692] Epoch 8693/12000
2024-11-05 00:59:20,186 - INFO - [diffusion][Epoch 8692] diffusion training Loss: 0.07170707359910011
2024-11-05 00:59:20,188 - INFO - [diffusion][Epoch 8692] diffusion learning rate: 0.001
2024-11-05 00:59:20,189 - INFO - [diffusion][Epoch 8692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:20,191 - INFO - [diffusion][Epoch 8693] Epoch 8694/12000
2024-11-05 00:59:23,309 - INFO - [diffusion][Epoch 8693] diffusion training Loss: 0.07217277027666569
2024-11-05 00:59:23,311 - INFO - [diffusion][Epoch 8693] diffusion learning rate: 0.001
2024-11-05 00:59:23,313 - INFO - [diffusion][Epoch 8693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:23,314 - INFO - [diffusion][Epoch 8694] Epoch 8695/12000
2024-11-05 00:59:26,919 - INFO - [diffusion][Epoch 8694] diffusion training Loss: 0.06797718536108732
2024-11-05 00:59:26,922 - INFO - [diffusion][Epoch 8694] diffusion learning rate: 0.001
2024-11-05 00:59:26,924 - INFO - [diffusion][Epoch 8694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:26,925 - INFO - [diffusion][Epoch 8695] Epoch 8696/12000
2024-11-05 00:59:30,076 - INFO - [diffusion][Epoch 8695] diffusion training Loss: 0.06465678289532661
2024-11-05 00:59:30,078 - INFO - [diffusion][Epoch 8695] diffusion learning rate: 0.001
2024-11-05 00:59:30,080 - INFO - [diffusion][Epoch 8695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:30,081 - INFO - [diffusion][Epoch 8696] Epoch 8697/12000
2024-11-05 00:59:33,329 - INFO - [diffusion][Epoch 8696] diffusion training Loss: 0.06694410927593708
2024-11-05 00:59:33,330 - INFO - [diffusion][Epoch 8696] diffusion learning rate: 0.001
2024-11-05 00:59:33,332 - INFO - [diffusion][Epoch 8696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:33,334 - INFO - [diffusion][Epoch 8697] Epoch 8698/12000
2024-11-05 00:59:36,624 - INFO - [diffusion][Epoch 8697] diffusion training Loss: 0.06545845046639442
2024-11-05 00:59:36,626 - INFO - [diffusion][Epoch 8697] diffusion learning rate: 0.001
2024-11-05 00:59:36,627 - INFO - [diffusion][Epoch 8697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:36,628 - INFO - [diffusion][Epoch 8698] Epoch 8699/12000
2024-11-05 00:59:39,936 - INFO - [diffusion][Epoch 8698] diffusion training Loss: 0.06980024464428425
2024-11-05 00:59:39,938 - INFO - [diffusion][Epoch 8698] diffusion learning rate: 0.001
2024-11-05 00:59:39,939 - INFO - [diffusion][Epoch 8698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:39,941 - INFO - [diffusion][Epoch 8699] Epoch 8700/12000
2024-11-05 00:59:43,566 - INFO - [diffusion][Epoch 8699] diffusion training Loss: 0.07647638767957687
2024-11-05 00:59:43,568 - INFO - [diffusion][Epoch 8699] diffusion learning rate: 0.001
2024-11-05 00:59:43,570 - INFO - [diffusion][Epoch 8699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:43,571 - INFO - [diffusion][Epoch 8700] Epoch 8701/12000
2024-11-05 00:59:47,086 - INFO - [diffusion][Epoch 8700] diffusion training Loss: 0.0725657157599926
2024-11-05 00:59:47,089 - INFO - [diffusion][Epoch 8700] diffusion learning rate: 0.001
2024-11-05 00:59:47,090 - INFO - [diffusion][Epoch 8700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:47,092 - INFO - [diffusion][Epoch 8701] Epoch 8702/12000
2024-11-05 00:59:50,619 - INFO - [diffusion][Epoch 8701] diffusion training Loss: 0.07091901451349258
2024-11-05 00:59:50,621 - INFO - [diffusion][Epoch 8701] diffusion learning rate: 0.001
2024-11-05 00:59:50,639 - INFO - [diffusion][Epoch 8701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:50,640 - INFO - [diffusion][Epoch 8702] Epoch 8703/12000
2024-11-05 00:59:53,745 - INFO - [diffusion][Epoch 8702] diffusion training Loss: 0.06914681568741798
2024-11-05 00:59:53,747 - INFO - [diffusion][Epoch 8702] diffusion learning rate: 0.001
2024-11-05 00:59:53,749 - INFO - [diffusion][Epoch 8702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:53,750 - INFO - [diffusion][Epoch 8703] Epoch 8704/12000
2024-11-05 00:59:56,907 - INFO - [diffusion][Epoch 8703] diffusion training Loss: 0.06911619752645493
2024-11-05 00:59:56,909 - INFO - [diffusion][Epoch 8703] diffusion learning rate: 0.001
2024-11-05 00:59:56,911 - INFO - [diffusion][Epoch 8703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:56,912 - INFO - [diffusion][Epoch 8704] Epoch 8705/12000
2024-11-05 01:00:00,388 - INFO - [diffusion][Epoch 8704] diffusion training Loss: 0.07067122496664524
2024-11-05 01:00:00,390 - INFO - [diffusion][Epoch 8704] diffusion learning rate: 0.001
2024-11-05 01:00:00,391 - INFO - [diffusion][Epoch 8704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:00,393 - INFO - [diffusion][Epoch 8705] Epoch 8706/12000
2024-11-05 01:00:04,024 - INFO - [diffusion][Epoch 8705] diffusion training Loss: 0.07244976237416267
2024-11-05 01:00:04,026 - INFO - [diffusion][Epoch 8705] diffusion learning rate: 0.001
2024-11-05 01:00:04,027 - INFO - [diffusion][Epoch 8705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:04,029 - INFO - [diffusion][Epoch 8706] Epoch 8707/12000
2024-11-05 01:00:07,407 - INFO - [diffusion][Epoch 8706] diffusion training Loss: 0.0684596449136734
2024-11-05 01:00:07,408 - INFO - [diffusion][Epoch 8706] diffusion learning rate: 0.001
2024-11-05 01:00:07,488 - INFO - [diffusion][Epoch 8706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:07,489 - INFO - [diffusion][Epoch 8707] Epoch 8708/12000
2024-11-05 01:00:10,531 - INFO - [diffusion][Epoch 8707] diffusion training Loss: 0.07399853877723217
2024-11-05 01:00:10,533 - INFO - [diffusion][Epoch 8707] diffusion learning rate: 0.001
2024-11-05 01:00:10,535 - INFO - [diffusion][Epoch 8707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:10,536 - INFO - [diffusion][Epoch 8708] Epoch 8709/12000
2024-11-05 01:00:13,630 - INFO - [diffusion][Epoch 8708] diffusion training Loss: 0.0683952271938324
2024-11-05 01:00:13,632 - INFO - [diffusion][Epoch 8708] diffusion learning rate: 0.001
2024-11-05 01:00:13,634 - INFO - [diffusion][Epoch 8708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:13,635 - INFO - [diffusion][Epoch 8709] Epoch 8710/12000
2024-11-05 01:00:17,037 - INFO - [diffusion][Epoch 8709] diffusion training Loss: 0.07815725728869438
2024-11-05 01:00:17,040 - INFO - [diffusion][Epoch 8709] diffusion learning rate: 0.001
2024-11-05 01:00:17,041 - INFO - [diffusion][Epoch 8709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:17,042 - INFO - [diffusion][Epoch 8710] Epoch 8711/12000
2024-11-05 01:00:20,519 - INFO - [diffusion][Epoch 8710] diffusion training Loss: 0.07202122919261456
2024-11-05 01:00:20,521 - INFO - [diffusion][Epoch 8710] diffusion learning rate: 0.001
2024-11-05 01:00:20,522 - INFO - [diffusion][Epoch 8710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:20,524 - INFO - [diffusion][Epoch 8711] Epoch 8712/12000
2024-11-05 01:00:23,639 - INFO - [diffusion][Epoch 8711] diffusion training Loss: 0.07284528575837612
2024-11-05 01:00:23,641 - INFO - [diffusion][Epoch 8711] diffusion learning rate: 0.001
2024-11-05 01:00:23,643 - INFO - [diffusion][Epoch 8711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:23,645 - INFO - [diffusion][Epoch 8712] Epoch 8713/12000
2024-11-05 01:00:26,760 - INFO - [diffusion][Epoch 8712] diffusion training Loss: 0.07662517204880714
2024-11-05 01:00:26,762 - INFO - [diffusion][Epoch 8712] diffusion learning rate: 0.001
2024-11-05 01:00:26,764 - INFO - [diffusion][Epoch 8712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:26,765 - INFO - [diffusion][Epoch 8713] Epoch 8714/12000
2024-11-05 01:00:29,847 - INFO - [diffusion][Epoch 8713] diffusion training Loss: 0.0661849994212389
2024-11-05 01:00:29,849 - INFO - [diffusion][Epoch 8713] diffusion learning rate: 0.001
2024-11-05 01:00:29,872 - INFO - [diffusion][Epoch 8713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:29,873 - INFO - [diffusion][Epoch 8714] Epoch 8715/12000
2024-11-05 01:00:33,394 - INFO - [diffusion][Epoch 8714] diffusion training Loss: 0.06604350358247757
2024-11-05 01:00:33,396 - INFO - [diffusion][Epoch 8714] diffusion learning rate: 0.001
2024-11-05 01:00:33,398 - INFO - [diffusion][Epoch 8714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:33,399 - INFO - [diffusion][Epoch 8715] Epoch 8716/12000
2024-11-05 01:00:36,946 - INFO - [diffusion][Epoch 8715] diffusion training Loss: 0.06856646202504635
2024-11-05 01:00:36,948 - INFO - [diffusion][Epoch 8715] diffusion learning rate: 0.001
2024-11-05 01:00:36,950 - INFO - [diffusion][Epoch 8715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:36,951 - INFO - [diffusion][Epoch 8716] Epoch 8717/12000
2024-11-05 01:00:40,053 - INFO - [diffusion][Epoch 8716] diffusion training Loss: 0.06872518174350262
2024-11-05 01:00:40,055 - INFO - [diffusion][Epoch 8716] diffusion learning rate: 0.001
2024-11-05 01:00:40,057 - INFO - [diffusion][Epoch 8716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:40,058 - INFO - [diffusion][Epoch 8717] Epoch 8718/12000
2024-11-05 01:00:43,160 - INFO - [diffusion][Epoch 8717] diffusion training Loss: 0.06891009770333767
2024-11-05 01:00:43,162 - INFO - [diffusion][Epoch 8717] diffusion learning rate: 0.001
2024-11-05 01:00:43,164 - INFO - [diffusion][Epoch 8717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:43,165 - INFO - [diffusion][Epoch 8718] Epoch 8719/12000
2024-11-05 01:00:46,294 - INFO - [diffusion][Epoch 8718] diffusion training Loss: 0.07009802758693695
2024-11-05 01:00:46,296 - INFO - [diffusion][Epoch 8718] diffusion learning rate: 0.001
2024-11-05 01:00:46,298 - INFO - [diffusion][Epoch 8718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:46,299 - INFO - [diffusion][Epoch 8719] Epoch 8720/12000
2024-11-05 01:00:49,933 - INFO - [diffusion][Epoch 8719] diffusion training Loss: 0.07112482748925686
2024-11-05 01:00:49,936 - INFO - [diffusion][Epoch 8719] diffusion learning rate: 0.001
2024-11-05 01:00:49,938 - INFO - [diffusion][Epoch 8719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:49,939 - INFO - [diffusion][Epoch 8720] Epoch 8721/12000
2024-11-05 01:00:52,975 - INFO - [diffusion][Epoch 8720] diffusion training Loss: 0.06753231212496758
2024-11-05 01:00:52,977 - INFO - [diffusion][Epoch 8720] diffusion learning rate: 0.001
2024-11-05 01:00:52,979 - INFO - [diffusion][Epoch 8720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:52,980 - INFO - [diffusion][Epoch 8721] Epoch 8722/12000
2024-11-05 01:00:56,252 - INFO - [diffusion][Epoch 8721] diffusion training Loss: 0.062124249525368214
2024-11-05 01:00:56,253 - INFO - [diffusion][Epoch 8721] diffusion learning rate: 0.001
2024-11-05 01:00:56,255 - INFO - [diffusion][Epoch 8721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:56,256 - INFO - [diffusion][Epoch 8722] Epoch 8723/12000
2024-11-05 01:00:59,406 - INFO - [diffusion][Epoch 8722] diffusion training Loss: 0.06433504540473223
2024-11-05 01:00:59,408 - INFO - [diffusion][Epoch 8722] diffusion learning rate: 0.001
2024-11-05 01:00:59,410 - INFO - [diffusion][Epoch 8722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:59,411 - INFO - [diffusion][Epoch 8723] Epoch 8724/12000
2024-11-05 01:01:02,792 - INFO - [diffusion][Epoch 8723] diffusion training Loss: 0.0732015036046505
2024-11-05 01:01:02,794 - INFO - [diffusion][Epoch 8723] diffusion learning rate: 0.001
2024-11-05 01:01:02,795 - INFO - [diffusion][Epoch 8723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:02,797 - INFO - [diffusion][Epoch 8724] Epoch 8725/12000
2024-11-05 01:01:06,485 - INFO - [diffusion][Epoch 8724] diffusion training Loss: 0.07080479804426432
2024-11-05 01:01:06,487 - INFO - [diffusion][Epoch 8724] diffusion learning rate: 0.001
2024-11-05 01:01:06,489 - INFO - [diffusion][Epoch 8724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:06,490 - INFO - [diffusion][Epoch 8725] Epoch 8726/12000
2024-11-05 01:01:09,920 - INFO - [diffusion][Epoch 8725] diffusion training Loss: 0.0648907208815217
2024-11-05 01:01:09,922 - INFO - [diffusion][Epoch 8725] diffusion learning rate: 0.001
2024-11-05 01:01:09,924 - INFO - [diffusion][Epoch 8725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:09,925 - INFO - [diffusion][Epoch 8726] Epoch 8727/12000
2024-11-05 01:01:13,054 - INFO - [diffusion][Epoch 8726] diffusion training Loss: 0.06109750457108021
2024-11-05 01:01:13,056 - INFO - [diffusion][Epoch 8726] diffusion learning rate: 0.001
2024-11-05 01:01:13,057 - INFO - [diffusion][Epoch 8726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:13,063 - INFO - [diffusion][Epoch 8727] Epoch 8728/12000
2024-11-05 01:01:16,257 - INFO - [diffusion][Epoch 8727] diffusion training Loss: 0.06976278871297836
2024-11-05 01:01:16,259 - INFO - [diffusion][Epoch 8727] diffusion learning rate: 0.001
2024-11-05 01:01:16,261 - INFO - [diffusion][Epoch 8727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:16,262 - INFO - [diffusion][Epoch 8728] Epoch 8729/12000
2024-11-05 01:01:19,532 - INFO - [diffusion][Epoch 8728] diffusion training Loss: 0.06764336489140987
2024-11-05 01:01:19,535 - INFO - [diffusion][Epoch 8728] diffusion learning rate: 0.001
2024-11-05 01:01:19,575 - INFO - [diffusion][Epoch 8728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:19,576 - INFO - [diffusion][Epoch 8729] Epoch 8730/12000
2024-11-05 01:01:23,166 - INFO - [diffusion][Epoch 8729] diffusion training Loss: 0.07190538942813873
2024-11-05 01:01:23,169 - INFO - [diffusion][Epoch 8729] diffusion learning rate: 0.001
2024-11-05 01:01:23,170 - INFO - [diffusion][Epoch 8729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:23,172 - INFO - [diffusion][Epoch 8730] Epoch 8731/12000
2024-11-05 01:01:26,673 - INFO - [diffusion][Epoch 8730] diffusion training Loss: 0.0692363828420639
2024-11-05 01:01:26,675 - INFO - [diffusion][Epoch 8730] diffusion learning rate: 0.001
2024-11-05 01:01:26,677 - INFO - [diffusion][Epoch 8730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:26,678 - INFO - [diffusion][Epoch 8731] Epoch 8732/12000
2024-11-05 01:01:29,758 - INFO - [diffusion][Epoch 8731] diffusion training Loss: 0.06694929488003254
2024-11-05 01:01:29,760 - INFO - [diffusion][Epoch 8731] diffusion learning rate: 0.001
2024-11-05 01:01:29,793 - INFO - [diffusion][Epoch 8731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:29,795 - INFO - [diffusion][Epoch 8732] Epoch 8733/12000
2024-11-05 01:01:32,764 - INFO - [diffusion][Epoch 8732] diffusion training Loss: 0.07479158602654934
2024-11-05 01:01:32,766 - INFO - [diffusion][Epoch 8732] diffusion learning rate: 0.001
2024-11-05 01:01:32,768 - INFO - [diffusion][Epoch 8732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:32,769 - INFO - [diffusion][Epoch 8733] Epoch 8734/12000
2024-11-05 01:01:36,061 - INFO - [diffusion][Epoch 8733] diffusion training Loss: 0.07047414034605026
2024-11-05 01:01:36,062 - INFO - [diffusion][Epoch 8733] diffusion learning rate: 0.001
2024-11-05 01:01:36,064 - INFO - [diffusion][Epoch 8733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:36,066 - INFO - [diffusion][Epoch 8734] Epoch 8735/12000
2024-11-05 01:01:39,641 - INFO - [diffusion][Epoch 8734] diffusion training Loss: 0.06941068172454834
2024-11-05 01:01:39,642 - INFO - [diffusion][Epoch 8734] diffusion learning rate: 0.001
2024-11-05 01:01:39,644 - INFO - [diffusion][Epoch 8734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:39,645 - INFO - [diffusion][Epoch 8735] Epoch 8736/12000
2024-11-05 01:01:43,158 - INFO - [diffusion][Epoch 8735] diffusion training Loss: 0.07382812723517418
2024-11-05 01:01:43,160 - INFO - [diffusion][Epoch 8735] diffusion learning rate: 0.001
2024-11-05 01:01:43,162 - INFO - [diffusion][Epoch 8735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:43,163 - INFO - [diffusion][Epoch 8736] Epoch 8737/12000
2024-11-05 01:01:46,257 - INFO - [diffusion][Epoch 8736] diffusion training Loss: 0.06850048340857029
2024-11-05 01:01:46,259 - INFO - [diffusion][Epoch 8736] diffusion learning rate: 0.001
2024-11-05 01:01:46,280 - INFO - [diffusion][Epoch 8736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:46,281 - INFO - [diffusion][Epoch 8737] Epoch 8738/12000
2024-11-05 01:01:49,392 - INFO - [diffusion][Epoch 8737] diffusion training Loss: 0.0712632592767477
2024-11-05 01:01:49,395 - INFO - [diffusion][Epoch 8737] diffusion learning rate: 0.001
2024-11-05 01:01:49,396 - INFO - [diffusion][Epoch 8737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:49,397 - INFO - [diffusion][Epoch 8738] Epoch 8739/12000
2024-11-05 01:01:52,532 - INFO - [diffusion][Epoch 8738] diffusion training Loss: 0.07084898091852665
2024-11-05 01:01:52,534 - INFO - [diffusion][Epoch 8738] diffusion learning rate: 0.001
2024-11-05 01:01:52,536 - INFO - [diffusion][Epoch 8738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:52,537 - INFO - [diffusion][Epoch 8739] Epoch 8740/12000
2024-11-05 01:01:56,054 - INFO - [diffusion][Epoch 8739] diffusion training Loss: 0.06696895696222782
2024-11-05 01:01:56,056 - INFO - [diffusion][Epoch 8739] diffusion learning rate: 0.001
2024-11-05 01:01:56,058 - INFO - [diffusion][Epoch 8739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:56,060 - INFO - [diffusion][Epoch 8740] Epoch 8741/12000
2024-11-05 01:01:59,752 - INFO - [diffusion][Epoch 8740] diffusion training Loss: 0.07084104605019093
2024-11-05 01:01:59,754 - INFO - [diffusion][Epoch 8740] diffusion learning rate: 0.001
2024-11-05 01:01:59,756 - INFO - [diffusion][Epoch 8740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:59,757 - INFO - [diffusion][Epoch 8741] Epoch 8742/12000
2024-11-05 01:02:03,965 - INFO - [diffusion][Epoch 8741] diffusion training Loss: 0.07451441884040833
2024-11-05 01:02:03,967 - INFO - [diffusion][Epoch 8741] diffusion learning rate: 0.001
2024-11-05 01:02:03,969 - INFO - [diffusion][Epoch 8741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:03,971 - INFO - [diffusion][Epoch 8742] Epoch 8743/12000
2024-11-05 01:02:07,320 - INFO - [diffusion][Epoch 8742] diffusion training Loss: 0.06874550133943558
2024-11-05 01:02:07,322 - INFO - [diffusion][Epoch 8742] diffusion learning rate: 0.001
2024-11-05 01:02:07,324 - INFO - [diffusion][Epoch 8742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:07,325 - INFO - [diffusion][Epoch 8743] Epoch 8744/12000
2024-11-05 01:02:10,615 - INFO - [diffusion][Epoch 8743] diffusion training Loss: 0.0664778221398592
2024-11-05 01:02:10,616 - INFO - [diffusion][Epoch 8743] diffusion learning rate: 0.001
2024-11-05 01:02:10,618 - INFO - [diffusion][Epoch 8743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:10,619 - INFO - [diffusion][Epoch 8744] Epoch 8745/12000
2024-11-05 01:02:13,488 - INFO - [diffusion][Epoch 8744] diffusion training Loss: 0.06996532343327999
2024-11-05 01:02:13,490 - INFO - [diffusion][Epoch 8744] diffusion learning rate: 0.001
2024-11-05 01:02:13,542 - INFO - [diffusion][Epoch 8744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:13,543 - INFO - [diffusion][Epoch 8745] Epoch 8746/12000
2024-11-05 01:02:17,064 - INFO - [diffusion][Epoch 8745] diffusion training Loss: 0.06924065575003624
2024-11-05 01:02:17,066 - INFO - [diffusion][Epoch 8745] diffusion learning rate: 0.001
2024-11-05 01:02:17,068 - INFO - [diffusion][Epoch 8745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:17,069 - INFO - [diffusion][Epoch 8746] Epoch 8747/12000
2024-11-05 01:02:20,684 - INFO - [diffusion][Epoch 8746] diffusion training Loss: 0.06677917577326298
2024-11-05 01:02:20,687 - INFO - [diffusion][Epoch 8746] diffusion learning rate: 0.001
2024-11-05 01:02:20,688 - INFO - [diffusion][Epoch 8746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:20,690 - INFO - [diffusion][Epoch 8747] Epoch 8748/12000
2024-11-05 01:02:23,767 - INFO - [diffusion][Epoch 8747] diffusion training Loss: 0.07167736440896988
2024-11-05 01:02:23,769 - INFO - [diffusion][Epoch 8747] diffusion learning rate: 0.001
2024-11-05 01:02:23,771 - INFO - [diffusion][Epoch 8747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:23,772 - INFO - [diffusion][Epoch 8748] Epoch 8749/12000
2024-11-05 01:02:27,120 - INFO - [diffusion][Epoch 8748] diffusion training Loss: 0.07154669426381588
2024-11-05 01:02:27,122 - INFO - [diffusion][Epoch 8748] diffusion learning rate: 0.001
2024-11-05 01:02:27,124 - INFO - [diffusion][Epoch 8748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:27,125 - INFO - [diffusion][Epoch 8749] Epoch 8750/12000
2024-11-05 01:02:30,305 - INFO - [diffusion][Epoch 8749] diffusion training Loss: 0.06954111903905869
2024-11-05 01:02:30,307 - INFO - [diffusion][Epoch 8749] diffusion learning rate: 0.001
2024-11-05 01:02:30,309 - INFO - [diffusion][Epoch 8749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:30,310 - INFO - [diffusion][Epoch 8750] Epoch 8751/12000
2024-11-05 01:02:33,633 - INFO - [diffusion][Epoch 8750] diffusion training Loss: 0.06776370108127594
2024-11-05 01:02:33,635 - INFO - [diffusion][Epoch 8750] diffusion learning rate: 0.001
2024-11-05 01:02:33,637 - INFO - [diffusion][Epoch 8750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:33,639 - INFO - [diffusion][Epoch 8751] Epoch 8752/12000
2024-11-05 01:02:37,294 - INFO - [diffusion][Epoch 8751] diffusion training Loss: 0.08020859770476818
2024-11-05 01:02:37,296 - INFO - [diffusion][Epoch 8751] diffusion learning rate: 0.001
2024-11-05 01:02:37,298 - INFO - [diffusion][Epoch 8751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:37,299 - INFO - [diffusion][Epoch 8752] Epoch 8753/12000
2024-11-05 01:02:40,603 - INFO - [diffusion][Epoch 8752] diffusion training Loss: 0.06557958107441664
2024-11-05 01:02:40,605 - INFO - [diffusion][Epoch 8752] diffusion learning rate: 0.001
2024-11-05 01:02:40,607 - INFO - [diffusion][Epoch 8752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:40,608 - INFO - [diffusion][Epoch 8753] Epoch 8754/12000
2024-11-05 01:02:43,620 - INFO - [diffusion][Epoch 8753] diffusion training Loss: 0.06947186402976513
2024-11-05 01:02:43,622 - INFO - [diffusion][Epoch 8753] diffusion learning rate: 0.001
2024-11-05 01:02:43,624 - INFO - [diffusion][Epoch 8753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:43,625 - INFO - [diffusion][Epoch 8754] Epoch 8755/12000
2024-11-05 01:02:46,734 - INFO - [diffusion][Epoch 8754] diffusion training Loss: 0.06919131614267826
2024-11-05 01:02:46,736 - INFO - [diffusion][Epoch 8754] diffusion learning rate: 0.001
2024-11-05 01:02:46,737 - INFO - [diffusion][Epoch 8754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:46,739 - INFO - [diffusion][Epoch 8755] Epoch 8756/12000
2024-11-05 01:02:50,260 - INFO - [diffusion][Epoch 8755] diffusion training Loss: 0.06586399953812361
2024-11-05 01:02:50,262 - INFO - [diffusion][Epoch 8755] diffusion learning rate: 0.001
2024-11-05 01:02:50,264 - INFO - [diffusion][Epoch 8755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:50,265 - INFO - [diffusion][Epoch 8756] Epoch 8757/12000
2024-11-05 01:02:53,802 - INFO - [diffusion][Epoch 8756] diffusion training Loss: 0.0699735302478075
2024-11-05 01:02:53,804 - INFO - [diffusion][Epoch 8756] diffusion learning rate: 0.001
2024-11-05 01:02:53,806 - INFO - [diffusion][Epoch 8756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:53,807 - INFO - [diffusion][Epoch 8757] Epoch 8758/12000
2024-11-05 01:02:56,846 - INFO - [diffusion][Epoch 8757] diffusion training Loss: 0.07054287008941174
2024-11-05 01:02:56,868 - INFO - [diffusion][Epoch 8757] diffusion learning rate: 0.001
2024-11-05 01:02:56,870 - INFO - [diffusion][Epoch 8757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:56,871 - INFO - [diffusion][Epoch 8758] Epoch 8759/12000
2024-11-05 01:03:00,028 - INFO - [diffusion][Epoch 8758] diffusion training Loss: 0.06733748689293861
2024-11-05 01:03:00,030 - INFO - [diffusion][Epoch 8758] diffusion learning rate: 0.001
2024-11-05 01:03:00,032 - INFO - [diffusion][Epoch 8758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:00,033 - INFO - [diffusion][Epoch 8759] Epoch 8760/12000
2024-11-05 01:03:03,124 - INFO - [diffusion][Epoch 8759] diffusion training Loss: 0.06279722135514021
2024-11-05 01:03:03,126 - INFO - [diffusion][Epoch 8759] diffusion learning rate: 0.001
2024-11-05 01:03:03,128 - INFO - [diffusion][Epoch 8759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:03,129 - INFO - [diffusion][Epoch 8760] Epoch 8761/12000
2024-11-05 01:03:06,644 - INFO - [diffusion][Epoch 8760] diffusion training Loss: 0.07065743580460548
2024-11-05 01:03:06,646 - INFO - [diffusion][Epoch 8760] diffusion learning rate: 0.001
2024-11-05 01:03:06,648 - INFO - [diffusion][Epoch 8760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:06,649 - INFO - [diffusion][Epoch 8761] Epoch 8762/12000
2024-11-05 01:03:10,741 - INFO - [diffusion][Epoch 8761] diffusion training Loss: 0.06877163052558899
2024-11-05 01:03:10,743 - INFO - [diffusion][Epoch 8761] diffusion learning rate: 0.001
2024-11-05 01:03:10,745 - INFO - [diffusion][Epoch 8761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:10,746 - INFO - [diffusion][Epoch 8762] Epoch 8763/12000
2024-11-05 01:03:14,248 - INFO - [diffusion][Epoch 8762] diffusion training Loss: 0.0758013091981411
2024-11-05 01:03:14,250 - INFO - [diffusion][Epoch 8762] diffusion learning rate: 0.001
2024-11-05 01:03:14,252 - INFO - [diffusion][Epoch 8762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:14,253 - INFO - [diffusion][Epoch 8763] Epoch 8764/12000
2024-11-05 01:03:17,342 - INFO - [diffusion][Epoch 8763] diffusion training Loss: 0.07165680080652237
2024-11-05 01:03:17,344 - INFO - [diffusion][Epoch 8763] diffusion learning rate: 0.001
2024-11-05 01:03:17,346 - INFO - [diffusion][Epoch 8763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:17,347 - INFO - [diffusion][Epoch 8764] Epoch 8765/12000
2024-11-05 01:03:20,645 - INFO - [diffusion][Epoch 8764] diffusion training Loss: 0.06991214491426945
2024-11-05 01:03:20,646 - INFO - [diffusion][Epoch 8764] diffusion learning rate: 0.001
2024-11-05 01:03:20,648 - INFO - [diffusion][Epoch 8764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:20,649 - INFO - [diffusion][Epoch 8765] Epoch 8766/12000
2024-11-05 01:03:23,894 - INFO - [diffusion][Epoch 8765] diffusion training Loss: 0.07227078825235367
2024-11-05 01:03:23,896 - INFO - [diffusion][Epoch 8765] diffusion learning rate: 0.001
2024-11-05 01:03:23,898 - INFO - [diffusion][Epoch 8765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:23,899 - INFO - [diffusion][Epoch 8766] Epoch 8767/12000
2024-11-05 01:03:27,541 - INFO - [diffusion][Epoch 8766] diffusion training Loss: 0.06920116767287254
2024-11-05 01:03:27,543 - INFO - [diffusion][Epoch 8766] diffusion learning rate: 0.001
2024-11-05 01:03:27,544 - INFO - [diffusion][Epoch 8766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:27,546 - INFO - [diffusion][Epoch 8767] Epoch 8768/12000
2024-11-05 01:03:30,978 - INFO - [diffusion][Epoch 8767] diffusion training Loss: 0.07374158687889576
2024-11-05 01:03:30,980 - INFO - [diffusion][Epoch 8767] diffusion learning rate: 0.001
2024-11-05 01:03:30,982 - INFO - [diffusion][Epoch 8767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:30,983 - INFO - [diffusion][Epoch 8768] Epoch 8769/12000
2024-11-05 01:03:34,190 - INFO - [diffusion][Epoch 8768] diffusion training Loss: 0.06514061614871025
2024-11-05 01:03:34,192 - INFO - [diffusion][Epoch 8768] diffusion learning rate: 0.001
2024-11-05 01:03:34,194 - INFO - [diffusion][Epoch 8768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:34,195 - INFO - [diffusion][Epoch 8769] Epoch 8770/12000
2024-11-05 01:03:37,033 - INFO - [diffusion][Epoch 8769] diffusion training Loss: 0.0682714618742466
2024-11-05 01:03:37,035 - INFO - [diffusion][Epoch 8769] diffusion learning rate: 0.001
2024-11-05 01:03:37,036 - INFO - [diffusion][Epoch 8769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:37,037 - INFO - [diffusion][Epoch 8770] Epoch 8771/12000
2024-11-05 01:03:40,531 - INFO - [diffusion][Epoch 8770] diffusion training Loss: 0.0682928841561079
2024-11-05 01:03:40,533 - INFO - [diffusion][Epoch 8770] diffusion learning rate: 0.001
2024-11-05 01:03:40,535 - INFO - [diffusion][Epoch 8770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:40,536 - INFO - [diffusion][Epoch 8771] Epoch 8772/12000
2024-11-05 01:03:44,007 - INFO - [diffusion][Epoch 8771] diffusion training Loss: 0.06996458023786545
2024-11-05 01:03:44,009 - INFO - [diffusion][Epoch 8771] diffusion learning rate: 0.001
2024-11-05 01:03:44,011 - INFO - [diffusion][Epoch 8771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:44,012 - INFO - [diffusion][Epoch 8772] Epoch 8773/12000
2024-11-05 01:03:47,087 - INFO - [diffusion][Epoch 8772] diffusion training Loss: 0.07039892114698887
2024-11-05 01:03:47,089 - INFO - [diffusion][Epoch 8772] diffusion learning rate: 0.001
2024-11-05 01:03:47,091 - INFO - [diffusion][Epoch 8772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:47,092 - INFO - [diffusion][Epoch 8773] Epoch 8774/12000
2024-11-05 01:03:50,171 - INFO - [diffusion][Epoch 8773] diffusion training Loss: 0.06522670201957226
2024-11-05 01:03:50,173 - INFO - [diffusion][Epoch 8773] diffusion learning rate: 0.001
2024-11-05 01:03:50,175 - INFO - [diffusion][Epoch 8773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:50,176 - INFO - [diffusion][Epoch 8774] Epoch 8775/12000
2024-11-05 01:03:53,274 - INFO - [diffusion][Epoch 8774] diffusion training Loss: 0.062340592965483665
2024-11-05 01:03:53,276 - INFO - [diffusion][Epoch 8774] diffusion learning rate: 0.001
2024-11-05 01:03:53,277 - INFO - [diffusion][Epoch 8774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:53,279 - INFO - [diffusion][Epoch 8775] Epoch 8776/12000
2024-11-05 01:03:56,735 - INFO - [diffusion][Epoch 8775] diffusion training Loss: 0.06830768380314112
2024-11-05 01:03:56,737 - INFO - [diffusion][Epoch 8775] diffusion learning rate: 0.001
2024-11-05 01:03:56,739 - INFO - [diffusion][Epoch 8775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:56,740 - INFO - [diffusion][Epoch 8776] Epoch 8777/12000
2024-11-05 01:04:00,252 - INFO - [diffusion][Epoch 8776] diffusion training Loss: 0.0642661415040493
2024-11-05 01:04:00,254 - INFO - [diffusion][Epoch 8776] diffusion learning rate: 0.001
2024-11-05 01:04:00,256 - INFO - [diffusion][Epoch 8776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:00,258 - INFO - [diffusion][Epoch 8777] Epoch 8778/12000
2024-11-05 01:04:03,359 - INFO - [diffusion][Epoch 8777] diffusion training Loss: 0.07359876483678818
2024-11-05 01:04:03,361 - INFO - [diffusion][Epoch 8777] diffusion learning rate: 0.001
2024-11-05 01:04:03,363 - INFO - [diffusion][Epoch 8777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:03,364 - INFO - [diffusion][Epoch 8778] Epoch 8779/12000
2024-11-05 01:04:06,591 - INFO - [diffusion][Epoch 8778] diffusion training Loss: 0.07250811532139778
2024-11-05 01:04:06,593 - INFO - [diffusion][Epoch 8778] diffusion learning rate: 0.001
2024-11-05 01:04:06,595 - INFO - [diffusion][Epoch 8778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:06,596 - INFO - [diffusion][Epoch 8779] Epoch 8780/12000
2024-11-05 01:04:09,701 - INFO - [diffusion][Epoch 8779] diffusion training Loss: 0.06829236634075642
2024-11-05 01:04:09,703 - INFO - [diffusion][Epoch 8779] diffusion learning rate: 0.001
2024-11-05 01:04:09,705 - INFO - [diffusion][Epoch 8779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:09,706 - INFO - [diffusion][Epoch 8780] Epoch 8781/12000
2024-11-05 01:04:13,195 - INFO - [diffusion][Epoch 8780] diffusion training Loss: 0.06725928001105785
2024-11-05 01:04:13,197 - INFO - [diffusion][Epoch 8780] diffusion learning rate: 0.001
2024-11-05 01:04:13,198 - INFO - [diffusion][Epoch 8780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:13,200 - INFO - [diffusion][Epoch 8781] Epoch 8782/12000
2024-11-05 01:04:17,117 - INFO - [diffusion][Epoch 8781] diffusion training Loss: 0.0685475692152977
2024-11-05 01:04:17,119 - INFO - [diffusion][Epoch 8781] diffusion learning rate: 0.001
2024-11-05 01:04:17,121 - INFO - [diffusion][Epoch 8781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:17,122 - INFO - [diffusion][Epoch 8782] Epoch 8783/12000
2024-11-05 01:04:20,405 - INFO - [diffusion][Epoch 8782] diffusion training Loss: 0.0697905570268631
2024-11-05 01:04:20,407 - INFO - [diffusion][Epoch 8782] diffusion learning rate: 0.001
2024-11-05 01:04:20,408 - INFO - [diffusion][Epoch 8782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:20,410 - INFO - [diffusion][Epoch 8783] Epoch 8784/12000
2024-11-05 01:04:23,532 - INFO - [diffusion][Epoch 8783] diffusion training Loss: 0.07013865374028683
2024-11-05 01:04:23,534 - INFO - [diffusion][Epoch 8783] diffusion learning rate: 0.001
2024-11-05 01:04:23,536 - INFO - [diffusion][Epoch 8783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:23,537 - INFO - [diffusion][Epoch 8784] Epoch 8785/12000
2024-11-05 01:04:26,609 - INFO - [diffusion][Epoch 8784] diffusion training Loss: 0.06588895805180073
2024-11-05 01:04:26,612 - INFO - [diffusion][Epoch 8784] diffusion learning rate: 0.001
2024-11-05 01:04:26,614 - INFO - [diffusion][Epoch 8784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:26,615 - INFO - [diffusion][Epoch 8785] Epoch 8786/12000
2024-11-05 01:04:30,132 - INFO - [diffusion][Epoch 8785] diffusion training Loss: 0.06957022752612829
2024-11-05 01:04:30,134 - INFO - [diffusion][Epoch 8785] diffusion learning rate: 0.001
2024-11-05 01:04:30,136 - INFO - [diffusion][Epoch 8785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:30,137 - INFO - [diffusion][Epoch 8786] Epoch 8787/12000
2024-11-05 01:04:33,858 - INFO - [diffusion][Epoch 8786] diffusion training Loss: 0.06821034289896488
2024-11-05 01:04:33,860 - INFO - [diffusion][Epoch 8786] diffusion learning rate: 0.001
2024-11-05 01:04:33,861 - INFO - [diffusion][Epoch 8786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:33,862 - INFO - [diffusion][Epoch 8787] Epoch 8788/12000
2024-11-05 01:04:37,134 - INFO - [diffusion][Epoch 8787] diffusion training Loss: 0.07105731777846813
2024-11-05 01:04:37,136 - INFO - [diffusion][Epoch 8787] diffusion learning rate: 0.001
2024-11-05 01:04:37,138 - INFO - [diffusion][Epoch 8787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:37,139 - INFO - [diffusion][Epoch 8788] Epoch 8789/12000
2024-11-05 01:04:40,261 - INFO - [diffusion][Epoch 8788] diffusion training Loss: 0.07613833993673325
2024-11-05 01:04:40,262 - INFO - [diffusion][Epoch 8788] diffusion learning rate: 0.001
2024-11-05 01:04:40,264 - INFO - [diffusion][Epoch 8788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:40,265 - INFO - [diffusion][Epoch 8789] Epoch 8790/12000
2024-11-05 01:04:43,480 - INFO - [diffusion][Epoch 8789] diffusion training Loss: 0.0705681461840868
2024-11-05 01:04:43,482 - INFO - [diffusion][Epoch 8789] diffusion learning rate: 0.001
2024-11-05 01:04:43,484 - INFO - [diffusion][Epoch 8789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:43,485 - INFO - [diffusion][Epoch 8790] Epoch 8791/12000
2024-11-05 01:04:46,879 - INFO - [diffusion][Epoch 8790] diffusion training Loss: 0.06528997421264648
2024-11-05 01:04:46,881 - INFO - [diffusion][Epoch 8790] diffusion learning rate: 0.001
2024-11-05 01:04:46,883 - INFO - [diffusion][Epoch 8790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:46,884 - INFO - [diffusion][Epoch 8791] Epoch 8792/12000
2024-11-05 01:04:50,505 - INFO - [diffusion][Epoch 8791] diffusion training Loss: 0.07524867728352547
2024-11-05 01:04:50,507 - INFO - [diffusion][Epoch 8791] diffusion learning rate: 0.001
2024-11-05 01:04:50,508 - INFO - [diffusion][Epoch 8791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:50,510 - INFO - [diffusion][Epoch 8792] Epoch 8793/12000
2024-11-05 01:04:53,934 - INFO - [diffusion][Epoch 8792] diffusion training Loss: 0.0766108687967062
2024-11-05 01:04:53,937 - INFO - [diffusion][Epoch 8792] diffusion learning rate: 0.001
2024-11-05 01:04:53,938 - INFO - [diffusion][Epoch 8792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:53,940 - INFO - [diffusion][Epoch 8793] Epoch 8794/12000
2024-11-05 01:04:56,990 - INFO - [diffusion][Epoch 8793] diffusion training Loss: 0.069833530113101
2024-11-05 01:04:56,992 - INFO - [diffusion][Epoch 8793] diffusion learning rate: 0.001
2024-11-05 01:04:56,993 - INFO - [diffusion][Epoch 8793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:56,995 - INFO - [diffusion][Epoch 8794] Epoch 8795/12000
2024-11-05 01:05:00,031 - INFO - [diffusion][Epoch 8794] diffusion training Loss: 0.0702336560934782
2024-11-05 01:05:00,033 - INFO - [diffusion][Epoch 8794] diffusion learning rate: 0.001
2024-11-05 01:05:00,036 - INFO - [diffusion][Epoch 8794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:00,038 - INFO - [diffusion][Epoch 8795] Epoch 8796/12000
2024-11-05 01:05:03,434 - INFO - [diffusion][Epoch 8795] diffusion training Loss: 0.06784035079181194
2024-11-05 01:05:03,436 - INFO - [diffusion][Epoch 8795] diffusion learning rate: 0.001
2024-11-05 01:05:03,438 - INFO - [diffusion][Epoch 8795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:03,439 - INFO - [diffusion][Epoch 8796] Epoch 8797/12000
2024-11-05 01:05:06,808 - INFO - [diffusion][Epoch 8796] diffusion training Loss: 0.06514285132288933
2024-11-05 01:05:06,810 - INFO - [diffusion][Epoch 8796] diffusion learning rate: 0.001
2024-11-05 01:05:06,811 - INFO - [diffusion][Epoch 8796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:06,813 - INFO - [diffusion][Epoch 8797] Epoch 8798/12000
2024-11-05 01:05:09,876 - INFO - [diffusion][Epoch 8797] diffusion training Loss: 0.07222986221313477
2024-11-05 01:05:09,878 - INFO - [diffusion][Epoch 8797] diffusion learning rate: 0.001
2024-11-05 01:05:09,879 - INFO - [diffusion][Epoch 8797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:09,881 - INFO - [diffusion][Epoch 8798] Epoch 8799/12000
2024-11-05 01:05:12,850 - INFO - [diffusion][Epoch 8798] diffusion training Loss: 0.07937912456691265
2024-11-05 01:05:12,852 - INFO - [diffusion][Epoch 8798] diffusion learning rate: 0.001
2024-11-05 01:05:12,854 - INFO - [diffusion][Epoch 8798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:12,855 - INFO - [diffusion][Epoch 8799] Epoch 8800/12000
2024-11-05 01:05:16,378 - INFO - [diffusion][Epoch 8799] diffusion training Loss: 0.0648095179349184
2024-11-05 01:05:16,380 - INFO - [diffusion][Epoch 8799] diffusion learning rate: 0.001
2024-11-05 01:05:16,382 - INFO - [diffusion][Epoch 8799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:16,384 - INFO - [diffusion][Epoch 8800] Epoch 8801/12000
2024-11-05 01:05:19,905 - INFO - [diffusion][Epoch 8800] diffusion training Loss: 0.06727627664804459
2024-11-05 01:05:19,907 - INFO - [diffusion][Epoch 8800] diffusion learning rate: 0.001
2024-11-05 01:05:19,909 - INFO - [diffusion][Epoch 8800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:19,910 - INFO - [diffusion][Epoch 8801] Epoch 8802/12000
2024-11-05 01:05:23,033 - INFO - [diffusion][Epoch 8801] diffusion training Loss: 0.07171324081718922
2024-11-05 01:05:23,035 - INFO - [diffusion][Epoch 8801] diffusion learning rate: 0.001
2024-11-05 01:05:23,037 - INFO - [diffusion][Epoch 8801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:23,038 - INFO - [diffusion][Epoch 8802] Epoch 8803/12000
2024-11-05 01:05:26,228 - INFO - [diffusion][Epoch 8802] diffusion training Loss: 0.06782049685716629
2024-11-05 01:05:26,230 - INFO - [diffusion][Epoch 8802] diffusion learning rate: 0.001
2024-11-05 01:05:26,232 - INFO - [diffusion][Epoch 8802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:26,233 - INFO - [diffusion][Epoch 8803] Epoch 8804/12000
2024-11-05 01:05:29,844 - INFO - [diffusion][Epoch 8803] diffusion training Loss: 0.0655811382457614
2024-11-05 01:05:29,847 - INFO - [diffusion][Epoch 8803] diffusion learning rate: 0.001
2024-11-05 01:05:29,849 - INFO - [diffusion][Epoch 8803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:29,850 - INFO - [diffusion][Epoch 8804] Epoch 8805/12000
2024-11-05 01:05:33,071 - INFO - [diffusion][Epoch 8804] diffusion training Loss: 0.07321690768003464
2024-11-05 01:05:33,073 - INFO - [diffusion][Epoch 8804] diffusion learning rate: 0.001
2024-11-05 01:05:33,075 - INFO - [diffusion][Epoch 8804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:33,076 - INFO - [diffusion][Epoch 8805] Epoch 8806/12000
2024-11-05 01:05:36,472 - INFO - [diffusion][Epoch 8805] diffusion training Loss: 0.06700798869132996
2024-11-05 01:05:36,474 - INFO - [diffusion][Epoch 8805] diffusion learning rate: 0.001
2024-11-05 01:05:36,522 - INFO - [diffusion][Epoch 8805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:36,524 - INFO - [diffusion][Epoch 8806] Epoch 8807/12000
2024-11-05 01:05:40,143 - INFO - [diffusion][Epoch 8806] diffusion training Loss: 0.07043375447392464
2024-11-05 01:05:40,144 - INFO - [diffusion][Epoch 8806] diffusion learning rate: 0.001
2024-11-05 01:05:40,146 - INFO - [diffusion][Epoch 8806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:40,147 - INFO - [diffusion][Epoch 8807] Epoch 8808/12000
2024-11-05 01:05:43,502 - INFO - [diffusion][Epoch 8807] diffusion training Loss: 0.06601028330624104
2024-11-05 01:05:43,504 - INFO - [diffusion][Epoch 8807] diffusion learning rate: 0.001
2024-11-05 01:05:43,506 - INFO - [diffusion][Epoch 8807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:43,507 - INFO - [diffusion][Epoch 8808] Epoch 8809/12000
2024-11-05 01:05:46,784 - INFO - [diffusion][Epoch 8808] diffusion training Loss: 0.06403039116412401
2024-11-05 01:05:46,786 - INFO - [diffusion][Epoch 8808] diffusion learning rate: 0.001
2024-11-05 01:05:46,814 - INFO - [diffusion][Epoch 8808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:46,815 - INFO - [diffusion][Epoch 8809] Epoch 8810/12000
2024-11-05 01:05:49,792 - INFO - [diffusion][Epoch 8809] diffusion training Loss: 0.07607445493340492
2024-11-05 01:05:49,793 - INFO - [diffusion][Epoch 8809] diffusion learning rate: 0.001
2024-11-05 01:05:49,795 - INFO - [diffusion][Epoch 8809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:49,796 - INFO - [diffusion][Epoch 8810] Epoch 8811/12000
2024-11-05 01:05:53,005 - INFO - [diffusion][Epoch 8810] diffusion training Loss: 0.07614394463598728
2024-11-05 01:05:53,007 - INFO - [diffusion][Epoch 8810] diffusion learning rate: 0.001
2024-11-05 01:05:53,009 - INFO - [diffusion][Epoch 8810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:53,010 - INFO - [diffusion][Epoch 8811] Epoch 8812/12000
2024-11-05 01:05:56,535 - INFO - [diffusion][Epoch 8811] diffusion training Loss: 0.06449033692479134
2024-11-05 01:05:56,537 - INFO - [diffusion][Epoch 8811] diffusion learning rate: 0.001
2024-11-05 01:05:56,539 - INFO - [diffusion][Epoch 8811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:56,540 - INFO - [diffusion][Epoch 8812] Epoch 8813/12000
2024-11-05 01:06:00,044 - INFO - [diffusion][Epoch 8812] diffusion training Loss: 0.07580601051449776
2024-11-05 01:06:00,046 - INFO - [diffusion][Epoch 8812] diffusion learning rate: 0.001
2024-11-05 01:06:00,048 - INFO - [diffusion][Epoch 8812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:00,049 - INFO - [diffusion][Epoch 8813] Epoch 8814/12000
2024-11-05 01:06:03,174 - INFO - [diffusion][Epoch 8813] diffusion training Loss: 0.06620302889496088
2024-11-05 01:06:03,176 - INFO - [diffusion][Epoch 8813] diffusion learning rate: 0.001
2024-11-05 01:06:03,178 - INFO - [diffusion][Epoch 8813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:03,179 - INFO - [diffusion][Epoch 8814] Epoch 8815/12000
2024-11-05 01:06:06,333 - INFO - [diffusion][Epoch 8814] diffusion training Loss: 0.07160991430282593
2024-11-05 01:06:06,335 - INFO - [diffusion][Epoch 8814] diffusion learning rate: 0.001
2024-11-05 01:06:06,336 - INFO - [diffusion][Epoch 8814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:06,338 - INFO - [diffusion][Epoch 8815] Epoch 8816/12000
2024-11-05 01:06:09,462 - INFO - [diffusion][Epoch 8815] diffusion training Loss: 0.07254837080836296
2024-11-05 01:06:09,464 - INFO - [diffusion][Epoch 8815] diffusion learning rate: 0.001
2024-11-05 01:06:09,466 - INFO - [diffusion][Epoch 8815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:09,467 - INFO - [diffusion][Epoch 8816] Epoch 8817/12000
2024-11-05 01:06:12,977 - INFO - [diffusion][Epoch 8816] diffusion training Loss: 0.07005452923476696
2024-11-05 01:06:12,978 - INFO - [diffusion][Epoch 8816] diffusion learning rate: 0.001
2024-11-05 01:06:12,980 - INFO - [diffusion][Epoch 8816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:12,981 - INFO - [diffusion][Epoch 8817] Epoch 8818/12000
2024-11-05 01:06:16,632 - INFO - [diffusion][Epoch 8817] diffusion training Loss: 0.07118833437561989
2024-11-05 01:06:16,634 - INFO - [diffusion][Epoch 8817] diffusion learning rate: 0.001
2024-11-05 01:06:16,635 - INFO - [diffusion][Epoch 8817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:16,637 - INFO - [diffusion][Epoch 8818] Epoch 8819/12000
2024-11-05 01:06:19,755 - INFO - [diffusion][Epoch 8818] diffusion training Loss: 0.06722643133252859
2024-11-05 01:06:19,756 - INFO - [diffusion][Epoch 8818] diffusion learning rate: 0.001
2024-11-05 01:06:19,798 - INFO - [diffusion][Epoch 8818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:19,799 - INFO - [diffusion][Epoch 8819] Epoch 8820/12000
2024-11-05 01:06:22,964 - INFO - [diffusion][Epoch 8819] diffusion training Loss: 0.06213967315852642
2024-11-05 01:06:22,966 - INFO - [diffusion][Epoch 8819] diffusion learning rate: 0.001
2024-11-05 01:06:22,967 - INFO - [diffusion][Epoch 8819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:22,969 - INFO - [diffusion][Epoch 8820] Epoch 8821/12000
2024-11-05 01:06:26,200 - INFO - [diffusion][Epoch 8820] diffusion training Loss: 0.06568898819386959
2024-11-05 01:06:26,202 - INFO - [diffusion][Epoch 8820] diffusion learning rate: 0.001
2024-11-05 01:06:26,204 - INFO - [diffusion][Epoch 8820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:26,205 - INFO - [diffusion][Epoch 8821] Epoch 8822/12000
2024-11-05 01:06:29,597 - INFO - [diffusion][Epoch 8821] diffusion training Loss: 0.06170445028692484
2024-11-05 01:06:29,603 - INFO - [diffusion][Epoch 8821] diffusion learning rate: 0.001
2024-11-05 01:06:29,605 - INFO - [diffusion][Epoch 8821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:29,606 - INFO - [diffusion][Epoch 8822] Epoch 8823/12000
2024-11-05 01:06:33,302 - INFO - [diffusion][Epoch 8822] diffusion training Loss: 0.07290531694889069
2024-11-05 01:06:33,306 - INFO - [diffusion][Epoch 8822] diffusion learning rate: 0.001
2024-11-05 01:06:33,307 - INFO - [diffusion][Epoch 8822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:33,308 - INFO - [diffusion][Epoch 8823] Epoch 8824/12000
2024-11-05 01:06:36,319 - INFO - [diffusion][Epoch 8823] diffusion training Loss: 0.06767364963889122
2024-11-05 01:06:36,321 - INFO - [diffusion][Epoch 8823] diffusion learning rate: 0.001
2024-11-05 01:06:36,343 - INFO - [diffusion][Epoch 8823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:36,344 - INFO - [diffusion][Epoch 8824] Epoch 8825/12000
2024-11-05 01:06:39,565 - INFO - [diffusion][Epoch 8824] diffusion training Loss: 0.06661525368690491
2024-11-05 01:06:39,567 - INFO - [diffusion][Epoch 8824] diffusion learning rate: 0.001
2024-11-05 01:06:39,568 - INFO - [diffusion][Epoch 8824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:39,570 - INFO - [diffusion][Epoch 8825] Epoch 8826/12000
2024-11-05 01:06:43,295 - INFO - [diffusion][Epoch 8825] diffusion training Loss: 0.06869998574256897
2024-11-05 01:06:43,297 - INFO - [diffusion][Epoch 8825] diffusion learning rate: 0.001
2024-11-05 01:06:43,298 - INFO - [diffusion][Epoch 8825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:43,300 - INFO - [diffusion][Epoch 8826] Epoch 8827/12000
2024-11-05 01:06:46,539 - INFO - [diffusion][Epoch 8826] diffusion training Loss: 0.06849896628409624
2024-11-05 01:06:46,541 - INFO - [diffusion][Epoch 8826] diffusion learning rate: 0.001
2024-11-05 01:06:46,543 - INFO - [diffusion][Epoch 8826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:46,544 - INFO - [diffusion][Epoch 8827] Epoch 8828/12000
2024-11-05 01:06:50,090 - INFO - [diffusion][Epoch 8827] diffusion training Loss: 0.0736193060874939
2024-11-05 01:06:50,091 - INFO - [diffusion][Epoch 8827] diffusion learning rate: 0.001
2024-11-05 01:06:50,093 - INFO - [diffusion][Epoch 8827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:50,094 - INFO - [diffusion][Epoch 8828] Epoch 8829/12000
2024-11-05 01:06:53,739 - INFO - [diffusion][Epoch 8828] diffusion training Loss: 0.0699495617300272
2024-11-05 01:06:53,741 - INFO - [diffusion][Epoch 8828] diffusion learning rate: 0.001
2024-11-05 01:06:53,742 - INFO - [diffusion][Epoch 8828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:53,744 - INFO - [diffusion][Epoch 8829] Epoch 8830/12000
2024-11-05 01:06:56,946 - INFO - [diffusion][Epoch 8829] diffusion training Loss: 0.07100718095898628
2024-11-05 01:06:56,949 - INFO - [diffusion][Epoch 8829] diffusion learning rate: 0.001
2024-11-05 01:06:56,950 - INFO - [diffusion][Epoch 8829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:56,952 - INFO - [diffusion][Epoch 8830] Epoch 8831/12000
2024-11-05 01:07:00,188 - INFO - [diffusion][Epoch 8830] diffusion training Loss: 0.0732556227594614
2024-11-05 01:07:00,189 - INFO - [diffusion][Epoch 8830] diffusion learning rate: 0.001
2024-11-05 01:07:00,191 - INFO - [diffusion][Epoch 8830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:00,192 - INFO - [diffusion][Epoch 8831] Epoch 8832/12000
2024-11-05 01:07:03,322 - INFO - [diffusion][Epoch 8831] diffusion training Loss: 0.07514198124408722
2024-11-05 01:07:03,324 - INFO - [diffusion][Epoch 8831] diffusion learning rate: 0.001
2024-11-05 01:07:03,326 - INFO - [diffusion][Epoch 8831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:03,327 - INFO - [diffusion][Epoch 8832] Epoch 8833/12000
2024-11-05 01:07:06,860 - INFO - [diffusion][Epoch 8832] diffusion training Loss: 0.07718007452785969
2024-11-05 01:07:06,862 - INFO - [diffusion][Epoch 8832] diffusion learning rate: 0.001
2024-11-05 01:07:06,864 - INFO - [diffusion][Epoch 8832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:06,865 - INFO - [diffusion][Epoch 8833] Epoch 8834/12000
2024-11-05 01:07:10,542 - INFO - [diffusion][Epoch 8833] diffusion training Loss: 0.0648313919082284
2024-11-05 01:07:10,544 - INFO - [diffusion][Epoch 8833] diffusion learning rate: 0.001
2024-11-05 01:07:10,546 - INFO - [diffusion][Epoch 8833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:10,547 - INFO - [diffusion][Epoch 8834] Epoch 8835/12000
2024-11-05 01:07:13,724 - INFO - [diffusion][Epoch 8834] diffusion training Loss: 0.06871825084090233
2024-11-05 01:07:13,726 - INFO - [diffusion][Epoch 8834] diffusion learning rate: 0.001
2024-11-05 01:07:13,728 - INFO - [diffusion][Epoch 8834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:13,729 - INFO - [diffusion][Epoch 8835] Epoch 8836/12000
2024-11-05 01:07:16,867 - INFO - [diffusion][Epoch 8835] diffusion training Loss: 0.07109505962580442
2024-11-05 01:07:16,869 - INFO - [diffusion][Epoch 8835] diffusion learning rate: 0.001
2024-11-05 01:07:16,871 - INFO - [diffusion][Epoch 8835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:16,872 - INFO - [diffusion][Epoch 8836] Epoch 8837/12000
2024-11-05 01:07:20,101 - INFO - [diffusion][Epoch 8836] diffusion training Loss: 0.07321743108332157
2024-11-05 01:07:20,103 - INFO - [diffusion][Epoch 8836] diffusion learning rate: 0.001
2024-11-05 01:07:20,105 - INFO - [diffusion][Epoch 8836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:20,106 - INFO - [diffusion][Epoch 8837] Epoch 8838/12000
2024-11-05 01:07:23,472 - INFO - [diffusion][Epoch 8837] diffusion training Loss: 0.06715994514524937
2024-11-05 01:07:23,474 - INFO - [diffusion][Epoch 8837] diffusion learning rate: 0.001
2024-11-05 01:07:23,476 - INFO - [diffusion][Epoch 8837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:23,477 - INFO - [diffusion][Epoch 8838] Epoch 8839/12000
2024-11-05 01:07:27,088 - INFO - [diffusion][Epoch 8838] diffusion training Loss: 0.06969435513019562
2024-11-05 01:07:27,089 - INFO - [diffusion][Epoch 8838] diffusion learning rate: 0.001
2024-11-05 01:07:27,091 - INFO - [diffusion][Epoch 8838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:27,092 - INFO - [diffusion][Epoch 8839] Epoch 8840/12000
2024-11-05 01:07:30,517 - INFO - [diffusion][Epoch 8839] diffusion training Loss: 0.0663080494850874
2024-11-05 01:07:30,723 - INFO - [diffusion][Epoch 8839] diffusion learning rate: 0.001
2024-11-05 01:07:30,725 - INFO - [diffusion][Epoch 8839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:30,726 - INFO - [diffusion][Epoch 8840] Epoch 8841/12000
2024-11-05 01:07:33,827 - INFO - [diffusion][Epoch 8840] diffusion training Loss: 0.0702791279181838
2024-11-05 01:07:33,829 - INFO - [diffusion][Epoch 8840] diffusion learning rate: 0.001
2024-11-05 01:07:33,831 - INFO - [diffusion][Epoch 8840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:33,833 - INFO - [diffusion][Epoch 8841] Epoch 8842/12000
2024-11-05 01:07:36,891 - INFO - [diffusion][Epoch 8841] diffusion training Loss: 0.07432354986667633
2024-11-05 01:07:36,893 - INFO - [diffusion][Epoch 8841] diffusion learning rate: 0.001
2024-11-05 01:07:36,895 - INFO - [diffusion][Epoch 8841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:36,896 - INFO - [diffusion][Epoch 8842] Epoch 8843/12000
2024-11-05 01:07:40,085 - INFO - [diffusion][Epoch 8842] diffusion training Loss: 0.06982450373470783
2024-11-05 01:07:40,087 - INFO - [diffusion][Epoch 8842] diffusion learning rate: 0.001
2024-11-05 01:07:40,089 - INFO - [diffusion][Epoch 8842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:40,090 - INFO - [diffusion][Epoch 8843] Epoch 8844/12000
2024-11-05 01:07:43,623 - INFO - [diffusion][Epoch 8843] diffusion training Loss: 0.06749357096850872
2024-11-05 01:07:43,625 - INFO - [diffusion][Epoch 8843] diffusion learning rate: 0.001
2024-11-05 01:07:43,628 - INFO - [diffusion][Epoch 8843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:43,630 - INFO - [diffusion][Epoch 8844] Epoch 8845/12000
2024-11-05 01:07:47,783 - INFO - [diffusion][Epoch 8844] diffusion training Loss: 0.07589929923415184
2024-11-05 01:07:47,785 - INFO - [diffusion][Epoch 8844] diffusion learning rate: 0.001
2024-11-05 01:07:47,787 - INFO - [diffusion][Epoch 8844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:47,788 - INFO - [diffusion][Epoch 8845] Epoch 8846/12000
2024-11-05 01:07:51,301 - INFO - [diffusion][Epoch 8845] diffusion training Loss: 0.07223382592201233
2024-11-05 01:07:51,303 - INFO - [diffusion][Epoch 8845] diffusion learning rate: 0.001
2024-11-05 01:07:51,305 - INFO - [diffusion][Epoch 8845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:51,306 - INFO - [diffusion][Epoch 8846] Epoch 8847/12000
2024-11-05 01:07:54,433 - INFO - [diffusion][Epoch 8846] diffusion training Loss: 0.07099868077784777
2024-11-05 01:07:54,435 - INFO - [diffusion][Epoch 8846] diffusion learning rate: 0.001
2024-11-05 01:07:54,436 - INFO - [diffusion][Epoch 8846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:54,438 - INFO - [diffusion][Epoch 8847] Epoch 8848/12000
2024-11-05 01:07:57,555 - INFO - [diffusion][Epoch 8847] diffusion training Loss: 0.06981115601956844
2024-11-05 01:07:57,558 - INFO - [diffusion][Epoch 8847] diffusion learning rate: 0.001
2024-11-05 01:07:57,560 - INFO - [diffusion][Epoch 8847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:57,561 - INFO - [diffusion][Epoch 8848] Epoch 8849/12000
2024-11-05 01:08:00,813 - INFO - [diffusion][Epoch 8848] diffusion training Loss: 0.0608404790982604
2024-11-05 01:08:00,815 - INFO - [diffusion][Epoch 8848] diffusion learning rate: 0.001
2024-11-05 01:08:00,816 - INFO - [diffusion][Epoch 8848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:00,818 - INFO - [diffusion][Epoch 8849] Epoch 8850/12000
2024-11-05 01:08:04,313 - INFO - [diffusion][Epoch 8849] diffusion training Loss: 0.06545250210911036
2024-11-05 01:08:04,316 - INFO - [diffusion][Epoch 8849] diffusion learning rate: 0.001
2024-11-05 01:08:04,318 - INFO - [diffusion][Epoch 8849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:04,320 - INFO - [diffusion][Epoch 8850] Epoch 8851/12000
2024-11-05 01:08:08,056 - INFO - [diffusion][Epoch 8850] diffusion training Loss: 0.06714392732828856
2024-11-05 01:08:08,058 - INFO - [diffusion][Epoch 8850] diffusion learning rate: 0.001
2024-11-05 01:08:08,060 - INFO - [diffusion][Epoch 8850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:08,062 - INFO - [diffusion][Epoch 8851] Epoch 8852/12000
2024-11-05 01:08:11,496 - INFO - [diffusion][Epoch 8851] diffusion training Loss: 0.07532137632369995
2024-11-05 01:08:11,498 - INFO - [diffusion][Epoch 8851] diffusion learning rate: 0.001
2024-11-05 01:08:11,500 - INFO - [diffusion][Epoch 8851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:11,501 - INFO - [diffusion][Epoch 8852] Epoch 8853/12000
2024-11-05 01:08:14,607 - INFO - [diffusion][Epoch 8852] diffusion training Loss: 0.06565336883068085
2024-11-05 01:08:14,609 - INFO - [diffusion][Epoch 8852] diffusion learning rate: 0.001
2024-11-05 01:08:14,611 - INFO - [diffusion][Epoch 8852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:14,612 - INFO - [diffusion][Epoch 8853] Epoch 8854/12000
2024-11-05 01:08:17,831 - INFO - [diffusion][Epoch 8853] diffusion training Loss: 0.06842507980763912
2024-11-05 01:08:17,833 - INFO - [diffusion][Epoch 8853] diffusion learning rate: 0.001
2024-11-05 01:08:17,835 - INFO - [diffusion][Epoch 8853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:17,836 - INFO - [diffusion][Epoch 8854] Epoch 8855/12000
2024-11-05 01:08:21,145 - INFO - [diffusion][Epoch 8854] diffusion training Loss: 0.0688070710748434
2024-11-05 01:08:21,146 - INFO - [diffusion][Epoch 8854] diffusion learning rate: 0.001
2024-11-05 01:08:21,148 - INFO - [diffusion][Epoch 8854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:21,149 - INFO - [diffusion][Epoch 8855] Epoch 8856/12000
2024-11-05 01:08:24,731 - INFO - [diffusion][Epoch 8855] diffusion training Loss: 0.06482903100550175
2024-11-05 01:08:24,733 - INFO - [diffusion][Epoch 8855] diffusion learning rate: 0.001
2024-11-05 01:08:24,735 - INFO - [diffusion][Epoch 8855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:24,736 - INFO - [diffusion][Epoch 8856] Epoch 8857/12000
2024-11-05 01:08:28,256 - INFO - [diffusion][Epoch 8856] diffusion training Loss: 0.07077248021960258
2024-11-05 01:08:28,258 - INFO - [diffusion][Epoch 8856] diffusion learning rate: 0.001
2024-11-05 01:08:28,260 - INFO - [diffusion][Epoch 8856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:28,261 - INFO - [diffusion][Epoch 8857] Epoch 8858/12000
2024-11-05 01:08:31,300 - INFO - [diffusion][Epoch 8857] diffusion training Loss: 0.06719198450446129
2024-11-05 01:08:31,301 - INFO - [diffusion][Epoch 8857] diffusion learning rate: 0.001
2024-11-05 01:08:31,303 - INFO - [diffusion][Epoch 8857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:31,304 - INFO - [diffusion][Epoch 8858] Epoch 8859/12000
2024-11-05 01:08:34,413 - INFO - [diffusion][Epoch 8858] diffusion training Loss: 0.06805430725216866
2024-11-05 01:08:34,416 - INFO - [diffusion][Epoch 8858] diffusion learning rate: 0.001
2024-11-05 01:08:34,418 - INFO - [diffusion][Epoch 8858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:34,419 - INFO - [diffusion][Epoch 8859] Epoch 8860/12000
2024-11-05 01:08:37,749 - INFO - [diffusion][Epoch 8859] diffusion training Loss: 0.06714930105954409
2024-11-05 01:08:37,751 - INFO - [diffusion][Epoch 8859] diffusion learning rate: 0.001
2024-11-05 01:08:37,752 - INFO - [diffusion][Epoch 8859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:37,754 - INFO - [diffusion][Epoch 8860] Epoch 8861/12000
2024-11-05 01:08:41,509 - INFO - [diffusion][Epoch 8860] diffusion training Loss: 0.06668866239488125
2024-11-05 01:08:41,510 - INFO - [diffusion][Epoch 8860] diffusion learning rate: 0.001
2024-11-05 01:08:41,512 - INFO - [diffusion][Epoch 8860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:41,513 - INFO - [diffusion][Epoch 8861] Epoch 8862/12000
2024-11-05 01:08:44,862 - INFO - [diffusion][Epoch 8861] diffusion training Loss: 0.06935848668217659
2024-11-05 01:08:44,864 - INFO - [diffusion][Epoch 8861] diffusion learning rate: 0.001
2024-11-05 01:08:44,865 - INFO - [diffusion][Epoch 8861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:44,866 - INFO - [diffusion][Epoch 8862] Epoch 8863/12000
2024-11-05 01:08:47,967 - INFO - [diffusion][Epoch 8862] diffusion training Loss: 0.06511667557060719
2024-11-05 01:08:47,968 - INFO - [diffusion][Epoch 8862] diffusion learning rate: 0.001
2024-11-05 01:08:47,970 - INFO - [diffusion][Epoch 8862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:47,972 - INFO - [diffusion][Epoch 8863] Epoch 8864/12000
2024-11-05 01:08:51,142 - INFO - [diffusion][Epoch 8863] diffusion training Loss: 0.06805542856454849
2024-11-05 01:08:51,144 - INFO - [diffusion][Epoch 8863] diffusion learning rate: 0.001
2024-11-05 01:08:51,145 - INFO - [diffusion][Epoch 8863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:51,147 - INFO - [diffusion][Epoch 8864] Epoch 8865/12000
2024-11-05 01:08:54,619 - INFO - [diffusion][Epoch 8864] diffusion training Loss: 0.06365714967250824
2024-11-05 01:08:54,621 - INFO - [diffusion][Epoch 8864] diffusion learning rate: 0.001
2024-11-05 01:08:54,623 - INFO - [diffusion][Epoch 8864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:54,624 - INFO - [diffusion][Epoch 8865] Epoch 8866/12000
2024-11-05 01:08:58,417 - INFO - [diffusion][Epoch 8865] diffusion training Loss: 0.06591573171317577
2024-11-05 01:08:58,420 - INFO - [diffusion][Epoch 8865] diffusion learning rate: 0.001
2024-11-05 01:08:58,421 - INFO - [diffusion][Epoch 8865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:58,423 - INFO - [diffusion][Epoch 8866] Epoch 8867/12000
2024-11-05 01:09:01,799 - INFO - [diffusion][Epoch 8866] diffusion training Loss: 0.06449643429368734
2024-11-05 01:09:01,841 - INFO - [diffusion][Epoch 8866] diffusion learning rate: 0.001
2024-11-05 01:09:01,842 - INFO - [diffusion][Epoch 8866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:01,844 - INFO - [diffusion][Epoch 8867] Epoch 8868/12000
2024-11-05 01:09:04,969 - INFO - [diffusion][Epoch 8867] diffusion training Loss: 0.06264952570199966
2024-11-05 01:09:04,971 - INFO - [diffusion][Epoch 8867] diffusion learning rate: 0.001
2024-11-05 01:09:04,973 - INFO - [diffusion][Epoch 8867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:04,974 - INFO - [diffusion][Epoch 8868] Epoch 8869/12000
2024-11-05 01:09:08,205 - INFO - [diffusion][Epoch 8868] diffusion training Loss: 0.06891747377812862
2024-11-05 01:09:08,207 - INFO - [diffusion][Epoch 8868] diffusion learning rate: 0.001
2024-11-05 01:09:08,209 - INFO - [diffusion][Epoch 8868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:08,210 - INFO - [diffusion][Epoch 8869] Epoch 8870/12000
2024-11-05 01:09:11,470 - INFO - [diffusion][Epoch 8869] diffusion training Loss: 0.06860538385808468
2024-11-05 01:09:11,472 - INFO - [diffusion][Epoch 8869] diffusion learning rate: 0.001
2024-11-05 01:09:11,473 - INFO - [diffusion][Epoch 8869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:11,474 - INFO - [diffusion][Epoch 8870] Epoch 8871/12000
2024-11-05 01:09:15,187 - INFO - [diffusion][Epoch 8870] diffusion training Loss: 0.06874407455325127
2024-11-05 01:09:15,189 - INFO - [diffusion][Epoch 8870] diffusion learning rate: 0.001
2024-11-05 01:09:15,191 - INFO - [diffusion][Epoch 8870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:15,192 - INFO - [diffusion][Epoch 8871] Epoch 8872/12000
2024-11-05 01:09:18,684 - INFO - [diffusion][Epoch 8871] diffusion training Loss: 0.07473255321383476
2024-11-05 01:09:18,686 - INFO - [diffusion][Epoch 8871] diffusion learning rate: 0.001
2024-11-05 01:09:18,688 - INFO - [diffusion][Epoch 8871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:18,689 - INFO - [diffusion][Epoch 8872] Epoch 8873/12000
2024-11-05 01:09:21,383 - INFO - [diffusion][Epoch 8872] diffusion training Loss: 0.06702163349837065
2024-11-05 01:09:21,386 - INFO - [diffusion][Epoch 8872] diffusion learning rate: 0.001
2024-11-05 01:09:21,387 - INFO - [diffusion][Epoch 8872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:21,389 - INFO - [diffusion][Epoch 8873] Epoch 8874/12000
2024-11-05 01:09:24,540 - INFO - [diffusion][Epoch 8873] diffusion training Loss: 0.06412981823086739
2024-11-05 01:09:24,542 - INFO - [diffusion][Epoch 8873] diffusion learning rate: 0.001
2024-11-05 01:09:24,544 - INFO - [diffusion][Epoch 8873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:24,545 - INFO - [diffusion][Epoch 8874] Epoch 8875/12000
2024-11-05 01:09:28,217 - INFO - [diffusion][Epoch 8874] diffusion training Loss: 0.06421366799622774
2024-11-05 01:09:28,220 - INFO - [diffusion][Epoch 8874] diffusion learning rate: 0.001
2024-11-05 01:09:28,222 - INFO - [diffusion][Epoch 8874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:28,223 - INFO - [diffusion][Epoch 8875] Epoch 8876/12000
2024-11-05 01:09:31,632 - INFO - [diffusion][Epoch 8875] diffusion training Loss: 0.06841974519193172
2024-11-05 01:09:31,634 - INFO - [diffusion][Epoch 8875] diffusion learning rate: 0.001
2024-11-05 01:09:31,636 - INFO - [diffusion][Epoch 8875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:31,637 - INFO - [diffusion][Epoch 8876] Epoch 8877/12000
2024-11-05 01:09:34,718 - INFO - [diffusion][Epoch 8876] diffusion training Loss: 0.06732454616576433
2024-11-05 01:09:34,721 - INFO - [diffusion][Epoch 8876] diffusion learning rate: 0.001
2024-11-05 01:09:34,723 - INFO - [diffusion][Epoch 8876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:34,724 - INFO - [diffusion][Epoch 8877] Epoch 8878/12000
2024-11-05 01:09:37,838 - INFO - [diffusion][Epoch 8877] diffusion training Loss: 0.06867635995149612
2024-11-05 01:09:37,841 - INFO - [diffusion][Epoch 8877] diffusion learning rate: 0.001
2024-11-05 01:09:37,842 - INFO - [diffusion][Epoch 8877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:37,844 - INFO - [diffusion][Epoch 8878] Epoch 8879/12000
2024-11-05 01:09:41,269 - INFO - [diffusion][Epoch 8878] diffusion training Loss: 0.07380556873977184
2024-11-05 01:09:41,270 - INFO - [diffusion][Epoch 8878] diffusion learning rate: 0.001
2024-11-05 01:09:41,272 - INFO - [diffusion][Epoch 8878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:41,273 - INFO - [diffusion][Epoch 8879] Epoch 8880/12000
2024-11-05 01:09:44,941 - INFO - [diffusion][Epoch 8879] diffusion training Loss: 0.06222989410161972
2024-11-05 01:09:44,943 - INFO - [diffusion][Epoch 8879] diffusion learning rate: 0.001
2024-11-05 01:09:44,985 - INFO - [diffusion][Epoch 8879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:44,987 - INFO - [diffusion][Epoch 8880] Epoch 8881/12000
2024-11-05 01:09:48,200 - INFO - [diffusion][Epoch 8880] diffusion training Loss: 0.06831196881830692
2024-11-05 01:09:48,205 - INFO - [diffusion][Epoch 8880] diffusion learning rate: 0.001
2024-11-05 01:09:48,207 - INFO - [diffusion][Epoch 8880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:48,208 - INFO - [diffusion][Epoch 8881] Epoch 8882/12000
2024-11-05 01:09:51,314 - INFO - [diffusion][Epoch 8881] diffusion training Loss: 0.06853470578789711
2024-11-05 01:09:51,315 - INFO - [diffusion][Epoch 8881] diffusion learning rate: 0.001
2024-11-05 01:09:51,317 - INFO - [diffusion][Epoch 8881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:51,318 - INFO - [diffusion][Epoch 8882] Epoch 8883/12000
2024-11-05 01:09:54,435 - INFO - [diffusion][Epoch 8882] diffusion training Loss: 0.07193814963102341
2024-11-05 01:09:54,437 - INFO - [diffusion][Epoch 8882] diffusion learning rate: 0.001
2024-11-05 01:09:54,438 - INFO - [diffusion][Epoch 8882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:54,439 - INFO - [diffusion][Epoch 8883] Epoch 8884/12000
2024-11-05 01:09:57,990 - INFO - [diffusion][Epoch 8883] diffusion training Loss: 0.06895114108920097
2024-11-05 01:09:57,992 - INFO - [diffusion][Epoch 8883] diffusion learning rate: 0.001
2024-11-05 01:09:57,994 - INFO - [diffusion][Epoch 8883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:57,995 - INFO - [diffusion][Epoch 8884] Epoch 8885/12000
2024-11-05 01:10:01,559 - INFO - [diffusion][Epoch 8884] diffusion training Loss: 0.0709020234644413
2024-11-05 01:10:01,562 - INFO - [diffusion][Epoch 8884] diffusion learning rate: 0.001
2024-11-05 01:10:01,563 - INFO - [diffusion][Epoch 8884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:01,564 - INFO - [diffusion][Epoch 8885] Epoch 8886/12000
2024-11-05 01:10:05,490 - INFO - [diffusion][Epoch 8885] diffusion training Loss: 0.06888006255030632
2024-11-05 01:10:05,492 - INFO - [diffusion][Epoch 8885] diffusion learning rate: 0.001
2024-11-05 01:10:05,494 - INFO - [diffusion][Epoch 8885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:05,495 - INFO - [diffusion][Epoch 8886] Epoch 8887/12000
2024-11-05 01:10:08,600 - INFO - [diffusion][Epoch 8886] diffusion training Loss: 0.0736799631267786
2024-11-05 01:10:08,603 - INFO - [diffusion][Epoch 8886] diffusion learning rate: 0.001
2024-11-05 01:10:08,605 - INFO - [diffusion][Epoch 8886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:08,607 - INFO - [diffusion][Epoch 8887] Epoch 8888/12000
2024-11-05 01:10:11,646 - INFO - [diffusion][Epoch 8887] diffusion training Loss: 0.07580778189003468
2024-11-05 01:10:11,648 - INFO - [diffusion][Epoch 8887] diffusion learning rate: 0.001
2024-11-05 01:10:11,650 - INFO - [diffusion][Epoch 8887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:11,651 - INFO - [diffusion][Epoch 8888] Epoch 8889/12000
2024-11-05 01:10:15,076 - INFO - [diffusion][Epoch 8888] diffusion training Loss: 0.06977809220552444
2024-11-05 01:10:15,079 - INFO - [diffusion][Epoch 8888] diffusion learning rate: 0.001
2024-11-05 01:10:15,081 - INFO - [diffusion][Epoch 8888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:15,082 - INFO - [diffusion][Epoch 8889] Epoch 8890/12000
2024-11-05 01:10:18,729 - INFO - [diffusion][Epoch 8889] diffusion training Loss: 0.06741737108677626
2024-11-05 01:10:18,731 - INFO - [diffusion][Epoch 8889] diffusion learning rate: 0.001
2024-11-05 01:10:18,732 - INFO - [diffusion][Epoch 8889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:18,734 - INFO - [diffusion][Epoch 8890] Epoch 8891/12000
2024-11-05 01:10:22,008 - INFO - [diffusion][Epoch 8890] diffusion training Loss: 0.07307895459234715
2024-11-05 01:10:22,010 - INFO - [diffusion][Epoch 8890] diffusion learning rate: 0.001
2024-11-05 01:10:22,012 - INFO - [diffusion][Epoch 8890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:22,013 - INFO - [diffusion][Epoch 8891] Epoch 8892/12000
2024-11-05 01:10:25,090 - INFO - [diffusion][Epoch 8891] diffusion training Loss: 0.06963969394564629
2024-11-05 01:10:25,093 - INFO - [diffusion][Epoch 8891] diffusion learning rate: 0.001
2024-11-05 01:10:25,095 - INFO - [diffusion][Epoch 8891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:25,097 - INFO - [diffusion][Epoch 8892] Epoch 8893/12000
2024-11-05 01:10:28,197 - INFO - [diffusion][Epoch 8892] diffusion training Loss: 0.07177956774830818
2024-11-05 01:10:28,199 - INFO - [diffusion][Epoch 8892] diffusion learning rate: 0.001
2024-11-05 01:10:28,201 - INFO - [diffusion][Epoch 8892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:28,202 - INFO - [diffusion][Epoch 8893] Epoch 8894/12000
2024-11-05 01:10:31,814 - INFO - [diffusion][Epoch 8893] diffusion training Loss: 0.06983137410134077
2024-11-05 01:10:31,816 - INFO - [diffusion][Epoch 8893] diffusion learning rate: 0.001
2024-11-05 01:10:31,818 - INFO - [diffusion][Epoch 8893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:31,819 - INFO - [diffusion][Epoch 8894] Epoch 8895/12000
2024-11-05 01:10:35,189 - INFO - [diffusion][Epoch 8894] diffusion training Loss: 0.0732323657721281
2024-11-05 01:10:35,192 - INFO - [diffusion][Epoch 8894] diffusion learning rate: 0.001
2024-11-05 01:10:35,193 - INFO - [diffusion][Epoch 8894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:35,195 - INFO - [diffusion][Epoch 8895] Epoch 8896/12000
2024-11-05 01:10:38,295 - INFO - [diffusion][Epoch 8895] diffusion training Loss: 0.0698253232985735
2024-11-05 01:10:38,297 - INFO - [diffusion][Epoch 8895] diffusion learning rate: 0.001
2024-11-05 01:10:38,299 - INFO - [diffusion][Epoch 8895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:38,300 - INFO - [diffusion][Epoch 8896] Epoch 8897/12000
2024-11-05 01:10:41,388 - INFO - [diffusion][Epoch 8896] diffusion training Loss: 0.06761765107512474
2024-11-05 01:10:41,390 - INFO - [diffusion][Epoch 8896] diffusion learning rate: 0.001
2024-11-05 01:10:41,392 - INFO - [diffusion][Epoch 8896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:41,393 - INFO - [diffusion][Epoch 8897] Epoch 8898/12000
2024-11-05 01:10:44,876 - INFO - [diffusion][Epoch 8897] diffusion training Loss: 0.07286659255623817
2024-11-05 01:10:44,878 - INFO - [diffusion][Epoch 8897] diffusion learning rate: 0.001
2024-11-05 01:10:44,880 - INFO - [diffusion][Epoch 8897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:44,881 - INFO - [diffusion][Epoch 8898] Epoch 8899/12000
2024-11-05 01:10:48,478 - INFO - [diffusion][Epoch 8898] diffusion training Loss: 0.06540141720324755
2024-11-05 01:10:48,479 - INFO - [diffusion][Epoch 8898] diffusion learning rate: 0.001
2024-11-05 01:10:48,481 - INFO - [diffusion][Epoch 8898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:48,482 - INFO - [diffusion][Epoch 8899] Epoch 8900/12000
2024-11-05 01:10:51,560 - INFO - [diffusion][Epoch 8899] diffusion training Loss: 0.07259315252304077
2024-11-05 01:10:51,562 - INFO - [diffusion][Epoch 8899] diffusion learning rate: 0.001
2024-11-05 01:10:51,564 - INFO - [diffusion][Epoch 8899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:51,565 - INFO - [diffusion][Epoch 8900] Epoch 8901/12000
2024-11-05 01:10:54,395 - INFO - [diffusion][Epoch 8900] diffusion training Loss: 0.06769455689936876
2024-11-05 01:10:54,397 - INFO - [diffusion][Epoch 8900] diffusion learning rate: 0.001
2024-11-05 01:10:54,399 - INFO - [diffusion][Epoch 8900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:54,401 - INFO - [diffusion][Epoch 8901] Epoch 8902/12000
2024-11-05 01:10:57,932 - INFO - [diffusion][Epoch 8901] diffusion training Loss: 0.07024613954126835
2024-11-05 01:10:57,934 - INFO - [diffusion][Epoch 8901] diffusion learning rate: 0.001
2024-11-05 01:10:57,935 - INFO - [diffusion][Epoch 8901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:57,937 - INFO - [diffusion][Epoch 8902] Epoch 8903/12000
2024-11-05 01:11:01,552 - INFO - [diffusion][Epoch 8902] diffusion training Loss: 0.0689063798636198
2024-11-05 01:11:01,554 - INFO - [diffusion][Epoch 8902] diffusion learning rate: 0.001
2024-11-05 01:11:01,556 - INFO - [diffusion][Epoch 8902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:01,557 - INFO - [diffusion][Epoch 8903] Epoch 8904/12000
2024-11-05 01:11:04,694 - INFO - [diffusion][Epoch 8903] diffusion training Loss: 0.07122797518968582
2024-11-05 01:11:04,696 - INFO - [diffusion][Epoch 8903] diffusion learning rate: 0.001
2024-11-05 01:11:04,698 - INFO - [diffusion][Epoch 8903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:04,699 - INFO - [diffusion][Epoch 8904] Epoch 8905/12000
2024-11-05 01:11:07,773 - INFO - [diffusion][Epoch 8904] diffusion training Loss: 0.061803731136024
2024-11-05 01:11:07,775 - INFO - [diffusion][Epoch 8904] diffusion learning rate: 0.001
2024-11-05 01:11:07,777 - INFO - [diffusion][Epoch 8904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:07,780 - INFO - [diffusion][Epoch 8905] Epoch 8906/12000
2024-11-05 01:11:10,916 - INFO - [diffusion][Epoch 8905] diffusion training Loss: 0.06803452409803867
2024-11-05 01:11:10,918 - INFO - [diffusion][Epoch 8905] diffusion learning rate: 0.001
2024-11-05 01:11:10,921 - INFO - [diffusion][Epoch 8905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:10,924 - INFO - [diffusion][Epoch 8906] Epoch 8907/12000
2024-11-05 01:11:14,454 - INFO - [diffusion][Epoch 8906] diffusion training Loss: 0.06827526725828648
2024-11-05 01:11:14,456 - INFO - [diffusion][Epoch 8906] diffusion learning rate: 0.001
2024-11-05 01:11:14,458 - INFO - [diffusion][Epoch 8906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:14,459 - INFO - [diffusion][Epoch 8907] Epoch 8908/12000
2024-11-05 01:11:17,962 - INFO - [diffusion][Epoch 8907] diffusion training Loss: 0.06936491280794144
2024-11-05 01:11:17,963 - INFO - [diffusion][Epoch 8907] diffusion learning rate: 0.001
2024-11-05 01:11:17,965 - INFO - [diffusion][Epoch 8907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:17,966 - INFO - [diffusion][Epoch 8908] Epoch 8909/12000
2024-11-05 01:11:21,025 - INFO - [diffusion][Epoch 8908] diffusion training Loss: 0.06986942049115896
2024-11-05 01:11:21,027 - INFO - [diffusion][Epoch 8908] diffusion learning rate: 0.001
2024-11-05 01:11:21,028 - INFO - [diffusion][Epoch 8908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:21,030 - INFO - [diffusion][Epoch 8909] Epoch 8910/12000
2024-11-05 01:11:24,168 - INFO - [diffusion][Epoch 8909] diffusion training Loss: 0.06981511693447828
2024-11-05 01:11:24,170 - INFO - [diffusion][Epoch 8909] diffusion learning rate: 0.001
2024-11-05 01:11:24,173 - INFO - [diffusion][Epoch 8909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:24,174 - INFO - [diffusion][Epoch 8910] Epoch 8911/12000
2024-11-05 01:11:27,401 - INFO - [diffusion][Epoch 8910] diffusion training Loss: 0.07556278631091118
2024-11-05 01:11:27,404 - INFO - [diffusion][Epoch 8910] diffusion learning rate: 0.001
2024-11-05 01:11:27,405 - INFO - [diffusion][Epoch 8910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:27,407 - INFO - [diffusion][Epoch 8911] Epoch 8912/12000
2024-11-05 01:11:30,994 - INFO - [diffusion][Epoch 8911] diffusion training Loss: 0.06837613694369793
2024-11-05 01:11:30,996 - INFO - [diffusion][Epoch 8911] diffusion learning rate: 0.001
2024-11-05 01:11:30,998 - INFO - [diffusion][Epoch 8911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:30,999 - INFO - [diffusion][Epoch 8912] Epoch 8913/12000
2024-11-05 01:11:34,302 - INFO - [diffusion][Epoch 8912] diffusion training Loss: 0.06681417487561703
2024-11-05 01:11:34,304 - INFO - [diffusion][Epoch 8912] diffusion learning rate: 0.001
2024-11-05 01:11:34,306 - INFO - [diffusion][Epoch 8912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:34,307 - INFO - [diffusion][Epoch 8913] Epoch 8914/12000
2024-11-05 01:11:37,361 - INFO - [diffusion][Epoch 8913] diffusion training Loss: 0.07256893254816532
2024-11-05 01:11:37,364 - INFO - [diffusion][Epoch 8913] diffusion learning rate: 0.001
2024-11-05 01:11:37,366 - INFO - [diffusion][Epoch 8913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:37,367 - INFO - [diffusion][Epoch 8914] Epoch 8915/12000
2024-11-05 01:11:40,452 - INFO - [diffusion][Epoch 8914] diffusion training Loss: 0.06991785392165184
2024-11-05 01:11:40,454 - INFO - [diffusion][Epoch 8914] diffusion learning rate: 0.001
2024-11-05 01:11:40,456 - INFO - [diffusion][Epoch 8914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:40,457 - INFO - [diffusion][Epoch 8915] Epoch 8916/12000
2024-11-05 01:11:44,081 - INFO - [diffusion][Epoch 8915] diffusion training Loss: 0.07009379379451275
2024-11-05 01:11:44,083 - INFO - [diffusion][Epoch 8915] diffusion learning rate: 0.001
2024-11-05 01:11:44,085 - INFO - [diffusion][Epoch 8915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:44,086 - INFO - [diffusion][Epoch 8916] Epoch 8917/12000
2024-11-05 01:11:47,480 - INFO - [diffusion][Epoch 8916] diffusion training Loss: 0.06563581619411707
2024-11-05 01:11:47,482 - INFO - [diffusion][Epoch 8916] diffusion learning rate: 0.001
2024-11-05 01:11:47,484 - INFO - [diffusion][Epoch 8916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:47,485 - INFO - [diffusion][Epoch 8917] Epoch 8918/12000
2024-11-05 01:11:50,501 - INFO - [diffusion][Epoch 8917] diffusion training Loss: 0.07436721585690975
2024-11-05 01:11:50,503 - INFO - [diffusion][Epoch 8917] diffusion learning rate: 0.001
2024-11-05 01:11:50,505 - INFO - [diffusion][Epoch 8917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:50,506 - INFO - [diffusion][Epoch 8918] Epoch 8919/12000
2024-11-05 01:11:53,575 - INFO - [diffusion][Epoch 8918] diffusion training Loss: 0.06856825668364763
2024-11-05 01:11:53,577 - INFO - [diffusion][Epoch 8918] diffusion learning rate: 0.001
2024-11-05 01:11:53,579 - INFO - [diffusion][Epoch 8918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:53,580 - INFO - [diffusion][Epoch 8919] Epoch 8920/12000
2024-11-05 01:11:57,133 - INFO - [diffusion][Epoch 8919] diffusion training Loss: 0.06416629627346992
2024-11-05 01:11:57,135 - INFO - [diffusion][Epoch 8919] diffusion learning rate: 0.001
2024-11-05 01:11:57,137 - INFO - [diffusion][Epoch 8919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:57,138 - INFO - [diffusion][Epoch 8920] Epoch 8921/12000
2024-11-05 01:12:00,670 - INFO - [diffusion][Epoch 8920] diffusion training Loss: 0.07040157914161682
2024-11-05 01:12:00,672 - INFO - [diffusion][Epoch 8920] diffusion learning rate: 0.001
2024-11-05 01:12:00,674 - INFO - [diffusion][Epoch 8920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:00,675 - INFO - [diffusion][Epoch 8921] Epoch 8922/12000
2024-11-05 01:12:03,772 - INFO - [diffusion][Epoch 8921] diffusion training Loss: 0.06988821364939213
2024-11-05 01:12:03,774 - INFO - [diffusion][Epoch 8921] diffusion learning rate: 0.001
2024-11-05 01:12:03,776 - INFO - [diffusion][Epoch 8921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:03,777 - INFO - [diffusion][Epoch 8922] Epoch 8923/12000
2024-11-05 01:12:06,878 - INFO - [diffusion][Epoch 8922] diffusion training Loss: 0.0710073821246624
2024-11-05 01:12:06,880 - INFO - [diffusion][Epoch 8922] diffusion learning rate: 0.001
2024-11-05 01:12:06,882 - INFO - [diffusion][Epoch 8922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:06,883 - INFO - [diffusion][Epoch 8923] Epoch 8924/12000
2024-11-05 01:12:10,176 - INFO - [diffusion][Epoch 8923] diffusion training Loss: 0.06666244566440582
2024-11-05 01:12:10,178 - INFO - [diffusion][Epoch 8923] diffusion learning rate: 0.001
2024-11-05 01:12:10,180 - INFO - [diffusion][Epoch 8923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:10,181 - INFO - [diffusion][Epoch 8924] Epoch 8925/12000
2024-11-05 01:12:13,792 - INFO - [diffusion][Epoch 8924] diffusion training Loss: 0.06418121326714754
2024-11-05 01:12:13,794 - INFO - [diffusion][Epoch 8924] diffusion learning rate: 0.001
2024-11-05 01:12:13,796 - INFO - [diffusion][Epoch 8924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:13,797 - INFO - [diffusion][Epoch 8925] Epoch 8926/12000
2024-11-05 01:12:17,303 - INFO - [diffusion][Epoch 8925] diffusion training Loss: 0.06379340589046478
2024-11-05 01:12:17,305 - INFO - [diffusion][Epoch 8925] diffusion learning rate: 0.001
2024-11-05 01:12:17,307 - INFO - [diffusion][Epoch 8925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:17,309 - INFO - [diffusion][Epoch 8926] Epoch 8927/12000
2024-11-05 01:12:20,954 - INFO - [diffusion][Epoch 8926] diffusion training Loss: 0.06262160278856754
2024-11-05 01:12:20,956 - INFO - [diffusion][Epoch 8926] diffusion learning rate: 0.001
2024-11-05 01:12:20,958 - INFO - [diffusion][Epoch 8926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:20,959 - INFO - [diffusion][Epoch 8927] Epoch 8928/12000
2024-11-05 01:12:24,035 - INFO - [diffusion][Epoch 8927] diffusion training Loss: 0.06436918769031763
2024-11-05 01:12:24,037 - INFO - [diffusion][Epoch 8927] diffusion learning rate: 0.001
2024-11-05 01:12:24,039 - INFO - [diffusion][Epoch 8927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:24,040 - INFO - [diffusion][Epoch 8928] Epoch 8929/12000
2024-11-05 01:12:27,135 - INFO - [diffusion][Epoch 8928] diffusion training Loss: 0.07636979594826698
2024-11-05 01:12:27,138 - INFO - [diffusion][Epoch 8928] diffusion learning rate: 0.001
2024-11-05 01:12:27,140 - INFO - [diffusion][Epoch 8928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:27,142 - INFO - [diffusion][Epoch 8929] Epoch 8930/12000
2024-11-05 01:12:30,831 - INFO - [diffusion][Epoch 8929] diffusion training Loss: 0.06804757751524448
2024-11-05 01:12:30,833 - INFO - [diffusion][Epoch 8929] diffusion learning rate: 0.001
2024-11-05 01:12:30,835 - INFO - [diffusion][Epoch 8929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:30,837 - INFO - [diffusion][Epoch 8930] Epoch 8931/12000
2024-11-05 01:12:34,325 - INFO - [diffusion][Epoch 8930] diffusion training Loss: 0.07130560092628002
2024-11-05 01:12:34,327 - INFO - [diffusion][Epoch 8930] diffusion learning rate: 0.001
2024-11-05 01:12:34,329 - INFO - [diffusion][Epoch 8930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:34,330 - INFO - [diffusion][Epoch 8931] Epoch 8932/12000
2024-11-05 01:12:37,347 - INFO - [diffusion][Epoch 8931] diffusion training Loss: 0.06693544611334801
2024-11-05 01:12:37,349 - INFO - [diffusion][Epoch 8931] diffusion learning rate: 0.001
2024-11-05 01:12:37,351 - INFO - [diffusion][Epoch 8931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:37,352 - INFO - [diffusion][Epoch 8932] Epoch 8933/12000
2024-11-05 01:12:40,461 - INFO - [diffusion][Epoch 8932] diffusion training Loss: 0.07101775705814362
2024-11-05 01:12:40,464 - INFO - [diffusion][Epoch 8932] diffusion learning rate: 0.001
2024-11-05 01:12:40,465 - INFO - [diffusion][Epoch 8932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:40,467 - INFO - [diffusion][Epoch 8933] Epoch 8934/12000
2024-11-05 01:12:43,947 - INFO - [diffusion][Epoch 8933] diffusion training Loss: 0.07047591730952263
2024-11-05 01:12:43,949 - INFO - [diffusion][Epoch 8933] diffusion learning rate: 0.001
2024-11-05 01:12:43,951 - INFO - [diffusion][Epoch 8933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:43,952 - INFO - [diffusion][Epoch 8934] Epoch 8935/12000
2024-11-05 01:12:47,527 - INFO - [diffusion][Epoch 8934] diffusion training Loss: 0.07171404734253883
2024-11-05 01:12:47,529 - INFO - [diffusion][Epoch 8934] diffusion learning rate: 0.001
2024-11-05 01:12:47,531 - INFO - [diffusion][Epoch 8934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:47,533 - INFO - [diffusion][Epoch 8935] Epoch 8936/12000
2024-11-05 01:12:50,619 - INFO - [diffusion][Epoch 8935] diffusion training Loss: 0.06888632662594318
2024-11-05 01:12:50,621 - INFO - [diffusion][Epoch 8935] diffusion learning rate: 0.001
2024-11-05 01:12:50,623 - INFO - [diffusion][Epoch 8935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:50,624 - INFO - [diffusion][Epoch 8936] Epoch 8937/12000
2024-11-05 01:12:53,741 - INFO - [diffusion][Epoch 8936] diffusion training Loss: 0.06620769202709198
2024-11-05 01:12:53,743 - INFO - [diffusion][Epoch 8936] diffusion learning rate: 0.001
2024-11-05 01:12:53,745 - INFO - [diffusion][Epoch 8936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:53,746 - INFO - [diffusion][Epoch 8937] Epoch 8938/12000
2024-11-05 01:12:56,843 - INFO - [diffusion][Epoch 8937] diffusion training Loss: 0.0714450255036354
2024-11-05 01:12:56,845 - INFO - [diffusion][Epoch 8937] diffusion learning rate: 0.001
2024-11-05 01:12:56,847 - INFO - [diffusion][Epoch 8937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:56,848 - INFO - [diffusion][Epoch 8938] Epoch 8939/12000
2024-11-05 01:13:00,502 - INFO - [diffusion][Epoch 8938] diffusion training Loss: 0.06940391846001148
2024-11-05 01:13:00,504 - INFO - [diffusion][Epoch 8938] diffusion learning rate: 0.001
2024-11-05 01:13:00,506 - INFO - [diffusion][Epoch 8938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:00,507 - INFO - [diffusion][Epoch 8939] Epoch 8940/12000
2024-11-05 01:13:03,975 - INFO - [diffusion][Epoch 8939] diffusion training Loss: 0.06816244125366211
2024-11-05 01:13:03,977 - INFO - [diffusion][Epoch 8939] diffusion learning rate: 0.001
2024-11-05 01:13:03,998 - INFO - [diffusion][Epoch 8939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:03,999 - INFO - [diffusion][Epoch 8940] Epoch 8941/12000
2024-11-05 01:13:07,065 - INFO - [diffusion][Epoch 8940] diffusion training Loss: 0.07606516219675541
2024-11-05 01:13:07,067 - INFO - [diffusion][Epoch 8940] diffusion learning rate: 0.001
2024-11-05 01:13:07,069 - INFO - [diffusion][Epoch 8940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:07,071 - INFO - [diffusion][Epoch 8941] Epoch 8942/12000
2024-11-05 01:13:10,152 - INFO - [diffusion][Epoch 8941] diffusion training Loss: 0.07682787626981735
2024-11-05 01:13:10,153 - INFO - [diffusion][Epoch 8941] diffusion learning rate: 0.001
2024-11-05 01:13:10,155 - INFO - [diffusion][Epoch 8941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:10,157 - INFO - [diffusion][Epoch 8942] Epoch 8943/12000
2024-11-05 01:13:13,630 - INFO - [diffusion][Epoch 8942] diffusion training Loss: 0.07498519122600555
2024-11-05 01:13:13,632 - INFO - [diffusion][Epoch 8942] diffusion learning rate: 0.001
2024-11-05 01:13:13,633 - INFO - [diffusion][Epoch 8942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:13,634 - INFO - [diffusion][Epoch 8943] Epoch 8944/12000
2024-11-05 01:13:17,131 - INFO - [diffusion][Epoch 8943] diffusion training Loss: 0.07933353446424007
2024-11-05 01:13:17,133 - INFO - [diffusion][Epoch 8943] diffusion learning rate: 0.001
2024-11-05 01:13:17,135 - INFO - [diffusion][Epoch 8943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:17,137 - INFO - [diffusion][Epoch 8944] Epoch 8945/12000
2024-11-05 01:13:20,271 - INFO - [diffusion][Epoch 8944] diffusion training Loss: 0.06832498870790005
2024-11-05 01:13:20,273 - INFO - [diffusion][Epoch 8944] diffusion learning rate: 0.001
2024-11-05 01:13:20,327 - INFO - [diffusion][Epoch 8944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:20,329 - INFO - [diffusion][Epoch 8945] Epoch 8946/12000
2024-11-05 01:13:23,429 - INFO - [diffusion][Epoch 8945] diffusion training Loss: 0.07109727337956429
2024-11-05 01:13:23,431 - INFO - [diffusion][Epoch 8945] diffusion learning rate: 0.001
2024-11-05 01:13:23,433 - INFO - [diffusion][Epoch 8945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:23,434 - INFO - [diffusion][Epoch 8946] Epoch 8947/12000
2024-11-05 01:13:26,817 - INFO - [diffusion][Epoch 8946] diffusion training Loss: 0.07107077538967133
2024-11-05 01:13:26,819 - INFO - [diffusion][Epoch 8946] diffusion learning rate: 0.001
2024-11-05 01:13:26,820 - INFO - [diffusion][Epoch 8946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:26,821 - INFO - [diffusion][Epoch 8947] Epoch 8948/12000
2024-11-05 01:13:30,178 - INFO - [diffusion][Epoch 8947] diffusion training Loss: 0.06384807918220758
2024-11-05 01:13:30,180 - INFO - [diffusion][Epoch 8947] diffusion learning rate: 0.001
2024-11-05 01:13:30,182 - INFO - [diffusion][Epoch 8947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:30,183 - INFO - [diffusion][Epoch 8948] Epoch 8949/12000
2024-11-05 01:13:33,978 - INFO - [diffusion][Epoch 8948] diffusion training Loss: 0.07358773984014988
2024-11-05 01:13:33,980 - INFO - [diffusion][Epoch 8948] diffusion learning rate: 0.001
2024-11-05 01:13:33,982 - INFO - [diffusion][Epoch 8948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:33,984 - INFO - [diffusion][Epoch 8949] Epoch 8950/12000
2024-11-05 01:13:37,333 - INFO - [diffusion][Epoch 8949] diffusion training Loss: 0.06616477202624083
2024-11-05 01:13:37,335 - INFO - [diffusion][Epoch 8949] diffusion learning rate: 0.001
2024-11-05 01:13:37,337 - INFO - [diffusion][Epoch 8949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:37,339 - INFO - [diffusion][Epoch 8950] Epoch 8951/12000
2024-11-05 01:13:40,392 - INFO - [diffusion][Epoch 8950] diffusion training Loss: 0.0645030839368701
2024-11-05 01:13:40,395 - INFO - [diffusion][Epoch 8950] diffusion learning rate: 0.001
2024-11-05 01:13:40,396 - INFO - [diffusion][Epoch 8950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:40,397 - INFO - [diffusion][Epoch 8951] Epoch 8952/12000
2024-11-05 01:13:43,396 - INFO - [diffusion][Epoch 8951] diffusion training Loss: 0.07261479087173939
2024-11-05 01:13:43,398 - INFO - [diffusion][Epoch 8951] diffusion learning rate: 0.001
2024-11-05 01:13:43,400 - INFO - [diffusion][Epoch 8951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:43,402 - INFO - [diffusion][Epoch 8952] Epoch 8953/12000
2024-11-05 01:13:47,175 - INFO - [diffusion][Epoch 8952] diffusion training Loss: 0.06911928579211235
2024-11-05 01:13:47,177 - INFO - [diffusion][Epoch 8952] diffusion learning rate: 0.001
2024-11-05 01:13:47,179 - INFO - [diffusion][Epoch 8952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:47,180 - INFO - [diffusion][Epoch 8953] Epoch 8954/12000
2024-11-05 01:13:50,700 - INFO - [diffusion][Epoch 8953] diffusion training Loss: 0.07252165861427784
2024-11-05 01:13:50,702 - INFO - [diffusion][Epoch 8953] diffusion learning rate: 0.001
2024-11-05 01:13:50,704 - INFO - [diffusion][Epoch 8953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:50,705 - INFO - [diffusion][Epoch 8954] Epoch 8955/12000
2024-11-05 01:13:53,894 - INFO - [diffusion][Epoch 8954] diffusion training Loss: 0.07044524699449539
2024-11-05 01:13:53,896 - INFO - [diffusion][Epoch 8954] diffusion learning rate: 0.001
2024-11-05 01:13:53,900 - INFO - [diffusion][Epoch 8954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:53,901 - INFO - [diffusion][Epoch 8955] Epoch 8956/12000
2024-11-05 01:13:56,924 - INFO - [diffusion][Epoch 8955] diffusion training Loss: 0.06877831555902958
2024-11-05 01:13:56,927 - INFO - [diffusion][Epoch 8955] diffusion learning rate: 0.001
2024-11-05 01:13:56,929 - INFO - [diffusion][Epoch 8955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:56,930 - INFO - [diffusion][Epoch 8956] Epoch 8957/12000
2024-11-05 01:14:00,173 - INFO - [diffusion][Epoch 8956] diffusion training Loss: 0.06813480891287327
2024-11-05 01:14:00,175 - INFO - [diffusion][Epoch 8956] diffusion learning rate: 0.001
2024-11-05 01:14:00,177 - INFO - [diffusion][Epoch 8956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:00,178 - INFO - [diffusion][Epoch 8957] Epoch 8958/12000
2024-11-05 01:14:03,719 - INFO - [diffusion][Epoch 8957] diffusion training Loss: 0.07162098214030266
2024-11-05 01:14:03,722 - INFO - [diffusion][Epoch 8957] diffusion learning rate: 0.001
2024-11-05 01:14:03,775 - INFO - [diffusion][Epoch 8957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:03,777 - INFO - [diffusion][Epoch 8958] Epoch 8959/12000
2024-11-05 01:14:07,227 - INFO - [diffusion][Epoch 8958] diffusion training Loss: 0.06781221181154251
2024-11-05 01:14:07,229 - INFO - [diffusion][Epoch 8958] diffusion learning rate: 0.001
2024-11-05 01:14:07,231 - INFO - [diffusion][Epoch 8958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:07,232 - INFO - [diffusion][Epoch 8959] Epoch 8960/12000
2024-11-05 01:14:10,320 - INFO - [diffusion][Epoch 8959] diffusion training Loss: 0.06829728092998266
2024-11-05 01:14:10,322 - INFO - [diffusion][Epoch 8959] diffusion learning rate: 0.001
2024-11-05 01:14:10,324 - INFO - [diffusion][Epoch 8959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:10,325 - INFO - [diffusion][Epoch 8960] Epoch 8961/12000
2024-11-05 01:14:13,608 - INFO - [diffusion][Epoch 8960] diffusion training Loss: 0.060721687972545624
2024-11-05 01:14:13,610 - INFO - [diffusion][Epoch 8960] diffusion learning rate: 0.001
2024-11-05 01:14:13,611 - INFO - [diffusion][Epoch 8960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:13,613 - INFO - [diffusion][Epoch 8961] Epoch 8962/12000
2024-11-05 01:14:16,835 - INFO - [diffusion][Epoch 8961] diffusion training Loss: 0.06965573132038116
2024-11-05 01:14:16,837 - INFO - [diffusion][Epoch 8961] diffusion learning rate: 0.001
2024-11-05 01:14:16,839 - INFO - [diffusion][Epoch 8961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:16,840 - INFO - [diffusion][Epoch 8962] Epoch 8963/12000
2024-11-05 01:14:20,537 - INFO - [diffusion][Epoch 8962] diffusion training Loss: 0.06818025931715965
2024-11-05 01:14:20,539 - INFO - [diffusion][Epoch 8962] diffusion learning rate: 0.001
2024-11-05 01:14:20,540 - INFO - [diffusion][Epoch 8962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:20,541 - INFO - [diffusion][Epoch 8963] Epoch 8964/12000
2024-11-05 01:14:24,044 - INFO - [diffusion][Epoch 8963] diffusion training Loss: 0.06360417418181896
2024-11-05 01:14:24,046 - INFO - [diffusion][Epoch 8963] diffusion learning rate: 0.001
2024-11-05 01:14:24,047 - INFO - [diffusion][Epoch 8963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:24,049 - INFO - [diffusion][Epoch 8964] Epoch 8965/12000
2024-11-05 01:14:27,650 - INFO - [diffusion][Epoch 8964] diffusion training Loss: 0.06682631466537714
2024-11-05 01:14:27,652 - INFO - [diffusion][Epoch 8964] diffusion learning rate: 0.001
2024-11-05 01:14:27,653 - INFO - [diffusion][Epoch 8964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:27,655 - INFO - [diffusion][Epoch 8965] Epoch 8966/12000
2024-11-05 01:14:31,253 - INFO - [diffusion][Epoch 8965] diffusion training Loss: 0.06891758739948273
2024-11-05 01:14:31,255 - INFO - [diffusion][Epoch 8965] diffusion learning rate: 0.001
2024-11-05 01:14:31,256 - INFO - [diffusion][Epoch 8965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:31,258 - INFO - [diffusion][Epoch 8966] Epoch 8967/12000
2024-11-05 01:14:34,313 - INFO - [diffusion][Epoch 8966] diffusion training Loss: 0.06873275712132454
2024-11-05 01:14:34,315 - INFO - [diffusion][Epoch 8966] diffusion learning rate: 0.001
2024-11-05 01:14:34,317 - INFO - [diffusion][Epoch 8966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:34,318 - INFO - [diffusion][Epoch 8967] Epoch 8968/12000
2024-11-05 01:14:37,464 - INFO - [diffusion][Epoch 8967] diffusion training Loss: 0.0686665028333664
2024-11-05 01:14:37,467 - INFO - [diffusion][Epoch 8967] diffusion learning rate: 0.001
2024-11-05 01:14:37,469 - INFO - [diffusion][Epoch 8967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:37,470 - INFO - [diffusion][Epoch 8968] Epoch 8969/12000
2024-11-05 01:14:41,022 - INFO - [diffusion][Epoch 8968] diffusion training Loss: 0.07188334874808788
2024-11-05 01:14:41,024 - INFO - [diffusion][Epoch 8968] diffusion learning rate: 0.001
2024-11-05 01:14:41,026 - INFO - [diffusion][Epoch 8968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:41,027 - INFO - [diffusion][Epoch 8969] Epoch 8970/12000
2024-11-05 01:14:44,585 - INFO - [diffusion][Epoch 8969] diffusion training Loss: 0.07107424922287464
2024-11-05 01:14:44,588 - INFO - [diffusion][Epoch 8969] diffusion learning rate: 0.001
2024-11-05 01:14:44,589 - INFO - [diffusion][Epoch 8969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:44,590 - INFO - [diffusion][Epoch 8970] Epoch 8971/12000
2024-11-05 01:14:47,706 - INFO - [diffusion][Epoch 8970] diffusion training Loss: 0.07083208300173283
2024-11-05 01:14:47,708 - INFO - [diffusion][Epoch 8970] diffusion learning rate: 0.001
2024-11-05 01:14:47,710 - INFO - [diffusion][Epoch 8970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:47,711 - INFO - [diffusion][Epoch 8971] Epoch 8972/12000
2024-11-05 01:14:50,783 - INFO - [diffusion][Epoch 8971] diffusion training Loss: 0.06656690686941147
2024-11-05 01:14:50,785 - INFO - [diffusion][Epoch 8971] diffusion learning rate: 0.001
2024-11-05 01:14:50,787 - INFO - [diffusion][Epoch 8971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:50,788 - INFO - [diffusion][Epoch 8972] Epoch 8973/12000
2024-11-05 01:14:53,873 - INFO - [diffusion][Epoch 8972] diffusion training Loss: 0.07245404832065105
2024-11-05 01:14:53,875 - INFO - [diffusion][Epoch 8972] diffusion learning rate: 0.001
2024-11-05 01:14:53,877 - INFO - [diffusion][Epoch 8972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:53,878 - INFO - [diffusion][Epoch 8973] Epoch 8974/12000
2024-11-05 01:14:57,396 - INFO - [diffusion][Epoch 8973] diffusion training Loss: 0.071267981082201
2024-11-05 01:14:57,398 - INFO - [diffusion][Epoch 8973] diffusion learning rate: 0.001
2024-11-05 01:14:57,400 - INFO - [diffusion][Epoch 8973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:57,401 - INFO - [diffusion][Epoch 8974] Epoch 8975/12000
2024-11-05 01:15:01,017 - INFO - [diffusion][Epoch 8974] diffusion training Loss: 0.06960819661617279
2024-11-05 01:15:01,020 - INFO - [diffusion][Epoch 8974] diffusion learning rate: 0.001
2024-11-05 01:15:01,022 - INFO - [diffusion][Epoch 8974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:01,023 - INFO - [diffusion][Epoch 8975] Epoch 8976/12000
2024-11-05 01:15:04,142 - INFO - [diffusion][Epoch 8975] diffusion training Loss: 0.07028142735362053
2024-11-05 01:15:04,144 - INFO - [diffusion][Epoch 8975] diffusion learning rate: 0.001
2024-11-05 01:15:04,146 - INFO - [diffusion][Epoch 8975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:04,147 - INFO - [diffusion][Epoch 8976] Epoch 8977/12000
2024-11-05 01:15:07,222 - INFO - [diffusion][Epoch 8976] diffusion training Loss: 0.07512424141168594
2024-11-05 01:15:07,224 - INFO - [diffusion][Epoch 8976] diffusion learning rate: 0.001
2024-11-05 01:15:07,226 - INFO - [diffusion][Epoch 8976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:07,227 - INFO - [diffusion][Epoch 8977] Epoch 8978/12000
2024-11-05 01:15:10,357 - INFO - [diffusion][Epoch 8977] diffusion training Loss: 0.06742504984140396
2024-11-05 01:15:10,359 - INFO - [diffusion][Epoch 8977] diffusion learning rate: 0.001
2024-11-05 01:15:10,417 - INFO - [diffusion][Epoch 8977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:10,418 - INFO - [diffusion][Epoch 8978] Epoch 8979/12000
2024-11-05 01:15:13,634 - INFO - [diffusion][Epoch 8978] diffusion training Loss: 0.07127052731812
2024-11-05 01:15:13,636 - INFO - [diffusion][Epoch 8978] diffusion learning rate: 0.001
2024-11-05 01:15:13,638 - INFO - [diffusion][Epoch 8978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:13,639 - INFO - [diffusion][Epoch 8979] Epoch 8980/12000
2024-11-05 01:15:16,782 - INFO - [diffusion][Epoch 8979] diffusion training Loss: 0.06004616618156433
2024-11-05 01:15:16,784 - INFO - [diffusion][Epoch 8979] diffusion learning rate: 0.001
2024-11-05 01:15:16,785 - INFO - [diffusion][Epoch 8979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:16,786 - INFO - [diffusion][Epoch 8980] Epoch 8981/12000
2024-11-05 01:15:19,693 - INFO - [diffusion][Epoch 8980] diffusion training Loss: 0.07456596754491329
2024-11-05 01:15:19,694 - INFO - [diffusion][Epoch 8980] diffusion learning rate: 0.001
2024-11-05 01:15:19,696 - INFO - [diffusion][Epoch 8980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:19,697 - INFO - [diffusion][Epoch 8981] Epoch 8982/12000
2024-11-05 01:15:22,928 - INFO - [diffusion][Epoch 8981] diffusion training Loss: 0.06633944623172283
2024-11-05 01:15:22,930 - INFO - [diffusion][Epoch 8981] diffusion learning rate: 0.001
2024-11-05 01:15:22,932 - INFO - [diffusion][Epoch 8981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:22,934 - INFO - [diffusion][Epoch 8982] Epoch 8983/12000
2024-11-05 01:15:26,322 - INFO - [diffusion][Epoch 8982] diffusion training Loss: 0.0746140107512474
2024-11-05 01:15:26,324 - INFO - [diffusion][Epoch 8982] diffusion learning rate: 0.001
2024-11-05 01:15:26,325 - INFO - [diffusion][Epoch 8982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:26,326 - INFO - [diffusion][Epoch 8983] Epoch 8984/12000
2024-11-05 01:15:29,408 - INFO - [diffusion][Epoch 8983] diffusion training Loss: 0.06615126505494118
2024-11-05 01:15:29,410 - INFO - [diffusion][Epoch 8983] diffusion learning rate: 0.001
2024-11-05 01:15:29,412 - INFO - [diffusion][Epoch 8983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:29,413 - INFO - [diffusion][Epoch 8984] Epoch 8985/12000
2024-11-05 01:15:32,510 - INFO - [diffusion][Epoch 8984] diffusion training Loss: 0.07430480979382992
2024-11-05 01:15:32,512 - INFO - [diffusion][Epoch 8984] diffusion learning rate: 0.001
2024-11-05 01:15:32,514 - INFO - [diffusion][Epoch 8984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:32,515 - INFO - [diffusion][Epoch 8985] Epoch 8986/12000
2024-11-05 01:15:35,622 - INFO - [diffusion][Epoch 8985] diffusion training Loss: 0.06593858078122139
2024-11-05 01:15:35,624 - INFO - [diffusion][Epoch 8985] diffusion learning rate: 0.001
2024-11-05 01:15:35,626 - INFO - [diffusion][Epoch 8985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:35,628 - INFO - [diffusion][Epoch 8986] Epoch 8987/12000
2024-11-05 01:15:39,158 - INFO - [diffusion][Epoch 8986] diffusion training Loss: 0.06427401304244995
2024-11-05 01:15:39,160 - INFO - [diffusion][Epoch 8986] diffusion learning rate: 0.001
2024-11-05 01:15:39,210 - INFO - [diffusion][Epoch 8986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:39,211 - INFO - [diffusion][Epoch 8987] Epoch 8988/12000
2024-11-05 01:15:42,731 - INFO - [diffusion][Epoch 8987] diffusion training Loss: 0.06400984525680542
2024-11-05 01:15:42,733 - INFO - [diffusion][Epoch 8987] diffusion learning rate: 0.001
2024-11-05 01:15:42,735 - INFO - [diffusion][Epoch 8987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:42,736 - INFO - [diffusion][Epoch 8988] Epoch 8989/12000
2024-11-05 01:15:45,837 - INFO - [diffusion][Epoch 8988] diffusion training Loss: 0.07072607055306435
2024-11-05 01:15:45,840 - INFO - [diffusion][Epoch 8988] diffusion learning rate: 0.001
2024-11-05 01:15:45,841 - INFO - [diffusion][Epoch 8988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:45,842 - INFO - [diffusion][Epoch 8989] Epoch 8990/12000
2024-11-05 01:15:49,052 - INFO - [diffusion][Epoch 8989] diffusion training Loss: 0.07648742385208607
2024-11-05 01:15:49,054 - INFO - [diffusion][Epoch 8989] diffusion learning rate: 0.001
2024-11-05 01:15:49,055 - INFO - [diffusion][Epoch 8989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:49,057 - INFO - [diffusion][Epoch 8990] Epoch 8991/12000
2024-11-05 01:15:52,183 - INFO - [diffusion][Epoch 8990] diffusion training Loss: 0.0682509746402502
2024-11-05 01:15:52,186 - INFO - [diffusion][Epoch 8990] diffusion learning rate: 0.001
2024-11-05 01:15:52,187 - INFO - [diffusion][Epoch 8990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:52,189 - INFO - [diffusion][Epoch 8991] Epoch 8992/12000
2024-11-05 01:15:55,724 - INFO - [diffusion][Epoch 8991] diffusion training Loss: 0.06737392023205757
2024-11-05 01:15:55,726 - INFO - [diffusion][Epoch 8991] diffusion learning rate: 0.001
2024-11-05 01:15:55,728 - INFO - [diffusion][Epoch 8991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:55,729 - INFO - [diffusion][Epoch 8992] Epoch 8993/12000
2024-11-05 01:15:59,286 - INFO - [diffusion][Epoch 8992] diffusion training Loss: 0.061704810708761215
2024-11-05 01:15:59,288 - INFO - [diffusion][Epoch 8992] diffusion learning rate: 0.001
2024-11-05 01:15:59,290 - INFO - [diffusion][Epoch 8992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:59,291 - INFO - [diffusion][Epoch 8993] Epoch 8994/12000
2024-11-05 01:16:02,449 - INFO - [diffusion][Epoch 8993] diffusion training Loss: 0.06580779608339071
2024-11-05 01:16:02,451 - INFO - [diffusion][Epoch 8993] diffusion learning rate: 0.001
2024-11-05 01:16:02,453 - INFO - [diffusion][Epoch 8993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:02,454 - INFO - [diffusion][Epoch 8994] Epoch 8995/12000
2024-11-05 01:16:05,651 - INFO - [diffusion][Epoch 8994] diffusion training Loss: 0.0733791645616293
2024-11-05 01:16:05,653 - INFO - [diffusion][Epoch 8994] diffusion learning rate: 0.001
2024-11-05 01:16:05,654 - INFO - [diffusion][Epoch 8994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:05,655 - INFO - [diffusion][Epoch 8995] Epoch 8996/12000
2024-11-05 01:16:08,871 - INFO - [diffusion][Epoch 8995] diffusion training Loss: 0.06573969684541225
2024-11-05 01:16:08,873 - INFO - [diffusion][Epoch 8995] diffusion learning rate: 0.001
2024-11-05 01:16:08,874 - INFO - [diffusion][Epoch 8995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:08,875 - INFO - [diffusion][Epoch 8996] Epoch 8997/12000
2024-11-05 01:16:12,307 - INFO - [diffusion][Epoch 8996] diffusion training Loss: 0.06907390989363194
2024-11-05 01:16:12,309 - INFO - [diffusion][Epoch 8996] diffusion learning rate: 0.001
2024-11-05 01:16:12,311 - INFO - [diffusion][Epoch 8996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:12,312 - INFO - [diffusion][Epoch 8997] Epoch 8998/12000
2024-11-05 01:16:16,085 - INFO - [diffusion][Epoch 8997] diffusion training Loss: 0.07308274507522583
2024-11-05 01:16:16,087 - INFO - [diffusion][Epoch 8997] diffusion learning rate: 0.001
2024-11-05 01:16:16,161 - INFO - [diffusion][Epoch 8997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:16,163 - INFO - [diffusion][Epoch 8998] Epoch 8999/12000
2024-11-05 01:16:19,598 - INFO - [diffusion][Epoch 8998] diffusion training Loss: 0.06718436442315578
2024-11-05 01:16:19,600 - INFO - [diffusion][Epoch 8998] diffusion learning rate: 0.001
2024-11-05 01:16:19,602 - INFO - [diffusion][Epoch 8998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:19,603 - INFO - [diffusion][Epoch 8999] Epoch 9000/12000
2024-11-05 01:16:22,671 - INFO - [diffusion][Epoch 8999] diffusion training Loss: 0.06858790665864944
2024-11-05 01:16:22,673 - INFO - [diffusion][Epoch 8999] diffusion learning rate: 0.001
2024-11-05 01:16:22,760 - INFO - [diffusion][Epoch 8999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:22,761 - INFO - [diffusion][Epoch 9000] Epoch 9001/12000
2024-11-05 01:16:25,990 - INFO - [diffusion][Epoch 9000] diffusion training Loss: 0.06732863932847977
2024-11-05 01:16:25,991 - INFO - [diffusion][Epoch 9000] diffusion learning rate: 0.001
2024-11-05 01:16:25,993 - INFO - [diffusion][Epoch 9000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:25,994 - INFO - [diffusion][Epoch 9001] Epoch 9002/12000
2024-11-05 01:16:29,281 - INFO - [diffusion][Epoch 9001] diffusion training Loss: 0.06314027961343527
2024-11-05 01:16:29,283 - INFO - [diffusion][Epoch 9001] diffusion learning rate: 0.001
2024-11-05 01:16:29,285 - INFO - [diffusion][Epoch 9001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:29,286 - INFO - [diffusion][Epoch 9002] Epoch 9003/12000
2024-11-05 01:16:32,889 - INFO - [diffusion][Epoch 9002] diffusion training Loss: 0.07006483618170023
2024-11-05 01:16:32,891 - INFO - [diffusion][Epoch 9002] diffusion learning rate: 0.001
2024-11-05 01:16:32,893 - INFO - [diffusion][Epoch 9002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:32,894 - INFO - [diffusion][Epoch 9003] Epoch 9004/12000
2024-11-05 01:16:36,394 - INFO - [diffusion][Epoch 9003] diffusion training Loss: 0.07550451159477234
2024-11-05 01:16:36,396 - INFO - [diffusion][Epoch 9003] diffusion learning rate: 0.001
2024-11-05 01:16:36,398 - INFO - [diffusion][Epoch 9003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:36,399 - INFO - [diffusion][Epoch 9004] Epoch 9005/12000
2024-11-05 01:16:38,995 - INFO - [diffusion][Epoch 9004] diffusion training Loss: 0.07153421081602573
2024-11-05 01:16:38,998 - INFO - [diffusion][Epoch 9004] diffusion learning rate: 0.001
2024-11-05 01:16:39,000 - INFO - [diffusion][Epoch 9004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:39,001 - INFO - [diffusion][Epoch 9005] Epoch 9006/12000
2024-11-05 01:16:42,063 - INFO - [diffusion][Epoch 9005] diffusion training Loss: 0.06925267167389393
2024-11-05 01:16:42,065 - INFO - [diffusion][Epoch 9005] diffusion learning rate: 0.001
2024-11-05 01:16:42,078 - INFO - [diffusion][Epoch 9005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:42,080 - INFO - [diffusion][Epoch 9006] Epoch 9007/12000
2024-11-05 01:16:45,842 - INFO - [diffusion][Epoch 9006] diffusion training Loss: 0.06776088103652
2024-11-05 01:16:45,845 - INFO - [diffusion][Epoch 9006] diffusion learning rate: 0.001
2024-11-05 01:16:45,847 - INFO - [diffusion][Epoch 9006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:45,848 - INFO - [diffusion][Epoch 9007] Epoch 9008/12000
2024-11-05 01:16:49,427 - INFO - [diffusion][Epoch 9007] diffusion training Loss: 0.06684238649904728
2024-11-05 01:16:49,429 - INFO - [diffusion][Epoch 9007] diffusion learning rate: 0.001
2024-11-05 01:16:49,431 - INFO - [diffusion][Epoch 9007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:49,432 - INFO - [diffusion][Epoch 9008] Epoch 9009/12000
2024-11-05 01:16:52,605 - INFO - [diffusion][Epoch 9008] diffusion training Loss: 0.07344360463321209
2024-11-05 01:16:52,607 - INFO - [diffusion][Epoch 9008] diffusion learning rate: 0.001
2024-11-05 01:16:52,609 - INFO - [diffusion][Epoch 9008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:52,610 - INFO - [diffusion][Epoch 9009] Epoch 9010/12000
2024-11-05 01:16:55,713 - INFO - [diffusion][Epoch 9009] diffusion training Loss: 0.07368341460824013
2024-11-05 01:16:55,715 - INFO - [diffusion][Epoch 9009] diffusion learning rate: 0.001
2024-11-05 01:16:55,716 - INFO - [diffusion][Epoch 9009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:55,718 - INFO - [diffusion][Epoch 9010] Epoch 9011/12000
2024-11-05 01:16:58,837 - INFO - [diffusion][Epoch 9010] diffusion training Loss: 0.07020430825650692
2024-11-05 01:16:58,838 - INFO - [diffusion][Epoch 9010] diffusion learning rate: 0.001
2024-11-05 01:16:58,840 - INFO - [diffusion][Epoch 9010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:58,841 - INFO - [diffusion][Epoch 9011] Epoch 9012/12000
2024-11-05 01:17:02,357 - INFO - [diffusion][Epoch 9011] diffusion training Loss: 0.06470718141645193
2024-11-05 01:17:02,359 - INFO - [diffusion][Epoch 9011] diffusion learning rate: 0.001
2024-11-05 01:17:02,361 - INFO - [diffusion][Epoch 9011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:02,362 - INFO - [diffusion][Epoch 9012] Epoch 9013/12000
2024-11-05 01:17:06,041 - INFO - [diffusion][Epoch 9012] diffusion training Loss: 0.06990659795701504
2024-11-05 01:17:06,043 - INFO - [diffusion][Epoch 9012] diffusion learning rate: 0.001
2024-11-05 01:17:06,045 - INFO - [diffusion][Epoch 9012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:06,046 - INFO - [diffusion][Epoch 9013] Epoch 9014/12000
2024-11-05 01:17:09,277 - INFO - [diffusion][Epoch 9013] diffusion training Loss: 0.06657842546701431
2024-11-05 01:17:09,279 - INFO - [diffusion][Epoch 9013] diffusion learning rate: 0.001
2024-11-05 01:17:09,281 - INFO - [diffusion][Epoch 9013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:09,282 - INFO - [diffusion][Epoch 9014] Epoch 9015/12000
2024-11-05 01:17:12,518 - INFO - [diffusion][Epoch 9014] diffusion training Loss: 0.06828754022717476
2024-11-05 01:17:12,520 - INFO - [diffusion][Epoch 9014] diffusion learning rate: 0.001
2024-11-05 01:17:12,522 - INFO - [diffusion][Epoch 9014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:12,523 - INFO - [diffusion][Epoch 9015] Epoch 9016/12000
2024-11-05 01:17:15,679 - INFO - [diffusion][Epoch 9015] diffusion training Loss: 0.06910923309624195
2024-11-05 01:17:15,681 - INFO - [diffusion][Epoch 9015] diffusion learning rate: 0.001
2024-11-05 01:17:15,682 - INFO - [diffusion][Epoch 9015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:15,683 - INFO - [diffusion][Epoch 9016] Epoch 9017/12000
2024-11-05 01:17:19,085 - INFO - [diffusion][Epoch 9016] diffusion training Loss: 0.06805024668574333
2024-11-05 01:17:19,088 - INFO - [diffusion][Epoch 9016] diffusion learning rate: 0.001
2024-11-05 01:17:19,090 - INFO - [diffusion][Epoch 9016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:19,093 - INFO - [diffusion][Epoch 9017] Epoch 9018/12000
2024-11-05 01:17:22,619 - INFO - [diffusion][Epoch 9017] diffusion training Loss: 0.0697705540806055
2024-11-05 01:17:22,621 - INFO - [diffusion][Epoch 9017] diffusion learning rate: 0.001
2024-11-05 01:17:22,623 - INFO - [diffusion][Epoch 9017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:22,624 - INFO - [diffusion][Epoch 9018] Epoch 9019/12000
2024-11-05 01:17:26,076 - INFO - [diffusion][Epoch 9018] diffusion training Loss: 0.06793334148824215
2024-11-05 01:17:26,078 - INFO - [diffusion][Epoch 9018] diffusion learning rate: 0.001
2024-11-05 01:17:26,079 - INFO - [diffusion][Epoch 9018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:26,081 - INFO - [diffusion][Epoch 9019] Epoch 9020/12000
2024-11-05 01:17:29,217 - INFO - [diffusion][Epoch 9019] diffusion training Loss: 0.06769846193492413
2024-11-05 01:17:29,219 - INFO - [diffusion][Epoch 9019] diffusion learning rate: 0.001
2024-11-05 01:17:29,221 - INFO - [diffusion][Epoch 9019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:29,222 - INFO - [diffusion][Epoch 9020] Epoch 9021/12000
2024-11-05 01:17:32,445 - INFO - [diffusion][Epoch 9020] diffusion training Loss: 0.07504124939441681
2024-11-05 01:17:32,447 - INFO - [diffusion][Epoch 9020] diffusion learning rate: 0.001
2024-11-05 01:17:32,448 - INFO - [diffusion][Epoch 9020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:32,450 - INFO - [diffusion][Epoch 9021] Epoch 9022/12000
2024-11-05 01:17:35,776 - INFO - [diffusion][Epoch 9021] diffusion training Loss: 0.07073293998837471
2024-11-05 01:17:35,778 - INFO - [diffusion][Epoch 9021] diffusion learning rate: 0.001
2024-11-05 01:17:35,779 - INFO - [diffusion][Epoch 9021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:35,781 - INFO - [diffusion][Epoch 9022] Epoch 9023/12000
2024-11-05 01:17:39,483 - INFO - [diffusion][Epoch 9022] diffusion training Loss: 0.07046009600162506
2024-11-05 01:17:39,485 - INFO - [diffusion][Epoch 9022] diffusion learning rate: 0.001
2024-11-05 01:17:39,487 - INFO - [diffusion][Epoch 9022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:39,488 - INFO - [diffusion][Epoch 9023] Epoch 9024/12000
2024-11-05 01:17:43,006 - INFO - [diffusion][Epoch 9023] diffusion training Loss: 0.07017454691231251
2024-11-05 01:17:43,008 - INFO - [diffusion][Epoch 9023] diffusion learning rate: 0.001
2024-11-05 01:17:43,009 - INFO - [diffusion][Epoch 9023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:43,011 - INFO - [diffusion][Epoch 9024] Epoch 9025/12000
2024-11-05 01:17:46,092 - INFO - [diffusion][Epoch 9024] diffusion training Loss: 0.06490644719451666
2024-11-05 01:17:46,095 - INFO - [diffusion][Epoch 9024] diffusion learning rate: 0.001
2024-11-05 01:17:46,096 - INFO - [diffusion][Epoch 9024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:46,098 - INFO - [diffusion][Epoch 9025] Epoch 9026/12000
2024-11-05 01:17:49,268 - INFO - [diffusion][Epoch 9025] diffusion training Loss: 0.06794757768511772
2024-11-05 01:17:49,270 - INFO - [diffusion][Epoch 9025] diffusion learning rate: 0.001
2024-11-05 01:17:49,272 - INFO - [diffusion][Epoch 9025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:49,273 - INFO - [diffusion][Epoch 9026] Epoch 9027/12000
2024-11-05 01:17:52,564 - INFO - [diffusion][Epoch 9026] diffusion training Loss: 0.06852135248482227
2024-11-05 01:17:52,566 - INFO - [diffusion][Epoch 9026] diffusion learning rate: 0.001
2024-11-05 01:17:52,568 - INFO - [diffusion][Epoch 9026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:52,569 - INFO - [diffusion][Epoch 9027] Epoch 9028/12000
2024-11-05 01:17:56,349 - INFO - [diffusion][Epoch 9027] diffusion training Loss: 0.06758271437138319
2024-11-05 01:17:56,351 - INFO - [diffusion][Epoch 9027] diffusion learning rate: 0.001
2024-11-05 01:17:56,353 - INFO - [diffusion][Epoch 9027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:56,354 - INFO - [diffusion][Epoch 9028] Epoch 9029/12000
2024-11-05 01:18:00,031 - INFO - [diffusion][Epoch 9028] diffusion training Loss: 0.07084303162992
2024-11-05 01:18:00,033 - INFO - [diffusion][Epoch 9028] diffusion learning rate: 0.001
2024-11-05 01:18:00,035 - INFO - [diffusion][Epoch 9028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:00,036 - INFO - [diffusion][Epoch 9029] Epoch 9030/12000
2024-11-05 01:18:03,199 - INFO - [diffusion][Epoch 9029] diffusion training Loss: 0.06508286111056805
2024-11-05 01:18:03,200 - INFO - [diffusion][Epoch 9029] diffusion learning rate: 0.001
2024-11-05 01:18:03,202 - INFO - [diffusion][Epoch 9029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:03,203 - INFO - [diffusion][Epoch 9030] Epoch 9031/12000
2024-11-05 01:18:06,298 - INFO - [diffusion][Epoch 9030] diffusion training Loss: 0.0721615981310606
2024-11-05 01:18:06,300 - INFO - [diffusion][Epoch 9030] diffusion learning rate: 0.001
2024-11-05 01:18:06,302 - INFO - [diffusion][Epoch 9030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:06,304 - INFO - [diffusion][Epoch 9031] Epoch 9032/12000
2024-11-05 01:18:09,384 - INFO - [diffusion][Epoch 9031] diffusion training Loss: 0.06901044026017189
2024-11-05 01:18:09,386 - INFO - [diffusion][Epoch 9031] diffusion learning rate: 0.001
2024-11-05 01:18:09,388 - INFO - [diffusion][Epoch 9031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:09,389 - INFO - [diffusion][Epoch 9032] Epoch 9033/12000
2024-11-05 01:18:12,920 - INFO - [diffusion][Epoch 9032] diffusion training Loss: 0.06881673634052277
2024-11-05 01:18:12,921 - INFO - [diffusion][Epoch 9032] diffusion learning rate: 0.001
2024-11-05 01:18:12,923 - INFO - [diffusion][Epoch 9032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:12,924 - INFO - [diffusion][Epoch 9033] Epoch 9034/12000
2024-11-05 01:18:16,517 - INFO - [diffusion][Epoch 9033] diffusion training Loss: 0.06901741772890091
2024-11-05 01:18:16,519 - INFO - [diffusion][Epoch 9033] diffusion learning rate: 0.001
2024-11-05 01:18:16,521 - INFO - [diffusion][Epoch 9033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:16,522 - INFO - [diffusion][Epoch 9034] Epoch 9035/12000
2024-11-05 01:18:19,616 - INFO - [diffusion][Epoch 9034] diffusion training Loss: 0.0718350037932396
2024-11-05 01:18:19,619 - INFO - [diffusion][Epoch 9034] diffusion learning rate: 0.001
2024-11-05 01:18:19,621 - INFO - [diffusion][Epoch 9034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:19,622 - INFO - [diffusion][Epoch 9035] Epoch 9036/12000
2024-11-05 01:18:22,741 - INFO - [diffusion][Epoch 9035] diffusion training Loss: 0.06754018925130367
2024-11-05 01:18:22,743 - INFO - [diffusion][Epoch 9035] diffusion learning rate: 0.001
2024-11-05 01:18:22,745 - INFO - [diffusion][Epoch 9035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:22,746 - INFO - [diffusion][Epoch 9036] Epoch 9037/12000
2024-11-05 01:18:25,966 - INFO - [diffusion][Epoch 9036] diffusion training Loss: 0.07041829451918602
2024-11-05 01:18:25,968 - INFO - [diffusion][Epoch 9036] diffusion learning rate: 0.001
2024-11-05 01:18:25,970 - INFO - [diffusion][Epoch 9036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:25,972 - INFO - [diffusion][Epoch 9037] Epoch 9038/12000
2024-11-05 01:18:29,437 - INFO - [diffusion][Epoch 9037] diffusion training Loss: 0.07017295621335506
2024-11-05 01:18:29,439 - INFO - [diffusion][Epoch 9037] diffusion learning rate: 0.001
2024-11-05 01:18:29,441 - INFO - [diffusion][Epoch 9037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:29,442 - INFO - [diffusion][Epoch 9038] Epoch 9039/12000
2024-11-05 01:18:33,101 - INFO - [diffusion][Epoch 9038] diffusion training Loss: 0.06916140951216221
2024-11-05 01:18:33,103 - INFO - [diffusion][Epoch 9038] diffusion learning rate: 0.001
2024-11-05 01:18:33,105 - INFO - [diffusion][Epoch 9038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:33,106 - INFO - [diffusion][Epoch 9039] Epoch 9040/12000
2024-11-05 01:18:36,358 - INFO - [diffusion][Epoch 9039] diffusion training Loss: 0.07189032435417175
2024-11-05 01:18:36,360 - INFO - [diffusion][Epoch 9039] diffusion learning rate: 0.001
2024-11-05 01:18:36,362 - INFO - [diffusion][Epoch 9039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:36,363 - INFO - [diffusion][Epoch 9040] Epoch 9041/12000
2024-11-05 01:18:39,507 - INFO - [diffusion][Epoch 9040] diffusion training Loss: 0.06642022263258696
2024-11-05 01:18:39,509 - INFO - [diffusion][Epoch 9040] diffusion learning rate: 0.001
2024-11-05 01:18:39,511 - INFO - [diffusion][Epoch 9040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:39,513 - INFO - [diffusion][Epoch 9041] Epoch 9042/12000
2024-11-05 01:18:42,730 - INFO - [diffusion][Epoch 9041] diffusion training Loss: 0.06605223380029202
2024-11-05 01:18:42,732 - INFO - [diffusion][Epoch 9041] diffusion learning rate: 0.001
2024-11-05 01:18:42,734 - INFO - [diffusion][Epoch 9041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:42,735 - INFO - [diffusion][Epoch 9042] Epoch 9043/12000
2024-11-05 01:18:46,098 - INFO - [diffusion][Epoch 9042] diffusion training Loss: 0.07218713872134686
2024-11-05 01:18:46,101 - INFO - [diffusion][Epoch 9042] diffusion learning rate: 0.001
2024-11-05 01:18:46,103 - INFO - [diffusion][Epoch 9042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:46,104 - INFO - [diffusion][Epoch 9043] Epoch 9044/12000
2024-11-05 01:18:49,720 - INFO - [diffusion][Epoch 9043] diffusion training Loss: 0.06875580828636885
2024-11-05 01:18:49,721 - INFO - [diffusion][Epoch 9043] diffusion learning rate: 0.001
2024-11-05 01:18:49,723 - INFO - [diffusion][Epoch 9043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:49,724 - INFO - [diffusion][Epoch 9044] Epoch 9045/12000
2024-11-05 01:18:53,166 - INFO - [diffusion][Epoch 9044] diffusion training Loss: 0.06025899387896061
2024-11-05 01:18:53,168 - INFO - [diffusion][Epoch 9044] diffusion learning rate: 0.001
2024-11-05 01:18:53,170 - INFO - [diffusion][Epoch 9044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:53,171 - INFO - [diffusion][Epoch 9045] Epoch 9046/12000
2024-11-05 01:18:56,271 - INFO - [diffusion][Epoch 9045] diffusion training Loss: 0.06672689132392406
2024-11-05 01:18:56,274 - INFO - [diffusion][Epoch 9045] diffusion learning rate: 0.001
2024-11-05 01:18:56,276 - INFO - [diffusion][Epoch 9045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:56,278 - INFO - [diffusion][Epoch 9046] Epoch 9047/12000
2024-11-05 01:18:59,511 - INFO - [diffusion][Epoch 9046] diffusion training Loss: 0.06759936362504959
2024-11-05 01:18:59,513 - INFO - [diffusion][Epoch 9046] diffusion learning rate: 0.001
2024-11-05 01:18:59,515 - INFO - [diffusion][Epoch 9046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:59,516 - INFO - [diffusion][Epoch 9047] Epoch 9048/12000
2024-11-05 01:19:03,038 - INFO - [diffusion][Epoch 9047] diffusion training Loss: 0.07043156027793884
2024-11-05 01:19:03,040 - INFO - [diffusion][Epoch 9047] diffusion learning rate: 0.001
2024-11-05 01:19:03,042 - INFO - [diffusion][Epoch 9047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:03,043 - INFO - [diffusion][Epoch 9048] Epoch 9049/12000
2024-11-05 01:19:06,573 - INFO - [diffusion][Epoch 9048] diffusion training Loss: 0.07205747812986374
2024-11-05 01:19:06,575 - INFO - [diffusion][Epoch 9048] diffusion learning rate: 0.001
2024-11-05 01:19:06,577 - INFO - [diffusion][Epoch 9048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:06,579 - INFO - [diffusion][Epoch 9049] Epoch 9050/12000
2024-11-05 01:19:10,209 - INFO - [diffusion][Epoch 9049] diffusion training Loss: 0.06619382463395596
2024-11-05 01:19:10,212 - INFO - [diffusion][Epoch 9049] diffusion learning rate: 0.001
2024-11-05 01:19:10,214 - INFO - [diffusion][Epoch 9049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:10,216 - INFO - [diffusion][Epoch 9050] Epoch 9051/12000
2024-11-05 01:19:13,302 - INFO - [diffusion][Epoch 9050] diffusion training Loss: 0.07062765583395958
2024-11-05 01:19:13,303 - INFO - [diffusion][Epoch 9050] diffusion learning rate: 0.001
2024-11-05 01:19:13,305 - INFO - [diffusion][Epoch 9050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:13,306 - INFO - [diffusion][Epoch 9051] Epoch 9052/12000
2024-11-05 01:19:16,341 - INFO - [diffusion][Epoch 9051] diffusion training Loss: 0.0681402264162898
2024-11-05 01:19:16,344 - INFO - [diffusion][Epoch 9051] diffusion learning rate: 0.001
2024-11-05 01:19:16,346 - INFO - [diffusion][Epoch 9051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:16,347 - INFO - [diffusion][Epoch 9052] Epoch 9053/12000
2024-11-05 01:19:19,680 - INFO - [diffusion][Epoch 9052] diffusion training Loss: 0.06671034544706345
2024-11-05 01:19:19,682 - INFO - [diffusion][Epoch 9052] diffusion learning rate: 0.001
2024-11-05 01:19:19,684 - INFO - [diffusion][Epoch 9052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:19,685 - INFO - [diffusion][Epoch 9053] Epoch 9054/12000
2024-11-05 01:19:23,129 - INFO - [diffusion][Epoch 9053] diffusion training Loss: 0.07015695981681347
2024-11-05 01:19:23,131 - INFO - [diffusion][Epoch 9053] diffusion learning rate: 0.001
2024-11-05 01:19:23,132 - INFO - [diffusion][Epoch 9053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:23,134 - INFO - [diffusion][Epoch 9054] Epoch 9055/12000
2024-11-05 01:19:26,849 - INFO - [diffusion][Epoch 9054] diffusion training Loss: 0.06682807207107544
2024-11-05 01:19:26,851 - INFO - [diffusion][Epoch 9054] diffusion learning rate: 0.001
2024-11-05 01:19:26,853 - INFO - [diffusion][Epoch 9054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:26,854 - INFO - [diffusion][Epoch 9055] Epoch 9056/12000
2024-11-05 01:19:30,113 - INFO - [diffusion][Epoch 9055] diffusion training Loss: 0.06732749193906784
2024-11-05 01:19:30,115 - INFO - [diffusion][Epoch 9055] diffusion learning rate: 0.001
2024-11-05 01:19:30,116 - INFO - [diffusion][Epoch 9055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:30,117 - INFO - [diffusion][Epoch 9056] Epoch 9057/12000
2024-11-05 01:19:33,142 - INFO - [diffusion][Epoch 9056] diffusion training Loss: 0.06968674063682556
2024-11-05 01:19:33,144 - INFO - [diffusion][Epoch 9056] diffusion learning rate: 0.001
2024-11-05 01:19:33,146 - INFO - [diffusion][Epoch 9056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:33,147 - INFO - [diffusion][Epoch 9057] Epoch 9058/12000
2024-11-05 01:19:36,272 - INFO - [diffusion][Epoch 9057] diffusion training Loss: 0.07027787063270807
2024-11-05 01:19:36,273 - INFO - [diffusion][Epoch 9057] diffusion learning rate: 0.001
2024-11-05 01:19:36,275 - INFO - [diffusion][Epoch 9057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:36,276 - INFO - [diffusion][Epoch 9058] Epoch 9059/12000
2024-11-05 01:19:39,837 - INFO - [diffusion][Epoch 9058] diffusion training Loss: 0.0682720523327589
2024-11-05 01:19:39,839 - INFO - [diffusion][Epoch 9058] diffusion learning rate: 0.001
2024-11-05 01:19:39,841 - INFO - [diffusion][Epoch 9058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:39,842 - INFO - [diffusion][Epoch 9059] Epoch 9060/12000
2024-11-05 01:19:43,364 - INFO - [diffusion][Epoch 9059] diffusion training Loss: 0.06884109042584896
2024-11-05 01:19:43,366 - INFO - [diffusion][Epoch 9059] diffusion learning rate: 0.001
2024-11-05 01:19:43,391 - INFO - [diffusion][Epoch 9059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:43,392 - INFO - [diffusion][Epoch 9060] Epoch 9061/12000
2024-11-05 01:19:46,469 - INFO - [diffusion][Epoch 9060] diffusion training Loss: 0.06739355809986591
2024-11-05 01:19:46,472 - INFO - [diffusion][Epoch 9060] diffusion learning rate: 0.001
2024-11-05 01:19:46,474 - INFO - [diffusion][Epoch 9060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:46,476 - INFO - [diffusion][Epoch 9061] Epoch 9062/12000
2024-11-05 01:19:49,589 - INFO - [diffusion][Epoch 9061] diffusion training Loss: 0.06609120406210423
2024-11-05 01:19:49,591 - INFO - [diffusion][Epoch 9061] diffusion learning rate: 0.001
2024-11-05 01:19:49,593 - INFO - [diffusion][Epoch 9061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:49,594 - INFO - [diffusion][Epoch 9062] Epoch 9063/12000
2024-11-05 01:19:52,686 - INFO - [diffusion][Epoch 9062] diffusion training Loss: 0.06858587451279163
2024-11-05 01:19:52,689 - INFO - [diffusion][Epoch 9062] diffusion learning rate: 0.001
2024-11-05 01:19:52,691 - INFO - [diffusion][Epoch 9062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:52,692 - INFO - [diffusion][Epoch 9063] Epoch 9064/12000
2024-11-05 01:19:56,195 - INFO - [diffusion][Epoch 9063] diffusion training Loss: 0.07208951376378536
2024-11-05 01:19:56,197 - INFO - [diffusion][Epoch 9063] diffusion learning rate: 0.001
2024-11-05 01:19:56,199 - INFO - [diffusion][Epoch 9063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:56,200 - INFO - [diffusion][Epoch 9064] Epoch 9065/12000
2024-11-05 01:19:59,802 - INFO - [diffusion][Epoch 9064] diffusion training Loss: 0.06739451177418232
2024-11-05 01:19:59,805 - INFO - [diffusion][Epoch 9064] diffusion learning rate: 0.001
2024-11-05 01:19:59,829 - INFO - [diffusion][Epoch 9064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:59,830 - INFO - [diffusion][Epoch 9065] Epoch 9066/12000
2024-11-05 01:20:02,990 - INFO - [diffusion][Epoch 9065] diffusion training Loss: 0.0658597033470869
2024-11-05 01:20:02,992 - INFO - [diffusion][Epoch 9065] diffusion learning rate: 0.001
2024-11-05 01:20:02,994 - INFO - [diffusion][Epoch 9065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:02,995 - INFO - [diffusion][Epoch 9066] Epoch 9067/12000
2024-11-05 01:20:06,068 - INFO - [diffusion][Epoch 9066] diffusion training Loss: 0.0721731074154377
2024-11-05 01:20:06,070 - INFO - [diffusion][Epoch 9066] diffusion learning rate: 0.001
2024-11-05 01:20:06,072 - INFO - [diffusion][Epoch 9066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:06,073 - INFO - [diffusion][Epoch 9067] Epoch 9068/12000
2024-11-05 01:20:09,810 - INFO - [diffusion][Epoch 9067] diffusion training Loss: 0.07631376385688782
2024-11-05 01:20:09,812 - INFO - [diffusion][Epoch 9067] diffusion learning rate: 0.001
2024-11-05 01:20:09,814 - INFO - [diffusion][Epoch 9067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:09,816 - INFO - [diffusion][Epoch 9068] Epoch 9069/12000
2024-11-05 01:20:12,930 - INFO - [diffusion][Epoch 9068] diffusion training Loss: 0.06724156066775322
2024-11-05 01:20:12,932 - INFO - [diffusion][Epoch 9068] diffusion learning rate: 0.001
2024-11-05 01:20:12,933 - INFO - [diffusion][Epoch 9068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:12,934 - INFO - [diffusion][Epoch 9069] Epoch 9070/12000
2024-11-05 01:20:16,594 - INFO - [diffusion][Epoch 9069] diffusion training Loss: 0.06981640309095383
2024-11-05 01:20:16,596 - INFO - [diffusion][Epoch 9069] diffusion learning rate: 0.001
2024-11-05 01:20:16,597 - INFO - [diffusion][Epoch 9069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:16,599 - INFO - [diffusion][Epoch 9070] Epoch 9071/12000
2024-11-05 01:20:20,139 - INFO - [diffusion][Epoch 9070] diffusion training Loss: 0.07482189498841763
2024-11-05 01:20:20,141 - INFO - [diffusion][Epoch 9070] diffusion learning rate: 0.001
2024-11-05 01:20:20,143 - INFO - [diffusion][Epoch 9070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:20,144 - INFO - [diffusion][Epoch 9071] Epoch 9072/12000
2024-11-05 01:20:23,274 - INFO - [diffusion][Epoch 9071] diffusion training Loss: 0.0726560577750206
2024-11-05 01:20:23,277 - INFO - [diffusion][Epoch 9071] diffusion learning rate: 0.001
2024-11-05 01:20:23,279 - INFO - [diffusion][Epoch 9071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:23,280 - INFO - [diffusion][Epoch 9072] Epoch 9073/12000
2024-11-05 01:20:26,330 - INFO - [diffusion][Epoch 9072] diffusion training Loss: 0.07169499062001705
2024-11-05 01:20:26,332 - INFO - [diffusion][Epoch 9072] diffusion learning rate: 0.001
2024-11-05 01:20:26,334 - INFO - [diffusion][Epoch 9072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:26,335 - INFO - [diffusion][Epoch 9073] Epoch 9074/12000
2024-11-05 01:20:29,744 - INFO - [diffusion][Epoch 9073] diffusion training Loss: 0.06995570473372936
2024-11-05 01:20:29,746 - INFO - [diffusion][Epoch 9073] diffusion learning rate: 0.001
2024-11-05 01:20:29,747 - INFO - [diffusion][Epoch 9073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:29,749 - INFO - [diffusion][Epoch 9074] Epoch 9075/12000
2024-11-05 01:20:33,529 - INFO - [diffusion][Epoch 9074] diffusion training Loss: 0.06864037737250328
2024-11-05 01:20:33,531 - INFO - [diffusion][Epoch 9074] diffusion learning rate: 0.001
2024-11-05 01:20:33,533 - INFO - [diffusion][Epoch 9074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:33,534 - INFO - [diffusion][Epoch 9075] Epoch 9076/12000
2024-11-05 01:20:36,880 - INFO - [diffusion][Epoch 9075] diffusion training Loss: 0.07665536180138588
2024-11-05 01:20:36,882 - INFO - [diffusion][Epoch 9075] diffusion learning rate: 0.001
2024-11-05 01:20:36,884 - INFO - [diffusion][Epoch 9075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:36,886 - INFO - [diffusion][Epoch 9076] Epoch 9077/12000
2024-11-05 01:20:40,041 - INFO - [diffusion][Epoch 9076] diffusion training Loss: 0.07764703594148159
2024-11-05 01:20:40,042 - INFO - [diffusion][Epoch 9076] diffusion learning rate: 0.001
2024-11-05 01:20:40,044 - INFO - [diffusion][Epoch 9076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:40,045 - INFO - [diffusion][Epoch 9077] Epoch 9078/12000
2024-11-05 01:20:43,144 - INFO - [diffusion][Epoch 9077] diffusion training Loss: 0.07374196127057076
2024-11-05 01:20:43,146 - INFO - [diffusion][Epoch 9077] diffusion learning rate: 0.001
2024-11-05 01:20:43,147 - INFO - [diffusion][Epoch 9077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:43,149 - INFO - [diffusion][Epoch 9078] Epoch 9079/12000
2024-11-05 01:20:46,607 - INFO - [diffusion][Epoch 9078] diffusion training Loss: 0.06813867948949337
2024-11-05 01:20:46,611 - INFO - [diffusion][Epoch 9078] diffusion learning rate: 0.001
2024-11-05 01:20:46,613 - INFO - [diffusion][Epoch 9078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:46,615 - INFO - [diffusion][Epoch 9079] Epoch 9080/12000
2024-11-05 01:20:50,301 - INFO - [diffusion][Epoch 9079] diffusion training Loss: 0.07126633822917938
2024-11-05 01:20:50,303 - INFO - [diffusion][Epoch 9079] diffusion learning rate: 0.001
2024-11-05 01:20:50,305 - INFO - [diffusion][Epoch 9079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:50,307 - INFO - [diffusion][Epoch 9080] Epoch 9081/12000
2024-11-05 01:20:53,708 - INFO - [diffusion][Epoch 9080] diffusion training Loss: 0.06587152183055878
2024-11-05 01:20:53,710 - INFO - [diffusion][Epoch 9080] diffusion learning rate: 0.001
2024-11-05 01:20:53,712 - INFO - [diffusion][Epoch 9080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:53,713 - INFO - [diffusion][Epoch 9081] Epoch 9082/12000
2024-11-05 01:20:56,858 - INFO - [diffusion][Epoch 9081] diffusion training Loss: 0.06639226526021957
2024-11-05 01:20:56,860 - INFO - [diffusion][Epoch 9081] diffusion learning rate: 0.001
2024-11-05 01:20:56,861 - INFO - [diffusion][Epoch 9081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:56,862 - INFO - [diffusion][Epoch 9082] Epoch 9083/12000
2024-11-05 01:20:59,850 - INFO - [diffusion][Epoch 9082] diffusion training Loss: 0.07136829383671284
2024-11-05 01:20:59,852 - INFO - [diffusion][Epoch 9082] diffusion learning rate: 0.001
2024-11-05 01:20:59,854 - INFO - [diffusion][Epoch 9082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:59,855 - INFO - [diffusion][Epoch 9083] Epoch 9084/12000
2024-11-05 01:21:03,691 - INFO - [diffusion][Epoch 9083] diffusion training Loss: 0.06498017720878124
2024-11-05 01:21:03,693 - INFO - [diffusion][Epoch 9083] diffusion learning rate: 0.001
2024-11-05 01:21:03,694 - INFO - [diffusion][Epoch 9083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:03,696 - INFO - [diffusion][Epoch 9084] Epoch 9085/12000
2024-11-05 01:21:07,106 - INFO - [diffusion][Epoch 9084] diffusion training Loss: 0.0698060467839241
2024-11-05 01:21:07,108 - INFO - [diffusion][Epoch 9084] diffusion learning rate: 0.001
2024-11-05 01:21:07,110 - INFO - [diffusion][Epoch 9084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:07,111 - INFO - [diffusion][Epoch 9085] Epoch 9086/12000
2024-11-05 01:21:10,259 - INFO - [diffusion][Epoch 9085] diffusion training Loss: 0.06931605283170938
2024-11-05 01:21:10,261 - INFO - [diffusion][Epoch 9085] diffusion learning rate: 0.001
2024-11-05 01:21:10,263 - INFO - [diffusion][Epoch 9085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:10,264 - INFO - [diffusion][Epoch 9086] Epoch 9087/12000
2024-11-05 01:21:13,494 - INFO - [diffusion][Epoch 9086] diffusion training Loss: 0.07914148829877377
2024-11-05 01:21:13,497 - INFO - [diffusion][Epoch 9086] diffusion learning rate: 0.001
2024-11-05 01:21:13,545 - INFO - [diffusion][Epoch 9086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:13,546 - INFO - [diffusion][Epoch 9087] Epoch 9088/12000
2024-11-05 01:21:16,713 - INFO - [diffusion][Epoch 9087] diffusion training Loss: 0.07247495092451572
2024-11-05 01:21:16,715 - INFO - [diffusion][Epoch 9087] diffusion learning rate: 0.001
2024-11-05 01:21:16,717 - INFO - [diffusion][Epoch 9087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:16,718 - INFO - [diffusion][Epoch 9088] Epoch 9089/12000
2024-11-05 01:21:20,301 - INFO - [diffusion][Epoch 9088] diffusion training Loss: 0.0715444590896368
2024-11-05 01:21:20,303 - INFO - [diffusion][Epoch 9088] diffusion learning rate: 0.001
2024-11-05 01:21:20,305 - INFO - [diffusion][Epoch 9088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:20,306 - INFO - [diffusion][Epoch 9089] Epoch 9090/12000
2024-11-05 01:21:23,846 - INFO - [diffusion][Epoch 9089] diffusion training Loss: 0.06622873339802027
2024-11-05 01:21:23,847 - INFO - [diffusion][Epoch 9089] diffusion learning rate: 0.001
2024-11-05 01:21:23,849 - INFO - [diffusion][Epoch 9089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:23,850 - INFO - [diffusion][Epoch 9090] Epoch 9091/12000
2024-11-05 01:21:26,959 - INFO - [diffusion][Epoch 9090] diffusion training Loss: 0.07041595783084631
2024-11-05 01:21:26,961 - INFO - [diffusion][Epoch 9090] diffusion learning rate: 0.001
2024-11-05 01:21:26,962 - INFO - [diffusion][Epoch 9090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:26,964 - INFO - [diffusion][Epoch 9091] Epoch 9092/12000
2024-11-05 01:21:30,084 - INFO - [diffusion][Epoch 9091] diffusion training Loss: 0.06845608074218035
2024-11-05 01:21:30,097 - INFO - [diffusion][Epoch 9091] diffusion learning rate: 0.001
2024-11-05 01:21:30,098 - INFO - [diffusion][Epoch 9091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:30,100 - INFO - [diffusion][Epoch 9092] Epoch 9093/12000
2024-11-05 01:21:33,247 - INFO - [diffusion][Epoch 9092] diffusion training Loss: 0.06839942280203104
2024-11-05 01:21:33,249 - INFO - [diffusion][Epoch 9092] diffusion learning rate: 0.001
2024-11-05 01:21:33,251 - INFO - [diffusion][Epoch 9092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:33,252 - INFO - [diffusion][Epoch 9093] Epoch 9094/12000
2024-11-05 01:21:36,772 - INFO - [diffusion][Epoch 9093] diffusion training Loss: 0.06793443113565445
2024-11-05 01:21:36,774 - INFO - [diffusion][Epoch 9093] diffusion learning rate: 0.001
2024-11-05 01:21:36,776 - INFO - [diffusion][Epoch 9093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:36,777 - INFO - [diffusion][Epoch 9094] Epoch 9095/12000
2024-11-05 01:21:40,481 - INFO - [diffusion][Epoch 9094] diffusion training Loss: 0.0621720002964139
2024-11-05 01:21:40,483 - INFO - [diffusion][Epoch 9094] diffusion learning rate: 0.001
2024-11-05 01:21:40,511 - INFO - [diffusion][Epoch 9094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:40,512 - INFO - [diffusion][Epoch 9095] Epoch 9096/12000
2024-11-05 01:21:43,952 - INFO - [diffusion][Epoch 9095] diffusion training Loss: 0.07012548297643661
2024-11-05 01:21:43,954 - INFO - [diffusion][Epoch 9095] diffusion learning rate: 0.001
2024-11-05 01:21:43,955 - INFO - [diffusion][Epoch 9095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:43,956 - INFO - [diffusion][Epoch 9096] Epoch 9097/12000
2024-11-05 01:21:47,163 - INFO - [diffusion][Epoch 9096] diffusion training Loss: 0.06556928530335426
2024-11-05 01:21:47,165 - INFO - [diffusion][Epoch 9096] diffusion learning rate: 0.001
2024-11-05 01:21:47,167 - INFO - [diffusion][Epoch 9096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:47,169 - INFO - [diffusion][Epoch 9097] Epoch 9098/12000
2024-11-05 01:21:50,258 - INFO - [diffusion][Epoch 9097] diffusion training Loss: 0.07032217271625996
2024-11-05 01:21:50,260 - INFO - [diffusion][Epoch 9097] diffusion learning rate: 0.001
2024-11-05 01:21:50,262 - INFO - [diffusion][Epoch 9097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:50,263 - INFO - [diffusion][Epoch 9098] Epoch 9099/12000
2024-11-05 01:21:53,402 - INFO - [diffusion][Epoch 9098] diffusion training Loss: 0.07364596985280514
2024-11-05 01:21:53,404 - INFO - [diffusion][Epoch 9098] diffusion learning rate: 0.001
2024-11-05 01:21:53,406 - INFO - [diffusion][Epoch 9098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:53,407 - INFO - [diffusion][Epoch 9099] Epoch 9100/12000
2024-11-05 01:21:56,939 - INFO - [diffusion][Epoch 9099] diffusion training Loss: 0.06894095987081528
2024-11-05 01:21:56,941 - INFO - [diffusion][Epoch 9099] diffusion learning rate: 0.001
2024-11-05 01:21:56,943 - INFO - [diffusion][Epoch 9099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:56,945 - INFO - [diffusion][Epoch 9100] Epoch 9101/12000
2024-11-05 01:22:00,434 - INFO - [diffusion][Epoch 9100] diffusion training Loss: 0.06835479475557804
2024-11-05 01:22:00,436 - INFO - [diffusion][Epoch 9100] diffusion learning rate: 0.001
2024-11-05 01:22:00,438 - INFO - [diffusion][Epoch 9100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:00,439 - INFO - [diffusion][Epoch 9101] Epoch 9102/12000
2024-11-05 01:22:03,549 - INFO - [diffusion][Epoch 9101] diffusion training Loss: 0.06657205894589424
2024-11-05 01:22:03,551 - INFO - [diffusion][Epoch 9101] diffusion learning rate: 0.001
2024-11-05 01:22:03,553 - INFO - [diffusion][Epoch 9101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:03,554 - INFO - [diffusion][Epoch 9102] Epoch 9103/12000
2024-11-05 01:22:06,721 - INFO - [diffusion][Epoch 9102] diffusion training Loss: 0.07161811739206314
2024-11-05 01:22:06,724 - INFO - [diffusion][Epoch 9102] diffusion learning rate: 0.001
2024-11-05 01:22:06,725 - INFO - [diffusion][Epoch 9102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:06,727 - INFO - [diffusion][Epoch 9103] Epoch 9104/12000
2024-11-05 01:22:09,946 - INFO - [diffusion][Epoch 9103] diffusion training Loss: 0.06492685154080391
2024-11-05 01:22:09,947 - INFO - [diffusion][Epoch 9103] diffusion learning rate: 0.001
2024-11-05 01:22:09,949 - INFO - [diffusion][Epoch 9103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:09,950 - INFO - [diffusion][Epoch 9104] Epoch 9105/12000
2024-11-05 01:22:13,607 - INFO - [diffusion][Epoch 9104] diffusion training Loss: 0.07060046680271626
2024-11-05 01:22:13,610 - INFO - [diffusion][Epoch 9104] diffusion learning rate: 0.001
2024-11-05 01:22:13,611 - INFO - [diffusion][Epoch 9104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:13,612 - INFO - [diffusion][Epoch 9105] Epoch 9106/12000
2024-11-05 01:22:17,114 - INFO - [diffusion][Epoch 9105] diffusion training Loss: 0.07156174629926682
2024-11-05 01:22:17,116 - INFO - [diffusion][Epoch 9105] diffusion learning rate: 0.001
2024-11-05 01:22:17,142 - INFO - [diffusion][Epoch 9105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:17,143 - INFO - [diffusion][Epoch 9106] Epoch 9107/12000
2024-11-05 01:22:20,491 - INFO - [diffusion][Epoch 9106] diffusion training Loss: 0.06525847781449556
2024-11-05 01:22:20,493 - INFO - [diffusion][Epoch 9106] diffusion learning rate: 0.001
2024-11-05 01:22:20,495 - INFO - [diffusion][Epoch 9106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:20,496 - INFO - [diffusion][Epoch 9107] Epoch 9108/12000
2024-11-05 01:22:23,617 - INFO - [diffusion][Epoch 9107] diffusion training Loss: 0.07355607487261295
2024-11-05 01:22:23,619 - INFO - [diffusion][Epoch 9107] diffusion learning rate: 0.001
2024-11-05 01:22:23,620 - INFO - [diffusion][Epoch 9107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:23,622 - INFO - [diffusion][Epoch 9108] Epoch 9109/12000
2024-11-05 01:22:26,759 - INFO - [diffusion][Epoch 9108] diffusion training Loss: 0.07526283711194992
2024-11-05 01:22:26,761 - INFO - [diffusion][Epoch 9108] diffusion learning rate: 0.001
2024-11-05 01:22:26,762 - INFO - [diffusion][Epoch 9108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:26,764 - INFO - [diffusion][Epoch 9109] Epoch 9110/12000
2024-11-05 01:22:30,303 - INFO - [diffusion][Epoch 9109] diffusion training Loss: 0.06967103108763695
2024-11-05 01:22:30,306 - INFO - [diffusion][Epoch 9109] diffusion learning rate: 0.001
2024-11-05 01:22:30,307 - INFO - [diffusion][Epoch 9109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:30,309 - INFO - [diffusion][Epoch 9110] Epoch 9111/12000
2024-11-05 01:22:33,904 - INFO - [diffusion][Epoch 9110] diffusion training Loss: 0.06571016274392605
2024-11-05 01:22:33,906 - INFO - [diffusion][Epoch 9110] diffusion learning rate: 0.001
2024-11-05 01:22:33,928 - INFO - [diffusion][Epoch 9110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:33,929 - INFO - [diffusion][Epoch 9111] Epoch 9112/12000
2024-11-05 01:22:37,178 - INFO - [diffusion][Epoch 9111] diffusion training Loss: 0.06848045997321606
2024-11-05 01:22:37,180 - INFO - [diffusion][Epoch 9111] diffusion learning rate: 0.001
2024-11-05 01:22:37,182 - INFO - [diffusion][Epoch 9111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:37,183 - INFO - [diffusion][Epoch 9112] Epoch 9113/12000
2024-11-05 01:22:40,290 - INFO - [diffusion][Epoch 9112] diffusion training Loss: 0.06761697493493557
2024-11-05 01:22:40,292 - INFO - [diffusion][Epoch 9112] diffusion learning rate: 0.001
2024-11-05 01:22:40,294 - INFO - [diffusion][Epoch 9112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:40,295 - INFO - [diffusion][Epoch 9113] Epoch 9114/12000
2024-11-05 01:22:43,367 - INFO - [diffusion][Epoch 9113] diffusion training Loss: 0.07252035476267338
2024-11-05 01:22:43,369 - INFO - [diffusion][Epoch 9113] diffusion learning rate: 0.001
2024-11-05 01:22:43,371 - INFO - [diffusion][Epoch 9113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:43,372 - INFO - [diffusion][Epoch 9114] Epoch 9115/12000
2024-11-05 01:22:46,884 - INFO - [diffusion][Epoch 9114] diffusion training Loss: 0.06823692843317986
2024-11-05 01:22:46,886 - INFO - [diffusion][Epoch 9114] diffusion learning rate: 0.001
2024-11-05 01:22:46,888 - INFO - [diffusion][Epoch 9114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:46,889 - INFO - [diffusion][Epoch 9115] Epoch 9116/12000
2024-11-05 01:22:50,415 - INFO - [diffusion][Epoch 9115] diffusion training Loss: 0.07018427737057209
2024-11-05 01:22:50,418 - INFO - [diffusion][Epoch 9115] diffusion learning rate: 0.001
2024-11-05 01:22:50,463 - INFO - [diffusion][Epoch 9115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:50,465 - INFO - [diffusion][Epoch 9116] Epoch 9117/12000
2024-11-05 01:22:53,577 - INFO - [diffusion][Epoch 9116] diffusion training Loss: 0.07240656390786171
2024-11-05 01:22:53,579 - INFO - [diffusion][Epoch 9116] diffusion learning rate: 0.001
2024-11-05 01:22:53,581 - INFO - [diffusion][Epoch 9116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:53,582 - INFO - [diffusion][Epoch 9117] Epoch 9118/12000
2024-11-05 01:22:56,681 - INFO - [diffusion][Epoch 9117] diffusion training Loss: 0.06935220770537853
2024-11-05 01:22:56,682 - INFO - [diffusion][Epoch 9117] diffusion learning rate: 0.001
2024-11-05 01:22:56,684 - INFO - [diffusion][Epoch 9117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:56,685 - INFO - [diffusion][Epoch 9118] Epoch 9119/12000
2024-11-05 01:22:59,789 - INFO - [diffusion][Epoch 9118] diffusion training Loss: 0.06851902045309544
2024-11-05 01:22:59,791 - INFO - [diffusion][Epoch 9118] diffusion learning rate: 0.001
2024-11-05 01:22:59,793 - INFO - [diffusion][Epoch 9118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:59,794 - INFO - [diffusion][Epoch 9119] Epoch 9120/12000
2024-11-05 01:23:03,344 - INFO - [diffusion][Epoch 9119] diffusion training Loss: 0.06453292071819305
2024-11-05 01:23:03,345 - INFO - [diffusion][Epoch 9119] diffusion learning rate: 0.001
2024-11-05 01:23:03,347 - INFO - [diffusion][Epoch 9119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:03,348 - INFO - [diffusion][Epoch 9120] Epoch 9121/12000
2024-11-05 01:23:06,881 - INFO - [diffusion][Epoch 9120] diffusion training Loss: 0.06658065132796764
2024-11-05 01:23:06,883 - INFO - [diffusion][Epoch 9120] diffusion learning rate: 0.001
2024-11-05 01:23:06,885 - INFO - [diffusion][Epoch 9120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:06,886 - INFO - [diffusion][Epoch 9121] Epoch 9122/12000
2024-11-05 01:23:10,020 - INFO - [diffusion][Epoch 9121] diffusion training Loss: 0.06577328592538834
2024-11-05 01:23:10,022 - INFO - [diffusion][Epoch 9121] diffusion learning rate: 0.001
2024-11-05 01:23:10,024 - INFO - [diffusion][Epoch 9121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:10,025 - INFO - [diffusion][Epoch 9122] Epoch 9123/12000
2024-11-05 01:23:13,304 - INFO - [diffusion][Epoch 9122] diffusion training Loss: 0.07070250809192657
2024-11-05 01:23:13,306 - INFO - [diffusion][Epoch 9122] diffusion learning rate: 0.001
2024-11-05 01:23:13,307 - INFO - [diffusion][Epoch 9122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:13,309 - INFO - [diffusion][Epoch 9123] Epoch 9124/12000
2024-11-05 01:23:16,432 - INFO - [diffusion][Epoch 9123] diffusion training Loss: 0.06737586203962564
2024-11-05 01:23:16,434 - INFO - [diffusion][Epoch 9123] diffusion learning rate: 0.001
2024-11-05 01:23:16,447 - INFO - [diffusion][Epoch 9123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:16,448 - INFO - [diffusion][Epoch 9124] Epoch 9125/12000
2024-11-05 01:23:19,976 - INFO - [diffusion][Epoch 9124] diffusion training Loss: 0.06710433680564165
2024-11-05 01:23:19,978 - INFO - [diffusion][Epoch 9124] diffusion learning rate: 0.001
2024-11-05 01:23:19,980 - INFO - [diffusion][Epoch 9124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:19,982 - INFO - [diffusion][Epoch 9125] Epoch 9126/12000
2024-11-05 01:23:23,524 - INFO - [diffusion][Epoch 9125] diffusion training Loss: 0.06568031013011932
2024-11-05 01:23:23,526 - INFO - [diffusion][Epoch 9125] diffusion learning rate: 0.001
2024-11-05 01:23:23,528 - INFO - [diffusion][Epoch 9125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:23,529 - INFO - [diffusion][Epoch 9126] Epoch 9127/12000
2024-11-05 01:23:26,613 - INFO - [diffusion][Epoch 9126] diffusion training Loss: 0.07354205287992954
2024-11-05 01:23:26,615 - INFO - [diffusion][Epoch 9126] diffusion learning rate: 0.001
2024-11-05 01:23:26,617 - INFO - [diffusion][Epoch 9126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:26,618 - INFO - [diffusion][Epoch 9127] Epoch 9128/12000
2024-11-05 01:23:29,735 - INFO - [diffusion][Epoch 9127] diffusion training Loss: 0.06661254446953535
2024-11-05 01:23:29,737 - INFO - [diffusion][Epoch 9127] diffusion learning rate: 0.001
2024-11-05 01:23:29,738 - INFO - [diffusion][Epoch 9127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:29,740 - INFO - [diffusion][Epoch 9128] Epoch 9129/12000
2024-11-05 01:23:33,380 - INFO - [diffusion][Epoch 9128] diffusion training Loss: 0.07335629127919674
2024-11-05 01:23:33,382 - INFO - [diffusion][Epoch 9128] diffusion learning rate: 0.001
2024-11-05 01:23:33,383 - INFO - [diffusion][Epoch 9128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:33,385 - INFO - [diffusion][Epoch 9129] Epoch 9130/12000
2024-11-05 01:23:36,618 - INFO - [diffusion][Epoch 9129] diffusion training Loss: 0.06783824041485786
2024-11-05 01:23:36,620 - INFO - [diffusion][Epoch 9129] diffusion learning rate: 0.001
2024-11-05 01:23:36,622 - INFO - [diffusion][Epoch 9129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:36,623 - INFO - [diffusion][Epoch 9130] Epoch 9131/12000
2024-11-05 01:23:40,244 - INFO - [diffusion][Epoch 9130] diffusion training Loss: 0.06760421767830849
2024-11-05 01:23:40,246 - INFO - [diffusion][Epoch 9130] diffusion learning rate: 0.001
2024-11-05 01:23:40,248 - INFO - [diffusion][Epoch 9130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:40,249 - INFO - [diffusion][Epoch 9131] Epoch 9132/12000
2024-11-05 01:23:43,695 - INFO - [diffusion][Epoch 9131] diffusion training Loss: 0.07799039408564568
2024-11-05 01:23:43,697 - INFO - [diffusion][Epoch 9131] diffusion learning rate: 0.001
2024-11-05 01:23:43,699 - INFO - [diffusion][Epoch 9131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:43,700 - INFO - [diffusion][Epoch 9132] Epoch 9133/12000
2024-11-05 01:23:46,901 - INFO - [diffusion][Epoch 9132] diffusion training Loss: 0.06614343542605639
2024-11-05 01:23:46,903 - INFO - [diffusion][Epoch 9132] diffusion learning rate: 0.001
2024-11-05 01:23:46,905 - INFO - [diffusion][Epoch 9132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:46,906 - INFO - [diffusion][Epoch 9133] Epoch 9134/12000
2024-11-05 01:23:49,980 - INFO - [diffusion][Epoch 9133] diffusion training Loss: 0.06523497588932514
2024-11-05 01:23:49,981 - INFO - [diffusion][Epoch 9133] diffusion learning rate: 0.001
2024-11-05 01:23:49,983 - INFO - [diffusion][Epoch 9133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:49,984 - INFO - [diffusion][Epoch 9134] Epoch 9135/12000
2024-11-05 01:23:53,501 - INFO - [diffusion][Epoch 9134] diffusion training Loss: 0.06738206464797258
2024-11-05 01:23:53,503 - INFO - [diffusion][Epoch 9134] diffusion learning rate: 0.001
2024-11-05 01:23:53,505 - INFO - [diffusion][Epoch 9134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:53,506 - INFO - [diffusion][Epoch 9135] Epoch 9136/12000
2024-11-05 01:23:57,072 - INFO - [diffusion][Epoch 9135] diffusion training Loss: 0.07001513801515102
2024-11-05 01:23:57,074 - INFO - [diffusion][Epoch 9135] diffusion learning rate: 0.001
2024-11-05 01:23:57,076 - INFO - [diffusion][Epoch 9135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:57,077 - INFO - [diffusion][Epoch 9136] Epoch 9137/12000
2024-11-05 01:24:00,197 - INFO - [diffusion][Epoch 9136] diffusion training Loss: 0.07398530654609203
2024-11-05 01:24:00,199 - INFO - [diffusion][Epoch 9136] diffusion learning rate: 0.001
2024-11-05 01:24:00,201 - INFO - [diffusion][Epoch 9136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:00,202 - INFO - [diffusion][Epoch 9137] Epoch 9138/12000
2024-11-05 01:24:03,282 - INFO - [diffusion][Epoch 9137] diffusion training Loss: 0.07444080710411072
2024-11-05 01:24:03,284 - INFO - [diffusion][Epoch 9137] diffusion learning rate: 0.001
2024-11-05 01:24:03,286 - INFO - [diffusion][Epoch 9137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:03,287 - INFO - [diffusion][Epoch 9138] Epoch 9139/12000
2024-11-05 01:24:06,456 - INFO - [diffusion][Epoch 9138] diffusion training Loss: 0.06507730204612017
2024-11-05 01:24:06,457 - INFO - [diffusion][Epoch 9138] diffusion learning rate: 0.001
2024-11-05 01:24:06,459 - INFO - [diffusion][Epoch 9138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:06,460 - INFO - [diffusion][Epoch 9139] Epoch 9140/12000
2024-11-05 01:24:09,981 - INFO - [diffusion][Epoch 9139] diffusion training Loss: 0.06417960580438375
2024-11-05 01:24:09,985 - INFO - [diffusion][Epoch 9139] diffusion learning rate: 0.001
2024-11-05 01:24:09,987 - INFO - [diffusion][Epoch 9139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:09,988 - INFO - [diffusion][Epoch 9140] Epoch 9141/12000
2024-11-05 01:24:13,521 - INFO - [diffusion][Epoch 9140] diffusion training Loss: 0.06641282886266708
2024-11-05 01:24:13,524 - INFO - [diffusion][Epoch 9140] diffusion learning rate: 0.001
2024-11-05 01:24:13,527 - INFO - [diffusion][Epoch 9140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:13,528 - INFO - [diffusion][Epoch 9141] Epoch 9142/12000
2024-11-05 01:24:16,631 - INFO - [diffusion][Epoch 9141] diffusion training Loss: 0.06243569403886795
2024-11-05 01:24:16,633 - INFO - [diffusion][Epoch 9141] diffusion learning rate: 0.001
2024-11-05 01:24:16,635 - INFO - [diffusion][Epoch 9141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:16,637 - INFO - [diffusion][Epoch 9142] Epoch 9143/12000
2024-11-05 01:24:19,865 - INFO - [diffusion][Epoch 9142] diffusion training Loss: 0.0691020917147398
2024-11-05 01:24:19,867 - INFO - [diffusion][Epoch 9142] diffusion learning rate: 0.001
2024-11-05 01:24:19,868 - INFO - [diffusion][Epoch 9142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:19,870 - INFO - [diffusion][Epoch 9143] Epoch 9144/12000
2024-11-05 01:24:23,061 - INFO - [diffusion][Epoch 9143] diffusion training Loss: 0.06914528831839561
2024-11-05 01:24:23,063 - INFO - [diffusion][Epoch 9143] diffusion learning rate: 0.001
2024-11-05 01:24:23,065 - INFO - [diffusion][Epoch 9143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:23,067 - INFO - [diffusion][Epoch 9144] Epoch 9145/12000
2024-11-05 01:24:26,650 - INFO - [diffusion][Epoch 9144] diffusion training Loss: 0.06771461851894855
2024-11-05 01:24:26,652 - INFO - [diffusion][Epoch 9144] diffusion learning rate: 0.001
2024-11-05 01:24:26,654 - INFO - [diffusion][Epoch 9144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:26,655 - INFO - [diffusion][Epoch 9145] Epoch 9146/12000
2024-11-05 01:24:30,180 - INFO - [diffusion][Epoch 9145] diffusion training Loss: 0.06620413810014725
2024-11-05 01:24:30,182 - INFO - [diffusion][Epoch 9145] diffusion learning rate: 0.001
2024-11-05 01:24:30,183 - INFO - [diffusion][Epoch 9145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:30,185 - INFO - [diffusion][Epoch 9146] Epoch 9147/12000
2024-11-05 01:24:33,276 - INFO - [diffusion][Epoch 9146] diffusion training Loss: 0.06928043439984322
2024-11-05 01:24:33,278 - INFO - [diffusion][Epoch 9146] diffusion learning rate: 0.001
2024-11-05 01:24:33,279 - INFO - [diffusion][Epoch 9146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:33,281 - INFO - [diffusion][Epoch 9147] Epoch 9148/12000
2024-11-05 01:24:36,380 - INFO - [diffusion][Epoch 9147] diffusion training Loss: 0.06609917990863323
2024-11-05 01:24:36,382 - INFO - [diffusion][Epoch 9147] diffusion learning rate: 0.001
2024-11-05 01:24:36,384 - INFO - [diffusion][Epoch 9147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:36,385 - INFO - [diffusion][Epoch 9148] Epoch 9149/12000
2024-11-05 01:24:39,523 - INFO - [diffusion][Epoch 9148] diffusion training Loss: 0.06470620166510344
2024-11-05 01:24:39,525 - INFO - [diffusion][Epoch 9148] diffusion learning rate: 0.001
2024-11-05 01:24:39,527 - INFO - [diffusion][Epoch 9148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:39,528 - INFO - [diffusion][Epoch 9149] Epoch 9150/12000
2024-11-05 01:24:43,033 - INFO - [diffusion][Epoch 9149] diffusion training Loss: 0.07559161074459553
2024-11-05 01:24:43,035 - INFO - [diffusion][Epoch 9149] diffusion learning rate: 0.001
2024-11-05 01:24:43,036 - INFO - [diffusion][Epoch 9149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:43,038 - INFO - [diffusion][Epoch 9150] Epoch 9151/12000
2024-11-05 01:24:46,561 - INFO - [diffusion][Epoch 9150] diffusion training Loss: 0.07144565135240555
2024-11-05 01:24:46,564 - INFO - [diffusion][Epoch 9150] diffusion learning rate: 0.001
2024-11-05 01:24:46,566 - INFO - [diffusion][Epoch 9150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:46,567 - INFO - [diffusion][Epoch 9151] Epoch 9152/12000
2024-11-05 01:24:49,700 - INFO - [diffusion][Epoch 9151] diffusion training Loss: 0.06827031448483467
2024-11-05 01:24:49,702 - INFO - [diffusion][Epoch 9151] diffusion learning rate: 0.001
2024-11-05 01:24:49,704 - INFO - [diffusion][Epoch 9151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:49,705 - INFO - [diffusion][Epoch 9152] Epoch 9153/12000
2024-11-05 01:24:52,818 - INFO - [diffusion][Epoch 9152] diffusion training Loss: 0.06469610519707203
2024-11-05 01:24:52,820 - INFO - [diffusion][Epoch 9152] diffusion learning rate: 0.001
2024-11-05 01:24:52,822 - INFO - [diffusion][Epoch 9152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:52,823 - INFO - [diffusion][Epoch 9153] Epoch 9154/12000
2024-11-05 01:24:56,060 - INFO - [diffusion][Epoch 9153] diffusion training Loss: 0.0682755671441555
2024-11-05 01:24:56,063 - INFO - [diffusion][Epoch 9153] diffusion learning rate: 0.001
2024-11-05 01:24:56,065 - INFO - [diffusion][Epoch 9153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:56,067 - INFO - [diffusion][Epoch 9154] Epoch 9155/12000
2024-11-05 01:24:59,698 - INFO - [diffusion][Epoch 9154] diffusion training Loss: 0.06813889555633068
2024-11-05 01:24:59,700 - INFO - [diffusion][Epoch 9154] diffusion learning rate: 0.001
2024-11-05 01:24:59,702 - INFO - [diffusion][Epoch 9154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:59,704 - INFO - [diffusion][Epoch 9155] Epoch 9156/12000
2024-11-05 01:25:03,207 - INFO - [diffusion][Epoch 9155] diffusion training Loss: 0.07123891450464725
2024-11-05 01:25:03,209 - INFO - [diffusion][Epoch 9155] diffusion learning rate: 0.001
2024-11-05 01:25:03,210 - INFO - [diffusion][Epoch 9155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:03,212 - INFO - [diffusion][Epoch 9156] Epoch 9157/12000
2024-11-05 01:25:06,301 - INFO - [diffusion][Epoch 9156] diffusion training Loss: 0.06666786968708038
2024-11-05 01:25:06,303 - INFO - [diffusion][Epoch 9156] diffusion learning rate: 0.001
2024-11-05 01:25:06,305 - INFO - [diffusion][Epoch 9156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:06,306 - INFO - [diffusion][Epoch 9157] Epoch 9158/12000
2024-11-05 01:25:09,409 - INFO - [diffusion][Epoch 9157] diffusion training Loss: 0.06504282541573048
2024-11-05 01:25:09,411 - INFO - [diffusion][Epoch 9157] diffusion learning rate: 0.001
2024-11-05 01:25:09,413 - INFO - [diffusion][Epoch 9157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:09,414 - INFO - [diffusion][Epoch 9158] Epoch 9159/12000
2024-11-05 01:25:12,724 - INFO - [diffusion][Epoch 9158] diffusion training Loss: 0.07161585614085197
2024-11-05 01:25:12,726 - INFO - [diffusion][Epoch 9158] diffusion learning rate: 0.001
2024-11-05 01:25:12,728 - INFO - [diffusion][Epoch 9158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:12,729 - INFO - [diffusion][Epoch 9159] Epoch 9160/12000
2024-11-05 01:25:16,264 - INFO - [diffusion][Epoch 9159] diffusion training Loss: 0.06886138767004013
2024-11-05 01:25:16,266 - INFO - [diffusion][Epoch 9159] diffusion learning rate: 0.001
2024-11-05 01:25:16,268 - INFO - [diffusion][Epoch 9159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:16,269 - INFO - [diffusion][Epoch 9160] Epoch 9161/12000
2024-11-05 01:25:19,376 - INFO - [diffusion][Epoch 9160] diffusion training Loss: 0.07389688305556774
2024-11-05 01:25:19,378 - INFO - [diffusion][Epoch 9160] diffusion learning rate: 0.001
2024-11-05 01:25:19,380 - INFO - [diffusion][Epoch 9160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:19,381 - INFO - [diffusion][Epoch 9161] Epoch 9162/12000
2024-11-05 01:25:22,452 - INFO - [diffusion][Epoch 9161] diffusion training Loss: 0.06713663786649704
2024-11-05 01:25:22,454 - INFO - [diffusion][Epoch 9161] diffusion learning rate: 0.001
2024-11-05 01:25:22,456 - INFO - [diffusion][Epoch 9161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:22,457 - INFO - [diffusion][Epoch 9162] Epoch 9163/12000
2024-11-05 01:25:25,688 - INFO - [diffusion][Epoch 9162] diffusion training Loss: 0.06691036093980074
2024-11-05 01:25:25,690 - INFO - [diffusion][Epoch 9162] diffusion learning rate: 0.001
2024-11-05 01:25:25,692 - INFO - [diffusion][Epoch 9162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:25,693 - INFO - [diffusion][Epoch 9163] Epoch 9164/12000
2024-11-05 01:25:29,329 - INFO - [diffusion][Epoch 9163] diffusion training Loss: 0.06713087484240532
2024-11-05 01:25:29,331 - INFO - [diffusion][Epoch 9163] diffusion learning rate: 0.001
2024-11-05 01:25:29,333 - INFO - [diffusion][Epoch 9163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:29,334 - INFO - [diffusion][Epoch 9164] Epoch 9165/12000
2024-11-05 01:25:32,639 - INFO - [diffusion][Epoch 9164] diffusion training Loss: 0.06634338945150375
2024-11-05 01:25:32,641 - INFO - [diffusion][Epoch 9164] diffusion learning rate: 0.001
2024-11-05 01:25:32,643 - INFO - [diffusion][Epoch 9164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:32,644 - INFO - [diffusion][Epoch 9165] Epoch 9166/12000
2024-11-05 01:25:35,865 - INFO - [diffusion][Epoch 9165] diffusion training Loss: 0.07158654183149338
2024-11-05 01:25:35,867 - INFO - [diffusion][Epoch 9165] diffusion learning rate: 0.001
2024-11-05 01:25:35,869 - INFO - [diffusion][Epoch 9165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:35,871 - INFO - [diffusion][Epoch 9166] Epoch 9167/12000
2024-11-05 01:25:38,908 - INFO - [diffusion][Epoch 9166] diffusion training Loss: 0.06759263016283512
2024-11-05 01:25:38,910 - INFO - [diffusion][Epoch 9166] diffusion learning rate: 0.001
2024-11-05 01:25:38,912 - INFO - [diffusion][Epoch 9166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:38,913 - INFO - [diffusion][Epoch 9167] Epoch 9168/12000
2024-11-05 01:25:42,474 - INFO - [diffusion][Epoch 9167] diffusion training Loss: 0.06437539588660002
2024-11-05 01:25:42,476 - INFO - [diffusion][Epoch 9167] diffusion learning rate: 0.001
2024-11-05 01:25:42,478 - INFO - [diffusion][Epoch 9167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:42,479 - INFO - [diffusion][Epoch 9168] Epoch 9169/12000
2024-11-05 01:25:46,429 - INFO - [diffusion][Epoch 9168] diffusion training Loss: 0.07053202576935291
2024-11-05 01:25:46,432 - INFO - [diffusion][Epoch 9168] diffusion learning rate: 0.001
2024-11-05 01:25:46,433 - INFO - [diffusion][Epoch 9168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:46,435 - INFO - [diffusion][Epoch 9169] Epoch 9170/12000
2024-11-05 01:25:49,881 - INFO - [diffusion][Epoch 9169] diffusion training Loss: 0.06979791447520256
2024-11-05 01:25:49,883 - INFO - [diffusion][Epoch 9169] diffusion learning rate: 0.001
2024-11-05 01:25:49,885 - INFO - [diffusion][Epoch 9169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:49,886 - INFO - [diffusion][Epoch 9170] Epoch 9171/12000
2024-11-05 01:25:52,990 - INFO - [diffusion][Epoch 9170] diffusion training Loss: 0.06711764074862003
2024-11-05 01:25:52,992 - INFO - [diffusion][Epoch 9170] diffusion learning rate: 0.001
2024-11-05 01:25:52,994 - INFO - [diffusion][Epoch 9170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:52,995 - INFO - [diffusion][Epoch 9171] Epoch 9172/12000
2024-11-05 01:25:56,340 - INFO - [diffusion][Epoch 9171] diffusion training Loss: 0.07114642858505249
2024-11-05 01:25:56,343 - INFO - [diffusion][Epoch 9171] diffusion learning rate: 0.001
2024-11-05 01:25:56,350 - INFO - [diffusion][Epoch 9171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:56,352 - INFO - [diffusion][Epoch 9172] Epoch 9173/12000
2024-11-05 01:25:59,514 - INFO - [diffusion][Epoch 9172] diffusion training Loss: 0.07061510719358921
2024-11-05 01:25:59,516 - INFO - [diffusion][Epoch 9172] diffusion learning rate: 0.001
2024-11-05 01:25:59,518 - INFO - [diffusion][Epoch 9172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:59,520 - INFO - [diffusion][Epoch 9173] Epoch 9174/12000
2024-11-05 01:26:03,106 - INFO - [diffusion][Epoch 9173] diffusion training Loss: 0.07012510672211647
2024-11-05 01:26:03,108 - INFO - [diffusion][Epoch 9173] diffusion learning rate: 0.001
2024-11-05 01:26:03,110 - INFO - [diffusion][Epoch 9173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:03,111 - INFO - [diffusion][Epoch 9174] Epoch 9175/12000
2024-11-05 01:26:06,600 - INFO - [diffusion][Epoch 9174] diffusion training Loss: 0.06622791942209005
2024-11-05 01:26:06,602 - INFO - [diffusion][Epoch 9174] diffusion learning rate: 0.001
2024-11-05 01:26:06,649 - INFO - [diffusion][Epoch 9174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:06,650 - INFO - [diffusion][Epoch 9175] Epoch 9176/12000
2024-11-05 01:26:09,744 - INFO - [diffusion][Epoch 9175] diffusion training Loss: 0.06760590709745884
2024-11-05 01:26:09,746 - INFO - [diffusion][Epoch 9175] diffusion learning rate: 0.001
2024-11-05 01:26:09,748 - INFO - [diffusion][Epoch 9175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:09,749 - INFO - [diffusion][Epoch 9176] Epoch 9177/12000
2024-11-05 01:26:12,848 - INFO - [diffusion][Epoch 9176] diffusion training Loss: 0.06507688388228416
2024-11-05 01:26:12,850 - INFO - [diffusion][Epoch 9176] diffusion learning rate: 0.001
2024-11-05 01:26:12,852 - INFO - [diffusion][Epoch 9176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:12,853 - INFO - [diffusion][Epoch 9177] Epoch 9178/12000
2024-11-05 01:26:15,961 - INFO - [diffusion][Epoch 9177] diffusion training Loss: 0.07019762136042118
2024-11-05 01:26:15,962 - INFO - [diffusion][Epoch 9177] diffusion learning rate: 0.001
2024-11-05 01:26:15,964 - INFO - [diffusion][Epoch 9177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:15,965 - INFO - [diffusion][Epoch 9178] Epoch 9179/12000
2024-11-05 01:26:19,444 - INFO - [diffusion][Epoch 9178] diffusion training Loss: 0.07054900005459785
2024-11-05 01:26:19,446 - INFO - [diffusion][Epoch 9178] diffusion learning rate: 0.001
2024-11-05 01:26:19,448 - INFO - [diffusion][Epoch 9178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:19,449 - INFO - [diffusion][Epoch 9179] Epoch 9180/12000
2024-11-05 01:26:22,839 - INFO - [diffusion][Epoch 9179] diffusion training Loss: 0.06832421012222767
2024-11-05 01:26:22,841 - INFO - [diffusion][Epoch 9179] diffusion learning rate: 0.001
2024-11-05 01:26:22,844 - INFO - [diffusion][Epoch 9179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:22,845 - INFO - [diffusion][Epoch 9180] Epoch 9181/12000
2024-11-05 01:26:25,782 - INFO - [diffusion][Epoch 9180] diffusion training Loss: 0.06507298350334167
2024-11-05 01:26:25,785 - INFO - [diffusion][Epoch 9180] diffusion learning rate: 0.001
2024-11-05 01:26:25,786 - INFO - [diffusion][Epoch 9180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:25,788 - INFO - [diffusion][Epoch 9181] Epoch 9182/12000
2024-11-05 01:26:28,841 - INFO - [diffusion][Epoch 9181] diffusion training Loss: 0.06533747166395187
2024-11-05 01:26:28,843 - INFO - [diffusion][Epoch 9181] diffusion learning rate: 0.001
2024-11-05 01:26:28,845 - INFO - [diffusion][Epoch 9181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:28,846 - INFO - [diffusion][Epoch 9182] Epoch 9183/12000
2024-11-05 01:26:32,411 - INFO - [diffusion][Epoch 9182] diffusion training Loss: 0.06829367578029633
2024-11-05 01:26:32,414 - INFO - [diffusion][Epoch 9182] diffusion learning rate: 0.001
2024-11-05 01:26:32,417 - INFO - [diffusion][Epoch 9182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:32,419 - INFO - [diffusion][Epoch 9183] Epoch 9184/12000
2024-11-05 01:26:35,899 - INFO - [diffusion][Epoch 9183] diffusion training Loss: 0.06862948089838028
2024-11-05 01:26:35,901 - INFO - [diffusion][Epoch 9183] diffusion learning rate: 0.001
2024-11-05 01:26:35,903 - INFO - [diffusion][Epoch 9183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:35,904 - INFO - [diffusion][Epoch 9184] Epoch 9185/12000
2024-11-05 01:26:38,719 - INFO - [diffusion][Epoch 9184] diffusion training Loss: 0.07162188366055489
2024-11-05 01:26:38,721 - INFO - [diffusion][Epoch 9184] diffusion learning rate: 0.001
2024-11-05 01:26:38,766 - INFO - [diffusion][Epoch 9184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:38,767 - INFO - [diffusion][Epoch 9185] Epoch 9186/12000
2024-11-05 01:26:41,876 - INFO - [diffusion][Epoch 9185] diffusion training Loss: 0.06542023923248053
2024-11-05 01:26:41,878 - INFO - [diffusion][Epoch 9185] diffusion learning rate: 0.001
2024-11-05 01:26:41,880 - INFO - [diffusion][Epoch 9185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:41,881 - INFO - [diffusion][Epoch 9186] Epoch 9187/12000
2024-11-05 01:26:45,313 - INFO - [diffusion][Epoch 9186] diffusion training Loss: 0.0630574468523264
2024-11-05 01:26:45,315 - INFO - [diffusion][Epoch 9186] diffusion learning rate: 0.001
2024-11-05 01:26:45,317 - INFO - [diffusion][Epoch 9186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:45,318 - INFO - [diffusion][Epoch 9187] Epoch 9188/12000
2024-11-05 01:26:48,851 - INFO - [diffusion][Epoch 9187] diffusion training Loss: 0.07232625968754292
2024-11-05 01:26:48,853 - INFO - [diffusion][Epoch 9187] diffusion learning rate: 0.001
2024-11-05 01:26:48,855 - INFO - [diffusion][Epoch 9187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:48,857 - INFO - [diffusion][Epoch 9188] Epoch 9189/12000
2024-11-05 01:26:52,316 - INFO - [diffusion][Epoch 9188] diffusion training Loss: 0.06576511822640896
2024-11-05 01:26:52,317 - INFO - [diffusion][Epoch 9188] diffusion learning rate: 0.001
2024-11-05 01:26:52,319 - INFO - [diffusion][Epoch 9188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:52,320 - INFO - [diffusion][Epoch 9189] Epoch 9190/12000
2024-11-05 01:26:55,387 - INFO - [diffusion][Epoch 9189] diffusion training Loss: 0.06302437372505665
2024-11-05 01:26:55,389 - INFO - [diffusion][Epoch 9189] diffusion learning rate: 0.001
2024-11-05 01:26:55,391 - INFO - [diffusion][Epoch 9189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:55,392 - INFO - [diffusion][Epoch 9190] Epoch 9191/12000
2024-11-05 01:26:58,466 - INFO - [diffusion][Epoch 9190] diffusion training Loss: 0.06578880827873945
2024-11-05 01:26:58,468 - INFO - [diffusion][Epoch 9190] diffusion learning rate: 0.001
2024-11-05 01:26:58,470 - INFO - [diffusion][Epoch 9190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:58,471 - INFO - [diffusion][Epoch 9191] Epoch 9192/12000
2024-11-05 01:27:01,991 - INFO - [diffusion][Epoch 9191] diffusion training Loss: 0.06239078938961029
2024-11-05 01:27:01,993 - INFO - [diffusion][Epoch 9191] diffusion learning rate: 0.001
2024-11-05 01:27:01,995 - INFO - [diffusion][Epoch 9191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:01,996 - INFO - [diffusion][Epoch 9192] Epoch 9193/12000
2024-11-05 01:27:05,727 - INFO - [diffusion][Epoch 9192] diffusion training Loss: 0.070774395018816
2024-11-05 01:27:05,729 - INFO - [diffusion][Epoch 9192] diffusion learning rate: 0.001
2024-11-05 01:27:05,730 - INFO - [diffusion][Epoch 9192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:05,732 - INFO - [diffusion][Epoch 9193] Epoch 9194/12000
2024-11-05 01:27:08,843 - INFO - [diffusion][Epoch 9193] diffusion training Loss: 0.07063491549342871
2024-11-05 01:27:08,845 - INFO - [diffusion][Epoch 9193] diffusion learning rate: 0.001
2024-11-05 01:27:08,847 - INFO - [diffusion][Epoch 9193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:08,848 - INFO - [diffusion][Epoch 9194] Epoch 9195/12000
2024-11-05 01:27:11,922 - INFO - [diffusion][Epoch 9194] diffusion training Loss: 0.06813841871917248
2024-11-05 01:27:11,924 - INFO - [diffusion][Epoch 9194] diffusion learning rate: 0.001
2024-11-05 01:27:11,926 - INFO - [diffusion][Epoch 9194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:11,927 - INFO - [diffusion][Epoch 9195] Epoch 9196/12000
2024-11-05 01:27:15,010 - INFO - [diffusion][Epoch 9195] diffusion training Loss: 0.0614798478782177
2024-11-05 01:27:15,011 - INFO - [diffusion][Epoch 9195] diffusion learning rate: 0.001
2024-11-05 01:27:15,013 - INFO - [diffusion][Epoch 9195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:15,015 - INFO - [diffusion][Epoch 9196] Epoch 9197/12000
2024-11-05 01:27:18,531 - INFO - [diffusion][Epoch 9196] diffusion training Loss: 0.06754706799983978
2024-11-05 01:27:18,533 - INFO - [diffusion][Epoch 9196] diffusion learning rate: 0.001
2024-11-05 01:27:18,535 - INFO - [diffusion][Epoch 9196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:18,536 - INFO - [diffusion][Epoch 9197] Epoch 9198/12000
2024-11-05 01:27:22,079 - INFO - [diffusion][Epoch 9197] diffusion training Loss: 0.06822602264583111
2024-11-05 01:27:22,081 - INFO - [diffusion][Epoch 9197] diffusion learning rate: 0.001
2024-11-05 01:27:22,083 - INFO - [diffusion][Epoch 9197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:22,084 - INFO - [diffusion][Epoch 9198] Epoch 9199/12000
2024-11-05 01:27:25,213 - INFO - [diffusion][Epoch 9198] diffusion training Loss: 0.0675894096493721
2024-11-05 01:27:25,215 - INFO - [diffusion][Epoch 9198] diffusion learning rate: 0.001
2024-11-05 01:27:25,217 - INFO - [diffusion][Epoch 9198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:25,218 - INFO - [diffusion][Epoch 9199] Epoch 9200/12000
2024-11-05 01:27:28,300 - INFO - [diffusion][Epoch 9199] diffusion training Loss: 0.06998399365693331
2024-11-05 01:27:28,302 - INFO - [diffusion][Epoch 9199] diffusion learning rate: 0.001
2024-11-05 01:27:28,304 - INFO - [diffusion][Epoch 9199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:28,305 - INFO - [diffusion][Epoch 9200] Epoch 9201/12000
2024-11-05 01:27:31,342 - INFO - [diffusion][Epoch 9200] diffusion training Loss: 0.06732446886599064
2024-11-05 01:27:31,344 - INFO - [diffusion][Epoch 9200] diffusion learning rate: 0.001
2024-11-05 01:27:31,346 - INFO - [diffusion][Epoch 9200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:31,348 - INFO - [diffusion][Epoch 9201] Epoch 9202/12000
2024-11-05 01:27:34,814 - INFO - [diffusion][Epoch 9201] diffusion training Loss: 0.06506071984767914
2024-11-05 01:27:34,828 - INFO - [diffusion][Epoch 9201] diffusion learning rate: 0.001
2024-11-05 01:27:34,830 - INFO - [diffusion][Epoch 9201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:34,831 - INFO - [diffusion][Epoch 9202] Epoch 9203/12000
2024-11-05 01:27:37,959 - INFO - [diffusion][Epoch 9202] diffusion training Loss: 0.07321728393435478
2024-11-05 01:27:37,962 - INFO - [diffusion][Epoch 9202] diffusion learning rate: 0.001
2024-11-05 01:27:37,967 - INFO - [diffusion][Epoch 9202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:37,971 - INFO - [diffusion][Epoch 9203] Epoch 9204/12000
2024-11-05 01:27:41,054 - INFO - [diffusion][Epoch 9203] diffusion training Loss: 0.07449714094400406
2024-11-05 01:27:41,056 - INFO - [diffusion][Epoch 9203] diffusion learning rate: 0.001
2024-11-05 01:27:41,057 - INFO - [diffusion][Epoch 9203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:41,059 - INFO - [diffusion][Epoch 9204] Epoch 9205/12000
2024-11-05 01:27:44,170 - INFO - [diffusion][Epoch 9204] diffusion training Loss: 0.06822844967246056
2024-11-05 01:27:44,171 - INFO - [diffusion][Epoch 9204] diffusion learning rate: 0.001
2024-11-05 01:27:44,173 - INFO - [diffusion][Epoch 9204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:44,174 - INFO - [diffusion][Epoch 9205] Epoch 9206/12000
2024-11-05 01:27:47,748 - INFO - [diffusion][Epoch 9205] diffusion training Loss: 0.06929808668792248
2024-11-05 01:27:47,750 - INFO - [diffusion][Epoch 9205] diffusion learning rate: 0.001
2024-11-05 01:27:47,752 - INFO - [diffusion][Epoch 9205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:47,753 - INFO - [diffusion][Epoch 9206] Epoch 9207/12000
2024-11-05 01:27:51,312 - INFO - [diffusion][Epoch 9206] diffusion training Loss: 0.07072085049003363
2024-11-05 01:27:51,315 - INFO - [diffusion][Epoch 9206] diffusion learning rate: 0.001
2024-11-05 01:27:51,316 - INFO - [diffusion][Epoch 9206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:51,318 - INFO - [diffusion][Epoch 9207] Epoch 9208/12000
2024-11-05 01:27:54,489 - INFO - [diffusion][Epoch 9207] diffusion training Loss: 0.0667915316298604
2024-11-05 01:27:54,491 - INFO - [diffusion][Epoch 9207] diffusion learning rate: 0.001
2024-11-05 01:27:54,493 - INFO - [diffusion][Epoch 9207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:54,494 - INFO - [diffusion][Epoch 9208] Epoch 9209/12000
2024-11-05 01:27:57,610 - INFO - [diffusion][Epoch 9208] diffusion training Loss: 0.06788296066224575
2024-11-05 01:27:57,613 - INFO - [diffusion][Epoch 9208] diffusion learning rate: 0.001
2024-11-05 01:27:57,615 - INFO - [diffusion][Epoch 9208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:57,616 - INFO - [diffusion][Epoch 9209] Epoch 9210/12000
2024-11-05 01:28:01,430 - INFO - [diffusion][Epoch 9209] diffusion training Loss: 0.07038640324026346
2024-11-05 01:28:01,433 - INFO - [diffusion][Epoch 9209] diffusion learning rate: 0.001
2024-11-05 01:28:01,434 - INFO - [diffusion][Epoch 9209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:01,435 - INFO - [diffusion][Epoch 9210] Epoch 9211/12000
2024-11-05 01:28:04,572 - INFO - [diffusion][Epoch 9210] diffusion training Loss: 0.06684909760951996
2024-11-05 01:28:04,574 - INFO - [diffusion][Epoch 9210] diffusion learning rate: 0.001
2024-11-05 01:28:04,576 - INFO - [diffusion][Epoch 9210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:04,577 - INFO - [diffusion][Epoch 9211] Epoch 9212/12000
2024-11-05 01:28:08,133 - INFO - [diffusion][Epoch 9211] diffusion training Loss: 0.06928938254714012
2024-11-05 01:28:08,135 - INFO - [diffusion][Epoch 9211] diffusion learning rate: 0.001
2024-11-05 01:28:08,137 - INFO - [diffusion][Epoch 9211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:08,140 - INFO - [diffusion][Epoch 9212] Epoch 9213/12000
2024-11-05 01:28:11,563 - INFO - [diffusion][Epoch 9212] diffusion training Loss: 0.06994894705712795
2024-11-05 01:28:11,565 - INFO - [diffusion][Epoch 9212] diffusion learning rate: 0.001
2024-11-05 01:28:11,567 - INFO - [diffusion][Epoch 9212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:11,568 - INFO - [diffusion][Epoch 9213] Epoch 9214/12000
2024-11-05 01:28:14,612 - INFO - [diffusion][Epoch 9213] diffusion training Loss: 0.07122797891497612
2024-11-05 01:28:14,613 - INFO - [diffusion][Epoch 9213] diffusion learning rate: 0.001
2024-11-05 01:28:14,646 - INFO - [diffusion][Epoch 9213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:14,647 - INFO - [diffusion][Epoch 9214] Epoch 9215/12000
2024-11-05 01:28:17,720 - INFO - [diffusion][Epoch 9214] diffusion training Loss: 0.07111586537212133
2024-11-05 01:28:17,722 - INFO - [diffusion][Epoch 9214] diffusion learning rate: 0.001
2024-11-05 01:28:17,724 - INFO - [diffusion][Epoch 9214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:17,725 - INFO - [diffusion][Epoch 9215] Epoch 9216/12000
2024-11-05 01:28:21,211 - INFO - [diffusion][Epoch 9215] diffusion training Loss: 0.0673280255869031
2024-11-05 01:28:21,213 - INFO - [diffusion][Epoch 9215] diffusion learning rate: 0.001
2024-11-05 01:28:21,215 - INFO - [diffusion][Epoch 9215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:21,216 - INFO - [diffusion][Epoch 9216] Epoch 9217/12000
2024-11-05 01:28:24,812 - INFO - [diffusion][Epoch 9216] diffusion training Loss: 0.06596798915416002
2024-11-05 01:28:24,845 - INFO - [diffusion][Epoch 9216] diffusion learning rate: 0.001
2024-11-05 01:28:24,847 - INFO - [diffusion][Epoch 9216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:24,848 - INFO - [diffusion][Epoch 9217] Epoch 9218/12000
2024-11-05 01:28:27,991 - INFO - [diffusion][Epoch 9217] diffusion training Loss: 0.06985917780548334
2024-11-05 01:28:27,993 - INFO - [diffusion][Epoch 9217] diffusion learning rate: 0.001
2024-11-05 01:28:27,995 - INFO - [diffusion][Epoch 9217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:27,996 - INFO - [diffusion][Epoch 9218] Epoch 9219/12000
2024-11-05 01:28:31,051 - INFO - [diffusion][Epoch 9218] diffusion training Loss: 0.06815756298601627
2024-11-05 01:28:31,053 - INFO - [diffusion][Epoch 9218] diffusion learning rate: 0.001
2024-11-05 01:28:31,055 - INFO - [diffusion][Epoch 9218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:31,057 - INFO - [diffusion][Epoch 9219] Epoch 9220/12000
2024-11-05 01:28:34,178 - INFO - [diffusion][Epoch 9219] diffusion training Loss: 0.07052136957645416
2024-11-05 01:28:34,180 - INFO - [diffusion][Epoch 9219] diffusion learning rate: 0.001
2024-11-05 01:28:34,182 - INFO - [diffusion][Epoch 9219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:34,184 - INFO - [diffusion][Epoch 9220] Epoch 9221/12000
2024-11-05 01:28:37,647 - INFO - [diffusion][Epoch 9220] diffusion training Loss: 0.06921179126948118
2024-11-05 01:28:37,648 - INFO - [diffusion][Epoch 9220] diffusion learning rate: 0.001
2024-11-05 01:28:37,650 - INFO - [diffusion][Epoch 9220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:37,651 - INFO - [diffusion][Epoch 9221] Epoch 9222/12000
2024-11-05 01:28:41,122 - INFO - [diffusion][Epoch 9221] diffusion training Loss: 0.0661712046712637
2024-11-05 01:28:41,124 - INFO - [diffusion][Epoch 9221] diffusion learning rate: 0.001
2024-11-05 01:28:41,163 - INFO - [diffusion][Epoch 9221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:41,164 - INFO - [diffusion][Epoch 9222] Epoch 9223/12000
2024-11-05 01:28:44,205 - INFO - [diffusion][Epoch 9222] diffusion training Loss: 0.06869456358253956
2024-11-05 01:28:44,207 - INFO - [diffusion][Epoch 9222] diffusion learning rate: 0.001
2024-11-05 01:28:44,208 - INFO - [diffusion][Epoch 9222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:44,210 - INFO - [diffusion][Epoch 9223] Epoch 9224/12000
2024-11-05 01:28:47,400 - INFO - [diffusion][Epoch 9223] diffusion training Loss: 0.07136497646570206
2024-11-05 01:28:47,402 - INFO - [diffusion][Epoch 9223] diffusion learning rate: 0.001
2024-11-05 01:28:47,404 - INFO - [diffusion][Epoch 9223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:47,405 - INFO - [diffusion][Epoch 9224] Epoch 9225/12000
2024-11-05 01:28:50,706 - INFO - [diffusion][Epoch 9224] diffusion training Loss: 0.0659706313163042
2024-11-05 01:28:50,708 - INFO - [diffusion][Epoch 9224] diffusion learning rate: 0.001
2024-11-05 01:28:50,710 - INFO - [diffusion][Epoch 9224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:50,711 - INFO - [diffusion][Epoch 9225] Epoch 9226/12000
2024-11-05 01:28:54,365 - INFO - [diffusion][Epoch 9225] diffusion training Loss: 0.06795844994485378
2024-11-05 01:28:54,368 - INFO - [diffusion][Epoch 9225] diffusion learning rate: 0.001
2024-11-05 01:28:54,370 - INFO - [diffusion][Epoch 9225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:54,371 - INFO - [diffusion][Epoch 9226] Epoch 9227/12000
2024-11-05 01:28:57,888 - INFO - [diffusion][Epoch 9226] diffusion training Loss: 0.06987892277538776
2024-11-05 01:28:57,889 - INFO - [diffusion][Epoch 9226] diffusion learning rate: 0.001
2024-11-05 01:28:57,891 - INFO - [diffusion][Epoch 9226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:57,893 - INFO - [diffusion][Epoch 9227] Epoch 9228/12000
2024-11-05 01:29:01,013 - INFO - [diffusion][Epoch 9227] diffusion training Loss: 0.06813307665288448
2024-11-05 01:29:01,015 - INFO - [diffusion][Epoch 9227] diffusion learning rate: 0.001
2024-11-05 01:29:01,017 - INFO - [diffusion][Epoch 9227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:01,018 - INFO - [diffusion][Epoch 9228] Epoch 9229/12000
2024-11-05 01:29:04,145 - INFO - [diffusion][Epoch 9228] diffusion training Loss: 0.06836925260722637
2024-11-05 01:29:04,147 - INFO - [diffusion][Epoch 9228] diffusion learning rate: 0.001
2024-11-05 01:29:04,149 - INFO - [diffusion][Epoch 9228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:04,150 - INFO - [diffusion][Epoch 9229] Epoch 9230/12000
2024-11-05 01:29:07,683 - INFO - [diffusion][Epoch 9229] diffusion training Loss: 0.06537443585693836
2024-11-05 01:29:07,685 - INFO - [diffusion][Epoch 9229] diffusion learning rate: 0.001
2024-11-05 01:29:07,686 - INFO - [diffusion][Epoch 9229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:07,688 - INFO - [diffusion][Epoch 9230] Epoch 9231/12000
2024-11-05 01:29:11,186 - INFO - [diffusion][Epoch 9230] diffusion training Loss: 0.06422445923089981
2024-11-05 01:29:11,188 - INFO - [diffusion][Epoch 9230] diffusion learning rate: 0.001
2024-11-05 01:29:11,190 - INFO - [diffusion][Epoch 9230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:11,191 - INFO - [diffusion][Epoch 9231] Epoch 9232/12000
2024-11-05 01:29:14,810 - INFO - [diffusion][Epoch 9231] diffusion training Loss: 0.06969025731086731
2024-11-05 01:29:14,812 - INFO - [diffusion][Epoch 9231] diffusion learning rate: 0.001
2024-11-05 01:29:14,814 - INFO - [diffusion][Epoch 9231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:14,815 - INFO - [diffusion][Epoch 9232] Epoch 9233/12000
2024-11-05 01:29:18,109 - INFO - [diffusion][Epoch 9232] diffusion training Loss: 0.0648756492882967
2024-11-05 01:29:18,111 - INFO - [diffusion][Epoch 9232] diffusion learning rate: 0.001
2024-11-05 01:29:18,163 - INFO - [diffusion][Epoch 9232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:18,165 - INFO - [diffusion][Epoch 9233] Epoch 9234/12000
2024-11-05 01:29:21,372 - INFO - [diffusion][Epoch 9233] diffusion training Loss: 0.06915678083896637
2024-11-05 01:29:21,374 - INFO - [diffusion][Epoch 9233] diffusion learning rate: 0.001
2024-11-05 01:29:21,376 - INFO - [diffusion][Epoch 9233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:21,377 - INFO - [diffusion][Epoch 9234] Epoch 9235/12000
2024-11-05 01:29:24,466 - INFO - [diffusion][Epoch 9234] diffusion training Loss: 0.071506692096591
2024-11-05 01:29:24,468 - INFO - [diffusion][Epoch 9234] diffusion learning rate: 0.001
2024-11-05 01:29:24,469 - INFO - [diffusion][Epoch 9234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:24,471 - INFO - [diffusion][Epoch 9235] Epoch 9236/12000
2024-11-05 01:29:27,893 - INFO - [diffusion][Epoch 9235] diffusion training Loss: 0.06854305509477854
2024-11-05 01:29:27,895 - INFO - [diffusion][Epoch 9235] diffusion learning rate: 0.001
2024-11-05 01:29:27,897 - INFO - [diffusion][Epoch 9235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:27,898 - INFO - [diffusion][Epoch 9236] Epoch 9237/12000
2024-11-05 01:29:31,561 - INFO - [diffusion][Epoch 9236] diffusion training Loss: 0.06570730078965425
2024-11-05 01:29:31,563 - INFO - [diffusion][Epoch 9236] diffusion learning rate: 0.001
2024-11-05 01:29:31,565 - INFO - [diffusion][Epoch 9236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:31,566 - INFO - [diffusion][Epoch 9237] Epoch 9238/12000
2024-11-05 01:29:34,662 - INFO - [diffusion][Epoch 9237] diffusion training Loss: 0.0701728630810976
2024-11-05 01:29:34,664 - INFO - [diffusion][Epoch 9237] diffusion learning rate: 0.001
2024-11-05 01:29:34,698 - INFO - [diffusion][Epoch 9237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:34,699 - INFO - [diffusion][Epoch 9238] Epoch 9239/12000
2024-11-05 01:29:37,810 - INFO - [diffusion][Epoch 9238] diffusion training Loss: 0.06775948125869036
2024-11-05 01:29:37,812 - INFO - [diffusion][Epoch 9238] diffusion learning rate: 0.001
2024-11-05 01:29:37,814 - INFO - [diffusion][Epoch 9238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:37,816 - INFO - [diffusion][Epoch 9239] Epoch 9240/12000
2024-11-05 01:29:40,864 - INFO - [diffusion][Epoch 9239] diffusion training Loss: 0.06457679532468319
2024-11-05 01:29:40,866 - INFO - [diffusion][Epoch 9239] diffusion learning rate: 0.001
2024-11-05 01:29:40,868 - INFO - [diffusion][Epoch 9239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:40,870 - INFO - [diffusion][Epoch 9240] Epoch 9241/12000
2024-11-05 01:29:44,363 - INFO - [diffusion][Epoch 9240] diffusion training Loss: 0.07051211223006248
2024-11-05 01:29:44,365 - INFO - [diffusion][Epoch 9240] diffusion learning rate: 0.001
2024-11-05 01:29:44,367 - INFO - [diffusion][Epoch 9240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:44,368 - INFO - [diffusion][Epoch 9241] Epoch 9242/12000
2024-11-05 01:29:47,865 - INFO - [diffusion][Epoch 9241] diffusion training Loss: 0.07336996681988239
2024-11-05 01:29:47,867 - INFO - [diffusion][Epoch 9241] diffusion learning rate: 0.001
2024-11-05 01:29:47,869 - INFO - [diffusion][Epoch 9241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:47,870 - INFO - [diffusion][Epoch 9242] Epoch 9243/12000
2024-11-05 01:29:50,970 - INFO - [diffusion][Epoch 9242] diffusion training Loss: 0.07180403918027878
2024-11-05 01:29:50,972 - INFO - [diffusion][Epoch 9242] diffusion learning rate: 0.001
2024-11-05 01:29:50,974 - INFO - [diffusion][Epoch 9242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:50,975 - INFO - [diffusion][Epoch 9243] Epoch 9244/12000
2024-11-05 01:29:54,095 - INFO - [diffusion][Epoch 9243] diffusion training Loss: 0.07104678265750408
2024-11-05 01:29:54,097 - INFO - [diffusion][Epoch 9243] diffusion learning rate: 0.001
2024-11-05 01:29:54,099 - INFO - [diffusion][Epoch 9243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:54,100 - INFO - [diffusion][Epoch 9244] Epoch 9245/12000
2024-11-05 01:29:57,248 - INFO - [diffusion][Epoch 9244] diffusion training Loss: 0.07465612143278122
2024-11-05 01:29:57,250 - INFO - [diffusion][Epoch 9244] diffusion learning rate: 0.001
2024-11-05 01:29:57,252 - INFO - [diffusion][Epoch 9244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:57,253 - INFO - [diffusion][Epoch 9245] Epoch 9246/12000
2024-11-05 01:30:00,758 - INFO - [diffusion][Epoch 9245] diffusion training Loss: 0.06987268105149269
2024-11-05 01:30:00,760 - INFO - [diffusion][Epoch 9245] diffusion learning rate: 0.001
2024-11-05 01:30:00,762 - INFO - [diffusion][Epoch 9245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:00,763 - INFO - [diffusion][Epoch 9246] Epoch 9247/12000
2024-11-05 01:30:04,272 - INFO - [diffusion][Epoch 9246] diffusion training Loss: 0.07069512084126472
2024-11-05 01:30:04,275 - INFO - [diffusion][Epoch 9246] diffusion learning rate: 0.001
2024-11-05 01:30:04,277 - INFO - [diffusion][Epoch 9246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:04,278 - INFO - [diffusion][Epoch 9247] Epoch 9248/12000
2024-11-05 01:30:07,373 - INFO - [diffusion][Epoch 9247] diffusion training Loss: 0.07160446792840958
2024-11-05 01:30:07,375 - INFO - [diffusion][Epoch 9247] diffusion learning rate: 0.001
2024-11-05 01:30:07,377 - INFO - [diffusion][Epoch 9247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:07,379 - INFO - [diffusion][Epoch 9248] Epoch 9249/12000
2024-11-05 01:30:10,544 - INFO - [diffusion][Epoch 9248] diffusion training Loss: 0.0734314639121294
2024-11-05 01:30:10,546 - INFO - [diffusion][Epoch 9248] diffusion learning rate: 0.001
2024-11-05 01:30:10,548 - INFO - [diffusion][Epoch 9248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:10,549 - INFO - [diffusion][Epoch 9249] Epoch 9250/12000
2024-11-05 01:30:13,620 - INFO - [diffusion][Epoch 9249] diffusion training Loss: 0.06884843669831753
2024-11-05 01:30:13,662 - INFO - [diffusion][Epoch 9249] diffusion learning rate: 0.001
2024-11-05 01:30:13,664 - INFO - [diffusion][Epoch 9249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:13,665 - INFO - [diffusion][Epoch 9250] Epoch 9251/12000
2024-11-05 01:30:17,259 - INFO - [diffusion][Epoch 9250] diffusion training Loss: 0.07185480743646622
2024-11-05 01:30:17,261 - INFO - [diffusion][Epoch 9250] diffusion learning rate: 0.001
2024-11-05 01:30:17,263 - INFO - [diffusion][Epoch 9250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:17,265 - INFO - [diffusion][Epoch 9251] Epoch 9252/12000
2024-11-05 01:30:20,765 - INFO - [diffusion][Epoch 9251] diffusion training Loss: 0.06555204652249813
2024-11-05 01:30:20,767 - INFO - [diffusion][Epoch 9251] diffusion learning rate: 0.001
2024-11-05 01:30:20,769 - INFO - [diffusion][Epoch 9251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:20,771 - INFO - [diffusion][Epoch 9252] Epoch 9253/12000
2024-11-05 01:30:23,955 - INFO - [diffusion][Epoch 9252] diffusion training Loss: 0.07194852456450462
2024-11-05 01:30:23,957 - INFO - [diffusion][Epoch 9252] diffusion learning rate: 0.001
2024-11-05 01:30:23,959 - INFO - [diffusion][Epoch 9252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:23,960 - INFO - [diffusion][Epoch 9253] Epoch 9254/12000
2024-11-05 01:30:26,898 - INFO - [diffusion][Epoch 9253] diffusion training Loss: 0.06831644847989082
2024-11-05 01:30:26,900 - INFO - [diffusion][Epoch 9253] diffusion learning rate: 0.001
2024-11-05 01:30:26,901 - INFO - [diffusion][Epoch 9253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:26,903 - INFO - [diffusion][Epoch 9254] Epoch 9255/12000
2024-11-05 01:30:30,216 - INFO - [diffusion][Epoch 9254] diffusion training Loss: 0.06293505616486073
2024-11-05 01:30:30,218 - INFO - [diffusion][Epoch 9254] diffusion learning rate: 0.001
2024-11-05 01:30:30,220 - INFO - [diffusion][Epoch 9254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:30,221 - INFO - [diffusion][Epoch 9255] Epoch 9256/12000
2024-11-05 01:30:33,869 - INFO - [diffusion][Epoch 9255] diffusion training Loss: 0.0703621581196785
2024-11-05 01:30:33,871 - INFO - [diffusion][Epoch 9255] diffusion learning rate: 0.001
2024-11-05 01:30:33,873 - INFO - [diffusion][Epoch 9255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:33,874 - INFO - [diffusion][Epoch 9256] Epoch 9257/12000
2024-11-05 01:30:37,072 - INFO - [diffusion][Epoch 9256] diffusion training Loss: 0.0691837165504694
2024-11-05 01:30:37,074 - INFO - [diffusion][Epoch 9256] diffusion learning rate: 0.001
2024-11-05 01:30:37,076 - INFO - [diffusion][Epoch 9256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:37,077 - INFO - [diffusion][Epoch 9257] Epoch 9258/12000
2024-11-05 01:30:40,281 - INFO - [diffusion][Epoch 9257] diffusion training Loss: 0.07027158327400684
2024-11-05 01:30:40,283 - INFO - [diffusion][Epoch 9257] diffusion learning rate: 0.001
2024-11-05 01:30:40,285 - INFO - [diffusion][Epoch 9257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:40,286 - INFO - [diffusion][Epoch 9258] Epoch 9259/12000
2024-11-05 01:30:43,523 - INFO - [diffusion][Epoch 9258] diffusion training Loss: 0.06583023443818092
2024-11-05 01:30:43,524 - INFO - [diffusion][Epoch 9258] diffusion learning rate: 0.001
2024-11-05 01:30:43,526 - INFO - [diffusion][Epoch 9258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:43,527 - INFO - [diffusion][Epoch 9259] Epoch 9260/12000
2024-11-05 01:30:46,898 - INFO - [diffusion][Epoch 9259] diffusion training Loss: 0.06605433858931065
2024-11-05 01:30:46,900 - INFO - [diffusion][Epoch 9259] diffusion learning rate: 0.001
2024-11-05 01:30:46,921 - INFO - [diffusion][Epoch 9259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:46,923 - INFO - [diffusion][Epoch 9260] Epoch 9261/12000
2024-11-05 01:30:50,495 - INFO - [diffusion][Epoch 9260] diffusion training Loss: 0.07186499796807766
2024-11-05 01:30:50,498 - INFO - [diffusion][Epoch 9260] diffusion learning rate: 0.001
2024-11-05 01:30:50,501 - INFO - [diffusion][Epoch 9260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:50,502 - INFO - [diffusion][Epoch 9261] Epoch 9262/12000
2024-11-05 01:30:53,595 - INFO - [diffusion][Epoch 9261] diffusion training Loss: 0.06446544174104929
2024-11-05 01:30:53,597 - INFO - [diffusion][Epoch 9261] diffusion learning rate: 0.001
2024-11-05 01:30:53,598 - INFO - [diffusion][Epoch 9261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:53,600 - INFO - [diffusion][Epoch 9262] Epoch 9263/12000
2024-11-05 01:30:56,705 - INFO - [diffusion][Epoch 9262] diffusion training Loss: 0.07099264208227396
2024-11-05 01:30:56,707 - INFO - [diffusion][Epoch 9262] diffusion learning rate: 0.001
2024-11-05 01:30:56,709 - INFO - [diffusion][Epoch 9262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:56,710 - INFO - [diffusion][Epoch 9263] Epoch 9264/12000
2024-11-05 01:30:59,856 - INFO - [diffusion][Epoch 9263] diffusion training Loss: 0.06668243184685707
2024-11-05 01:30:59,858 - INFO - [diffusion][Epoch 9263] diffusion learning rate: 0.001
2024-11-05 01:30:59,860 - INFO - [diffusion][Epoch 9263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:59,861 - INFO - [diffusion][Epoch 9264] Epoch 9265/12000
2024-11-05 01:31:03,568 - INFO - [diffusion][Epoch 9264] diffusion training Loss: 0.07401753030717373
2024-11-05 01:31:03,570 - INFO - [diffusion][Epoch 9264] diffusion learning rate: 0.001
2024-11-05 01:31:03,639 - INFO - [diffusion][Epoch 9264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:03,640 - INFO - [diffusion][Epoch 9265] Epoch 9266/12000
2024-11-05 01:31:07,073 - INFO - [diffusion][Epoch 9265] diffusion training Loss: 0.06882100459188223
2024-11-05 01:31:07,075 - INFO - [diffusion][Epoch 9265] diffusion learning rate: 0.001
2024-11-05 01:31:07,077 - INFO - [diffusion][Epoch 9265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:07,078 - INFO - [diffusion][Epoch 9266] Epoch 9267/12000
2024-11-05 01:31:10,159 - INFO - [diffusion][Epoch 9266] diffusion training Loss: 0.07104234024882317
2024-11-05 01:31:10,161 - INFO - [diffusion][Epoch 9266] diffusion learning rate: 0.001
2024-11-05 01:31:10,163 - INFO - [diffusion][Epoch 9266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:10,164 - INFO - [diffusion][Epoch 9267] Epoch 9268/12000
2024-11-05 01:31:13,235 - INFO - [diffusion][Epoch 9267] diffusion training Loss: 0.07000906206667423
2024-11-05 01:31:13,237 - INFO - [diffusion][Epoch 9267] diffusion learning rate: 0.001
2024-11-05 01:31:13,239 - INFO - [diffusion][Epoch 9267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:13,240 - INFO - [diffusion][Epoch 9268] Epoch 9269/12000
2024-11-05 01:31:16,693 - INFO - [diffusion][Epoch 9268] diffusion training Loss: 0.06656347494572401
2024-11-05 01:31:16,695 - INFO - [diffusion][Epoch 9268] diffusion learning rate: 0.001
2024-11-05 01:31:16,696 - INFO - [diffusion][Epoch 9268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:16,698 - INFO - [diffusion][Epoch 9269] Epoch 9270/12000
2024-11-05 01:31:20,378 - INFO - [diffusion][Epoch 9269] diffusion training Loss: 0.06627548206597567
2024-11-05 01:31:20,379 - INFO - [diffusion][Epoch 9269] diffusion learning rate: 0.001
2024-11-05 01:31:20,381 - INFO - [diffusion][Epoch 9269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:20,382 - INFO - [diffusion][Epoch 9270] Epoch 9271/12000
2024-11-05 01:31:23,986 - INFO - [diffusion][Epoch 9270] diffusion training Loss: 0.07024305686354637
2024-11-05 01:31:23,989 - INFO - [diffusion][Epoch 9270] diffusion learning rate: 0.001
2024-11-05 01:31:23,990 - INFO - [diffusion][Epoch 9270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:23,992 - INFO - [diffusion][Epoch 9271] Epoch 9272/12000
2024-11-05 01:31:27,142 - INFO - [diffusion][Epoch 9271] diffusion training Loss: 0.06473531294614077
2024-11-05 01:31:27,144 - INFO - [diffusion][Epoch 9271] diffusion learning rate: 0.001
2024-11-05 01:31:27,145 - INFO - [diffusion][Epoch 9271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:27,147 - INFO - [diffusion][Epoch 9272] Epoch 9273/12000
2024-11-05 01:31:30,278 - INFO - [diffusion][Epoch 9272] diffusion training Loss: 0.0698488187044859
2024-11-05 01:31:30,280 - INFO - [diffusion][Epoch 9272] diffusion learning rate: 0.001
2024-11-05 01:31:30,282 - INFO - [diffusion][Epoch 9272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:30,283 - INFO - [diffusion][Epoch 9273] Epoch 9274/12000
2024-11-05 01:31:33,537 - INFO - [diffusion][Epoch 9273] diffusion training Loss: 0.07275992259383202
2024-11-05 01:31:33,539 - INFO - [diffusion][Epoch 9273] diffusion learning rate: 0.001
2024-11-05 01:31:33,541 - INFO - [diffusion][Epoch 9273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:33,542 - INFO - [diffusion][Epoch 9274] Epoch 9275/12000
2024-11-05 01:31:37,122 - INFO - [diffusion][Epoch 9274] diffusion training Loss: 0.06864911317825317
2024-11-05 01:31:37,123 - INFO - [diffusion][Epoch 9274] diffusion learning rate: 0.001
2024-11-05 01:31:37,125 - INFO - [diffusion][Epoch 9274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:37,126 - INFO - [diffusion][Epoch 9275] Epoch 9276/12000
2024-11-05 01:31:40,648 - INFO - [diffusion][Epoch 9275] diffusion training Loss: 0.07047323510050774
2024-11-05 01:31:40,650 - INFO - [diffusion][Epoch 9275] diffusion learning rate: 0.001
2024-11-05 01:31:40,651 - INFO - [diffusion][Epoch 9275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:40,653 - INFO - [diffusion][Epoch 9276] Epoch 9277/12000
2024-11-05 01:31:43,731 - INFO - [diffusion][Epoch 9276] diffusion training Loss: 0.07256978936493397
2024-11-05 01:31:43,733 - INFO - [diffusion][Epoch 9276] diffusion learning rate: 0.001
2024-11-05 01:31:43,735 - INFO - [diffusion][Epoch 9276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:43,736 - INFO - [diffusion][Epoch 9277] Epoch 9278/12000
2024-11-05 01:31:46,811 - INFO - [diffusion][Epoch 9277] diffusion training Loss: 0.0693304417654872
2024-11-05 01:31:46,812 - INFO - [diffusion][Epoch 9277] diffusion learning rate: 0.001
2024-11-05 01:31:46,814 - INFO - [diffusion][Epoch 9277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:46,815 - INFO - [diffusion][Epoch 9278] Epoch 9279/12000
2024-11-05 01:31:49,904 - INFO - [diffusion][Epoch 9278] diffusion training Loss: 0.06598641909658909
2024-11-05 01:31:49,906 - INFO - [diffusion][Epoch 9278] diffusion learning rate: 0.001
2024-11-05 01:31:49,908 - INFO - [diffusion][Epoch 9278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:49,909 - INFO - [diffusion][Epoch 9279] Epoch 9280/12000
2024-11-05 01:31:53,503 - INFO - [diffusion][Epoch 9279] diffusion training Loss: 0.07094330340623856
2024-11-05 01:31:53,504 - INFO - [diffusion][Epoch 9279] diffusion learning rate: 0.001
2024-11-05 01:31:53,506 - INFO - [diffusion][Epoch 9279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:53,507 - INFO - [diffusion][Epoch 9280] Epoch 9281/12000
2024-11-05 01:31:57,053 - INFO - [diffusion][Epoch 9280] diffusion training Loss: 0.06895488873124123
2024-11-05 01:31:57,055 - INFO - [diffusion][Epoch 9280] diffusion learning rate: 0.001
2024-11-05 01:31:57,056 - INFO - [diffusion][Epoch 9280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:57,057 - INFO - [diffusion][Epoch 9281] Epoch 9282/12000
2024-11-05 01:32:00,134 - INFO - [diffusion][Epoch 9281] diffusion training Loss: 0.06928211264312267
2024-11-05 01:32:00,136 - INFO - [diffusion][Epoch 9281] diffusion learning rate: 0.001
2024-11-05 01:32:00,138 - INFO - [diffusion][Epoch 9281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:00,139 - INFO - [diffusion][Epoch 9282] Epoch 9283/12000
2024-11-05 01:32:03,153 - INFO - [diffusion][Epoch 9282] diffusion training Loss: 0.07328624837100506
2024-11-05 01:32:03,155 - INFO - [diffusion][Epoch 9282] diffusion learning rate: 0.001
2024-11-05 01:32:03,157 - INFO - [diffusion][Epoch 9282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:03,158 - INFO - [diffusion][Epoch 9283] Epoch 9284/12000
2024-11-05 01:32:06,457 - INFO - [diffusion][Epoch 9283] diffusion training Loss: 0.06743316352367401
2024-11-05 01:32:06,459 - INFO - [diffusion][Epoch 9283] diffusion learning rate: 0.001
2024-11-05 01:32:06,461 - INFO - [diffusion][Epoch 9283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:06,462 - INFO - [diffusion][Epoch 9284] Epoch 9285/12000
2024-11-05 01:32:10,092 - INFO - [diffusion][Epoch 9284] diffusion training Loss: 0.07226807065308094
2024-11-05 01:32:10,095 - INFO - [diffusion][Epoch 9284] diffusion learning rate: 0.001
2024-11-05 01:32:10,097 - INFO - [diffusion][Epoch 9284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:10,098 - INFO - [diffusion][Epoch 9285] Epoch 9286/12000
2024-11-05 01:32:13,686 - INFO - [diffusion][Epoch 9285] diffusion training Loss: 0.0646224357187748
2024-11-05 01:32:13,688 - INFO - [diffusion][Epoch 9285] diffusion learning rate: 0.001
2024-11-05 01:32:13,690 - INFO - [diffusion][Epoch 9285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:13,692 - INFO - [diffusion][Epoch 9286] Epoch 9287/12000
2024-11-05 01:32:16,803 - INFO - [diffusion][Epoch 9286] diffusion training Loss: 0.06551015190780163
2024-11-05 01:32:16,805 - INFO - [diffusion][Epoch 9286] diffusion learning rate: 0.001
2024-11-05 01:32:16,807 - INFO - [diffusion][Epoch 9286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:16,808 - INFO - [diffusion][Epoch 9287] Epoch 9288/12000
2024-11-05 01:32:19,995 - INFO - [diffusion][Epoch 9287] diffusion training Loss: 0.07250088639557362
2024-11-05 01:32:19,997 - INFO - [diffusion][Epoch 9287] diffusion learning rate: 0.001
2024-11-05 01:32:19,999 - INFO - [diffusion][Epoch 9287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:20,000 - INFO - [diffusion][Epoch 9288] Epoch 9289/12000
2024-11-05 01:32:23,202 - INFO - [diffusion][Epoch 9288] diffusion training Loss: 0.06903676502406597
2024-11-05 01:32:23,204 - INFO - [diffusion][Epoch 9288] diffusion learning rate: 0.001
2024-11-05 01:32:23,206 - INFO - [diffusion][Epoch 9288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:23,207 - INFO - [diffusion][Epoch 9289] Epoch 9290/12000
2024-11-05 01:32:26,889 - INFO - [diffusion][Epoch 9289] diffusion training Loss: 0.06640217546373606
2024-11-05 01:32:26,890 - INFO - [diffusion][Epoch 9289] diffusion learning rate: 0.001
2024-11-05 01:32:26,892 - INFO - [diffusion][Epoch 9289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:26,894 - INFO - [diffusion][Epoch 9290] Epoch 9291/12000
2024-11-05 01:32:30,039 - INFO - [diffusion][Epoch 9290] diffusion training Loss: 0.07112917862832546
2024-11-05 01:32:30,041 - INFO - [diffusion][Epoch 9290] diffusion learning rate: 0.001
2024-11-05 01:32:30,043 - INFO - [diffusion][Epoch 9290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:30,044 - INFO - [diffusion][Epoch 9291] Epoch 9292/12000
2024-11-05 01:32:33,132 - INFO - [diffusion][Epoch 9291] diffusion training Loss: 0.06786696426570415
2024-11-05 01:32:33,134 - INFO - [diffusion][Epoch 9291] diffusion learning rate: 0.001
2024-11-05 01:32:33,136 - INFO - [diffusion][Epoch 9291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:33,137 - INFO - [diffusion][Epoch 9292] Epoch 9293/12000
2024-11-05 01:32:36,147 - INFO - [diffusion][Epoch 9292] diffusion training Loss: 0.07120832614600658
2024-11-05 01:32:36,149 - INFO - [diffusion][Epoch 9292] diffusion learning rate: 0.001
2024-11-05 01:32:36,150 - INFO - [diffusion][Epoch 9292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:36,152 - INFO - [diffusion][Epoch 9293] Epoch 9294/12000
2024-11-05 01:32:39,599 - INFO - [diffusion][Epoch 9293] diffusion training Loss: 0.06043890863656998
2024-11-05 01:32:39,600 - INFO - [diffusion][Epoch 9293] diffusion learning rate: 0.001
2024-11-05 01:32:39,626 - INFO - [diffusion][Epoch 9293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:39,627 - INFO - [diffusion][Epoch 9294] Epoch 9295/12000
2024-11-05 01:32:43,006 - INFO - [diffusion][Epoch 9294] diffusion training Loss: 0.06835847347974777
2024-11-05 01:32:43,008 - INFO - [diffusion][Epoch 9294] diffusion learning rate: 0.001
2024-11-05 01:32:43,010 - INFO - [diffusion][Epoch 9294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:43,012 - INFO - [diffusion][Epoch 9295] Epoch 9296/12000
2024-11-05 01:32:45,932 - INFO - [diffusion][Epoch 9295] diffusion training Loss: 0.0659788902848959
2024-11-05 01:32:45,934 - INFO - [diffusion][Epoch 9295] diffusion learning rate: 0.001
2024-11-05 01:32:45,936 - INFO - [diffusion][Epoch 9295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:45,937 - INFO - [diffusion][Epoch 9296] Epoch 9297/12000
2024-11-05 01:32:49,008 - INFO - [diffusion][Epoch 9296] diffusion training Loss: 0.06870281137526035
2024-11-05 01:32:49,009 - INFO - [diffusion][Epoch 9296] diffusion learning rate: 0.001
2024-11-05 01:32:49,011 - INFO - [diffusion][Epoch 9296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:49,012 - INFO - [diffusion][Epoch 9297] Epoch 9298/12000
2024-11-05 01:32:52,499 - INFO - [diffusion][Epoch 9297] diffusion training Loss: 0.06491347402334213
2024-11-05 01:32:52,501 - INFO - [diffusion][Epoch 9297] diffusion learning rate: 0.001
2024-11-05 01:32:52,502 - INFO - [diffusion][Epoch 9297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:52,504 - INFO - [diffusion][Epoch 9298] Epoch 9299/12000
2024-11-05 01:32:56,123 - INFO - [diffusion][Epoch 9298] diffusion training Loss: 0.0659025926142931
2024-11-05 01:32:56,125 - INFO - [diffusion][Epoch 9298] diffusion learning rate: 0.001
2024-11-05 01:32:56,149 - INFO - [diffusion][Epoch 9298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:56,151 - INFO - [diffusion][Epoch 9299] Epoch 9300/12000
2024-11-05 01:32:59,449 - INFO - [diffusion][Epoch 9299] diffusion training Loss: 0.07177523151040077
2024-11-05 01:32:59,451 - INFO - [diffusion][Epoch 9299] diffusion learning rate: 0.001
2024-11-05 01:32:59,453 - INFO - [diffusion][Epoch 9299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:59,454 - INFO - [diffusion][Epoch 9300] Epoch 9301/12000
2024-11-05 01:33:02,484 - INFO - [diffusion][Epoch 9300] diffusion training Loss: 0.0639342712238431
2024-11-05 01:33:02,486 - INFO - [diffusion][Epoch 9300] diffusion learning rate: 0.001
2024-11-05 01:33:02,487 - INFO - [diffusion][Epoch 9300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:02,489 - INFO - [diffusion][Epoch 9301] Epoch 9302/12000
2024-11-05 01:33:05,575 - INFO - [diffusion][Epoch 9301] diffusion training Loss: 0.06798353604972363
2024-11-05 01:33:05,577 - INFO - [diffusion][Epoch 9301] diffusion learning rate: 0.001
2024-11-05 01:33:05,578 - INFO - [diffusion][Epoch 9301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:05,580 - INFO - [diffusion][Epoch 9302] Epoch 9303/12000
2024-11-05 01:33:09,101 - INFO - [diffusion][Epoch 9302] diffusion training Loss: 0.06679322756826878
2024-11-05 01:33:09,103 - INFO - [diffusion][Epoch 9302] diffusion learning rate: 0.001
2024-11-05 01:33:09,105 - INFO - [diffusion][Epoch 9302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:09,106 - INFO - [diffusion][Epoch 9303] Epoch 9304/12000
2024-11-05 01:33:12,812 - INFO - [diffusion][Epoch 9303] diffusion training Loss: 0.07169809751212597
2024-11-05 01:33:12,814 - INFO - [diffusion][Epoch 9303] diffusion learning rate: 0.001
2024-11-05 01:33:12,816 - INFO - [diffusion][Epoch 9303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:12,817 - INFO - [diffusion][Epoch 9304] Epoch 9305/12000
2024-11-05 01:33:16,277 - INFO - [diffusion][Epoch 9304] diffusion training Loss: 0.07197504490613937
2024-11-05 01:33:16,279 - INFO - [diffusion][Epoch 9304] diffusion learning rate: 0.001
2024-11-05 01:33:16,281 - INFO - [diffusion][Epoch 9304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:16,282 - INFO - [diffusion][Epoch 9305] Epoch 9306/12000
2024-11-05 01:33:19,477 - INFO - [diffusion][Epoch 9305] diffusion training Loss: 0.0662255547940731
2024-11-05 01:33:19,479 - INFO - [diffusion][Epoch 9305] diffusion learning rate: 0.001
2024-11-05 01:33:19,481 - INFO - [diffusion][Epoch 9305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:19,483 - INFO - [diffusion][Epoch 9306] Epoch 9307/12000
2024-11-05 01:33:22,633 - INFO - [diffusion][Epoch 9306] diffusion training Loss: 0.07181165739893913
2024-11-05 01:33:22,635 - INFO - [diffusion][Epoch 9306] diffusion learning rate: 0.001
2024-11-05 01:33:22,637 - INFO - [diffusion][Epoch 9306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:22,638 - INFO - [diffusion][Epoch 9307] Epoch 9308/12000
2024-11-05 01:33:25,745 - INFO - [diffusion][Epoch 9307] diffusion training Loss: 0.06097456719726324
2024-11-05 01:33:25,747 - INFO - [diffusion][Epoch 9307] diffusion learning rate: 0.001
2024-11-05 01:33:25,749 - INFO - [diffusion][Epoch 9307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:25,750 - INFO - [diffusion][Epoch 9308] Epoch 9309/12000
2024-11-05 01:33:29,273 - INFO - [diffusion][Epoch 9308] diffusion training Loss: 0.06671654991805553
2024-11-05 01:33:29,276 - INFO - [diffusion][Epoch 9308] diffusion learning rate: 0.001
2024-11-05 01:33:29,277 - INFO - [diffusion][Epoch 9308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:29,279 - INFO - [diffusion][Epoch 9309] Epoch 9310/12000
2024-11-05 01:33:33,290 - INFO - [diffusion][Epoch 9309] diffusion training Loss: 0.0710956584662199
2024-11-05 01:33:33,292 - INFO - [diffusion][Epoch 9309] diffusion learning rate: 0.001
2024-11-05 01:33:33,294 - INFO - [diffusion][Epoch 9309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:33,295 - INFO - [diffusion][Epoch 9310] Epoch 9311/12000
2024-11-05 01:33:36,690 - INFO - [diffusion][Epoch 9310] diffusion training Loss: 0.06738144345581532
2024-11-05 01:33:36,692 - INFO - [diffusion][Epoch 9310] diffusion learning rate: 0.001
2024-11-05 01:33:36,693 - INFO - [diffusion][Epoch 9310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:36,695 - INFO - [diffusion][Epoch 9311] Epoch 9312/12000
2024-11-05 01:33:39,727 - INFO - [diffusion][Epoch 9311] diffusion training Loss: 0.06793783605098724
2024-11-05 01:33:39,728 - INFO - [diffusion][Epoch 9311] diffusion learning rate: 0.001
2024-11-05 01:33:39,785 - INFO - [diffusion][Epoch 9311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:39,786 - INFO - [diffusion][Epoch 9312] Epoch 9313/12000
2024-11-05 01:33:42,990 - INFO - [diffusion][Epoch 9312] diffusion training Loss: 0.0689978376030922
2024-11-05 01:33:42,991 - INFO - [diffusion][Epoch 9312] diffusion learning rate: 0.001
2024-11-05 01:33:42,993 - INFO - [diffusion][Epoch 9312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:42,994 - INFO - [diffusion][Epoch 9313] Epoch 9314/12000
2024-11-05 01:33:46,218 - INFO - [diffusion][Epoch 9313] diffusion training Loss: 0.06968091987073421
2024-11-05 01:33:46,220 - INFO - [diffusion][Epoch 9313] diffusion learning rate: 0.001
2024-11-05 01:33:46,223 - INFO - [diffusion][Epoch 9313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:46,224 - INFO - [diffusion][Epoch 9314] Epoch 9315/12000
2024-11-05 01:33:49,666 - INFO - [diffusion][Epoch 9314] diffusion training Loss: 0.06813159584999084
2024-11-05 01:33:49,668 - INFO - [diffusion][Epoch 9314] diffusion learning rate: 0.001
2024-11-05 01:33:49,670 - INFO - [diffusion][Epoch 9314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:49,671 - INFO - [diffusion][Epoch 9315] Epoch 9316/12000
2024-11-05 01:33:52,790 - INFO - [diffusion][Epoch 9315] diffusion training Loss: 0.0693430658429861
2024-11-05 01:33:52,792 - INFO - [diffusion][Epoch 9315] diffusion learning rate: 0.001
2024-11-05 01:33:52,794 - INFO - [diffusion][Epoch 9315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:52,795 - INFO - [diffusion][Epoch 9316] Epoch 9317/12000
2024-11-05 01:33:56,088 - INFO - [diffusion][Epoch 9316] diffusion training Loss: 0.06333361193537712
2024-11-05 01:33:56,090 - INFO - [diffusion][Epoch 9316] diffusion learning rate: 0.001
2024-11-05 01:33:56,091 - INFO - [diffusion][Epoch 9316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:56,093 - INFO - [diffusion][Epoch 9317] Epoch 9318/12000
2024-11-05 01:33:59,310 - INFO - [diffusion][Epoch 9317] diffusion training Loss: 0.0669461851939559
2024-11-05 01:33:59,312 - INFO - [diffusion][Epoch 9317] diffusion learning rate: 0.001
2024-11-05 01:33:59,314 - INFO - [diffusion][Epoch 9317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:59,315 - INFO - [diffusion][Epoch 9318] Epoch 9319/12000
2024-11-05 01:34:03,027 - INFO - [diffusion][Epoch 9318] diffusion training Loss: 0.06736860610544682
2024-11-05 01:34:03,029 - INFO - [diffusion][Epoch 9318] diffusion learning rate: 0.001
2024-11-05 01:34:03,031 - INFO - [diffusion][Epoch 9318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:03,032 - INFO - [diffusion][Epoch 9319] Epoch 9320/12000
2024-11-05 01:34:06,539 - INFO - [diffusion][Epoch 9319] diffusion training Loss: 0.06523083429783583
2024-11-05 01:34:06,541 - INFO - [diffusion][Epoch 9319] diffusion learning rate: 0.001
2024-11-05 01:34:06,543 - INFO - [diffusion][Epoch 9319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:06,544 - INFO - [diffusion][Epoch 9320] Epoch 9321/12000
2024-11-05 01:34:09,663 - INFO - [diffusion][Epoch 9320] diffusion training Loss: 0.06731119938194752
2024-11-05 01:34:09,665 - INFO - [diffusion][Epoch 9320] diffusion learning rate: 0.001
2024-11-05 01:34:09,667 - INFO - [diffusion][Epoch 9320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:09,668 - INFO - [diffusion][Epoch 9321] Epoch 9322/12000
2024-11-05 01:34:12,789 - INFO - [diffusion][Epoch 9321] diffusion training Loss: 0.06588047742843628
2024-11-05 01:34:12,790 - INFO - [diffusion][Epoch 9321] diffusion learning rate: 0.001
2024-11-05 01:34:12,792 - INFO - [diffusion][Epoch 9321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:12,793 - INFO - [diffusion][Epoch 9322] Epoch 9323/12000
2024-11-05 01:34:15,927 - INFO - [diffusion][Epoch 9322] diffusion training Loss: 0.06374853476881981
2024-11-05 01:34:15,929 - INFO - [diffusion][Epoch 9322] diffusion learning rate: 0.001
2024-11-05 01:34:15,931 - INFO - [diffusion][Epoch 9322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:15,932 - INFO - [diffusion][Epoch 9323] Epoch 9324/12000
2024-11-05 01:34:19,467 - INFO - [diffusion][Epoch 9323] diffusion training Loss: 0.0675461171194911
2024-11-05 01:34:19,470 - INFO - [diffusion][Epoch 9323] diffusion learning rate: 0.001
2024-11-05 01:34:19,472 - INFO - [diffusion][Epoch 9323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:19,473 - INFO - [diffusion][Epoch 9324] Epoch 9325/12000
2024-11-05 01:34:23,199 - INFO - [diffusion][Epoch 9324] diffusion training Loss: 0.06580939516425133
2024-11-05 01:34:23,201 - INFO - [diffusion][Epoch 9324] diffusion learning rate: 0.001
2024-11-05 01:34:23,203 - INFO - [diffusion][Epoch 9324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:23,204 - INFO - [diffusion][Epoch 9325] Epoch 9326/12000
2024-11-05 01:34:26,508 - INFO - [diffusion][Epoch 9325] diffusion training Loss: 0.06700577307492495
2024-11-05 01:34:26,510 - INFO - [diffusion][Epoch 9325] diffusion learning rate: 0.001
2024-11-05 01:34:26,511 - INFO - [diffusion][Epoch 9325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:26,513 - INFO - [diffusion][Epoch 9326] Epoch 9327/12000
2024-11-05 01:34:29,822 - INFO - [diffusion][Epoch 9326] diffusion training Loss: 0.06684870272874832
2024-11-05 01:34:29,826 - INFO - [diffusion][Epoch 9326] diffusion learning rate: 0.001
2024-11-05 01:34:29,827 - INFO - [diffusion][Epoch 9326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:29,829 - INFO - [diffusion][Epoch 9327] Epoch 9328/12000
2024-11-05 01:34:33,088 - INFO - [diffusion][Epoch 9327] diffusion training Loss: 0.06407568603754044
2024-11-05 01:34:33,090 - INFO - [diffusion][Epoch 9327] diffusion learning rate: 0.001
2024-11-05 01:34:33,092 - INFO - [diffusion][Epoch 9327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:33,093 - INFO - [diffusion][Epoch 9328] Epoch 9329/12000
2024-11-05 01:34:36,265 - INFO - [diffusion][Epoch 9328] diffusion training Loss: 0.06971181090921164
2024-11-05 01:34:36,267 - INFO - [diffusion][Epoch 9328] diffusion learning rate: 0.001
2024-11-05 01:34:36,269 - INFO - [diffusion][Epoch 9328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:36,270 - INFO - [diffusion][Epoch 9329] Epoch 9330/12000
2024-11-05 01:34:39,912 - INFO - [diffusion][Epoch 9329] diffusion training Loss: 0.07169105857610703
2024-11-05 01:34:39,914 - INFO - [diffusion][Epoch 9329] diffusion learning rate: 0.001
2024-11-05 01:34:39,915 - INFO - [diffusion][Epoch 9329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:39,917 - INFO - [diffusion][Epoch 9330] Epoch 9331/12000
2024-11-05 01:34:43,512 - INFO - [diffusion][Epoch 9330] diffusion training Loss: 0.0656994292512536
2024-11-05 01:34:43,514 - INFO - [diffusion][Epoch 9330] diffusion learning rate: 0.001
2024-11-05 01:34:43,516 - INFO - [diffusion][Epoch 9330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:43,517 - INFO - [diffusion][Epoch 9331] Epoch 9332/12000
2024-11-05 01:34:47,015 - INFO - [diffusion][Epoch 9331] diffusion training Loss: 0.06620282959192991
2024-11-05 01:34:47,017 - INFO - [diffusion][Epoch 9331] diffusion learning rate: 0.001
2024-11-05 01:34:47,019 - INFO - [diffusion][Epoch 9331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:47,020 - INFO - [diffusion][Epoch 9332] Epoch 9333/12000
2024-11-05 01:34:50,082 - INFO - [diffusion][Epoch 9332] diffusion training Loss: 0.06846116669476032
2024-11-05 01:34:50,084 - INFO - [diffusion][Epoch 9332] diffusion learning rate: 0.001
2024-11-05 01:34:50,086 - INFO - [diffusion][Epoch 9332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:50,087 - INFO - [diffusion][Epoch 9333] Epoch 9334/12000
2024-11-05 01:34:53,183 - INFO - [diffusion][Epoch 9333] diffusion training Loss: 0.06629099696874619
2024-11-05 01:34:53,185 - INFO - [diffusion][Epoch 9333] diffusion learning rate: 0.001
2024-11-05 01:34:53,187 - INFO - [diffusion][Epoch 9333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:53,188 - INFO - [diffusion][Epoch 9334] Epoch 9335/12000
2024-11-05 01:34:56,612 - INFO - [diffusion][Epoch 9334] diffusion training Loss: 0.07113842479884624
2024-11-05 01:34:56,613 - INFO - [diffusion][Epoch 9334] diffusion learning rate: 0.001
2024-11-05 01:34:56,615 - INFO - [diffusion][Epoch 9334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:56,617 - INFO - [diffusion][Epoch 9335] Epoch 9336/12000
2024-11-05 01:35:00,394 - INFO - [diffusion][Epoch 9335] diffusion training Loss: 0.07411449030041695
2024-11-05 01:35:00,396 - INFO - [diffusion][Epoch 9335] diffusion learning rate: 0.001
2024-11-05 01:35:00,398 - INFO - [diffusion][Epoch 9335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:00,400 - INFO - [diffusion][Epoch 9336] Epoch 9337/12000
2024-11-05 01:35:03,841 - INFO - [diffusion][Epoch 9336] diffusion training Loss: 0.0689471960067749
2024-11-05 01:35:03,843 - INFO - [diffusion][Epoch 9336] diffusion learning rate: 0.001
2024-11-05 01:35:03,844 - INFO - [diffusion][Epoch 9336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:03,846 - INFO - [diffusion][Epoch 9337] Epoch 9338/12000
2024-11-05 01:35:07,037 - INFO - [diffusion][Epoch 9337] diffusion training Loss: 0.06939915753901005
2024-11-05 01:35:07,039 - INFO - [diffusion][Epoch 9337] diffusion learning rate: 0.001
2024-11-05 01:35:07,040 - INFO - [diffusion][Epoch 9337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:07,042 - INFO - [diffusion][Epoch 9338] Epoch 9339/12000
2024-11-05 01:35:09,982 - INFO - [diffusion][Epoch 9338] diffusion training Loss: 0.06073477864265442
2024-11-05 01:35:09,984 - INFO - [diffusion][Epoch 9338] diffusion learning rate: 0.001
2024-11-05 01:35:09,986 - INFO - [diffusion][Epoch 9338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:09,987 - INFO - [diffusion][Epoch 9339] Epoch 9340/12000
2024-11-05 01:35:13,481 - INFO - [diffusion][Epoch 9339] diffusion training Loss: 0.07060259766876698
2024-11-05 01:35:13,483 - INFO - [diffusion][Epoch 9339] diffusion learning rate: 0.001
2024-11-05 01:35:13,484 - INFO - [diffusion][Epoch 9339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:13,486 - INFO - [diffusion][Epoch 9340] Epoch 9341/12000
2024-11-05 01:35:16,978 - INFO - [diffusion][Epoch 9340] diffusion training Loss: 0.07169758155941963
2024-11-05 01:35:16,980 - INFO - [diffusion][Epoch 9340] diffusion learning rate: 0.001
2024-11-05 01:35:16,982 - INFO - [diffusion][Epoch 9340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:16,983 - INFO - [diffusion][Epoch 9341] Epoch 9342/12000
2024-11-05 01:35:20,085 - INFO - [diffusion][Epoch 9341] diffusion training Loss: 0.07164030522108078
2024-11-05 01:35:20,087 - INFO - [diffusion][Epoch 9341] diffusion learning rate: 0.001
2024-11-05 01:35:20,089 - INFO - [diffusion][Epoch 9341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:20,090 - INFO - [diffusion][Epoch 9342] Epoch 9343/12000
2024-11-05 01:35:23,184 - INFO - [diffusion][Epoch 9342] diffusion training Loss: 0.07409448362886906
2024-11-05 01:35:23,186 - INFO - [diffusion][Epoch 9342] diffusion learning rate: 0.001
2024-11-05 01:35:23,187 - INFO - [diffusion][Epoch 9342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:23,189 - INFO - [diffusion][Epoch 9343] Epoch 9344/12000
2024-11-05 01:35:26,337 - INFO - [diffusion][Epoch 9343] diffusion training Loss: 0.0733803790062666
2024-11-05 01:35:26,339 - INFO - [diffusion][Epoch 9343] diffusion learning rate: 0.001
2024-11-05 01:35:26,341 - INFO - [diffusion][Epoch 9343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:26,342 - INFO - [diffusion][Epoch 9344] Epoch 9345/12000
2024-11-05 01:35:29,861 - INFO - [diffusion][Epoch 9344] diffusion training Loss: 0.07125738449394703
2024-11-05 01:35:29,864 - INFO - [diffusion][Epoch 9344] diffusion learning rate: 0.001
2024-11-05 01:35:29,865 - INFO - [diffusion][Epoch 9344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:29,871 - INFO - [diffusion][Epoch 9345] Epoch 9346/12000
2024-11-05 01:35:33,615 - INFO - [diffusion][Epoch 9345] diffusion training Loss: 0.06432182434946299
2024-11-05 01:35:33,617 - INFO - [diffusion][Epoch 9345] diffusion learning rate: 0.001
2024-11-05 01:35:33,619 - INFO - [diffusion][Epoch 9345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:33,621 - INFO - [diffusion][Epoch 9346] Epoch 9347/12000
2024-11-05 01:35:36,865 - INFO - [diffusion][Epoch 9346] diffusion training Loss: 0.06453606765717268
2024-11-05 01:35:36,867 - INFO - [diffusion][Epoch 9346] diffusion learning rate: 0.001
2024-11-05 01:35:36,868 - INFO - [diffusion][Epoch 9346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:36,870 - INFO - [diffusion][Epoch 9347] Epoch 9348/12000
2024-11-05 01:35:40,007 - INFO - [diffusion][Epoch 9347] diffusion training Loss: 0.06435199361294508
2024-11-05 01:35:40,008 - INFO - [diffusion][Epoch 9347] diffusion learning rate: 0.001
2024-11-05 01:35:40,010 - INFO - [diffusion][Epoch 9347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:40,011 - INFO - [diffusion][Epoch 9348] Epoch 9349/12000
2024-11-05 01:35:43,103 - INFO - [diffusion][Epoch 9348] diffusion training Loss: 0.07700902782380581
2024-11-05 01:35:43,104 - INFO - [diffusion][Epoch 9348] diffusion learning rate: 0.001
2024-11-05 01:35:43,106 - INFO - [diffusion][Epoch 9348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:43,107 - INFO - [diffusion][Epoch 9349] Epoch 9350/12000
2024-11-05 01:35:46,713 - INFO - [diffusion][Epoch 9349] diffusion training Loss: 0.06452060863375664
2024-11-05 01:35:46,715 - INFO - [diffusion][Epoch 9349] diffusion learning rate: 0.001
2024-11-05 01:35:46,717 - INFO - [diffusion][Epoch 9349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:46,718 - INFO - [diffusion][Epoch 9350] Epoch 9351/12000
2024-11-05 01:35:50,450 - INFO - [diffusion][Epoch 9350] diffusion training Loss: 0.06668241415172815
2024-11-05 01:35:50,452 - INFO - [diffusion][Epoch 9350] diffusion learning rate: 0.001
2024-11-05 01:35:50,453 - INFO - [diffusion][Epoch 9350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:50,455 - INFO - [diffusion][Epoch 9351] Epoch 9352/12000
2024-11-05 01:35:53,793 - INFO - [diffusion][Epoch 9351] diffusion training Loss: 0.06658054329454899
2024-11-05 01:35:53,795 - INFO - [diffusion][Epoch 9351] diffusion learning rate: 0.001
2024-11-05 01:35:53,797 - INFO - [diffusion][Epoch 9351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:53,798 - INFO - [diffusion][Epoch 9352] Epoch 9353/12000
2024-11-05 01:35:56,891 - INFO - [diffusion][Epoch 9352] diffusion training Loss: 0.06985144037753344
2024-11-05 01:35:56,893 - INFO - [diffusion][Epoch 9352] diffusion learning rate: 0.001
2024-11-05 01:35:56,894 - INFO - [diffusion][Epoch 9352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:56,895 - INFO - [diffusion][Epoch 9353] Epoch 9354/12000
2024-11-05 01:36:00,039 - INFO - [diffusion][Epoch 9353] diffusion training Loss: 0.06542321853339672
2024-11-05 01:36:00,041 - INFO - [diffusion][Epoch 9353] diffusion learning rate: 0.001
2024-11-05 01:36:00,043 - INFO - [diffusion][Epoch 9353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:00,044 - INFO - [diffusion][Epoch 9354] Epoch 9355/12000
2024-11-05 01:36:03,488 - INFO - [diffusion][Epoch 9354] diffusion training Loss: 0.0698903538286686
2024-11-05 01:36:03,490 - INFO - [diffusion][Epoch 9354] diffusion learning rate: 0.001
2024-11-05 01:36:03,492 - INFO - [diffusion][Epoch 9354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:03,493 - INFO - [diffusion][Epoch 9355] Epoch 9356/12000
2024-11-05 01:36:07,131 - INFO - [diffusion][Epoch 9355] diffusion training Loss: 0.06987202167510986
2024-11-05 01:36:07,132 - INFO - [diffusion][Epoch 9355] diffusion learning rate: 0.001
2024-11-05 01:36:07,134 - INFO - [diffusion][Epoch 9355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:07,135 - INFO - [diffusion][Epoch 9356] Epoch 9357/12000
2024-11-05 01:36:10,384 - INFO - [diffusion][Epoch 9356] diffusion training Loss: 0.0648008231073618
2024-11-05 01:36:10,386 - INFO - [diffusion][Epoch 9356] diffusion learning rate: 0.001
2024-11-05 01:36:10,388 - INFO - [diffusion][Epoch 9356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:10,389 - INFO - [diffusion][Epoch 9357] Epoch 9358/12000
2024-11-05 01:36:13,502 - INFO - [diffusion][Epoch 9357] diffusion training Loss: 0.07048539817333221
2024-11-05 01:36:13,504 - INFO - [diffusion][Epoch 9357] diffusion learning rate: 0.001
2024-11-05 01:36:13,506 - INFO - [diffusion][Epoch 9357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:13,507 - INFO - [diffusion][Epoch 9358] Epoch 9359/12000
2024-11-05 01:36:16,602 - INFO - [diffusion][Epoch 9358] diffusion training Loss: 0.07396149728447199
2024-11-05 01:36:16,604 - INFO - [diffusion][Epoch 9358] diffusion learning rate: 0.001
2024-11-05 01:36:16,605 - INFO - [diffusion][Epoch 9358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:16,607 - INFO - [diffusion][Epoch 9359] Epoch 9360/12000
2024-11-05 01:36:20,128 - INFO - [diffusion][Epoch 9359] diffusion training Loss: 0.06424064375460148
2024-11-05 01:36:20,131 - INFO - [diffusion][Epoch 9359] diffusion learning rate: 0.001
2024-11-05 01:36:20,133 - INFO - [diffusion][Epoch 9359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:20,134 - INFO - [diffusion][Epoch 9360] Epoch 9361/12000
2024-11-05 01:36:23,667 - INFO - [diffusion][Epoch 9360] diffusion training Loss: 0.07319878786802292
2024-11-05 01:36:23,669 - INFO - [diffusion][Epoch 9360] diffusion learning rate: 0.001
2024-11-05 01:36:23,671 - INFO - [diffusion][Epoch 9360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:23,672 - INFO - [diffusion][Epoch 9361] Epoch 9362/12000
2024-11-05 01:36:26,778 - INFO - [diffusion][Epoch 9361] diffusion training Loss: 0.06647215969860554
2024-11-05 01:36:26,780 - INFO - [diffusion][Epoch 9361] diffusion learning rate: 0.001
2024-11-05 01:36:26,781 - INFO - [diffusion][Epoch 9361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:26,783 - INFO - [diffusion][Epoch 9362] Epoch 9363/12000
2024-11-05 01:36:29,903 - INFO - [diffusion][Epoch 9362] diffusion training Loss: 0.07205644249916077
2024-11-05 01:36:29,905 - INFO - [diffusion][Epoch 9362] diffusion learning rate: 0.001
2024-11-05 01:36:29,907 - INFO - [diffusion][Epoch 9362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:29,908 - INFO - [diffusion][Epoch 9363] Epoch 9364/12000
2024-11-05 01:36:33,039 - INFO - [diffusion][Epoch 9363] diffusion training Loss: 0.06588163133710623
2024-11-05 01:36:33,041 - INFO - [diffusion][Epoch 9363] diffusion learning rate: 0.001
2024-11-05 01:36:33,043 - INFO - [diffusion][Epoch 9363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:33,045 - INFO - [diffusion][Epoch 9364] Epoch 9365/12000
2024-11-05 01:36:36,574 - INFO - [diffusion][Epoch 9364] diffusion training Loss: 0.0671764463186264
2024-11-05 01:36:36,576 - INFO - [diffusion][Epoch 9364] diffusion learning rate: 0.001
2024-11-05 01:36:36,641 - INFO - [diffusion][Epoch 9364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:36,642 - INFO - [diffusion][Epoch 9365] Epoch 9366/12000
2024-11-05 01:36:39,798 - INFO - [diffusion][Epoch 9365] diffusion training Loss: 0.06649949308484793
2024-11-05 01:36:39,800 - INFO - [diffusion][Epoch 9365] diffusion learning rate: 0.001
2024-11-05 01:36:39,802 - INFO - [diffusion][Epoch 9365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:39,804 - INFO - [diffusion][Epoch 9366] Epoch 9367/12000
2024-11-05 01:36:43,017 - INFO - [diffusion][Epoch 9366] diffusion training Loss: 0.06693116948008537
2024-11-05 01:36:43,019 - INFO - [diffusion][Epoch 9366] diffusion learning rate: 0.001
2024-11-05 01:36:43,020 - INFO - [diffusion][Epoch 9366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:43,022 - INFO - [diffusion][Epoch 9367] Epoch 9368/12000
2024-11-05 01:36:46,296 - INFO - [diffusion][Epoch 9367] diffusion training Loss: 0.06867842935025692
2024-11-05 01:36:46,298 - INFO - [diffusion][Epoch 9367] diffusion learning rate: 0.001
2024-11-05 01:36:46,300 - INFO - [diffusion][Epoch 9367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:46,301 - INFO - [diffusion][Epoch 9368] Epoch 9369/12000
2024-11-05 01:36:49,647 - INFO - [diffusion][Epoch 9368] diffusion training Loss: 0.07070063892751932
2024-11-05 01:36:49,649 - INFO - [diffusion][Epoch 9368] diffusion learning rate: 0.001
2024-11-05 01:36:49,650 - INFO - [diffusion][Epoch 9368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:49,651 - INFO - [diffusion][Epoch 9369] Epoch 9370/12000
2024-11-05 01:36:53,342 - INFO - [diffusion][Epoch 9369] diffusion training Loss: 0.06804905273020267
2024-11-05 01:36:53,344 - INFO - [diffusion][Epoch 9369] diffusion learning rate: 0.001
2024-11-05 01:36:53,346 - INFO - [diffusion][Epoch 9369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:53,347 - INFO - [diffusion][Epoch 9370] Epoch 9371/12000
2024-11-05 01:36:57,194 - INFO - [diffusion][Epoch 9370] diffusion training Loss: 0.06662797555327415
2024-11-05 01:36:57,196 - INFO - [diffusion][Epoch 9370] diffusion learning rate: 0.001
2024-11-05 01:36:57,197 - INFO - [diffusion][Epoch 9370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:57,198 - INFO - [diffusion][Epoch 9371] Epoch 9372/12000
2024-11-05 01:37:00,281 - INFO - [diffusion][Epoch 9371] diffusion training Loss: 0.06780869327485561
2024-11-05 01:37:00,283 - INFO - [diffusion][Epoch 9371] diffusion learning rate: 0.001
2024-11-05 01:37:00,284 - INFO - [diffusion][Epoch 9371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:00,285 - INFO - [diffusion][Epoch 9372] Epoch 9373/12000
2024-11-05 01:37:03,439 - INFO - [diffusion][Epoch 9372] diffusion training Loss: 0.06837080977857113
2024-11-05 01:37:03,441 - INFO - [diffusion][Epoch 9372] diffusion learning rate: 0.001
2024-11-05 01:37:03,480 - INFO - [diffusion][Epoch 9372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:03,481 - INFO - [diffusion][Epoch 9373] Epoch 9374/12000
2024-11-05 01:37:06,600 - INFO - [diffusion][Epoch 9373] diffusion training Loss: 0.06431949976831675
2024-11-05 01:37:06,602 - INFO - [diffusion][Epoch 9373] diffusion learning rate: 0.001
2024-11-05 01:37:06,604 - INFO - [diffusion][Epoch 9373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:06,605 - INFO - [diffusion][Epoch 9374] Epoch 9375/12000
2024-11-05 01:37:10,164 - INFO - [diffusion][Epoch 9374] diffusion training Loss: 0.06701776012778282
2024-11-05 01:37:10,166 - INFO - [diffusion][Epoch 9374] diffusion learning rate: 0.001
2024-11-05 01:37:10,167 - INFO - [diffusion][Epoch 9374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:10,169 - INFO - [diffusion][Epoch 9375] Epoch 9376/12000
2024-11-05 01:37:13,697 - INFO - [diffusion][Epoch 9375] diffusion training Loss: 0.06543146073818207
2024-11-05 01:37:13,699 - INFO - [diffusion][Epoch 9375] diffusion learning rate: 0.001
2024-11-05 01:37:13,756 - INFO - [diffusion][Epoch 9375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:13,757 - INFO - [diffusion][Epoch 9376] Epoch 9377/12000
2024-11-05 01:37:16,866 - INFO - [diffusion][Epoch 9376] diffusion training Loss: 0.063686846755445
2024-11-05 01:37:16,868 - INFO - [diffusion][Epoch 9376] diffusion learning rate: 0.001
2024-11-05 01:37:16,870 - INFO - [diffusion][Epoch 9376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:16,871 - INFO - [diffusion][Epoch 9377] Epoch 9378/12000
2024-11-05 01:37:19,938 - INFO - [diffusion][Epoch 9377] diffusion training Loss: 0.06981070525944233
2024-11-05 01:37:19,940 - INFO - [diffusion][Epoch 9377] diffusion learning rate: 0.001
2024-11-05 01:37:19,942 - INFO - [diffusion][Epoch 9377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:19,943 - INFO - [diffusion][Epoch 9378] Epoch 9379/12000
2024-11-05 01:37:23,144 - INFO - [diffusion][Epoch 9378] diffusion training Loss: 0.07281111925840378
2024-11-05 01:37:23,146 - INFO - [diffusion][Epoch 9378] diffusion learning rate: 0.001
2024-11-05 01:37:23,148 - INFO - [diffusion][Epoch 9378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:23,149 - INFO - [diffusion][Epoch 9379] Epoch 9380/12000
2024-11-05 01:37:26,743 - INFO - [diffusion][Epoch 9379] diffusion training Loss: 0.07202056050300598
2024-11-05 01:37:26,745 - INFO - [diffusion][Epoch 9379] diffusion learning rate: 0.001
2024-11-05 01:37:26,748 - INFO - [diffusion][Epoch 9379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:26,749 - INFO - [diffusion][Epoch 9380] Epoch 9381/12000
2024-11-05 01:37:30,279 - INFO - [diffusion][Epoch 9380] diffusion training Loss: 0.0694270022213459
2024-11-05 01:37:30,282 - INFO - [diffusion][Epoch 9380] diffusion learning rate: 0.001
2024-11-05 01:37:30,309 - INFO - [diffusion][Epoch 9380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:30,310 - INFO - [diffusion][Epoch 9381] Epoch 9382/12000
2024-11-05 01:37:33,361 - INFO - [diffusion][Epoch 9381] diffusion training Loss: 0.07054052501916885
2024-11-05 01:37:33,363 - INFO - [diffusion][Epoch 9381] diffusion learning rate: 0.001
2024-11-05 01:37:33,365 - INFO - [diffusion][Epoch 9381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:33,366 - INFO - [diffusion][Epoch 9382] Epoch 9383/12000
2024-11-05 01:37:36,435 - INFO - [diffusion][Epoch 9382] diffusion training Loss: 0.06572378892451525
2024-11-05 01:37:36,437 - INFO - [diffusion][Epoch 9382] diffusion learning rate: 0.001
2024-11-05 01:37:36,439 - INFO - [diffusion][Epoch 9382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:36,440 - INFO - [diffusion][Epoch 9383] Epoch 9384/12000
2024-11-05 01:37:39,819 - INFO - [diffusion][Epoch 9383] diffusion training Loss: 0.0674011567607522
2024-11-05 01:37:39,821 - INFO - [diffusion][Epoch 9383] diffusion learning rate: 0.001
2024-11-05 01:37:39,823 - INFO - [diffusion][Epoch 9383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:39,825 - INFO - [diffusion][Epoch 9384] Epoch 9385/12000
2024-11-05 01:37:43,653 - INFO - [diffusion][Epoch 9384] diffusion training Loss: 0.06714924331754446
2024-11-05 01:37:43,655 - INFO - [diffusion][Epoch 9384] diffusion learning rate: 0.001
2024-11-05 01:37:43,657 - INFO - [diffusion][Epoch 9384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:43,658 - INFO - [diffusion][Epoch 9385] Epoch 9386/12000
2024-11-05 01:37:47,190 - INFO - [diffusion][Epoch 9385] diffusion training Loss: 0.07117309607565403
2024-11-05 01:37:47,192 - INFO - [diffusion][Epoch 9385] diffusion learning rate: 0.001
2024-11-05 01:37:47,194 - INFO - [diffusion][Epoch 9385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:47,195 - INFO - [diffusion][Epoch 9386] Epoch 9387/12000
2024-11-05 01:37:50,285 - INFO - [diffusion][Epoch 9386] diffusion training Loss: 0.06548747606575489
2024-11-05 01:37:50,288 - INFO - [diffusion][Epoch 9386] diffusion learning rate: 0.001
2024-11-05 01:37:50,289 - INFO - [diffusion][Epoch 9386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:50,292 - INFO - [diffusion][Epoch 9387] Epoch 9388/12000
2024-11-05 01:37:53,407 - INFO - [diffusion][Epoch 9387] diffusion training Loss: 0.0716954804956913
2024-11-05 01:37:53,410 - INFO - [diffusion][Epoch 9387] diffusion learning rate: 0.001
2024-11-05 01:37:53,412 - INFO - [diffusion][Epoch 9387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:53,413 - INFO - [diffusion][Epoch 9388] Epoch 9389/12000
2024-11-05 01:37:56,779 - INFO - [diffusion][Epoch 9388] diffusion training Loss: 0.06847266480326653
2024-11-05 01:37:56,781 - INFO - [diffusion][Epoch 9388] diffusion learning rate: 0.001
2024-11-05 01:37:56,782 - INFO - [diffusion][Epoch 9388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:56,784 - INFO - [diffusion][Epoch 9389] Epoch 9390/12000
2024-11-05 01:38:00,344 - INFO - [diffusion][Epoch 9389] diffusion training Loss: 0.0686853788793087
2024-11-05 01:38:00,345 - INFO - [diffusion][Epoch 9389] diffusion learning rate: 0.001
2024-11-05 01:38:00,347 - INFO - [diffusion][Epoch 9389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:00,348 - INFO - [diffusion][Epoch 9390] Epoch 9391/12000
2024-11-05 01:38:03,422 - INFO - [diffusion][Epoch 9390] diffusion training Loss: 0.06442220881581306
2024-11-05 01:38:03,424 - INFO - [diffusion][Epoch 9390] diffusion learning rate: 0.001
2024-11-05 01:38:03,509 - INFO - [diffusion][Epoch 9390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:03,511 - INFO - [diffusion][Epoch 9391] Epoch 9392/12000
2024-11-05 01:38:06,733 - INFO - [diffusion][Epoch 9391] diffusion training Loss: 0.06680644676089287
2024-11-05 01:38:06,735 - INFO - [diffusion][Epoch 9391] diffusion learning rate: 0.001
2024-11-05 01:38:06,737 - INFO - [diffusion][Epoch 9391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:06,738 - INFO - [diffusion][Epoch 9392] Epoch 9393/12000
2024-11-05 01:38:09,924 - INFO - [diffusion][Epoch 9392] diffusion training Loss: 0.06646033562719822
2024-11-05 01:38:09,927 - INFO - [diffusion][Epoch 9392] diffusion learning rate: 0.001
2024-11-05 01:38:09,929 - INFO - [diffusion][Epoch 9392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:09,930 - INFO - [diffusion][Epoch 9393] Epoch 9394/12000
2024-11-05 01:38:13,498 - INFO - [diffusion][Epoch 9393] diffusion training Loss: 0.06688376888632774
2024-11-05 01:38:13,500 - INFO - [diffusion][Epoch 9393] diffusion learning rate: 0.001
2024-11-05 01:38:13,502 - INFO - [diffusion][Epoch 9393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:13,504 - INFO - [diffusion][Epoch 9394] Epoch 9395/12000
2024-11-05 01:38:16,908 - INFO - [diffusion][Epoch 9394] diffusion training Loss: 0.06873366050422192
2024-11-05 01:38:16,911 - INFO - [diffusion][Epoch 9394] diffusion learning rate: 0.001
2024-11-05 01:38:16,912 - INFO - [diffusion][Epoch 9394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:16,914 - INFO - [diffusion][Epoch 9395] Epoch 9396/12000
2024-11-05 01:38:19,873 - INFO - [diffusion][Epoch 9395] diffusion training Loss: 0.06628645956516266
2024-11-05 01:38:19,876 - INFO - [diffusion][Epoch 9395] diffusion learning rate: 0.001
2024-11-05 01:38:19,879 - INFO - [diffusion][Epoch 9395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:19,881 - INFO - [diffusion][Epoch 9396] Epoch 9397/12000
2024-11-05 01:38:22,881 - INFO - [diffusion][Epoch 9396] diffusion training Loss: 0.06945103127509356
2024-11-05 01:38:22,884 - INFO - [diffusion][Epoch 9396] diffusion learning rate: 0.001
2024-11-05 01:38:22,886 - INFO - [diffusion][Epoch 9396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:22,888 - INFO - [diffusion][Epoch 9397] Epoch 9398/12000
2024-11-05 01:38:26,347 - INFO - [diffusion][Epoch 9397] diffusion training Loss: 0.06472116056829691
2024-11-05 01:38:26,349 - INFO - [diffusion][Epoch 9397] diffusion learning rate: 0.001
2024-11-05 01:38:26,351 - INFO - [diffusion][Epoch 9397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:26,352 - INFO - [diffusion][Epoch 9398] Epoch 9399/12000
2024-11-05 01:38:29,863 - INFO - [diffusion][Epoch 9398] diffusion training Loss: 0.06732440367341042
2024-11-05 01:38:29,865 - INFO - [diffusion][Epoch 9398] diffusion learning rate: 0.001
2024-11-05 01:38:29,866 - INFO - [diffusion][Epoch 9398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:29,868 - INFO - [diffusion][Epoch 9399] Epoch 9400/12000
2024-11-05 01:38:32,905 - INFO - [diffusion][Epoch 9399] diffusion training Loss: 0.06645951978862286
2024-11-05 01:38:32,907 - INFO - [diffusion][Epoch 9399] diffusion learning rate: 0.001
2024-11-05 01:38:32,909 - INFO - [diffusion][Epoch 9399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:32,910 - INFO - [diffusion][Epoch 9400] Epoch 9401/12000
2024-11-05 01:38:36,033 - INFO - [diffusion][Epoch 9400] diffusion training Loss: 0.06306484527885914
2024-11-05 01:38:36,035 - INFO - [diffusion][Epoch 9400] diffusion learning rate: 0.001
2024-11-05 01:38:36,037 - INFO - [diffusion][Epoch 9400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:36,038 - INFO - [diffusion][Epoch 9401] Epoch 9402/12000
2024-11-05 01:38:39,246 - INFO - [diffusion][Epoch 9401] diffusion training Loss: 0.06817759200930595
2024-11-05 01:38:39,248 - INFO - [diffusion][Epoch 9401] diffusion learning rate: 0.001
2024-11-05 01:38:39,249 - INFO - [diffusion][Epoch 9401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:39,251 - INFO - [diffusion][Epoch 9402] Epoch 9403/12000
2024-11-05 01:38:42,610 - INFO - [diffusion][Epoch 9402] diffusion training Loss: 0.07602301426231861
2024-11-05 01:38:42,612 - INFO - [diffusion][Epoch 9402] diffusion learning rate: 0.001
2024-11-05 01:38:42,613 - INFO - [diffusion][Epoch 9402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:42,615 - INFO - [diffusion][Epoch 9403] Epoch 9404/12000
2024-11-05 01:38:45,830 - INFO - [diffusion][Epoch 9403] diffusion training Loss: 0.06839269585907459
2024-11-05 01:38:45,831 - INFO - [diffusion][Epoch 9403] diffusion learning rate: 0.001
2024-11-05 01:38:45,833 - INFO - [diffusion][Epoch 9403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:45,834 - INFO - [diffusion][Epoch 9404] Epoch 9405/12000
2024-11-05 01:38:48,974 - INFO - [diffusion][Epoch 9404] diffusion training Loss: 0.06983176246285439
2024-11-05 01:38:48,976 - INFO - [diffusion][Epoch 9404] diffusion learning rate: 0.001
2024-11-05 01:38:48,978 - INFO - [diffusion][Epoch 9404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:48,979 - INFO - [diffusion][Epoch 9405] Epoch 9406/12000
2024-11-05 01:38:52,057 - INFO - [diffusion][Epoch 9405] diffusion training Loss: 0.06728094816207886
2024-11-05 01:38:52,059 - INFO - [diffusion][Epoch 9405] diffusion learning rate: 0.001
2024-11-05 01:38:52,061 - INFO - [diffusion][Epoch 9405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:52,062 - INFO - [diffusion][Epoch 9406] Epoch 9407/12000
2024-11-05 01:38:55,584 - INFO - [diffusion][Epoch 9406] diffusion training Loss: 0.067084526643157
2024-11-05 01:38:55,586 - INFO - [diffusion][Epoch 9406] diffusion learning rate: 0.001
2024-11-05 01:38:55,588 - INFO - [diffusion][Epoch 9406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:55,589 - INFO - [diffusion][Epoch 9407] Epoch 9408/12000
2024-11-05 01:38:59,101 - INFO - [diffusion][Epoch 9407] diffusion training Loss: 0.06518302112817764
2024-11-05 01:38:59,103 - INFO - [diffusion][Epoch 9407] diffusion learning rate: 0.001
2024-11-05 01:38:59,104 - INFO - [diffusion][Epoch 9407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:59,106 - INFO - [diffusion][Epoch 9408] Epoch 9409/12000
2024-11-05 01:39:02,186 - INFO - [diffusion][Epoch 9408] diffusion training Loss: 0.06349667813628912
2024-11-05 01:39:02,188 - INFO - [diffusion][Epoch 9408] diffusion learning rate: 0.001
2024-11-05 01:39:02,189 - INFO - [diffusion][Epoch 9408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:02,190 - INFO - [diffusion][Epoch 9409] Epoch 9410/12000
2024-11-05 01:39:05,292 - INFO - [diffusion][Epoch 9409] diffusion training Loss: 0.06682944856584072
2024-11-05 01:39:05,294 - INFO - [diffusion][Epoch 9409] diffusion learning rate: 0.001
2024-11-05 01:39:05,296 - INFO - [diffusion][Epoch 9409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:05,297 - INFO - [diffusion][Epoch 9410] Epoch 9411/12000
2024-11-05 01:39:08,498 - INFO - [diffusion][Epoch 9410] diffusion training Loss: 0.060949441976845264
2024-11-05 01:39:08,500 - INFO - [diffusion][Epoch 9410] diffusion learning rate: 0.001
2024-11-05 01:39:08,501 - INFO - [diffusion][Epoch 9410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:08,503 - INFO - [diffusion][Epoch 9411] Epoch 9412/12000
2024-11-05 01:39:12,199 - INFO - [diffusion][Epoch 9411] diffusion training Loss: 0.06893257144838572
2024-11-05 01:39:12,200 - INFO - [diffusion][Epoch 9411] diffusion learning rate: 0.001
2024-11-05 01:39:12,202 - INFO - [diffusion][Epoch 9411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:12,203 - INFO - [diffusion][Epoch 9412] Epoch 9413/12000
2024-11-05 01:39:15,397 - INFO - [diffusion][Epoch 9412] diffusion training Loss: 0.06330959312617779
2024-11-05 01:39:15,399 - INFO - [diffusion][Epoch 9412] diffusion learning rate: 0.001
2024-11-05 01:39:15,401 - INFO - [diffusion][Epoch 9412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:15,402 - INFO - [diffusion][Epoch 9413] Epoch 9414/12000
2024-11-05 01:39:19,018 - INFO - [diffusion][Epoch 9413] diffusion training Loss: 0.061222875490784645
2024-11-05 01:39:19,020 - INFO - [diffusion][Epoch 9413] diffusion learning rate: 0.001
2024-11-05 01:39:19,022 - INFO - [diffusion][Epoch 9413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:19,023 - INFO - [diffusion][Epoch 9414] Epoch 9415/12000
2024-11-05 01:39:22,114 - INFO - [diffusion][Epoch 9414] diffusion training Loss: 0.06800420209765434
2024-11-05 01:39:22,116 - INFO - [diffusion][Epoch 9414] diffusion learning rate: 0.001
2024-11-05 01:39:22,117 - INFO - [diffusion][Epoch 9414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:22,118 - INFO - [diffusion][Epoch 9415] Epoch 9416/12000
2024-11-05 01:39:25,282 - INFO - [diffusion][Epoch 9415] diffusion training Loss: 0.07076363824307919
2024-11-05 01:39:25,284 - INFO - [diffusion][Epoch 9415] diffusion learning rate: 0.001
2024-11-05 01:39:25,286 - INFO - [diffusion][Epoch 9415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:25,287 - INFO - [diffusion][Epoch 9416] Epoch 9417/12000
2024-11-05 01:39:28,781 - INFO - [diffusion][Epoch 9416] diffusion training Loss: 0.0687440037727356
2024-11-05 01:39:28,784 - INFO - [diffusion][Epoch 9416] diffusion learning rate: 0.001
2024-11-05 01:39:28,786 - INFO - [diffusion][Epoch 9416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:28,787 - INFO - [diffusion][Epoch 9417] Epoch 9418/12000
2024-11-05 01:39:32,344 - INFO - [diffusion][Epoch 9417] diffusion training Loss: 0.06903195101767778
2024-11-05 01:39:32,347 - INFO - [diffusion][Epoch 9417] diffusion learning rate: 0.001
2024-11-05 01:39:32,348 - INFO - [diffusion][Epoch 9417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:32,350 - INFO - [diffusion][Epoch 9418] Epoch 9419/12000
2024-11-05 01:39:35,416 - INFO - [diffusion][Epoch 9418] diffusion training Loss: 0.0699275890365243
2024-11-05 01:39:35,418 - INFO - [diffusion][Epoch 9418] diffusion learning rate: 0.001
2024-11-05 01:39:35,420 - INFO - [diffusion][Epoch 9418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:35,421 - INFO - [diffusion][Epoch 9419] Epoch 9420/12000
2024-11-05 01:39:38,508 - INFO - [diffusion][Epoch 9419] diffusion training Loss: 0.07106256298720837
2024-11-05 01:39:38,510 - INFO - [diffusion][Epoch 9419] diffusion learning rate: 0.001
2024-11-05 01:39:38,512 - INFO - [diffusion][Epoch 9419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:38,513 - INFO - [diffusion][Epoch 9420] Epoch 9421/12000
2024-11-05 01:39:41,819 - INFO - [diffusion][Epoch 9420] diffusion training Loss: 0.07582636084407568
2024-11-05 01:39:41,821 - INFO - [diffusion][Epoch 9420] diffusion learning rate: 0.001
2024-11-05 01:39:41,823 - INFO - [diffusion][Epoch 9420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:41,824 - INFO - [diffusion][Epoch 9421] Epoch 9422/12000
2024-11-05 01:39:45,501 - INFO - [diffusion][Epoch 9421] diffusion training Loss: 0.0625269552692771
2024-11-05 01:39:45,503 - INFO - [diffusion][Epoch 9421] diffusion learning rate: 0.001
2024-11-05 01:39:45,505 - INFO - [diffusion][Epoch 9421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:45,506 - INFO - [diffusion][Epoch 9422] Epoch 9423/12000
2024-11-05 01:39:48,869 - INFO - [diffusion][Epoch 9422] diffusion training Loss: 0.06936736404895782
2024-11-05 01:39:48,871 - INFO - [diffusion][Epoch 9422] diffusion learning rate: 0.001
2024-11-05 01:39:48,895 - INFO - [diffusion][Epoch 9422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:48,896 - INFO - [diffusion][Epoch 9423] Epoch 9424/12000
2024-11-05 01:39:52,009 - INFO - [diffusion][Epoch 9423] diffusion training Loss: 0.06303123012185097
2024-11-05 01:39:52,011 - INFO - [diffusion][Epoch 9423] diffusion learning rate: 0.001
2024-11-05 01:39:52,013 - INFO - [diffusion][Epoch 9423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:52,015 - INFO - [diffusion][Epoch 9424] Epoch 9425/12000
2024-11-05 01:39:55,016 - INFO - [diffusion][Epoch 9424] diffusion training Loss: 0.06804008036851883
2024-11-05 01:39:55,018 - INFO - [diffusion][Epoch 9424] diffusion learning rate: 0.001
2024-11-05 01:39:55,020 - INFO - [diffusion][Epoch 9424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:55,021 - INFO - [diffusion][Epoch 9425] Epoch 9426/12000
2024-11-05 01:39:58,540 - INFO - [diffusion][Epoch 9425] diffusion training Loss: 0.0687454305589199
2024-11-05 01:39:58,542 - INFO - [diffusion][Epoch 9425] diffusion learning rate: 0.001
2024-11-05 01:39:58,543 - INFO - [diffusion][Epoch 9425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:58,545 - INFO - [diffusion][Epoch 9426] Epoch 9427/12000
2024-11-05 01:40:02,013 - INFO - [diffusion][Epoch 9426] diffusion training Loss: 0.060651401057839394
2024-11-05 01:40:02,015 - INFO - [diffusion][Epoch 9426] diffusion learning rate: 0.001
2024-11-05 01:40:02,016 - INFO - [diffusion][Epoch 9426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:02,018 - INFO - [diffusion][Epoch 9427] Epoch 9428/12000
2024-11-05 01:40:05,100 - INFO - [diffusion][Epoch 9427] diffusion training Loss: 0.06924188509583473
2024-11-05 01:40:05,104 - INFO - [diffusion][Epoch 9427] diffusion learning rate: 0.001
2024-11-05 01:40:05,106 - INFO - [diffusion][Epoch 9427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:05,107 - INFO - [diffusion][Epoch 9428] Epoch 9429/12000
2024-11-05 01:40:08,250 - INFO - [diffusion][Epoch 9428] diffusion training Loss: 0.07049569673836231
2024-11-05 01:40:08,252 - INFO - [diffusion][Epoch 9428] diffusion learning rate: 0.001
2024-11-05 01:40:08,254 - INFO - [diffusion][Epoch 9428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:08,256 - INFO - [diffusion][Epoch 9429] Epoch 9430/12000
2024-11-05 01:40:11,520 - INFO - [diffusion][Epoch 9429] diffusion training Loss: 0.06534217856824398
2024-11-05 01:40:11,522 - INFO - [diffusion][Epoch 9429] diffusion learning rate: 0.001
2024-11-05 01:40:11,524 - INFO - [diffusion][Epoch 9429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:11,525 - INFO - [diffusion][Epoch 9430] Epoch 9431/12000
2024-11-05 01:40:15,251 - INFO - [diffusion][Epoch 9430] diffusion training Loss: 0.06586542446166277
2024-11-05 01:40:15,253 - INFO - [diffusion][Epoch 9430] diffusion learning rate: 0.001
2024-11-05 01:40:15,255 - INFO - [diffusion][Epoch 9430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:15,256 - INFO - [diffusion][Epoch 9431] Epoch 9432/12000
2024-11-05 01:40:18,646 - INFO - [diffusion][Epoch 9431] diffusion training Loss: 0.06808760203421116
2024-11-05 01:40:18,648 - INFO - [diffusion][Epoch 9431] diffusion learning rate: 0.001
2024-11-05 01:40:18,650 - INFO - [diffusion][Epoch 9431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:18,651 - INFO - [diffusion][Epoch 9432] Epoch 9433/12000
2024-11-05 01:40:21,753 - INFO - [diffusion][Epoch 9432] diffusion training Loss: 0.06898857280611992
2024-11-05 01:40:21,757 - INFO - [diffusion][Epoch 9432] diffusion learning rate: 0.001
2024-11-05 01:40:21,758 - INFO - [diffusion][Epoch 9432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:21,760 - INFO - [diffusion][Epoch 9433] Epoch 9434/12000
2024-11-05 01:40:25,470 - INFO - [diffusion][Epoch 9433] diffusion training Loss: 0.07126563787460327
2024-11-05 01:40:25,472 - INFO - [diffusion][Epoch 9433] diffusion learning rate: 0.001
2024-11-05 01:40:25,474 - INFO - [diffusion][Epoch 9433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:25,475 - INFO - [diffusion][Epoch 9434] Epoch 9435/12000
2024-11-05 01:40:28,573 - INFO - [diffusion][Epoch 9434] diffusion training Loss: 0.06631529331207275
2024-11-05 01:40:28,575 - INFO - [diffusion][Epoch 9434] diffusion learning rate: 0.001
2024-11-05 01:40:28,577 - INFO - [diffusion][Epoch 9434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:28,578 - INFO - [diffusion][Epoch 9435] Epoch 9436/12000
2024-11-05 01:40:32,192 - INFO - [diffusion][Epoch 9435] diffusion training Loss: 0.06797465495765209
2024-11-05 01:40:32,195 - INFO - [diffusion][Epoch 9435] diffusion learning rate: 0.001
2024-11-05 01:40:32,245 - INFO - [diffusion][Epoch 9435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:32,247 - INFO - [diffusion][Epoch 9436] Epoch 9437/12000
2024-11-05 01:40:35,796 - INFO - [diffusion][Epoch 9436] diffusion training Loss: 0.07283416390419006
2024-11-05 01:40:35,799 - INFO - [diffusion][Epoch 9436] diffusion learning rate: 0.001
2024-11-05 01:40:35,800 - INFO - [diffusion][Epoch 9436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:35,802 - INFO - [diffusion][Epoch 9437] Epoch 9438/12000
2024-11-05 01:40:38,943 - INFO - [diffusion][Epoch 9437] diffusion training Loss: 0.07359872944653034
2024-11-05 01:40:38,945 - INFO - [diffusion][Epoch 9437] diffusion learning rate: 0.001
2024-11-05 01:40:38,947 - INFO - [diffusion][Epoch 9437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:38,948 - INFO - [diffusion][Epoch 9438] Epoch 9439/12000
2024-11-05 01:40:42,029 - INFO - [diffusion][Epoch 9438] diffusion training Loss: 0.06907838396728039
2024-11-05 01:40:42,031 - INFO - [diffusion][Epoch 9438] diffusion learning rate: 0.001
2024-11-05 01:40:42,033 - INFO - [diffusion][Epoch 9438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:42,034 - INFO - [diffusion][Epoch 9439] Epoch 9440/12000
2024-11-05 01:40:45,279 - INFO - [diffusion][Epoch 9439] diffusion training Loss: 0.06670521013438702
2024-11-05 01:40:45,281 - INFO - [diffusion][Epoch 9439] diffusion learning rate: 0.001
2024-11-05 01:40:45,283 - INFO - [diffusion][Epoch 9439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:45,284 - INFO - [diffusion][Epoch 9440] Epoch 9441/12000
2024-11-05 01:40:49,032 - INFO - [diffusion][Epoch 9440] diffusion training Loss: 0.0693000890314579
2024-11-05 01:40:49,034 - INFO - [diffusion][Epoch 9440] diffusion learning rate: 0.001
2024-11-05 01:40:49,036 - INFO - [diffusion][Epoch 9440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:49,037 - INFO - [diffusion][Epoch 9441] Epoch 9442/12000
2024-11-05 01:40:52,574 - INFO - [diffusion][Epoch 9441] diffusion training Loss: 0.06795450765639544
2024-11-05 01:40:52,576 - INFO - [diffusion][Epoch 9441] diffusion learning rate: 0.001
2024-11-05 01:40:52,577 - INFO - [diffusion][Epoch 9441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:52,579 - INFO - [diffusion][Epoch 9442] Epoch 9443/12000
2024-11-05 01:40:55,664 - INFO - [diffusion][Epoch 9442] diffusion training Loss: 0.06596020050346851
2024-11-05 01:40:55,665 - INFO - [diffusion][Epoch 9442] diffusion learning rate: 0.001
2024-11-05 01:40:55,667 - INFO - [diffusion][Epoch 9442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:55,668 - INFO - [diffusion][Epoch 9443] Epoch 9444/12000
2024-11-05 01:40:58,773 - INFO - [diffusion][Epoch 9443] diffusion training Loss: 0.0670007448643446
2024-11-05 01:40:58,775 - INFO - [diffusion][Epoch 9443] diffusion learning rate: 0.001
2024-11-05 01:40:58,776 - INFO - [diffusion][Epoch 9443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:58,778 - INFO - [diffusion][Epoch 9444] Epoch 9445/12000
2024-11-05 01:41:02,069 - INFO - [diffusion][Epoch 9444] diffusion training Loss: 0.06514369416981936
2024-11-05 01:41:02,071 - INFO - [diffusion][Epoch 9444] diffusion learning rate: 0.001
2024-11-05 01:41:02,073 - INFO - [diffusion][Epoch 9444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:02,074 - INFO - [diffusion][Epoch 9445] Epoch 9446/12000
2024-11-05 01:41:05,675 - INFO - [diffusion][Epoch 9445] diffusion training Loss: 0.06260744575411081
2024-11-05 01:41:05,678 - INFO - [diffusion][Epoch 9445] diffusion learning rate: 0.001
2024-11-05 01:41:05,679 - INFO - [diffusion][Epoch 9445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:05,681 - INFO - [diffusion][Epoch 9446] Epoch 9447/12000
2024-11-05 01:41:08,829 - INFO - [diffusion][Epoch 9446] diffusion training Loss: 0.06782283168286085
2024-11-05 01:41:08,831 - INFO - [diffusion][Epoch 9446] diffusion learning rate: 0.001
2024-11-05 01:41:08,832 - INFO - [diffusion][Epoch 9446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:08,834 - INFO - [diffusion][Epoch 9447] Epoch 9448/12000
2024-11-05 01:41:11,939 - INFO - [diffusion][Epoch 9447] diffusion training Loss: 0.06522228848189116
2024-11-05 01:41:11,941 - INFO - [diffusion][Epoch 9447] diffusion learning rate: 0.001
2024-11-05 01:41:11,943 - INFO - [diffusion][Epoch 9447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:11,944 - INFO - [diffusion][Epoch 9448] Epoch 9449/12000
2024-11-05 01:41:15,072 - INFO - [diffusion][Epoch 9448] diffusion training Loss: 0.06729230377823114
2024-11-05 01:41:15,074 - INFO - [diffusion][Epoch 9448] diffusion learning rate: 0.001
2024-11-05 01:41:15,088 - INFO - [diffusion][Epoch 9448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:15,090 - INFO - [diffusion][Epoch 9449] Epoch 9450/12000
2024-11-05 01:41:18,594 - INFO - [diffusion][Epoch 9449] diffusion training Loss: 0.06914522219449282
2024-11-05 01:41:18,596 - INFO - [diffusion][Epoch 9449] diffusion learning rate: 0.001
2024-11-05 01:41:18,598 - INFO - [diffusion][Epoch 9449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:18,599 - INFO - [diffusion][Epoch 9450] Epoch 9451/12000
2024-11-05 01:41:22,158 - INFO - [diffusion][Epoch 9450] diffusion training Loss: 0.07086906209588051
2024-11-05 01:41:22,160 - INFO - [diffusion][Epoch 9450] diffusion learning rate: 0.001
2024-11-05 01:41:22,161 - INFO - [diffusion][Epoch 9450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:22,163 - INFO - [diffusion][Epoch 9451] Epoch 9452/12000
2024-11-05 01:41:25,263 - INFO - [diffusion][Epoch 9451] diffusion training Loss: 0.06545668840408325
2024-11-05 01:41:25,265 - INFO - [diffusion][Epoch 9451] diffusion learning rate: 0.001
2024-11-05 01:41:25,266 - INFO - [diffusion][Epoch 9451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:25,268 - INFO - [diffusion][Epoch 9452] Epoch 9453/12000
2024-11-05 01:41:28,360 - INFO - [diffusion][Epoch 9452] diffusion training Loss: 0.06814827397465706
2024-11-05 01:41:28,362 - INFO - [diffusion][Epoch 9452] diffusion learning rate: 0.001
2024-11-05 01:41:28,364 - INFO - [diffusion][Epoch 9452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:28,365 - INFO - [diffusion][Epoch 9453] Epoch 9454/12000
2024-11-05 01:41:31,480 - INFO - [diffusion][Epoch 9453] diffusion training Loss: 0.07116262055933475
2024-11-05 01:41:31,482 - INFO - [diffusion][Epoch 9453] diffusion learning rate: 0.001
2024-11-05 01:41:31,484 - INFO - [diffusion][Epoch 9453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:31,485 - INFO - [diffusion][Epoch 9454] Epoch 9455/12000
2024-11-05 01:41:35,229 - INFO - [diffusion][Epoch 9454] diffusion training Loss: 0.0618981309235096
2024-11-05 01:41:35,231 - INFO - [diffusion][Epoch 9454] diffusion learning rate: 0.001
2024-11-05 01:41:35,233 - INFO - [diffusion][Epoch 9454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:35,234 - INFO - [diffusion][Epoch 9455] Epoch 9456/12000
2024-11-05 01:41:38,908 - INFO - [diffusion][Epoch 9455] diffusion training Loss: 0.07416216656565666
2024-11-05 01:41:38,910 - INFO - [diffusion][Epoch 9455] diffusion learning rate: 0.001
2024-11-05 01:41:38,912 - INFO - [diffusion][Epoch 9455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:38,913 - INFO - [diffusion][Epoch 9456] Epoch 9457/12000
2024-11-05 01:41:42,438 - INFO - [diffusion][Epoch 9456] diffusion training Loss: 0.07228113990277052
2024-11-05 01:41:42,440 - INFO - [diffusion][Epoch 9456] diffusion learning rate: 0.001
2024-11-05 01:41:42,442 - INFO - [diffusion][Epoch 9456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:42,443 - INFO - [diffusion][Epoch 9457] Epoch 9458/12000
2024-11-05 01:41:45,583 - INFO - [diffusion][Epoch 9457] diffusion training Loss: 0.07193973660469055
2024-11-05 01:41:45,585 - INFO - [diffusion][Epoch 9457] diffusion learning rate: 0.001
2024-11-05 01:41:45,587 - INFO - [diffusion][Epoch 9457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:45,588 - INFO - [diffusion][Epoch 9458] Epoch 9459/12000
2024-11-05 01:41:48,710 - INFO - [diffusion][Epoch 9458] diffusion training Loss: 0.0639808839187026
2024-11-05 01:41:48,712 - INFO - [diffusion][Epoch 9458] diffusion learning rate: 0.001
2024-11-05 01:41:48,714 - INFO - [diffusion][Epoch 9458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:48,715 - INFO - [diffusion][Epoch 9459] Epoch 9460/12000
2024-11-05 01:41:51,849 - INFO - [diffusion][Epoch 9459] diffusion training Loss: 0.06352207064628601
2024-11-05 01:41:51,851 - INFO - [diffusion][Epoch 9459] diffusion learning rate: 0.001
2024-11-05 01:41:51,854 - INFO - [diffusion][Epoch 9459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:51,855 - INFO - [diffusion][Epoch 9460] Epoch 9461/12000
2024-11-05 01:41:55,377 - INFO - [diffusion][Epoch 9460] diffusion training Loss: 0.07489811442792416
2024-11-05 01:41:55,379 - INFO - [diffusion][Epoch 9460] diffusion learning rate: 0.001
2024-11-05 01:41:55,381 - INFO - [diffusion][Epoch 9460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:55,382 - INFO - [diffusion][Epoch 9461] Epoch 9462/12000
2024-11-05 01:41:58,900 - INFO - [diffusion][Epoch 9461] diffusion training Loss: 0.06096638739109039
2024-11-05 01:41:58,902 - INFO - [diffusion][Epoch 9461] diffusion learning rate: 0.001
2024-11-05 01:41:58,904 - INFO - [diffusion][Epoch 9461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:58,906 - INFO - [diffusion][Epoch 9462] Epoch 9463/12000
2024-11-05 01:42:01,998 - INFO - [diffusion][Epoch 9462] diffusion training Loss: 0.06439019087702036
2024-11-05 01:42:02,000 - INFO - [diffusion][Epoch 9462] diffusion learning rate: 0.001
2024-11-05 01:42:02,002 - INFO - [diffusion][Epoch 9462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:02,003 - INFO - [diffusion][Epoch 9463] Epoch 9464/12000
2024-11-05 01:42:05,172 - INFO - [diffusion][Epoch 9463] diffusion training Loss: 0.06562142632901669
2024-11-05 01:42:05,174 - INFO - [diffusion][Epoch 9463] diffusion learning rate: 0.001
2024-11-05 01:42:05,176 - INFO - [diffusion][Epoch 9463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:05,177 - INFO - [diffusion][Epoch 9464] Epoch 9465/12000
2024-11-05 01:42:08,506 - INFO - [diffusion][Epoch 9464] diffusion training Loss: 0.06069811899214983
2024-11-05 01:42:08,507 - INFO - [diffusion][Epoch 9464] diffusion learning rate: 0.001
2024-11-05 01:42:08,509 - INFO - [diffusion][Epoch 9464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:08,510 - INFO - [diffusion][Epoch 9465] Epoch 9466/12000
2024-11-05 01:42:12,153 - INFO - [diffusion][Epoch 9465] diffusion training Loss: 0.06589268520474434
2024-11-05 01:42:12,155 - INFO - [diffusion][Epoch 9465] diffusion learning rate: 0.001
2024-11-05 01:42:12,157 - INFO - [diffusion][Epoch 9465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:12,158 - INFO - [diffusion][Epoch 9466] Epoch 9467/12000
2024-11-05 01:42:15,572 - INFO - [diffusion][Epoch 9466] diffusion training Loss: 0.06603624392300844
2024-11-05 01:42:15,574 - INFO - [diffusion][Epoch 9466] diffusion learning rate: 0.001
2024-11-05 01:42:15,609 - INFO - [diffusion][Epoch 9466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:15,610 - INFO - [diffusion][Epoch 9467] Epoch 9468/12000
2024-11-05 01:42:18,646 - INFO - [diffusion][Epoch 9467] diffusion training Loss: 0.06649824976921082
2024-11-05 01:42:18,647 - INFO - [diffusion][Epoch 9467] diffusion learning rate: 0.001
2024-11-05 01:42:18,649 - INFO - [diffusion][Epoch 9467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:18,651 - INFO - [diffusion][Epoch 9468] Epoch 9469/12000
2024-11-05 01:42:21,786 - INFO - [diffusion][Epoch 9468] diffusion training Loss: 0.06181226298213005
2024-11-05 01:42:21,788 - INFO - [diffusion][Epoch 9468] diffusion learning rate: 0.001
2024-11-05 01:42:21,790 - INFO - [diffusion][Epoch 9468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:21,791 - INFO - [diffusion][Epoch 9469] Epoch 9470/12000
2024-11-05 01:42:25,284 - INFO - [diffusion][Epoch 9469] diffusion training Loss: 0.06752168387174606
2024-11-05 01:42:25,286 - INFO - [diffusion][Epoch 9469] diffusion learning rate: 0.001
2024-11-05 01:42:25,287 - INFO - [diffusion][Epoch 9469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:25,289 - INFO - [diffusion][Epoch 9470] Epoch 9471/12000
2024-11-05 01:42:28,904 - INFO - [diffusion][Epoch 9470] diffusion training Loss: 0.0675343032926321
2024-11-05 01:42:28,906 - INFO - [diffusion][Epoch 9470] diffusion learning rate: 0.001
2024-11-05 01:42:28,908 - INFO - [diffusion][Epoch 9470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:28,910 - INFO - [diffusion][Epoch 9471] Epoch 9472/12000
2024-11-05 01:42:31,986 - INFO - [diffusion][Epoch 9471] diffusion training Loss: 0.06554244458675385
2024-11-05 01:42:31,988 - INFO - [diffusion][Epoch 9471] diffusion learning rate: 0.001
2024-11-05 01:42:31,990 - INFO - [diffusion][Epoch 9471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:31,992 - INFO - [diffusion][Epoch 9472] Epoch 9473/12000
2024-11-05 01:42:35,587 - INFO - [diffusion][Epoch 9472] diffusion training Loss: 0.06628069467842579
2024-11-05 01:42:35,590 - INFO - [diffusion][Epoch 9472] diffusion learning rate: 0.001
2024-11-05 01:42:35,591 - INFO - [diffusion][Epoch 9472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:35,593 - INFO - [diffusion][Epoch 9473] Epoch 9474/12000
2024-11-05 01:42:38,702 - INFO - [diffusion][Epoch 9473] diffusion training Loss: 0.06896437611430883
2024-11-05 01:42:38,704 - INFO - [diffusion][Epoch 9473] diffusion learning rate: 0.001
2024-11-05 01:42:38,706 - INFO - [diffusion][Epoch 9473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:38,707 - INFO - [diffusion][Epoch 9474] Epoch 9475/12000
2024-11-05 01:42:41,770 - INFO - [diffusion][Epoch 9474] diffusion training Loss: 0.06669656280428171
2024-11-05 01:42:41,772 - INFO - [diffusion][Epoch 9474] diffusion learning rate: 0.001
2024-11-05 01:42:41,774 - INFO - [diffusion][Epoch 9474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:41,775 - INFO - [diffusion][Epoch 9475] Epoch 9476/12000
2024-11-05 01:42:45,320 - INFO - [diffusion][Epoch 9475] diffusion training Loss: 0.06235282216221094
2024-11-05 01:42:45,322 - INFO - [diffusion][Epoch 9475] diffusion learning rate: 0.001
2024-11-05 01:42:45,324 - INFO - [diffusion][Epoch 9475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:45,325 - INFO - [diffusion][Epoch 9476] Epoch 9477/12000
2024-11-05 01:42:48,821 - INFO - [diffusion][Epoch 9476] diffusion training Loss: 0.06712127104401588
2024-11-05 01:42:48,823 - INFO - [diffusion][Epoch 9476] diffusion learning rate: 0.001
2024-11-05 01:42:48,877 - INFO - [diffusion][Epoch 9476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:48,878 - INFO - [diffusion][Epoch 9477] Epoch 9478/12000
2024-11-05 01:42:52,000 - INFO - [diffusion][Epoch 9477] diffusion training Loss: 0.06734371185302734
2024-11-05 01:42:52,001 - INFO - [diffusion][Epoch 9477] diffusion learning rate: 0.001
2024-11-05 01:42:52,003 - INFO - [diffusion][Epoch 9477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:52,004 - INFO - [diffusion][Epoch 9478] Epoch 9479/12000
2024-11-05 01:42:55,087 - INFO - [diffusion][Epoch 9478] diffusion training Loss: 0.06664859876036644
2024-11-05 01:42:55,089 - INFO - [diffusion][Epoch 9478] diffusion learning rate: 0.001
2024-11-05 01:42:55,091 - INFO - [diffusion][Epoch 9478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:55,092 - INFO - [diffusion][Epoch 9479] Epoch 9480/12000
2024-11-05 01:42:58,248 - INFO - [diffusion][Epoch 9479] diffusion training Loss: 0.06780877988785505
2024-11-05 01:42:58,250 - INFO - [diffusion][Epoch 9479] diffusion learning rate: 0.001
2024-11-05 01:42:58,252 - INFO - [diffusion][Epoch 9479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:58,254 - INFO - [diffusion][Epoch 9480] Epoch 9481/12000
2024-11-05 01:43:01,719 - INFO - [diffusion][Epoch 9480] diffusion training Loss: 0.0657091373577714
2024-11-05 01:43:01,720 - INFO - [diffusion][Epoch 9480] diffusion learning rate: 0.001
2024-11-05 01:43:01,722 - INFO - [diffusion][Epoch 9480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:01,723 - INFO - [diffusion][Epoch 9481] Epoch 9482/12000
2024-11-05 01:43:05,271 - INFO - [diffusion][Epoch 9481] diffusion training Loss: 0.07139192335307598
2024-11-05 01:43:05,273 - INFO - [diffusion][Epoch 9481] diffusion learning rate: 0.001
2024-11-05 01:43:05,275 - INFO - [diffusion][Epoch 9481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:05,276 - INFO - [diffusion][Epoch 9482] Epoch 9483/12000
2024-11-05 01:43:08,405 - INFO - [diffusion][Epoch 9482] diffusion training Loss: 0.06541006080806255
2024-11-05 01:43:08,407 - INFO - [diffusion][Epoch 9482] diffusion learning rate: 0.001
2024-11-05 01:43:08,409 - INFO - [diffusion][Epoch 9482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:08,410 - INFO - [diffusion][Epoch 9483] Epoch 9484/12000
2024-11-05 01:43:11,514 - INFO - [diffusion][Epoch 9483] diffusion training Loss: 0.06769576482474804
2024-11-05 01:43:11,516 - INFO - [diffusion][Epoch 9483] diffusion learning rate: 0.001
2024-11-05 01:43:11,518 - INFO - [diffusion][Epoch 9483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:11,519 - INFO - [diffusion][Epoch 9484] Epoch 9485/12000
2024-11-05 01:43:14,668 - INFO - [diffusion][Epoch 9484] diffusion training Loss: 0.06769920513033867
2024-11-05 01:43:14,670 - INFO - [diffusion][Epoch 9484] diffusion learning rate: 0.001
2024-11-05 01:43:14,671 - INFO - [diffusion][Epoch 9484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:14,672 - INFO - [diffusion][Epoch 9485] Epoch 9486/12000
2024-11-05 01:43:18,181 - INFO - [diffusion][Epoch 9485] diffusion training Loss: 0.06052693724632263
2024-11-05 01:43:18,183 - INFO - [diffusion][Epoch 9485] diffusion learning rate: 0.001
2024-11-05 01:43:18,185 - INFO - [diffusion][Epoch 9485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:18,186 - INFO - [diffusion][Epoch 9486] Epoch 9487/12000
2024-11-05 01:43:21,837 - INFO - [diffusion][Epoch 9486] diffusion training Loss: 0.06810545921325684
2024-11-05 01:43:21,839 - INFO - [diffusion][Epoch 9486] diffusion learning rate: 0.001
2024-11-05 01:43:21,841 - INFO - [diffusion][Epoch 9486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:21,842 - INFO - [diffusion][Epoch 9487] Epoch 9488/12000
2024-11-05 01:43:25,078 - INFO - [diffusion][Epoch 9487] diffusion training Loss: 0.0653770249336958
2024-11-05 01:43:25,080 - INFO - [diffusion][Epoch 9487] diffusion learning rate: 0.001
2024-11-05 01:43:25,081 - INFO - [diffusion][Epoch 9487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:25,082 - INFO - [diffusion][Epoch 9488] Epoch 9489/12000
2024-11-05 01:43:28,190 - INFO - [diffusion][Epoch 9488] diffusion training Loss: 0.06354853138327599
2024-11-05 01:43:28,192 - INFO - [diffusion][Epoch 9488] diffusion learning rate: 0.001
2024-11-05 01:43:28,194 - INFO - [diffusion][Epoch 9488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:28,195 - INFO - [diffusion][Epoch 9489] Epoch 9490/12000
2024-11-05 01:43:31,415 - INFO - [diffusion][Epoch 9489] diffusion training Loss: 0.06616839300841093
2024-11-05 01:43:31,417 - INFO - [diffusion][Epoch 9489] diffusion learning rate: 0.001
2024-11-05 01:43:31,418 - INFO - [diffusion][Epoch 9489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:31,420 - INFO - [diffusion][Epoch 9490] Epoch 9491/12000
2024-11-05 01:43:34,823 - INFO - [diffusion][Epoch 9490] diffusion training Loss: 0.06505683995783329
2024-11-05 01:43:34,825 - INFO - [diffusion][Epoch 9490] diffusion learning rate: 0.001
2024-11-05 01:43:34,827 - INFO - [diffusion][Epoch 9490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:34,829 - INFO - [diffusion][Epoch 9491] Epoch 9492/12000
2024-11-05 01:43:38,451 - INFO - [diffusion][Epoch 9491] diffusion training Loss: 0.06974962912499905
2024-11-05 01:43:38,456 - INFO - [diffusion][Epoch 9491] diffusion learning rate: 0.001
2024-11-05 01:43:38,458 - INFO - [diffusion][Epoch 9491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:38,459 - INFO - [diffusion][Epoch 9492] Epoch 9493/12000
2024-11-05 01:43:41,693 - INFO - [diffusion][Epoch 9492] diffusion training Loss: 0.0658181756734848
2024-11-05 01:43:41,695 - INFO - [diffusion][Epoch 9492] diffusion learning rate: 0.001
2024-11-05 01:43:41,697 - INFO - [diffusion][Epoch 9492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:41,698 - INFO - [diffusion][Epoch 9493] Epoch 9494/12000
2024-11-05 01:43:44,844 - INFO - [diffusion][Epoch 9493] diffusion training Loss: 0.06817477848380804
2024-11-05 01:43:44,846 - INFO - [diffusion][Epoch 9493] diffusion learning rate: 0.001
2024-11-05 01:43:44,848 - INFO - [diffusion][Epoch 9493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:44,849 - INFO - [diffusion][Epoch 9494] Epoch 9495/12000
2024-11-05 01:43:47,999 - INFO - [diffusion][Epoch 9494] diffusion training Loss: 0.06796947307884693
2024-11-05 01:43:48,001 - INFO - [diffusion][Epoch 9494] diffusion learning rate: 0.001
2024-11-05 01:43:48,004 - INFO - [diffusion][Epoch 9494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:48,006 - INFO - [diffusion][Epoch 9495] Epoch 9496/12000
2024-11-05 01:43:51,489 - INFO - [diffusion][Epoch 9495] diffusion training Loss: 0.06545846071094275
2024-11-05 01:43:51,491 - INFO - [diffusion][Epoch 9495] diffusion learning rate: 0.001
2024-11-05 01:43:51,493 - INFO - [diffusion][Epoch 9495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:51,494 - INFO - [diffusion][Epoch 9496] Epoch 9497/12000
2024-11-05 01:43:55,140 - INFO - [diffusion][Epoch 9496] diffusion training Loss: 0.06533362716436386
2024-11-05 01:43:55,142 - INFO - [diffusion][Epoch 9496] diffusion learning rate: 0.001
2024-11-05 01:43:55,144 - INFO - [diffusion][Epoch 9496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:55,145 - INFO - [diffusion][Epoch 9497] Epoch 9498/12000
2024-11-05 01:43:58,268 - INFO - [diffusion][Epoch 9497] diffusion training Loss: 0.07009958475828171
2024-11-05 01:43:58,270 - INFO - [diffusion][Epoch 9497] diffusion learning rate: 0.001
2024-11-05 01:43:58,272 - INFO - [diffusion][Epoch 9497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:58,273 - INFO - [diffusion][Epoch 9498] Epoch 9499/12000
2024-11-05 01:44:01,351 - INFO - [diffusion][Epoch 9498] diffusion training Loss: 0.06749505642801523
2024-11-05 01:44:01,353 - INFO - [diffusion][Epoch 9498] diffusion learning rate: 0.001
2024-11-05 01:44:01,355 - INFO - [diffusion][Epoch 9498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:01,356 - INFO - [diffusion][Epoch 9499] Epoch 9500/12000
2024-11-05 01:44:04,444 - INFO - [diffusion][Epoch 9499] diffusion training Loss: 0.06684963405132294
2024-11-05 01:44:04,446 - INFO - [diffusion][Epoch 9499] diffusion learning rate: 0.001
2024-11-05 01:44:04,447 - INFO - [diffusion][Epoch 9499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:04,449 - INFO - [diffusion][Epoch 9500] Epoch 9501/12000
2024-11-05 01:44:08,023 - INFO - [diffusion][Epoch 9500] diffusion training Loss: 0.0685621490702033
2024-11-05 01:44:08,025 - INFO - [diffusion][Epoch 9500] diffusion learning rate: 0.001
2024-11-05 01:44:08,027 - INFO - [diffusion][Epoch 9500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:08,028 - INFO - [diffusion][Epoch 9501] Epoch 9502/12000
2024-11-05 01:44:11,534 - INFO - [diffusion][Epoch 9501] diffusion training Loss: 0.06346760038286448
2024-11-05 01:44:11,536 - INFO - [diffusion][Epoch 9501] diffusion learning rate: 0.001
2024-11-05 01:44:11,538 - INFO - [diffusion][Epoch 9501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:11,539 - INFO - [diffusion][Epoch 9502] Epoch 9503/12000
2024-11-05 01:44:14,616 - INFO - [diffusion][Epoch 9502] diffusion training Loss: 0.06471876800060272
2024-11-05 01:44:14,618 - INFO - [diffusion][Epoch 9502] diffusion learning rate: 0.001
2024-11-05 01:44:14,620 - INFO - [diffusion][Epoch 9502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:14,621 - INFO - [diffusion][Epoch 9503] Epoch 9504/12000
2024-11-05 01:44:17,778 - INFO - [diffusion][Epoch 9503] diffusion training Loss: 0.067117421887815
2024-11-05 01:44:17,780 - INFO - [diffusion][Epoch 9503] diffusion learning rate: 0.001
2024-11-05 01:44:17,782 - INFO - [diffusion][Epoch 9503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:17,783 - INFO - [diffusion][Epoch 9504] Epoch 9505/12000
2024-11-05 01:44:21,103 - INFO - [diffusion][Epoch 9504] diffusion training Loss: 0.0636996878311038
2024-11-05 01:44:21,105 - INFO - [diffusion][Epoch 9504] diffusion learning rate: 0.001
2024-11-05 01:44:21,107 - INFO - [diffusion][Epoch 9504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:21,109 - INFO - [diffusion][Epoch 9505] Epoch 9506/12000
2024-11-05 01:44:24,745 - INFO - [diffusion][Epoch 9505] diffusion training Loss: 0.06505616568028927
2024-11-05 01:44:24,747 - INFO - [diffusion][Epoch 9505] diffusion learning rate: 0.001
2024-11-05 01:44:24,749 - INFO - [diffusion][Epoch 9505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:24,750 - INFO - [diffusion][Epoch 9506] Epoch 9507/12000
2024-11-05 01:44:28,264 - INFO - [diffusion][Epoch 9506] diffusion training Loss: 0.07005604542791843
2024-11-05 01:44:28,266 - INFO - [diffusion][Epoch 9506] diffusion learning rate: 0.001
2024-11-05 01:44:28,268 - INFO - [diffusion][Epoch 9506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:28,269 - INFO - [diffusion][Epoch 9507] Epoch 9508/12000
2024-11-05 01:44:31,375 - INFO - [diffusion][Epoch 9507] diffusion training Loss: 0.06566409207880497
2024-11-05 01:44:31,377 - INFO - [diffusion][Epoch 9507] diffusion learning rate: 0.001
2024-11-05 01:44:31,379 - INFO - [diffusion][Epoch 9507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:31,380 - INFO - [diffusion][Epoch 9508] Epoch 9509/12000
2024-11-05 01:44:34,586 - INFO - [diffusion][Epoch 9508] diffusion training Loss: 0.06309864390641451
2024-11-05 01:44:34,588 - INFO - [diffusion][Epoch 9508] diffusion learning rate: 0.001
2024-11-05 01:44:34,590 - INFO - [diffusion][Epoch 9508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:34,591 - INFO - [diffusion][Epoch 9509] Epoch 9510/12000
2024-11-05 01:44:37,791 - INFO - [diffusion][Epoch 9509] diffusion training Loss: 0.06748179439455271
2024-11-05 01:44:37,793 - INFO - [diffusion][Epoch 9509] diffusion learning rate: 0.001
2024-11-05 01:44:37,795 - INFO - [diffusion][Epoch 9509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:37,796 - INFO - [diffusion][Epoch 9510] Epoch 9511/12000
2024-11-05 01:44:41,383 - INFO - [diffusion][Epoch 9510] diffusion training Loss: 0.06752612628042698
2024-11-05 01:44:41,386 - INFO - [diffusion][Epoch 9510] diffusion learning rate: 0.001
2024-11-05 01:44:41,388 - INFO - [diffusion][Epoch 9510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:41,389 - INFO - [diffusion][Epoch 9511] Epoch 9512/12000
2024-11-05 01:44:44,908 - INFO - [diffusion][Epoch 9511] diffusion training Loss: 0.06927639804780483
2024-11-05 01:44:44,910 - INFO - [diffusion][Epoch 9511] diffusion learning rate: 0.001
2024-11-05 01:44:44,912 - INFO - [diffusion][Epoch 9511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:44,913 - INFO - [diffusion][Epoch 9512] Epoch 9513/12000
2024-11-05 01:44:48,016 - INFO - [diffusion][Epoch 9512] diffusion training Loss: 0.06737792305648327
2024-11-05 01:44:48,018 - INFO - [diffusion][Epoch 9512] diffusion learning rate: 0.001
2024-11-05 01:44:48,020 - INFO - [diffusion][Epoch 9512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:48,025 - INFO - [diffusion][Epoch 9513] Epoch 9514/12000
2024-11-05 01:44:51,546 - INFO - [diffusion][Epoch 9513] diffusion training Loss: 0.07074969168752432
2024-11-05 01:44:51,548 - INFO - [diffusion][Epoch 9513] diffusion learning rate: 0.001
2024-11-05 01:44:51,550 - INFO - [diffusion][Epoch 9513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:51,551 - INFO - [diffusion][Epoch 9514] Epoch 9515/12000
2024-11-05 01:44:54,926 - INFO - [diffusion][Epoch 9514] diffusion training Loss: 0.06424209475517273
2024-11-05 01:44:54,928 - INFO - [diffusion][Epoch 9514] diffusion learning rate: 0.001
2024-11-05 01:44:54,930 - INFO - [diffusion][Epoch 9514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:54,931 - INFO - [diffusion][Epoch 9515] Epoch 9516/12000
2024-11-05 01:44:58,103 - INFO - [diffusion][Epoch 9515] diffusion training Loss: 0.06703631393611431
2024-11-05 01:44:58,153 - INFO - [diffusion][Epoch 9515] diffusion learning rate: 0.001
2024-11-05 01:44:58,155 - INFO - [diffusion][Epoch 9515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:58,156 - INFO - [diffusion][Epoch 9516] Epoch 9517/12000
2024-11-05 01:45:01,530 - INFO - [diffusion][Epoch 9516] diffusion training Loss: 0.07173354644328356
2024-11-05 01:45:01,532 - INFO - [diffusion][Epoch 9516] diffusion learning rate: 0.001
2024-11-05 01:45:01,534 - INFO - [diffusion][Epoch 9516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:01,535 - INFO - [diffusion][Epoch 9517] Epoch 9518/12000
2024-11-05 01:45:04,914 - INFO - [diffusion][Epoch 9517] diffusion training Loss: 0.0692040678113699
2024-11-05 01:45:04,916 - INFO - [diffusion][Epoch 9517] diffusion learning rate: 0.001
2024-11-05 01:45:04,918 - INFO - [diffusion][Epoch 9517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:04,919 - INFO - [diffusion][Epoch 9518] Epoch 9519/12000
2024-11-05 01:45:08,083 - INFO - [diffusion][Epoch 9518] diffusion training Loss: 0.0628629894927144
2024-11-05 01:45:08,085 - INFO - [diffusion][Epoch 9518] diffusion learning rate: 0.001
2024-11-05 01:45:08,087 - INFO - [diffusion][Epoch 9518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:08,088 - INFO - [diffusion][Epoch 9519] Epoch 9520/12000
2024-11-05 01:45:11,269 - INFO - [diffusion][Epoch 9519] diffusion training Loss: 0.06642336398363113
2024-11-05 01:45:11,271 - INFO - [diffusion][Epoch 9519] diffusion learning rate: 0.001
2024-11-05 01:45:11,272 - INFO - [diffusion][Epoch 9519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:11,274 - INFO - [diffusion][Epoch 9520] Epoch 9521/12000
2024-11-05 01:45:14,578 - INFO - [diffusion][Epoch 9520] diffusion training Loss: 0.07042677327990532
2024-11-05 01:45:14,580 - INFO - [diffusion][Epoch 9520] diffusion learning rate: 0.001
2024-11-05 01:45:14,582 - INFO - [diffusion][Epoch 9520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:14,583 - INFO - [diffusion][Epoch 9521] Epoch 9522/12000
2024-11-05 01:45:18,346 - INFO - [diffusion][Epoch 9521] diffusion training Loss: 0.06675043795257807
2024-11-05 01:45:18,348 - INFO - [diffusion][Epoch 9521] diffusion learning rate: 0.001
2024-11-05 01:45:18,350 - INFO - [diffusion][Epoch 9521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:18,351 - INFO - [diffusion][Epoch 9522] Epoch 9523/12000
2024-11-05 01:45:21,857 - INFO - [diffusion][Epoch 9522] diffusion training Loss: 0.06661983300000429
2024-11-05 01:45:21,858 - INFO - [diffusion][Epoch 9522] diffusion learning rate: 0.001
2024-11-05 01:45:21,860 - INFO - [diffusion][Epoch 9522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:21,861 - INFO - [diffusion][Epoch 9523] Epoch 9524/12000
2024-11-05 01:45:24,980 - INFO - [diffusion][Epoch 9523] diffusion training Loss: 0.06061091274023056
2024-11-05 01:45:24,982 - INFO - [diffusion][Epoch 9523] diffusion learning rate: 0.001
2024-11-05 01:45:24,984 - INFO - [diffusion][Epoch 9523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:24,985 - INFO - [diffusion][Epoch 9524] Epoch 9525/12000
2024-11-05 01:45:28,161 - INFO - [diffusion][Epoch 9524] diffusion training Loss: 0.06416024453938007
2024-11-05 01:45:28,163 - INFO - [diffusion][Epoch 9524] diffusion learning rate: 0.001
2024-11-05 01:45:28,165 - INFO - [diffusion][Epoch 9524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:28,166 - INFO - [diffusion][Epoch 9525] Epoch 9526/12000
2024-11-05 01:45:31,313 - INFO - [diffusion][Epoch 9525] diffusion training Loss: 0.06452000513672829
2024-11-05 01:45:31,315 - INFO - [diffusion][Epoch 9525] diffusion learning rate: 0.001
2024-11-05 01:45:31,317 - INFO - [diffusion][Epoch 9525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:31,318 - INFO - [diffusion][Epoch 9526] Epoch 9527/12000
2024-11-05 01:45:34,914 - INFO - [diffusion][Epoch 9526] diffusion training Loss: 0.06599904783070087
2024-11-05 01:45:34,916 - INFO - [diffusion][Epoch 9526] diffusion learning rate: 0.001
2024-11-05 01:45:34,917 - INFO - [diffusion][Epoch 9526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:34,919 - INFO - [diffusion][Epoch 9527] Epoch 9528/12000
2024-11-05 01:45:38,420 - INFO - [diffusion][Epoch 9527] diffusion training Loss: 0.06163548678159714
2024-11-05 01:45:38,422 - INFO - [diffusion][Epoch 9527] diffusion learning rate: 0.001
2024-11-05 01:45:38,424 - INFO - [diffusion][Epoch 9527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:38,425 - INFO - [diffusion][Epoch 9528] Epoch 9529/12000
2024-11-05 01:45:41,452 - INFO - [diffusion][Epoch 9528] diffusion training Loss: 0.06340126600116491
2024-11-05 01:45:41,456 - INFO - [diffusion][Epoch 9528] diffusion learning rate: 0.001
2024-11-05 01:45:41,457 - INFO - [diffusion][Epoch 9528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:41,459 - INFO - [diffusion][Epoch 9529] Epoch 9530/12000
2024-11-05 01:45:44,655 - INFO - [diffusion][Epoch 9529] diffusion training Loss: 0.06623143516480923
2024-11-05 01:45:44,657 - INFO - [diffusion][Epoch 9529] diffusion learning rate: 0.001
2024-11-05 01:45:44,658 - INFO - [diffusion][Epoch 9529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:44,660 - INFO - [diffusion][Epoch 9530] Epoch 9531/12000
2024-11-05 01:45:47,822 - INFO - [diffusion][Epoch 9530] diffusion training Loss: 0.0664131622761488
2024-11-05 01:45:47,824 - INFO - [diffusion][Epoch 9530] diffusion learning rate: 0.001
2024-11-05 01:45:47,826 - INFO - [diffusion][Epoch 9530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:47,827 - INFO - [diffusion][Epoch 9531] Epoch 9532/12000
2024-11-05 01:45:51,364 - INFO - [diffusion][Epoch 9531] diffusion training Loss: 0.06778660044074059
2024-11-05 01:45:51,366 - INFO - [diffusion][Epoch 9531] diffusion learning rate: 0.001
2024-11-05 01:45:51,368 - INFO - [diffusion][Epoch 9531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:51,369 - INFO - [diffusion][Epoch 9532] Epoch 9533/12000
2024-11-05 01:45:54,887 - INFO - [diffusion][Epoch 9532] diffusion training Loss: 0.06605780310928822
2024-11-05 01:45:54,889 - INFO - [diffusion][Epoch 9532] diffusion learning rate: 0.001
2024-11-05 01:45:54,891 - INFO - [diffusion][Epoch 9532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:54,892 - INFO - [diffusion][Epoch 9533] Epoch 9534/12000
2024-11-05 01:45:57,999 - INFO - [diffusion][Epoch 9533] diffusion training Loss: 0.06930848583579063
2024-11-05 01:45:58,001 - INFO - [diffusion][Epoch 9533] diffusion learning rate: 0.001
2024-11-05 01:45:58,002 - INFO - [diffusion][Epoch 9533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:58,004 - INFO - [diffusion][Epoch 9534] Epoch 9535/12000
2024-11-05 01:46:01,131 - INFO - [diffusion][Epoch 9534] diffusion training Loss: 0.06679733842611313
2024-11-05 01:46:01,133 - INFO - [diffusion][Epoch 9534] diffusion learning rate: 0.001
2024-11-05 01:46:01,135 - INFO - [diffusion][Epoch 9534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:01,136 - INFO - [diffusion][Epoch 9535] Epoch 9536/12000
2024-11-05 01:46:04,334 - INFO - [diffusion][Epoch 9535] diffusion training Loss: 0.06293860264122486
2024-11-05 01:46:04,336 - INFO - [diffusion][Epoch 9535] diffusion learning rate: 0.001
2024-11-05 01:46:04,338 - INFO - [diffusion][Epoch 9535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:04,340 - INFO - [diffusion][Epoch 9536] Epoch 9537/12000
2024-11-05 01:46:07,937 - INFO - [diffusion][Epoch 9536] diffusion training Loss: 0.06598610989749432
2024-11-05 01:46:07,939 - INFO - [diffusion][Epoch 9536] diffusion learning rate: 0.001
2024-11-05 01:46:07,941 - INFO - [diffusion][Epoch 9536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:07,942 - INFO - [diffusion][Epoch 9537] Epoch 9538/12000
2024-11-05 01:46:11,450 - INFO - [diffusion][Epoch 9537] diffusion training Loss: 0.06539912521839142
2024-11-05 01:46:11,452 - INFO - [diffusion][Epoch 9537] diffusion learning rate: 0.001
2024-11-05 01:46:11,479 - INFO - [diffusion][Epoch 9537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:11,480 - INFO - [diffusion][Epoch 9538] Epoch 9539/12000
2024-11-05 01:46:14,561 - INFO - [diffusion][Epoch 9538] diffusion training Loss: 0.0700455941259861
2024-11-05 01:46:14,563 - INFO - [diffusion][Epoch 9538] diffusion learning rate: 0.001
2024-11-05 01:46:14,565 - INFO - [diffusion][Epoch 9538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:14,566 - INFO - [diffusion][Epoch 9539] Epoch 9540/12000
2024-11-05 01:46:17,680 - INFO - [diffusion][Epoch 9539] diffusion training Loss: 0.07009783200919628
2024-11-05 01:46:17,682 - INFO - [diffusion][Epoch 9539] diffusion learning rate: 0.001
2024-11-05 01:46:17,684 - INFO - [diffusion][Epoch 9539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:17,685 - INFO - [diffusion][Epoch 9540] Epoch 9541/12000
2024-11-05 01:46:20,801 - INFO - [diffusion][Epoch 9540] diffusion training Loss: 0.06714711058884859
2024-11-05 01:46:20,803 - INFO - [diffusion][Epoch 9540] diffusion learning rate: 0.001
2024-11-05 01:46:20,805 - INFO - [diffusion][Epoch 9540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:20,806 - INFO - [diffusion][Epoch 9541] Epoch 9542/12000
2024-11-05 01:46:24,322 - INFO - [diffusion][Epoch 9541] diffusion training Loss: 0.07082324661314487
2024-11-05 01:46:24,324 - INFO - [diffusion][Epoch 9541] diffusion learning rate: 0.001
2024-11-05 01:46:24,326 - INFO - [diffusion][Epoch 9541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:24,327 - INFO - [diffusion][Epoch 9542] Epoch 9543/12000
2024-11-05 01:46:27,869 - INFO - [diffusion][Epoch 9542] diffusion training Loss: 0.06783748790621758
2024-11-05 01:46:27,871 - INFO - [diffusion][Epoch 9542] diffusion learning rate: 0.001
2024-11-05 01:46:27,891 - INFO - [diffusion][Epoch 9542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:27,893 - INFO - [diffusion][Epoch 9543] Epoch 9544/12000
2024-11-05 01:46:31,058 - INFO - [diffusion][Epoch 9543] diffusion training Loss: 0.0626073507592082
2024-11-05 01:46:31,060 - INFO - [diffusion][Epoch 9543] diffusion learning rate: 0.001
2024-11-05 01:46:31,061 - INFO - [diffusion][Epoch 9543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:31,062 - INFO - [diffusion][Epoch 9544] Epoch 9545/12000
2024-11-05 01:46:34,144 - INFO - [diffusion][Epoch 9544] diffusion training Loss: 0.06603214982897043
2024-11-05 01:46:34,146 - INFO - [diffusion][Epoch 9544] diffusion learning rate: 0.001
2024-11-05 01:46:34,147 - INFO - [diffusion][Epoch 9544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:34,148 - INFO - [diffusion][Epoch 9545] Epoch 9546/12000
2024-11-05 01:46:37,225 - INFO - [diffusion][Epoch 9545] diffusion training Loss: 0.06831252761185169
2024-11-05 01:46:37,227 - INFO - [diffusion][Epoch 9545] diffusion learning rate: 0.001
2024-11-05 01:46:37,228 - INFO - [diffusion][Epoch 9545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:37,229 - INFO - [diffusion][Epoch 9546] Epoch 9547/12000
2024-11-05 01:46:40,824 - INFO - [diffusion][Epoch 9546] diffusion training Loss: 0.07583882100880146
2024-11-05 01:46:40,826 - INFO - [diffusion][Epoch 9546] diffusion learning rate: 0.001
2024-11-05 01:46:40,827 - INFO - [diffusion][Epoch 9546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:40,829 - INFO - [diffusion][Epoch 9547] Epoch 9548/12000
2024-11-05 01:46:44,366 - INFO - [diffusion][Epoch 9547] diffusion training Loss: 0.06402148399502039
2024-11-05 01:46:44,369 - INFO - [diffusion][Epoch 9547] diffusion learning rate: 0.001
2024-11-05 01:46:44,371 - INFO - [diffusion][Epoch 9547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:44,372 - INFO - [diffusion][Epoch 9548] Epoch 9549/12000
2024-11-05 01:46:47,438 - INFO - [diffusion][Epoch 9548] diffusion training Loss: 0.061500283889472485
2024-11-05 01:46:47,440 - INFO - [diffusion][Epoch 9548] diffusion learning rate: 0.001
2024-11-05 01:46:47,442 - INFO - [diffusion][Epoch 9548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:47,443 - INFO - [diffusion][Epoch 9549] Epoch 9550/12000
2024-11-05 01:46:50,541 - INFO - [diffusion][Epoch 9549] diffusion training Loss: 0.06705104932188988
2024-11-05 01:46:50,543 - INFO - [diffusion][Epoch 9549] diffusion learning rate: 0.001
2024-11-05 01:46:50,545 - INFO - [diffusion][Epoch 9549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:50,547 - INFO - [diffusion][Epoch 9550] Epoch 9551/12000
2024-11-05 01:46:53,660 - INFO - [diffusion][Epoch 9550] diffusion training Loss: 0.06800612807273865
2024-11-05 01:46:53,662 - INFO - [diffusion][Epoch 9550] diffusion learning rate: 0.001
2024-11-05 01:46:53,664 - INFO - [diffusion][Epoch 9550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:53,665 - INFO - [diffusion][Epoch 9551] Epoch 9552/12000
2024-11-05 01:46:57,224 - INFO - [diffusion][Epoch 9551] diffusion training Loss: 0.06585857272148132
2024-11-05 01:46:57,226 - INFO - [diffusion][Epoch 9551] diffusion learning rate: 0.001
2024-11-05 01:46:57,227 - INFO - [diffusion][Epoch 9551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:57,229 - INFO - [diffusion][Epoch 9552] Epoch 9553/12000
2024-11-05 01:47:00,778 - INFO - [diffusion][Epoch 9552] diffusion training Loss: 0.0661817118525505
2024-11-05 01:47:00,779 - INFO - [diffusion][Epoch 9552] diffusion learning rate: 0.001
2024-11-05 01:47:00,781 - INFO - [diffusion][Epoch 9552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:00,782 - INFO - [diffusion][Epoch 9553] Epoch 9554/12000
2024-11-05 01:47:04,342 - INFO - [diffusion][Epoch 9553] diffusion training Loss: 0.06554565764963627
2024-11-05 01:47:04,344 - INFO - [diffusion][Epoch 9553] diffusion learning rate: 0.001
2024-11-05 01:47:04,345 - INFO - [diffusion][Epoch 9553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:04,347 - INFO - [diffusion][Epoch 9554] Epoch 9555/12000
2024-11-05 01:47:07,874 - INFO - [diffusion][Epoch 9554] diffusion training Loss: 0.06887425761669874
2024-11-05 01:47:07,875 - INFO - [diffusion][Epoch 9554] diffusion learning rate: 0.001
2024-11-05 01:47:07,877 - INFO - [diffusion][Epoch 9554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:07,878 - INFO - [diffusion][Epoch 9555] Epoch 9556/12000
2024-11-05 01:47:11,048 - INFO - [diffusion][Epoch 9555] diffusion training Loss: 0.06548219732940197
2024-11-05 01:47:11,050 - INFO - [diffusion][Epoch 9555] diffusion learning rate: 0.001
2024-11-05 01:47:11,087 - INFO - [diffusion][Epoch 9555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:11,088 - INFO - [diffusion][Epoch 9556] Epoch 9557/12000
2024-11-05 01:47:14,360 - INFO - [diffusion][Epoch 9556] diffusion training Loss: 0.06528663169592619
2024-11-05 01:47:14,362 - INFO - [diffusion][Epoch 9556] diffusion learning rate: 0.001
2024-11-05 01:47:14,363 - INFO - [diffusion][Epoch 9556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:14,365 - INFO - [diffusion][Epoch 9557] Epoch 9558/12000
2024-11-05 01:47:17,702 - INFO - [diffusion][Epoch 9557] diffusion training Loss: 0.07146530598402023
2024-11-05 01:47:17,704 - INFO - [diffusion][Epoch 9557] diffusion learning rate: 0.001
2024-11-05 01:47:17,706 - INFO - [diffusion][Epoch 9557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:17,707 - INFO - [diffusion][Epoch 9558] Epoch 9559/12000
2024-11-05 01:47:21,371 - INFO - [diffusion][Epoch 9558] diffusion training Loss: 0.07041417621076107
2024-11-05 01:47:21,373 - INFO - [diffusion][Epoch 9558] diffusion learning rate: 0.001
2024-11-05 01:47:21,415 - INFO - [diffusion][Epoch 9558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:21,416 - INFO - [diffusion][Epoch 9559] Epoch 9560/12000
2024-11-05 01:47:24,938 - INFO - [diffusion][Epoch 9559] diffusion training Loss: 0.06871202494949102
2024-11-05 01:47:24,940 - INFO - [diffusion][Epoch 9559] diffusion learning rate: 0.001
2024-11-05 01:47:24,942 - INFO - [diffusion][Epoch 9559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:24,943 - INFO - [diffusion][Epoch 9560] Epoch 9561/12000
2024-11-05 01:47:28,061 - INFO - [diffusion][Epoch 9560] diffusion training Loss: 0.06577048450708389
2024-11-05 01:47:28,063 - INFO - [diffusion][Epoch 9560] diffusion learning rate: 0.001
2024-11-05 01:47:28,065 - INFO - [diffusion][Epoch 9560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:28,066 - INFO - [diffusion][Epoch 9561] Epoch 9562/12000
2024-11-05 01:47:31,159 - INFO - [diffusion][Epoch 9561] diffusion training Loss: 0.07023368962109089
2024-11-05 01:47:31,161 - INFO - [diffusion][Epoch 9561] diffusion learning rate: 0.001
2024-11-05 01:47:31,162 - INFO - [diffusion][Epoch 9561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:31,164 - INFO - [diffusion][Epoch 9562] Epoch 9563/12000
2024-11-05 01:47:34,325 - INFO - [diffusion][Epoch 9562] diffusion training Loss: 0.0664256103336811
2024-11-05 01:47:34,327 - INFO - [diffusion][Epoch 9562] diffusion learning rate: 0.001
2024-11-05 01:47:34,329 - INFO - [diffusion][Epoch 9562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:34,330 - INFO - [diffusion][Epoch 9563] Epoch 9564/12000
2024-11-05 01:47:37,941 - INFO - [diffusion][Epoch 9563] diffusion training Loss: 0.06713675428181887
2024-11-05 01:47:37,943 - INFO - [diffusion][Epoch 9563] diffusion learning rate: 0.001
2024-11-05 01:47:37,945 - INFO - [diffusion][Epoch 9563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:37,946 - INFO - [diffusion][Epoch 9564] Epoch 9565/12000
2024-11-05 01:47:41,223 - INFO - [diffusion][Epoch 9564] diffusion training Loss: 0.06186036113649607
2024-11-05 01:47:41,225 - INFO - [diffusion][Epoch 9564] diffusion learning rate: 0.001
2024-11-05 01:47:41,226 - INFO - [diffusion][Epoch 9564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:41,229 - INFO - [diffusion][Epoch 9565] Epoch 9566/12000
2024-11-05 01:47:44,351 - INFO - [diffusion][Epoch 9565] diffusion training Loss: 0.07408387027680874
2024-11-05 01:47:44,353 - INFO - [diffusion][Epoch 9565] diffusion learning rate: 0.001
2024-11-05 01:47:44,354 - INFO - [diffusion][Epoch 9565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:44,356 - INFO - [diffusion][Epoch 9566] Epoch 9567/12000
2024-11-05 01:47:47,622 - INFO - [diffusion][Epoch 9566] diffusion training Loss: 0.06657552532851696
2024-11-05 01:47:47,625 - INFO - [diffusion][Epoch 9566] diffusion learning rate: 0.001
2024-11-05 01:47:47,626 - INFO - [diffusion][Epoch 9566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:47,628 - INFO - [diffusion][Epoch 9567] Epoch 9568/12000
2024-11-05 01:47:50,896 - INFO - [diffusion][Epoch 9567] diffusion training Loss: 0.07125044241547585
2024-11-05 01:47:50,898 - INFO - [diffusion][Epoch 9567] diffusion learning rate: 0.001
2024-11-05 01:47:50,941 - INFO - [diffusion][Epoch 9567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:50,942 - INFO - [diffusion][Epoch 9568] Epoch 9569/12000
2024-11-05 01:47:54,642 - INFO - [diffusion][Epoch 9568] diffusion training Loss: 0.07228372804820538
2024-11-05 01:47:54,644 - INFO - [diffusion][Epoch 9568] diffusion learning rate: 0.001
2024-11-05 01:47:54,645 - INFO - [diffusion][Epoch 9568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:54,647 - INFO - [diffusion][Epoch 9569] Epoch 9570/12000
2024-11-05 01:47:58,150 - INFO - [diffusion][Epoch 9569] diffusion training Loss: 0.07249418180435896
2024-11-05 01:47:58,151 - INFO - [diffusion][Epoch 9569] diffusion learning rate: 0.001
2024-11-05 01:47:58,153 - INFO - [diffusion][Epoch 9569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:58,154 - INFO - [diffusion][Epoch 9570] Epoch 9571/12000
2024-11-05 01:48:00,921 - INFO - [diffusion][Epoch 9570] diffusion training Loss: 0.06876686867326498
2024-11-05 01:48:00,923 - INFO - [diffusion][Epoch 9570] diffusion learning rate: 0.001
2024-11-05 01:48:00,925 - INFO - [diffusion][Epoch 9570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:00,926 - INFO - [diffusion][Epoch 9571] Epoch 9572/12000
2024-11-05 01:48:04,087 - INFO - [diffusion][Epoch 9571] diffusion training Loss: 0.06597945187240839
2024-11-05 01:48:04,088 - INFO - [diffusion][Epoch 9571] diffusion learning rate: 0.001
2024-11-05 01:48:04,090 - INFO - [diffusion][Epoch 9571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:04,092 - INFO - [diffusion][Epoch 9572] Epoch 9573/12000
2024-11-05 01:48:07,837 - INFO - [diffusion][Epoch 9572] diffusion training Loss: 0.07260551117360592
2024-11-05 01:48:07,839 - INFO - [diffusion][Epoch 9572] diffusion learning rate: 0.001
2024-11-05 01:48:07,841 - INFO - [diffusion][Epoch 9572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:07,842 - INFO - [diffusion][Epoch 9573] Epoch 9574/12000
2024-11-05 01:48:11,491 - INFO - [diffusion][Epoch 9573] diffusion training Loss: 0.0699042696505785
2024-11-05 01:48:11,493 - INFO - [diffusion][Epoch 9573] diffusion learning rate: 0.001
2024-11-05 01:48:11,495 - INFO - [diffusion][Epoch 9573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:11,496 - INFO - [diffusion][Epoch 9574] Epoch 9575/12000
2024-11-05 01:48:14,649 - INFO - [diffusion][Epoch 9574] diffusion training Loss: 0.07538532465696335
2024-11-05 01:48:14,651 - INFO - [diffusion][Epoch 9574] diffusion learning rate: 0.001
2024-11-05 01:48:14,652 - INFO - [diffusion][Epoch 9574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:14,654 - INFO - [diffusion][Epoch 9575] Epoch 9576/12000
2024-11-05 01:48:17,754 - INFO - [diffusion][Epoch 9575] diffusion training Loss: 0.06530088372528553
2024-11-05 01:48:17,756 - INFO - [diffusion][Epoch 9575] diffusion learning rate: 0.001
2024-11-05 01:48:17,758 - INFO - [diffusion][Epoch 9575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:17,759 - INFO - [diffusion][Epoch 9576] Epoch 9577/12000
2024-11-05 01:48:20,943 - INFO - [diffusion][Epoch 9576] diffusion training Loss: 0.0642421655356884
2024-11-05 01:48:20,946 - INFO - [diffusion][Epoch 9576] diffusion learning rate: 0.001
2024-11-05 01:48:20,948 - INFO - [diffusion][Epoch 9576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:20,949 - INFO - [diffusion][Epoch 9577] Epoch 9578/12000
2024-11-05 01:48:24,445 - INFO - [diffusion][Epoch 9577] diffusion training Loss: 0.06776438653469086
2024-11-05 01:48:24,447 - INFO - [diffusion][Epoch 9577] diffusion learning rate: 0.001
2024-11-05 01:48:24,449 - INFO - [diffusion][Epoch 9577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:24,450 - INFO - [diffusion][Epoch 9578] Epoch 9579/12000
2024-11-05 01:48:28,170 - INFO - [diffusion][Epoch 9578] diffusion training Loss: 0.06322748400270939
2024-11-05 01:48:28,172 - INFO - [diffusion][Epoch 9578] diffusion learning rate: 0.001
2024-11-05 01:48:28,174 - INFO - [diffusion][Epoch 9578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:28,175 - INFO - [diffusion][Epoch 9579] Epoch 9580/12000
2024-11-05 01:48:31,479 - INFO - [diffusion][Epoch 9579] diffusion training Loss: 0.06751817651093006
2024-11-05 01:48:31,481 - INFO - [diffusion][Epoch 9579] diffusion learning rate: 0.001
2024-11-05 01:48:31,483 - INFO - [diffusion][Epoch 9579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:31,484 - INFO - [diffusion][Epoch 9580] Epoch 9581/12000
2024-11-05 01:48:34,577 - INFO - [diffusion][Epoch 9580] diffusion training Loss: 0.06743803154677153
2024-11-05 01:48:34,579 - INFO - [diffusion][Epoch 9580] diffusion learning rate: 0.001
2024-11-05 01:48:34,581 - INFO - [diffusion][Epoch 9580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:34,582 - INFO - [diffusion][Epoch 9581] Epoch 9582/12000
2024-11-05 01:48:37,774 - INFO - [diffusion][Epoch 9581] diffusion training Loss: 0.06897861696779728
2024-11-05 01:48:37,776 - INFO - [diffusion][Epoch 9581] diffusion learning rate: 0.001
2024-11-05 01:48:37,779 - INFO - [diffusion][Epoch 9581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:37,781 - INFO - [diffusion][Epoch 9582] Epoch 9583/12000
2024-11-05 01:48:41,201 - INFO - [diffusion][Epoch 9582] diffusion training Loss: 0.07214807718992233
2024-11-05 01:48:41,203 - INFO - [diffusion][Epoch 9582] diffusion learning rate: 0.001
2024-11-05 01:48:41,204 - INFO - [diffusion][Epoch 9582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:41,206 - INFO - [diffusion][Epoch 9583] Epoch 9584/12000
2024-11-05 01:48:44,912 - INFO - [diffusion][Epoch 9583] diffusion training Loss: 0.06321557238698006
2024-11-05 01:48:44,913 - INFO - [diffusion][Epoch 9583] diffusion learning rate: 0.001
2024-11-05 01:48:44,949 - INFO - [diffusion][Epoch 9583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:44,950 - INFO - [diffusion][Epoch 9584] Epoch 9585/12000
2024-11-05 01:48:48,300 - INFO - [diffusion][Epoch 9584] diffusion training Loss: 0.07646996527910233
2024-11-05 01:48:48,304 - INFO - [diffusion][Epoch 9584] diffusion learning rate: 0.001
2024-11-05 01:48:48,306 - INFO - [diffusion][Epoch 9584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:48,307 - INFO - [diffusion][Epoch 9585] Epoch 9586/12000
2024-11-05 01:48:51,539 - INFO - [diffusion][Epoch 9585] diffusion training Loss: 0.06381007097661495
2024-11-05 01:48:51,541 - INFO - [diffusion][Epoch 9585] diffusion learning rate: 0.001
2024-11-05 01:48:51,543 - INFO - [diffusion][Epoch 9585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:51,544 - INFO - [diffusion][Epoch 9586] Epoch 9587/12000
2024-11-05 01:48:54,724 - INFO - [diffusion][Epoch 9586] diffusion training Loss: 0.06732243206351995
2024-11-05 01:48:54,726 - INFO - [diffusion][Epoch 9586] diffusion learning rate: 0.001
2024-11-05 01:48:54,728 - INFO - [diffusion][Epoch 9586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:54,729 - INFO - [diffusion][Epoch 9587] Epoch 9588/12000
2024-11-05 01:48:57,971 - INFO - [diffusion][Epoch 9587] diffusion training Loss: 0.07038754224777222
2024-11-05 01:48:57,973 - INFO - [diffusion][Epoch 9587] diffusion learning rate: 0.001
2024-11-05 01:48:57,974 - INFO - [diffusion][Epoch 9587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:57,976 - INFO - [diffusion][Epoch 9588] Epoch 9589/12000
2024-11-05 01:49:01,673 - INFO - [diffusion][Epoch 9588] diffusion training Loss: 0.065550047904253
2024-11-05 01:49:01,674 - INFO - [diffusion][Epoch 9588] diffusion learning rate: 0.001
2024-11-05 01:49:01,676 - INFO - [diffusion][Epoch 9588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:01,678 - INFO - [diffusion][Epoch 9589] Epoch 9590/12000
2024-11-05 01:49:05,188 - INFO - [diffusion][Epoch 9589] diffusion training Loss: 0.06428038235753775
2024-11-05 01:49:05,190 - INFO - [diffusion][Epoch 9589] diffusion learning rate: 0.001
2024-11-05 01:49:05,191 - INFO - [diffusion][Epoch 9589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:05,193 - INFO - [diffusion][Epoch 9590] Epoch 9591/12000
2024-11-05 01:49:08,226 - INFO - [diffusion][Epoch 9590] diffusion training Loss: 0.06925272569060326
2024-11-05 01:49:08,228 - INFO - [diffusion][Epoch 9590] diffusion learning rate: 0.001
2024-11-05 01:49:08,230 - INFO - [diffusion][Epoch 9590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:08,231 - INFO - [diffusion][Epoch 9591] Epoch 9592/12000
2024-11-05 01:49:11,346 - INFO - [diffusion][Epoch 9591] diffusion training Loss: 0.06609921995550394
2024-11-05 01:49:11,348 - INFO - [diffusion][Epoch 9591] diffusion learning rate: 0.001
2024-11-05 01:49:11,350 - INFO - [diffusion][Epoch 9591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:11,351 - INFO - [diffusion][Epoch 9592] Epoch 9593/12000
2024-11-05 01:49:14,732 - INFO - [diffusion][Epoch 9592] diffusion training Loss: 0.07050245255231857
2024-11-05 01:49:14,734 - INFO - [diffusion][Epoch 9592] diffusion learning rate: 0.001
2024-11-05 01:49:14,736 - INFO - [diffusion][Epoch 9592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:14,737 - INFO - [diffusion][Epoch 9593] Epoch 9594/12000
2024-11-05 01:49:18,430 - INFO - [diffusion][Epoch 9593] diffusion training Loss: 0.07102033123373985
2024-11-05 01:49:18,432 - INFO - [diffusion][Epoch 9593] diffusion learning rate: 0.001
2024-11-05 01:49:18,434 - INFO - [diffusion][Epoch 9593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:18,436 - INFO - [diffusion][Epoch 9594] Epoch 9595/12000
2024-11-05 01:49:21,551 - INFO - [diffusion][Epoch 9594] diffusion training Loss: 0.07217579334974289
2024-11-05 01:49:21,553 - INFO - [diffusion][Epoch 9594] diffusion learning rate: 0.001
2024-11-05 01:49:21,555 - INFO - [diffusion][Epoch 9594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:21,556 - INFO - [diffusion][Epoch 9595] Epoch 9596/12000
2024-11-05 01:49:25,085 - INFO - [diffusion][Epoch 9595] diffusion training Loss: 0.06971554644405842
2024-11-05 01:49:25,087 - INFO - [diffusion][Epoch 9595] diffusion learning rate: 0.001
2024-11-05 01:49:25,089 - INFO - [diffusion][Epoch 9595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:25,091 - INFO - [diffusion][Epoch 9596] Epoch 9597/12000
2024-11-05 01:49:28,134 - INFO - [diffusion][Epoch 9596] diffusion training Loss: 0.06542062014341354
2024-11-05 01:49:28,136 - INFO - [diffusion][Epoch 9596] diffusion learning rate: 0.001
2024-11-05 01:49:28,138 - INFO - [diffusion][Epoch 9596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:28,139 - INFO - [diffusion][Epoch 9597] Epoch 9598/12000
2024-11-05 01:49:31,667 - INFO - [diffusion][Epoch 9597] diffusion training Loss: 0.06397301517426968
2024-11-05 01:49:31,669 - INFO - [diffusion][Epoch 9597] diffusion learning rate: 0.001
2024-11-05 01:49:31,671 - INFO - [diffusion][Epoch 9597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:31,672 - INFO - [diffusion][Epoch 9598] Epoch 9599/12000
2024-11-05 01:49:35,351 - INFO - [diffusion][Epoch 9598] diffusion training Loss: 0.07316475920379162
2024-11-05 01:49:35,352 - INFO - [diffusion][Epoch 9598] diffusion learning rate: 0.001
2024-11-05 01:49:35,354 - INFO - [diffusion][Epoch 9598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:35,356 - INFO - [diffusion][Epoch 9599] Epoch 9600/12000
2024-11-05 01:49:38,647 - INFO - [diffusion][Epoch 9599] diffusion training Loss: 0.06761933024972677
2024-11-05 01:49:38,649 - INFO - [diffusion][Epoch 9599] diffusion learning rate: 0.001
2024-11-05 01:49:38,651 - INFO - [diffusion][Epoch 9599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:38,652 - INFO - [diffusion][Epoch 9600] Epoch 9601/12000
2024-11-05 01:49:41,724 - INFO - [diffusion][Epoch 9600] diffusion training Loss: 0.06592665426433086
2024-11-05 01:49:41,726 - INFO - [diffusion][Epoch 9600] diffusion learning rate: 0.001
2024-11-05 01:49:41,728 - INFO - [diffusion][Epoch 9600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:41,729 - INFO - [diffusion][Epoch 9601] Epoch 9602/12000
2024-11-05 01:49:44,864 - INFO - [diffusion][Epoch 9601] diffusion training Loss: 0.06756523065268993
2024-11-05 01:49:44,866 - INFO - [diffusion][Epoch 9601] diffusion learning rate: 0.001
2024-11-05 01:49:44,910 - INFO - [diffusion][Epoch 9601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:44,912 - INFO - [diffusion][Epoch 9602] Epoch 9603/12000
2024-11-05 01:49:48,367 - INFO - [diffusion][Epoch 9602] diffusion training Loss: 0.06630251836031675
2024-11-05 01:49:48,370 - INFO - [diffusion][Epoch 9602] diffusion learning rate: 0.001
2024-11-05 01:49:48,373 - INFO - [diffusion][Epoch 9602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:48,378 - INFO - [diffusion][Epoch 9603] Epoch 9604/12000
2024-11-05 01:49:52,098 - INFO - [diffusion][Epoch 9603] diffusion training Loss: 0.0670524425804615
2024-11-05 01:49:52,100 - INFO - [diffusion][Epoch 9603] diffusion learning rate: 0.001
2024-11-05 01:49:52,101 - INFO - [diffusion][Epoch 9603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:52,103 - INFO - [diffusion][Epoch 9604] Epoch 9605/12000
2024-11-05 01:49:55,399 - INFO - [diffusion][Epoch 9604] diffusion training Loss: 0.06410285271704197
2024-11-05 01:49:55,401 - INFO - [diffusion][Epoch 9604] diffusion learning rate: 0.001
2024-11-05 01:49:55,486 - INFO - [diffusion][Epoch 9604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:55,487 - INFO - [diffusion][Epoch 9605] Epoch 9606/12000
2024-11-05 01:49:58,714 - INFO - [diffusion][Epoch 9605] diffusion training Loss: 0.07188709080219269
2024-11-05 01:49:58,716 - INFO - [diffusion][Epoch 9605] diffusion learning rate: 0.001
2024-11-05 01:49:58,718 - INFO - [diffusion][Epoch 9605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:58,719 - INFO - [diffusion][Epoch 9606] Epoch 9607/12000
2024-11-05 01:50:01,927 - INFO - [diffusion][Epoch 9606] diffusion training Loss: 0.06741649657487869
2024-11-05 01:50:01,929 - INFO - [diffusion][Epoch 9606] diffusion learning rate: 0.001
2024-11-05 01:50:01,931 - INFO - [diffusion][Epoch 9606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:01,933 - INFO - [diffusion][Epoch 9607] Epoch 9608/12000
2024-11-05 01:50:05,203 - INFO - [diffusion][Epoch 9607] diffusion training Loss: 0.07101373933255672
2024-11-05 01:50:05,205 - INFO - [diffusion][Epoch 9607] diffusion learning rate: 0.001
2024-11-05 01:50:05,207 - INFO - [diffusion][Epoch 9607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:05,208 - INFO - [diffusion][Epoch 9608] Epoch 9609/12000
2024-11-05 01:50:08,809 - INFO - [diffusion][Epoch 9608] diffusion training Loss: 0.06381848081946373
2024-11-05 01:50:08,811 - INFO - [diffusion][Epoch 9608] diffusion learning rate: 0.001
2024-11-05 01:50:08,813 - INFO - [diffusion][Epoch 9608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:08,814 - INFO - [diffusion][Epoch 9609] Epoch 9610/12000
2024-11-05 01:50:12,339 - INFO - [diffusion][Epoch 9609] diffusion training Loss: 0.06564728170633316
2024-11-05 01:50:12,496 - INFO - [diffusion][Epoch 9609] diffusion learning rate: 0.001
2024-11-05 01:50:12,498 - INFO - [diffusion][Epoch 9609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:12,499 - INFO - [diffusion][Epoch 9610] Epoch 9611/12000
2024-11-05 01:50:15,805 - INFO - [diffusion][Epoch 9610] diffusion training Loss: 0.06896316725760698
2024-11-05 01:50:15,807 - INFO - [diffusion][Epoch 9610] diffusion learning rate: 0.001
2024-11-05 01:50:15,808 - INFO - [diffusion][Epoch 9610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:15,809 - INFO - [diffusion][Epoch 9611] Epoch 9612/12000
2024-11-05 01:50:18,967 - INFO - [diffusion][Epoch 9611] diffusion training Loss: 0.07501422613859177
2024-11-05 01:50:18,969 - INFO - [diffusion][Epoch 9611] diffusion learning rate: 0.001
2024-11-05 01:50:18,971 - INFO - [diffusion][Epoch 9611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:18,972 - INFO - [diffusion][Epoch 9612] Epoch 9613/12000
2024-11-05 01:50:22,085 - INFO - [diffusion][Epoch 9612] diffusion training Loss: 0.06485235504806042
2024-11-05 01:50:22,087 - INFO - [diffusion][Epoch 9612] diffusion learning rate: 0.001
2024-11-05 01:50:22,150 - INFO - [diffusion][Epoch 9612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:22,152 - INFO - [diffusion][Epoch 9613] Epoch 9614/12000
2024-11-05 01:50:25,604 - INFO - [diffusion][Epoch 9613] diffusion training Loss: 0.07424425147473812
2024-11-05 01:50:25,606 - INFO - [diffusion][Epoch 9613] diffusion learning rate: 0.001
2024-11-05 01:50:25,608 - INFO - [diffusion][Epoch 9613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:25,609 - INFO - [diffusion][Epoch 9614] Epoch 9615/12000
2024-11-05 01:50:29,586 - INFO - [diffusion][Epoch 9614] diffusion training Loss: 0.059439267963171005
2024-11-05 01:50:29,588 - INFO - [diffusion][Epoch 9614] diffusion learning rate: 0.001
2024-11-05 01:50:29,590 - INFO - [diffusion][Epoch 9614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:29,591 - INFO - [diffusion][Epoch 9615] Epoch 9616/12000
2024-11-05 01:50:33,144 - INFO - [diffusion][Epoch 9615] diffusion training Loss: 0.06868054158985615
2024-11-05 01:50:33,146 - INFO - [diffusion][Epoch 9615] diffusion learning rate: 0.001
2024-11-05 01:50:33,148 - INFO - [diffusion][Epoch 9615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:33,149 - INFO - [diffusion][Epoch 9616] Epoch 9617/12000
2024-11-05 01:50:36,338 - INFO - [diffusion][Epoch 9616] diffusion training Loss: 0.06681324169039726
2024-11-05 01:50:36,341 - INFO - [diffusion][Epoch 9616] diffusion learning rate: 0.001
2024-11-05 01:50:36,343 - INFO - [diffusion][Epoch 9616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:36,344 - INFO - [diffusion][Epoch 9617] Epoch 9618/12000
2024-11-05 01:50:39,567 - INFO - [diffusion][Epoch 9617] diffusion training Loss: 0.06804088223725557
2024-11-05 01:50:39,569 - INFO - [diffusion][Epoch 9617] diffusion learning rate: 0.001
2024-11-05 01:50:39,571 - INFO - [diffusion][Epoch 9617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:39,572 - INFO - [diffusion][Epoch 9618] Epoch 9619/12000
2024-11-05 01:50:42,643 - INFO - [diffusion][Epoch 9618] diffusion training Loss: 0.06831684708595276
2024-11-05 01:50:42,644 - INFO - [diffusion][Epoch 9618] diffusion learning rate: 0.001
2024-11-05 01:50:42,646 - INFO - [diffusion][Epoch 9618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:42,647 - INFO - [diffusion][Epoch 9619] Epoch 9620/12000
2024-11-05 01:50:46,119 - INFO - [diffusion][Epoch 9619] diffusion training Loss: 0.06800773739814758
2024-11-05 01:50:46,121 - INFO - [diffusion][Epoch 9619] diffusion learning rate: 0.001
2024-11-05 01:50:46,122 - INFO - [diffusion][Epoch 9619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:46,124 - INFO - [diffusion][Epoch 9620] Epoch 9621/12000
2024-11-05 01:50:49,625 - INFO - [diffusion][Epoch 9620] diffusion training Loss: 0.06934771127998829
2024-11-05 01:50:49,627 - INFO - [diffusion][Epoch 9620] diffusion learning rate: 0.001
2024-11-05 01:50:49,661 - INFO - [diffusion][Epoch 9620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:49,662 - INFO - [diffusion][Epoch 9621] Epoch 9622/12000
2024-11-05 01:50:52,810 - INFO - [diffusion][Epoch 9621] diffusion training Loss: 0.06910761166363955
2024-11-05 01:50:52,812 - INFO - [diffusion][Epoch 9621] diffusion learning rate: 0.001
2024-11-05 01:50:52,813 - INFO - [diffusion][Epoch 9621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:52,815 - INFO - [diffusion][Epoch 9622] Epoch 9623/12000
2024-11-05 01:50:55,947 - INFO - [diffusion][Epoch 9622] diffusion training Loss: 0.06673245877027512
2024-11-05 01:50:55,949 - INFO - [diffusion][Epoch 9622] diffusion learning rate: 0.001
2024-11-05 01:50:55,950 - INFO - [diffusion][Epoch 9622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:55,952 - INFO - [diffusion][Epoch 9623] Epoch 9624/12000
2024-11-05 01:50:59,056 - INFO - [diffusion][Epoch 9623] diffusion training Loss: 0.05938127636909485
2024-11-05 01:50:59,058 - INFO - [diffusion][Epoch 9623] diffusion learning rate: 0.001
2024-11-05 01:50:59,060 - INFO - [diffusion][Epoch 9623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:59,061 - INFO - [diffusion][Epoch 9624] Epoch 9625/12000
2024-11-05 01:51:02,574 - INFO - [diffusion][Epoch 9624] diffusion training Loss: 0.06819221563637257
2024-11-05 01:51:02,576 - INFO - [diffusion][Epoch 9624] diffusion learning rate: 0.001
2024-11-05 01:51:02,577 - INFO - [diffusion][Epoch 9624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:02,581 - INFO - [diffusion][Epoch 9625] Epoch 9626/12000
2024-11-05 01:51:06,192 - INFO - [diffusion][Epoch 9625] diffusion training Loss: 0.06353561114519835
2024-11-05 01:51:06,194 - INFO - [diffusion][Epoch 9625] diffusion learning rate: 0.001
2024-11-05 01:51:06,195 - INFO - [diffusion][Epoch 9625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:06,196 - INFO - [diffusion][Epoch 9626] Epoch 9627/12000
2024-11-05 01:51:09,287 - INFO - [diffusion][Epoch 9626] diffusion training Loss: 0.06875415332615376
2024-11-05 01:51:09,290 - INFO - [diffusion][Epoch 9626] diffusion learning rate: 0.001
2024-11-05 01:51:09,292 - INFO - [diffusion][Epoch 9626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:09,293 - INFO - [diffusion][Epoch 9627] Epoch 9628/12000
2024-11-05 01:51:12,426 - INFO - [diffusion][Epoch 9627] diffusion training Loss: 0.06829723715782166
2024-11-05 01:51:12,431 - INFO - [diffusion][Epoch 9627] diffusion learning rate: 0.001
2024-11-05 01:51:12,432 - INFO - [diffusion][Epoch 9627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:12,436 - INFO - [diffusion][Epoch 9628] Epoch 9629/12000
2024-11-05 01:51:15,523 - INFO - [diffusion][Epoch 9628] diffusion training Loss: 0.06824242416769266
2024-11-05 01:51:15,525 - INFO - [diffusion][Epoch 9628] diffusion learning rate: 0.001
2024-11-05 01:51:15,527 - INFO - [diffusion][Epoch 9628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:15,528 - INFO - [diffusion][Epoch 9629] Epoch 9630/12000
2024-11-05 01:51:19,067 - INFO - [diffusion][Epoch 9629] diffusion training Loss: 0.07375989854335785
2024-11-05 01:51:19,069 - INFO - [diffusion][Epoch 9629] diffusion learning rate: 0.001
2024-11-05 01:51:19,070 - INFO - [diffusion][Epoch 9629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:19,072 - INFO - [diffusion][Epoch 9630] Epoch 9631/12000
2024-11-05 01:51:22,613 - INFO - [diffusion][Epoch 9630] diffusion training Loss: 0.07135457545518875
2024-11-05 01:51:22,615 - INFO - [diffusion][Epoch 9630] diffusion learning rate: 0.001
2024-11-05 01:51:22,662 - INFO - [diffusion][Epoch 9630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:22,663 - INFO - [diffusion][Epoch 9631] Epoch 9632/12000
2024-11-05 01:51:25,816 - INFO - [diffusion][Epoch 9631] diffusion training Loss: 0.06484760716557503
2024-11-05 01:51:25,817 - INFO - [diffusion][Epoch 9631] diffusion learning rate: 0.001
2024-11-05 01:51:25,819 - INFO - [diffusion][Epoch 9631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:25,820 - INFO - [diffusion][Epoch 9632] Epoch 9633/12000
2024-11-05 01:51:28,949 - INFO - [diffusion][Epoch 9632] diffusion training Loss: 0.06551690772175789
2024-11-05 01:51:28,951 - INFO - [diffusion][Epoch 9632] diffusion learning rate: 0.001
2024-11-05 01:51:28,953 - INFO - [diffusion][Epoch 9632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:28,954 - INFO - [diffusion][Epoch 9633] Epoch 9634/12000
2024-11-05 01:51:32,374 - INFO - [diffusion][Epoch 9633] diffusion training Loss: 0.06786356214433908
2024-11-05 01:51:32,376 - INFO - [diffusion][Epoch 9633] diffusion learning rate: 0.001
2024-11-05 01:51:32,377 - INFO - [diffusion][Epoch 9633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:32,379 - INFO - [diffusion][Epoch 9634] Epoch 9635/12000
2024-11-05 01:51:35,705 - INFO - [diffusion][Epoch 9634] diffusion training Loss: 0.06748924776911736
2024-11-05 01:51:35,707 - INFO - [diffusion][Epoch 9634] diffusion learning rate: 0.001
2024-11-05 01:51:35,709 - INFO - [diffusion][Epoch 9634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:35,710 - INFO - [diffusion][Epoch 9635] Epoch 9636/12000
2024-11-05 01:51:39,428 - INFO - [diffusion][Epoch 9635] diffusion training Loss: 0.06841154769062996
2024-11-05 01:51:39,430 - INFO - [diffusion][Epoch 9635] diffusion learning rate: 0.001
2024-11-05 01:51:39,432 - INFO - [diffusion][Epoch 9635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:39,433 - INFO - [diffusion][Epoch 9636] Epoch 9637/12000
2024-11-05 01:51:42,897 - INFO - [diffusion][Epoch 9636] diffusion training Loss: 0.06787513382732868
2024-11-05 01:51:42,899 - INFO - [diffusion][Epoch 9636] diffusion learning rate: 0.001
2024-11-05 01:51:42,901 - INFO - [diffusion][Epoch 9636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:42,903 - INFO - [diffusion][Epoch 9637] Epoch 9638/12000
2024-11-05 01:51:46,103 - INFO - [diffusion][Epoch 9637] diffusion training Loss: 0.06378332246094942
2024-11-05 01:51:46,105 - INFO - [diffusion][Epoch 9637] diffusion learning rate: 0.001
2024-11-05 01:51:46,107 - INFO - [diffusion][Epoch 9637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:46,108 - INFO - [diffusion][Epoch 9638] Epoch 9639/12000
2024-11-05 01:51:49,258 - INFO - [diffusion][Epoch 9638] diffusion training Loss: 0.07073353417217731
2024-11-05 01:51:49,260 - INFO - [diffusion][Epoch 9638] diffusion learning rate: 0.001
2024-11-05 01:51:49,262 - INFO - [diffusion][Epoch 9638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:49,263 - INFO - [diffusion][Epoch 9639] Epoch 9640/12000
2024-11-05 01:51:52,475 - INFO - [diffusion][Epoch 9639] diffusion training Loss: 0.06630865950137377
2024-11-05 01:51:52,477 - INFO - [diffusion][Epoch 9639] diffusion learning rate: 0.001
2024-11-05 01:51:52,479 - INFO - [diffusion][Epoch 9639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:52,480 - INFO - [diffusion][Epoch 9640] Epoch 9641/12000
2024-11-05 01:51:56,097 - INFO - [diffusion][Epoch 9640] diffusion training Loss: 0.06704907864332199
2024-11-05 01:51:56,099 - INFO - [diffusion][Epoch 9640] diffusion learning rate: 0.001
2024-11-05 01:51:56,100 - INFO - [diffusion][Epoch 9640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:56,102 - INFO - [diffusion][Epoch 9641] Epoch 9642/12000
2024-11-05 01:51:59,680 - INFO - [diffusion][Epoch 9641] diffusion training Loss: 0.06497302651405334
2024-11-05 01:51:59,681 - INFO - [diffusion][Epoch 9641] diffusion learning rate: 0.001
2024-11-05 01:51:59,683 - INFO - [diffusion][Epoch 9641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:59,684 - INFO - [diffusion][Epoch 9642] Epoch 9643/12000
2024-11-05 01:52:02,805 - INFO - [diffusion][Epoch 9642] diffusion training Loss: 0.06533928401768208
2024-11-05 01:52:02,807 - INFO - [diffusion][Epoch 9642] diffusion learning rate: 0.001
2024-11-05 01:52:02,808 - INFO - [diffusion][Epoch 9642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:02,810 - INFO - [diffusion][Epoch 9643] Epoch 9644/12000
2024-11-05 01:52:05,857 - INFO - [diffusion][Epoch 9643] diffusion training Loss: 0.06461274158209562
2024-11-05 01:52:05,859 - INFO - [diffusion][Epoch 9643] diffusion learning rate: 0.001
2024-11-05 01:52:05,861 - INFO - [diffusion][Epoch 9643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:05,862 - INFO - [diffusion][Epoch 9644] Epoch 9645/12000
2024-11-05 01:52:09,010 - INFO - [diffusion][Epoch 9644] diffusion training Loss: 0.06983121670782566
2024-11-05 01:52:09,012 - INFO - [diffusion][Epoch 9644] diffusion learning rate: 0.001
2024-11-05 01:52:09,014 - INFO - [diffusion][Epoch 9644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:09,015 - INFO - [diffusion][Epoch 9645] Epoch 9646/12000
2024-11-05 01:52:12,569 - INFO - [diffusion][Epoch 9645] diffusion training Loss: 0.06759244855493307
2024-11-05 01:52:12,570 - INFO - [diffusion][Epoch 9645] diffusion learning rate: 0.001
2024-11-05 01:52:12,572 - INFO - [diffusion][Epoch 9645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:12,573 - INFO - [diffusion][Epoch 9646] Epoch 9647/12000
2024-11-05 01:52:16,113 - INFO - [diffusion][Epoch 9646] diffusion training Loss: 0.07214835286140442
2024-11-05 01:52:16,115 - INFO - [diffusion][Epoch 9646] diffusion learning rate: 0.001
2024-11-05 01:52:16,117 - INFO - [diffusion][Epoch 9646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:16,118 - INFO - [diffusion][Epoch 9647] Epoch 9648/12000
2024-11-05 01:52:19,188 - INFO - [diffusion][Epoch 9647] diffusion training Loss: 0.0619485666975379
2024-11-05 01:52:19,190 - INFO - [diffusion][Epoch 9647] diffusion learning rate: 0.001
2024-11-05 01:52:19,193 - INFO - [diffusion][Epoch 9647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:19,194 - INFO - [diffusion][Epoch 9648] Epoch 9649/12000
2024-11-05 01:52:22,244 - INFO - [diffusion][Epoch 9648] diffusion training Loss: 0.07521170191466808
2024-11-05 01:52:22,246 - INFO - [diffusion][Epoch 9648] diffusion learning rate: 0.001
2024-11-05 01:52:22,248 - INFO - [diffusion][Epoch 9648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:22,249 - INFO - [diffusion][Epoch 9649] Epoch 9650/12000
2024-11-05 01:52:25,433 - INFO - [diffusion][Epoch 9649] diffusion training Loss: 0.06740828976035118
2024-11-05 01:52:25,435 - INFO - [diffusion][Epoch 9649] diffusion learning rate: 0.001
2024-11-05 01:52:25,436 - INFO - [diffusion][Epoch 9649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:25,439 - INFO - [diffusion][Epoch 9650] Epoch 9651/12000
2024-11-05 01:52:28,969 - INFO - [diffusion][Epoch 9650] diffusion training Loss: 0.07299965247511864
2024-11-05 01:52:28,971 - INFO - [diffusion][Epoch 9650] diffusion learning rate: 0.001
2024-11-05 01:52:28,972 - INFO - [diffusion][Epoch 9650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:28,974 - INFO - [diffusion][Epoch 9651] Epoch 9652/12000
2024-11-05 01:52:32,568 - INFO - [diffusion][Epoch 9651] diffusion training Loss: 0.06828009244054556
2024-11-05 01:52:32,569 - INFO - [diffusion][Epoch 9651] diffusion learning rate: 0.001
2024-11-05 01:52:32,571 - INFO - [diffusion][Epoch 9651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:32,572 - INFO - [diffusion][Epoch 9652] Epoch 9653/12000
2024-11-05 01:52:35,696 - INFO - [diffusion][Epoch 9652] diffusion training Loss: 0.06954141892492771
2024-11-05 01:52:35,698 - INFO - [diffusion][Epoch 9652] diffusion learning rate: 0.001
2024-11-05 01:52:35,699 - INFO - [diffusion][Epoch 9652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:35,701 - INFO - [diffusion][Epoch 9653] Epoch 9654/12000
2024-11-05 01:52:38,970 - INFO - [diffusion][Epoch 9653] diffusion training Loss: 0.06848860532045364
2024-11-05 01:52:38,972 - INFO - [diffusion][Epoch 9653] diffusion learning rate: 0.001
2024-11-05 01:52:38,973 - INFO - [diffusion][Epoch 9653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:38,975 - INFO - [diffusion][Epoch 9654] Epoch 9655/12000
2024-11-05 01:52:42,289 - INFO - [diffusion][Epoch 9654] diffusion training Loss: 0.06832660920917988
2024-11-05 01:52:42,292 - INFO - [diffusion][Epoch 9654] diffusion learning rate: 0.001
2024-11-05 01:52:42,342 - INFO - [diffusion][Epoch 9654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:42,343 - INFO - [diffusion][Epoch 9655] Epoch 9656/12000
2024-11-05 01:52:45,463 - INFO - [diffusion][Epoch 9655] diffusion training Loss: 0.05934480670839548
2024-11-05 01:52:45,465 - INFO - [diffusion][Epoch 9655] diffusion learning rate: 0.001
2024-11-05 01:52:45,467 - INFO - [diffusion][Epoch 9655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:45,468 - INFO - [diffusion][Epoch 9656] Epoch 9657/12000
2024-11-05 01:52:49,025 - INFO - [diffusion][Epoch 9656] diffusion training Loss: 0.07102439925074577
2024-11-05 01:52:49,027 - INFO - [diffusion][Epoch 9656] diffusion learning rate: 0.001
2024-11-05 01:52:49,029 - INFO - [diffusion][Epoch 9656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:49,030 - INFO - [diffusion][Epoch 9657] Epoch 9658/12000
2024-11-05 01:52:52,566 - INFO - [diffusion][Epoch 9657] diffusion training Loss: 0.06328419968485832
2024-11-05 01:52:52,569 - INFO - [diffusion][Epoch 9657] diffusion learning rate: 0.001
2024-11-05 01:52:52,600 - INFO - [diffusion][Epoch 9657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:52,601 - INFO - [diffusion][Epoch 9658] Epoch 9659/12000
2024-11-05 01:52:55,750 - INFO - [diffusion][Epoch 9658] diffusion training Loss: 0.06870714202523232
2024-11-05 01:52:55,752 - INFO - [diffusion][Epoch 9658] diffusion learning rate: 0.001
2024-11-05 01:52:55,754 - INFO - [diffusion][Epoch 9658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:55,755 - INFO - [diffusion][Epoch 9659] Epoch 9660/12000
2024-11-05 01:52:58,874 - INFO - [diffusion][Epoch 9659] diffusion training Loss: 0.07066339254379272
2024-11-05 01:52:58,876 - INFO - [diffusion][Epoch 9659] diffusion learning rate: 0.001
2024-11-05 01:52:58,877 - INFO - [diffusion][Epoch 9659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:58,878 - INFO - [diffusion][Epoch 9660] Epoch 9661/12000
2024-11-05 01:53:02,015 - INFO - [diffusion][Epoch 9660] diffusion training Loss: 0.06740992981940508
2024-11-05 01:53:02,018 - INFO - [diffusion][Epoch 9660] diffusion learning rate: 0.001
2024-11-05 01:53:02,020 - INFO - [diffusion][Epoch 9660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:02,021 - INFO - [diffusion][Epoch 9661] Epoch 9662/12000
2024-11-05 01:53:05,576 - INFO - [diffusion][Epoch 9661] diffusion training Loss: 0.06041772663593292
2024-11-05 01:53:05,578 - INFO - [diffusion][Epoch 9661] diffusion learning rate: 0.001
2024-11-05 01:53:05,580 - INFO - [diffusion][Epoch 9661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:05,581 - INFO - [diffusion][Epoch 9662] Epoch 9663/12000
2024-11-05 01:53:09,041 - INFO - [diffusion][Epoch 9662] diffusion training Loss: 0.062196253798902035
2024-11-05 01:53:09,043 - INFO - [diffusion][Epoch 9662] diffusion learning rate: 0.001
2024-11-05 01:53:09,057 - INFO - [diffusion][Epoch 9662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:09,058 - INFO - [diffusion][Epoch 9663] Epoch 9664/12000
2024-11-05 01:53:12,211 - INFO - [diffusion][Epoch 9663] diffusion training Loss: 0.07322634384036064
2024-11-05 01:53:12,213 - INFO - [diffusion][Epoch 9663] diffusion learning rate: 0.001
2024-11-05 01:53:12,215 - INFO - [diffusion][Epoch 9663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:12,216 - INFO - [diffusion][Epoch 9664] Epoch 9665/12000
2024-11-05 01:53:15,381 - INFO - [diffusion][Epoch 9664] diffusion training Loss: 0.06998778134584427
2024-11-05 01:53:15,383 - INFO - [diffusion][Epoch 9664] diffusion learning rate: 0.001
2024-11-05 01:53:15,385 - INFO - [diffusion][Epoch 9664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:15,386 - INFO - [diffusion][Epoch 9665] Epoch 9666/12000
2024-11-05 01:53:18,557 - INFO - [diffusion][Epoch 9665] diffusion training Loss: 0.06983463279902935
2024-11-05 01:53:18,559 - INFO - [diffusion][Epoch 9665] diffusion learning rate: 0.001
2024-11-05 01:53:18,561 - INFO - [diffusion][Epoch 9665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:18,562 - INFO - [diffusion][Epoch 9666] Epoch 9667/12000
2024-11-05 01:53:22,212 - INFO - [diffusion][Epoch 9666] diffusion training Loss: 0.06617337465286255
2024-11-05 01:53:22,215 - INFO - [diffusion][Epoch 9666] diffusion learning rate: 0.001
2024-11-05 01:53:22,217 - INFO - [diffusion][Epoch 9666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:22,218 - INFO - [diffusion][Epoch 9667] Epoch 9668/12000
2024-11-05 01:53:25,763 - INFO - [diffusion][Epoch 9667] diffusion training Loss: 0.06735046952962875
2024-11-05 01:53:25,765 - INFO - [diffusion][Epoch 9667] diffusion learning rate: 0.001
2024-11-05 01:53:25,767 - INFO - [diffusion][Epoch 9667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:25,768 - INFO - [diffusion][Epoch 9668] Epoch 9669/12000
2024-11-05 01:53:28,966 - INFO - [diffusion][Epoch 9668] diffusion training Loss: 0.06658699363470078
2024-11-05 01:53:28,968 - INFO - [diffusion][Epoch 9668] diffusion learning rate: 0.001
2024-11-05 01:53:28,969 - INFO - [diffusion][Epoch 9668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:28,971 - INFO - [diffusion][Epoch 9669] Epoch 9670/12000
2024-11-05 01:53:32,062 - INFO - [diffusion][Epoch 9669] diffusion training Loss: 0.06456298753619194
2024-11-05 01:53:32,063 - INFO - [diffusion][Epoch 9669] diffusion learning rate: 0.001
2024-11-05 01:53:32,065 - INFO - [diffusion][Epoch 9669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:32,066 - INFO - [diffusion][Epoch 9670] Epoch 9671/12000
2024-11-05 01:53:35,331 - INFO - [diffusion][Epoch 9670] diffusion training Loss: 0.06380973476916552
2024-11-05 01:53:35,332 - INFO - [diffusion][Epoch 9670] diffusion learning rate: 0.001
2024-11-05 01:53:35,334 - INFO - [diffusion][Epoch 9670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:35,335 - INFO - [diffusion][Epoch 9671] Epoch 9672/12000
2024-11-05 01:53:38,631 - INFO - [diffusion][Epoch 9671] diffusion training Loss: 0.07082787342369556
2024-11-05 01:53:38,633 - INFO - [diffusion][Epoch 9671] diffusion learning rate: 0.001
2024-11-05 01:53:38,635 - INFO - [diffusion][Epoch 9671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:38,636 - INFO - [diffusion][Epoch 9672] Epoch 9673/12000
2024-11-05 01:53:42,114 - INFO - [diffusion][Epoch 9672] diffusion training Loss: 0.06492396630346775
2024-11-05 01:53:42,116 - INFO - [diffusion][Epoch 9672] diffusion learning rate: 0.001
2024-11-05 01:53:42,118 - INFO - [diffusion][Epoch 9672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:42,119 - INFO - [diffusion][Epoch 9673] Epoch 9674/12000
2024-11-05 01:53:45,216 - INFO - [diffusion][Epoch 9673] diffusion training Loss: 0.06345189921557903
2024-11-05 01:53:45,218 - INFO - [diffusion][Epoch 9673] diffusion learning rate: 0.001
2024-11-05 01:53:45,220 - INFO - [diffusion][Epoch 9673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:45,221 - INFO - [diffusion][Epoch 9674] Epoch 9675/12000
2024-11-05 01:53:48,397 - INFO - [diffusion][Epoch 9674] diffusion training Loss: 0.06619235873222351
2024-11-05 01:53:48,399 - INFO - [diffusion][Epoch 9674] diffusion learning rate: 0.001
2024-11-05 01:53:48,401 - INFO - [diffusion][Epoch 9674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:48,402 - INFO - [diffusion][Epoch 9675] Epoch 9676/12000
2024-11-05 01:53:52,021 - INFO - [diffusion][Epoch 9675] diffusion training Loss: 0.06855841167271137
2024-11-05 01:53:52,023 - INFO - [diffusion][Epoch 9675] diffusion learning rate: 0.001
2024-11-05 01:53:52,024 - INFO - [diffusion][Epoch 9675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:52,026 - INFO - [diffusion][Epoch 9676] Epoch 9677/12000
2024-11-05 01:53:55,611 - INFO - [diffusion][Epoch 9676] diffusion training Loss: 0.0626677293330431
2024-11-05 01:53:55,615 - INFO - [diffusion][Epoch 9676] diffusion learning rate: 0.001
2024-11-05 01:53:55,617 - INFO - [diffusion][Epoch 9676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:55,618 - INFO - [diffusion][Epoch 9677] Epoch 9678/12000
2024-11-05 01:53:58,719 - INFO - [diffusion][Epoch 9677] diffusion training Loss: 0.0675716195255518
2024-11-05 01:53:58,721 - INFO - [diffusion][Epoch 9677] diffusion learning rate: 0.001
2024-11-05 01:53:58,723 - INFO - [diffusion][Epoch 9677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:58,725 - INFO - [diffusion][Epoch 9678] Epoch 9679/12000
2024-11-05 01:54:01,800 - INFO - [diffusion][Epoch 9678] diffusion training Loss: 0.06680493988096714
2024-11-05 01:54:01,803 - INFO - [diffusion][Epoch 9678] diffusion learning rate: 0.001
2024-11-05 01:54:01,804 - INFO - [diffusion][Epoch 9678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:01,806 - INFO - [diffusion][Epoch 9679] Epoch 9680/12000
2024-11-05 01:54:04,917 - INFO - [diffusion][Epoch 9679] diffusion training Loss: 0.06795309670269489
2024-11-05 01:54:04,919 - INFO - [diffusion][Epoch 9679] diffusion learning rate: 0.001
2024-11-05 01:54:04,975 - INFO - [diffusion][Epoch 9679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:04,976 - INFO - [diffusion][Epoch 9680] Epoch 9681/12000
2024-11-05 01:54:08,473 - INFO - [diffusion][Epoch 9680] diffusion training Loss: 0.06986194849014282
2024-11-05 01:54:08,475 - INFO - [diffusion][Epoch 9680] diffusion learning rate: 0.001
2024-11-05 01:54:08,477 - INFO - [diffusion][Epoch 9680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:08,478 - INFO - [diffusion][Epoch 9681] Epoch 9682/12000
2024-11-05 01:54:12,030 - INFO - [diffusion][Epoch 9681] diffusion training Loss: 0.06651554070413113
2024-11-05 01:54:12,032 - INFO - [diffusion][Epoch 9681] diffusion learning rate: 0.001
2024-11-05 01:54:12,034 - INFO - [diffusion][Epoch 9681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:12,035 - INFO - [diffusion][Epoch 9682] Epoch 9683/12000
2024-11-05 01:54:15,193 - INFO - [diffusion][Epoch 9682] diffusion training Loss: 0.06421651598066092
2024-11-05 01:54:15,195 - INFO - [diffusion][Epoch 9682] diffusion learning rate: 0.001
2024-11-05 01:54:15,273 - INFO - [diffusion][Epoch 9682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:15,274 - INFO - [diffusion][Epoch 9683] Epoch 9684/12000
2024-11-05 01:54:18,382 - INFO - [diffusion][Epoch 9683] diffusion training Loss: 0.06971130054444075
2024-11-05 01:54:18,384 - INFO - [diffusion][Epoch 9683] diffusion learning rate: 0.001
2024-11-05 01:54:18,386 - INFO - [diffusion][Epoch 9683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:18,388 - INFO - [diffusion][Epoch 9684] Epoch 9685/12000
2024-11-05 01:54:21,557 - INFO - [diffusion][Epoch 9684] diffusion training Loss: 0.07794753462076187
2024-11-05 01:54:21,559 - INFO - [diffusion][Epoch 9684] diffusion learning rate: 0.001
2024-11-05 01:54:21,561 - INFO - [diffusion][Epoch 9684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:21,562 - INFO - [diffusion][Epoch 9685] Epoch 9686/12000
2024-11-05 01:54:25,074 - INFO - [diffusion][Epoch 9685] diffusion training Loss: 0.07113023288547993
2024-11-05 01:54:25,076 - INFO - [diffusion][Epoch 9685] diffusion learning rate: 0.001
2024-11-05 01:54:25,078 - INFO - [diffusion][Epoch 9685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:25,079 - INFO - [diffusion][Epoch 9686] Epoch 9687/12000
2024-11-05 01:54:28,633 - INFO - [diffusion][Epoch 9686] diffusion training Loss: 0.05959474388509989
2024-11-05 01:54:28,635 - INFO - [diffusion][Epoch 9686] diffusion learning rate: 0.001
2024-11-05 01:54:28,637 - INFO - [diffusion][Epoch 9686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:28,638 - INFO - [diffusion][Epoch 9687] Epoch 9688/12000
2024-11-05 01:54:31,716 - INFO - [diffusion][Epoch 9687] diffusion training Loss: 0.06749912165105343
2024-11-05 01:54:31,718 - INFO - [diffusion][Epoch 9687] diffusion learning rate: 0.001
2024-11-05 01:54:31,720 - INFO - [diffusion][Epoch 9687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:31,721 - INFO - [diffusion][Epoch 9688] Epoch 9689/12000
2024-11-05 01:54:34,810 - INFO - [diffusion][Epoch 9688] diffusion training Loss: 0.0667075589299202
2024-11-05 01:54:34,812 - INFO - [diffusion][Epoch 9688] diffusion learning rate: 0.001
2024-11-05 01:54:34,814 - INFO - [diffusion][Epoch 9688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:34,815 - INFO - [diffusion][Epoch 9689] Epoch 9690/12000
2024-11-05 01:54:37,915 - INFO - [diffusion][Epoch 9689] diffusion training Loss: 0.06340220104902983
2024-11-05 01:54:37,917 - INFO - [diffusion][Epoch 9689] diffusion learning rate: 0.001
2024-11-05 01:54:37,918 - INFO - [diffusion][Epoch 9689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:37,920 - INFO - [diffusion][Epoch 9690] Epoch 9691/12000
2024-11-05 01:54:41,471 - INFO - [diffusion][Epoch 9690] diffusion training Loss: 0.06949100736528635
2024-11-05 01:54:41,473 - INFO - [diffusion][Epoch 9690] diffusion learning rate: 0.001
2024-11-05 01:54:41,475 - INFO - [diffusion][Epoch 9690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:41,476 - INFO - [diffusion][Epoch 9691] Epoch 9692/12000
2024-11-05 01:54:44,991 - INFO - [diffusion][Epoch 9691] diffusion training Loss: 0.05843508616089821
2024-11-05 01:54:44,994 - INFO - [diffusion][Epoch 9691] diffusion learning rate: 0.001
2024-11-05 01:54:44,996 - INFO - [diffusion][Epoch 9691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:44,997 - INFO - [diffusion][Epoch 9692] Epoch 9693/12000
2024-11-05 01:54:48,961 - INFO - [diffusion][Epoch 9692] diffusion training Loss: 0.07018553838133812
2024-11-05 01:54:48,963 - INFO - [diffusion][Epoch 9692] diffusion learning rate: 0.001
2024-11-05 01:54:48,965 - INFO - [diffusion][Epoch 9692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:48,966 - INFO - [diffusion][Epoch 9693] Epoch 9694/12000
2024-11-05 01:54:52,105 - INFO - [diffusion][Epoch 9693] diffusion training Loss: 0.06231206189841032
2024-11-05 01:54:52,107 - INFO - [diffusion][Epoch 9693] diffusion learning rate: 0.001
2024-11-05 01:54:52,109 - INFO - [diffusion][Epoch 9693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:52,110 - INFO - [diffusion][Epoch 9694] Epoch 9695/12000
2024-11-05 01:54:55,194 - INFO - [diffusion][Epoch 9694] diffusion training Loss: 0.062363085336983204
2024-11-05 01:54:55,196 - INFO - [diffusion][Epoch 9694] diffusion learning rate: 0.001
2024-11-05 01:54:55,198 - INFO - [diffusion][Epoch 9694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:55,199 - INFO - [diffusion][Epoch 9695] Epoch 9696/12000
2024-11-05 01:54:58,262 - INFO - [diffusion][Epoch 9695] diffusion training Loss: 0.06610325910151005
2024-11-05 01:54:58,265 - INFO - [diffusion][Epoch 9695] diffusion learning rate: 0.001
2024-11-05 01:54:58,267 - INFO - [diffusion][Epoch 9695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:58,268 - INFO - [diffusion][Epoch 9696] Epoch 9697/12000
2024-11-05 01:55:01,763 - INFO - [diffusion][Epoch 9696] diffusion training Loss: 0.06759501621127129
2024-11-05 01:55:01,765 - INFO - [diffusion][Epoch 9696] diffusion learning rate: 0.001
2024-11-05 01:55:01,767 - INFO - [diffusion][Epoch 9696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:01,768 - INFO - [diffusion][Epoch 9697] Epoch 9698/12000
2024-11-05 01:55:05,112 - INFO - [diffusion][Epoch 9697] diffusion training Loss: 0.06614157650619745
2024-11-05 01:55:05,114 - INFO - [diffusion][Epoch 9697] diffusion learning rate: 0.001
2024-11-05 01:55:05,115 - INFO - [diffusion][Epoch 9697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:05,117 - INFO - [diffusion][Epoch 9698] Epoch 9699/12000
2024-11-05 01:55:08,214 - INFO - [diffusion][Epoch 9698] diffusion training Loss: 0.06823946349322796
2024-11-05 01:55:08,216 - INFO - [diffusion][Epoch 9698] diffusion learning rate: 0.001
2024-11-05 01:55:08,217 - INFO - [diffusion][Epoch 9698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:08,219 - INFO - [diffusion][Epoch 9699] Epoch 9700/12000
2024-11-05 01:55:11,300 - INFO - [diffusion][Epoch 9699] diffusion training Loss: 0.06742754019796848
2024-11-05 01:55:11,302 - INFO - [diffusion][Epoch 9699] diffusion learning rate: 0.001
2024-11-05 01:55:11,304 - INFO - [diffusion][Epoch 9699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:11,305 - INFO - [diffusion][Epoch 9700] Epoch 9701/12000
2024-11-05 01:55:14,821 - INFO - [diffusion][Epoch 9700] diffusion training Loss: 0.06849828734993935
2024-11-05 01:55:14,822 - INFO - [diffusion][Epoch 9700] diffusion learning rate: 0.001
2024-11-05 01:55:14,824 - INFO - [diffusion][Epoch 9700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:14,825 - INFO - [diffusion][Epoch 9701] Epoch 9702/12000
2024-11-05 01:55:18,352 - INFO - [diffusion][Epoch 9701] diffusion training Loss: 0.06379783432930708
2024-11-05 01:55:18,354 - INFO - [diffusion][Epoch 9701] diffusion learning rate: 0.001
2024-11-05 01:55:18,356 - INFO - [diffusion][Epoch 9701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:18,357 - INFO - [diffusion][Epoch 9702] Epoch 9703/12000
2024-11-05 01:55:21,441 - INFO - [diffusion][Epoch 9702] diffusion training Loss: 0.0661602970212698
2024-11-05 01:55:21,442 - INFO - [diffusion][Epoch 9702] diffusion learning rate: 0.001
2024-11-05 01:55:21,444 - INFO - [diffusion][Epoch 9702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:21,445 - INFO - [diffusion][Epoch 9703] Epoch 9704/12000
2024-11-05 01:55:24,491 - INFO - [diffusion][Epoch 9703] diffusion training Loss: 0.06573300436139107
2024-11-05 01:55:24,493 - INFO - [diffusion][Epoch 9703] diffusion learning rate: 0.001
2024-11-05 01:55:24,494 - INFO - [diffusion][Epoch 9703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:24,495 - INFO - [diffusion][Epoch 9704] Epoch 9705/12000
2024-11-05 01:55:27,778 - INFO - [diffusion][Epoch 9704] diffusion training Loss: 0.07121332548558712
2024-11-05 01:55:27,780 - INFO - [diffusion][Epoch 9704] diffusion learning rate: 0.001
2024-11-05 01:55:27,782 - INFO - [diffusion][Epoch 9704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:27,783 - INFO - [diffusion][Epoch 9705] Epoch 9706/12000
2024-11-05 01:55:31,488 - INFO - [diffusion][Epoch 9705] diffusion training Loss: 0.06350632663816214
2024-11-05 01:55:31,490 - INFO - [diffusion][Epoch 9705] diffusion learning rate: 0.001
2024-11-05 01:55:31,492 - INFO - [diffusion][Epoch 9705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:31,493 - INFO - [diffusion][Epoch 9706] Epoch 9707/12000
2024-11-05 01:55:34,911 - INFO - [diffusion][Epoch 9706] diffusion training Loss: 0.06576637923717499
2024-11-05 01:55:34,912 - INFO - [diffusion][Epoch 9706] diffusion learning rate: 0.001
2024-11-05 01:55:34,914 - INFO - [diffusion][Epoch 9706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:34,915 - INFO - [diffusion][Epoch 9707] Epoch 9708/12000
2024-11-05 01:55:37,965 - INFO - [diffusion][Epoch 9707] diffusion training Loss: 0.06604436319321394
2024-11-05 01:55:37,967 - INFO - [diffusion][Epoch 9707] diffusion learning rate: 0.001
2024-11-05 01:55:37,968 - INFO - [diffusion][Epoch 9707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:37,969 - INFO - [diffusion][Epoch 9708] Epoch 9709/12000
2024-11-05 01:55:41,129 - INFO - [diffusion][Epoch 9708] diffusion training Loss: 0.06999679934233427
2024-11-05 01:55:41,131 - INFO - [diffusion][Epoch 9708] diffusion learning rate: 0.001
2024-11-05 01:55:41,132 - INFO - [diffusion][Epoch 9708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:41,134 - INFO - [diffusion][Epoch 9709] Epoch 9710/12000
2024-11-05 01:55:44,553 - INFO - [diffusion][Epoch 9709] diffusion training Loss: 0.06763864308595657
2024-11-05 01:55:44,555 - INFO - [diffusion][Epoch 9709] diffusion learning rate: 0.001
2024-11-05 01:55:44,557 - INFO - [diffusion][Epoch 9709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:44,558 - INFO - [diffusion][Epoch 9710] Epoch 9711/12000
2024-11-05 01:55:48,336 - INFO - [diffusion][Epoch 9710] diffusion training Loss: 0.06902858801186085
2024-11-05 01:55:48,338 - INFO - [diffusion][Epoch 9710] diffusion learning rate: 0.001
2024-11-05 01:55:48,340 - INFO - [diffusion][Epoch 9710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:48,342 - INFO - [diffusion][Epoch 9711] Epoch 9712/12000
2024-11-05 01:55:51,825 - INFO - [diffusion][Epoch 9711] diffusion training Loss: 0.06456063874065876
2024-11-05 01:55:51,827 - INFO - [diffusion][Epoch 9711] diffusion learning rate: 0.001
2024-11-05 01:55:51,829 - INFO - [diffusion][Epoch 9711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:51,830 - INFO - [diffusion][Epoch 9712] Epoch 9713/12000
2024-11-05 01:55:54,942 - INFO - [diffusion][Epoch 9712] diffusion training Loss: 0.06635335553437471
2024-11-05 01:55:54,944 - INFO - [diffusion][Epoch 9712] diffusion learning rate: 0.001
2024-11-05 01:55:54,945 - INFO - [diffusion][Epoch 9712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:54,947 - INFO - [diffusion][Epoch 9713] Epoch 9714/12000
2024-11-05 01:55:58,116 - INFO - [diffusion][Epoch 9713] diffusion training Loss: 0.06470986176282167
2024-11-05 01:55:58,118 - INFO - [diffusion][Epoch 9713] diffusion learning rate: 0.001
2024-11-05 01:55:58,120 - INFO - [diffusion][Epoch 9713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:58,121 - INFO - [diffusion][Epoch 9714] Epoch 9715/12000
2024-11-05 01:56:01,397 - INFO - [diffusion][Epoch 9714] diffusion training Loss: 0.068763243034482
2024-11-05 01:56:01,399 - INFO - [diffusion][Epoch 9714] diffusion learning rate: 0.001
2024-11-05 01:56:01,401 - INFO - [diffusion][Epoch 9714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:01,402 - INFO - [diffusion][Epoch 9715] Epoch 9716/12000
2024-11-05 01:56:05,127 - INFO - [diffusion][Epoch 9715] diffusion training Loss: 0.06447506323456764
2024-11-05 01:56:05,129 - INFO - [diffusion][Epoch 9715] diffusion learning rate: 0.001
2024-11-05 01:56:05,181 - INFO - [diffusion][Epoch 9715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:05,183 - INFO - [diffusion][Epoch 9716] Epoch 9717/12000
2024-11-05 01:56:08,744 - INFO - [diffusion][Epoch 9716] diffusion training Loss: 0.06776031572371721
2024-11-05 01:56:08,746 - INFO - [diffusion][Epoch 9716] diffusion learning rate: 0.001
2024-11-05 01:56:08,748 - INFO - [diffusion][Epoch 9716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:08,749 - INFO - [diffusion][Epoch 9717] Epoch 9718/12000
2024-11-05 01:56:11,875 - INFO - [diffusion][Epoch 9717] diffusion training Loss: 0.07028798013925552
2024-11-05 01:56:11,877 - INFO - [diffusion][Epoch 9717] diffusion learning rate: 0.001
2024-11-05 01:56:11,879 - INFO - [diffusion][Epoch 9717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:11,880 - INFO - [diffusion][Epoch 9718] Epoch 9719/12000
2024-11-05 01:56:15,006 - INFO - [diffusion][Epoch 9718] diffusion training Loss: 0.06245629210025072
2024-11-05 01:56:15,008 - INFO - [diffusion][Epoch 9718] diffusion learning rate: 0.001
2024-11-05 01:56:15,010 - INFO - [diffusion][Epoch 9718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:15,011 - INFO - [diffusion][Epoch 9719] Epoch 9720/12000
2024-11-05 01:56:18,200 - INFO - [diffusion][Epoch 9719] diffusion training Loss: 0.06710449047386646
2024-11-05 01:56:18,202 - INFO - [diffusion][Epoch 9719] diffusion learning rate: 0.001
2024-11-05 01:56:18,204 - INFO - [diffusion][Epoch 9719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:18,205 - INFO - [diffusion][Epoch 9720] Epoch 9721/12000
2024-11-05 01:56:21,791 - INFO - [diffusion][Epoch 9720] diffusion training Loss: 0.06439869292080402
2024-11-05 01:56:21,793 - INFO - [diffusion][Epoch 9720] diffusion learning rate: 0.001
2024-11-05 01:56:21,795 - INFO - [diffusion][Epoch 9720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:21,796 - INFO - [diffusion][Epoch 9721] Epoch 9722/12000
2024-11-05 01:56:25,298 - INFO - [diffusion][Epoch 9721] diffusion training Loss: 0.062289618887007236
2024-11-05 01:56:25,300 - INFO - [diffusion][Epoch 9721] diffusion learning rate: 0.001
2024-11-05 01:56:25,302 - INFO - [diffusion][Epoch 9721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:25,303 - INFO - [diffusion][Epoch 9722] Epoch 9723/12000
2024-11-05 01:56:28,171 - INFO - [diffusion][Epoch 9722] diffusion training Loss: 0.06820271722972393
2024-11-05 01:56:28,173 - INFO - [diffusion][Epoch 9722] diffusion learning rate: 0.001
2024-11-05 01:56:28,175 - INFO - [diffusion][Epoch 9722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:28,176 - INFO - [diffusion][Epoch 9723] Epoch 9724/12000
2024-11-05 01:56:31,258 - INFO - [diffusion][Epoch 9723] diffusion training Loss: 0.07178404554724693
2024-11-05 01:56:31,260 - INFO - [diffusion][Epoch 9723] diffusion learning rate: 0.001
2024-11-05 01:56:31,261 - INFO - [diffusion][Epoch 9723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:31,263 - INFO - [diffusion][Epoch 9724] Epoch 9725/12000
2024-11-05 01:56:34,812 - INFO - [diffusion][Epoch 9724] diffusion training Loss: 0.06098244246095419
2024-11-05 01:56:34,814 - INFO - [diffusion][Epoch 9724] diffusion learning rate: 0.001
2024-11-05 01:56:34,815 - INFO - [diffusion][Epoch 9724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:34,817 - INFO - [diffusion][Epoch 9725] Epoch 9726/12000
2024-11-05 01:56:38,514 - INFO - [diffusion][Epoch 9725] diffusion training Loss: 0.0648194570094347
2024-11-05 01:56:38,516 - INFO - [diffusion][Epoch 9725] diffusion learning rate: 0.001
2024-11-05 01:56:38,517 - INFO - [diffusion][Epoch 9725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:38,518 - INFO - [diffusion][Epoch 9726] Epoch 9727/12000
2024-11-05 01:56:41,745 - INFO - [diffusion][Epoch 9726] diffusion training Loss: 0.06815448589622974
2024-11-05 01:56:41,747 - INFO - [diffusion][Epoch 9726] diffusion learning rate: 0.001
2024-11-05 01:56:41,749 - INFO - [diffusion][Epoch 9726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:41,750 - INFO - [diffusion][Epoch 9727] Epoch 9728/12000
2024-11-05 01:56:45,043 - INFO - [diffusion][Epoch 9727] diffusion training Loss: 0.06810621730983257
2024-11-05 01:56:45,045 - INFO - [diffusion][Epoch 9727] diffusion learning rate: 0.001
2024-11-05 01:56:45,047 - INFO - [diffusion][Epoch 9727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:45,048 - INFO - [diffusion][Epoch 9728] Epoch 9729/12000
2024-11-05 01:56:48,241 - INFO - [diffusion][Epoch 9728] diffusion training Loss: 0.06742505449801683
2024-11-05 01:56:48,242 - INFO - [diffusion][Epoch 9728] diffusion learning rate: 0.001
2024-11-05 01:56:48,244 - INFO - [diffusion][Epoch 9728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:48,245 - INFO - [diffusion][Epoch 9729] Epoch 9730/12000
2024-11-05 01:56:51,596 - INFO - [diffusion][Epoch 9729] diffusion training Loss: 0.06483417004346848
2024-11-05 01:56:51,599 - INFO - [diffusion][Epoch 9729] diffusion learning rate: 0.001
2024-11-05 01:56:51,600 - INFO - [diffusion][Epoch 9729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:51,602 - INFO - [diffusion][Epoch 9730] Epoch 9731/12000
2024-11-05 01:56:55,161 - INFO - [diffusion][Epoch 9730] diffusion training Loss: 0.06450708769261837
2024-11-05 01:56:55,163 - INFO - [diffusion][Epoch 9730] diffusion learning rate: 0.001
2024-11-05 01:56:55,164 - INFO - [diffusion][Epoch 9730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:55,166 - INFO - [diffusion][Epoch 9731] Epoch 9732/12000
2024-11-05 01:56:58,701 - INFO - [diffusion][Epoch 9731] diffusion training Loss: 0.06884821876883507
2024-11-05 01:56:58,703 - INFO - [diffusion][Epoch 9731] diffusion learning rate: 0.001
2024-11-05 01:56:58,705 - INFO - [diffusion][Epoch 9731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:58,706 - INFO - [diffusion][Epoch 9732] Epoch 9733/12000
2024-11-05 01:57:02,217 - INFO - [diffusion][Epoch 9732] diffusion training Loss: 0.06818580906838179
2024-11-05 01:57:02,219 - INFO - [diffusion][Epoch 9732] diffusion learning rate: 0.001
2024-11-05 01:57:02,221 - INFO - [diffusion][Epoch 9732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:02,222 - INFO - [diffusion][Epoch 9733] Epoch 9734/12000
2024-11-05 01:57:05,028 - INFO - [diffusion][Epoch 9733] diffusion training Loss: 0.0699615366756916
2024-11-05 01:57:05,030 - INFO - [diffusion][Epoch 9733] diffusion learning rate: 0.001
2024-11-05 01:57:05,032 - INFO - [diffusion][Epoch 9733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:05,033 - INFO - [diffusion][Epoch 9734] Epoch 9735/12000
2024-11-05 01:57:08,274 - INFO - [diffusion][Epoch 9734] diffusion training Loss: 0.06344503723084927
2024-11-05 01:57:08,276 - INFO - [diffusion][Epoch 9734] diffusion learning rate: 0.001
2024-11-05 01:57:08,278 - INFO - [diffusion][Epoch 9734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:08,279 - INFO - [diffusion][Epoch 9735] Epoch 9736/12000
2024-11-05 01:57:11,486 - INFO - [diffusion][Epoch 9735] diffusion training Loss: 0.062491649761796
2024-11-05 01:57:11,488 - INFO - [diffusion][Epoch 9735] diffusion learning rate: 0.001
2024-11-05 01:57:11,490 - INFO - [diffusion][Epoch 9735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:11,491 - INFO - [diffusion][Epoch 9736] Epoch 9737/12000
2024-11-05 01:57:15,142 - INFO - [diffusion][Epoch 9736] diffusion training Loss: 0.06738031283020973
2024-11-05 01:57:15,143 - INFO - [diffusion][Epoch 9736] diffusion learning rate: 0.001
2024-11-05 01:57:15,145 - INFO - [diffusion][Epoch 9736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:15,146 - INFO - [diffusion][Epoch 9737] Epoch 9738/12000
2024-11-05 01:57:18,674 - INFO - [diffusion][Epoch 9737] diffusion training Loss: 0.0703478455543518
2024-11-05 01:57:18,675 - INFO - [diffusion][Epoch 9737] diffusion learning rate: 0.001
2024-11-05 01:57:18,677 - INFO - [diffusion][Epoch 9737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:18,678 - INFO - [diffusion][Epoch 9738] Epoch 9739/12000
2024-11-05 01:57:21,755 - INFO - [diffusion][Epoch 9738] diffusion training Loss: 0.06428397353738546
2024-11-05 01:57:21,757 - INFO - [diffusion][Epoch 9738] diffusion learning rate: 0.001
2024-11-05 01:57:21,759 - INFO - [diffusion][Epoch 9738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:21,760 - INFO - [diffusion][Epoch 9739] Epoch 9740/12000
2024-11-05 01:57:24,840 - INFO - [diffusion][Epoch 9739] diffusion training Loss: 0.0664385799318552
2024-11-05 01:57:24,843 - INFO - [diffusion][Epoch 9739] diffusion learning rate: 0.001
2024-11-05 01:57:24,844 - INFO - [diffusion][Epoch 9739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:24,846 - INFO - [diffusion][Epoch 9740] Epoch 9741/12000
2024-11-05 01:57:28,206 - INFO - [diffusion][Epoch 9740] diffusion training Loss: 0.07192662917077541
2024-11-05 01:57:28,208 - INFO - [diffusion][Epoch 9740] diffusion learning rate: 0.001
2024-11-05 01:57:28,210 - INFO - [diffusion][Epoch 9740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:28,212 - INFO - [diffusion][Epoch 9741] Epoch 9742/12000
2024-11-05 01:57:31,816 - INFO - [diffusion][Epoch 9741] diffusion training Loss: 0.0668602641671896
2024-11-05 01:57:31,817 - INFO - [diffusion][Epoch 9741] diffusion learning rate: 0.001
2024-11-05 01:57:31,819 - INFO - [diffusion][Epoch 9741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:31,820 - INFO - [diffusion][Epoch 9742] Epoch 9743/12000
2024-11-05 01:57:35,037 - INFO - [diffusion][Epoch 9742] diffusion training Loss: 0.06798458285629749
2024-11-05 01:57:35,039 - INFO - [diffusion][Epoch 9742] diffusion learning rate: 0.001
2024-11-05 01:57:35,041 - INFO - [diffusion][Epoch 9742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:35,042 - INFO - [diffusion][Epoch 9743] Epoch 9744/12000
2024-11-05 01:57:38,189 - INFO - [diffusion][Epoch 9743] diffusion training Loss: 0.0681452788412571
2024-11-05 01:57:38,191 - INFO - [diffusion][Epoch 9743] diffusion learning rate: 0.001
2024-11-05 01:57:38,192 - INFO - [diffusion][Epoch 9743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:38,193 - INFO - [diffusion][Epoch 9744] Epoch 9745/12000
2024-11-05 01:57:41,234 - INFO - [diffusion][Epoch 9744] diffusion training Loss: 0.0684151379391551
2024-11-05 01:57:41,236 - INFO - [diffusion][Epoch 9744] diffusion learning rate: 0.001
2024-11-05 01:57:41,238 - INFO - [diffusion][Epoch 9744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:41,239 - INFO - [diffusion][Epoch 9745] Epoch 9746/12000
2024-11-05 01:57:44,875 - INFO - [diffusion][Epoch 9745] diffusion training Loss: 0.06177642196416855
2024-11-05 01:57:44,877 - INFO - [diffusion][Epoch 9745] diffusion learning rate: 0.001
2024-11-05 01:57:44,878 - INFO - [diffusion][Epoch 9745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:44,880 - INFO - [diffusion][Epoch 9746] Epoch 9747/12000
2024-11-05 01:57:48,462 - INFO - [diffusion][Epoch 9746] diffusion training Loss: 0.06503081507980824
2024-11-05 01:57:48,464 - INFO - [diffusion][Epoch 9746] diffusion learning rate: 0.001
2024-11-05 01:57:48,503 - INFO - [diffusion][Epoch 9746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:48,504 - INFO - [diffusion][Epoch 9747] Epoch 9748/12000
2024-11-05 01:57:51,613 - INFO - [diffusion][Epoch 9747] diffusion training Loss: 0.06278351228684187
2024-11-05 01:57:51,615 - INFO - [diffusion][Epoch 9747] diffusion learning rate: 0.001
2024-11-05 01:57:51,617 - INFO - [diffusion][Epoch 9747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:51,618 - INFO - [diffusion][Epoch 9748] Epoch 9749/12000
2024-11-05 01:57:54,775 - INFO - [diffusion][Epoch 9748] diffusion training Loss: 0.06389513984322548
2024-11-05 01:57:54,777 - INFO - [diffusion][Epoch 9748] diffusion learning rate: 0.001
2024-11-05 01:57:54,779 - INFO - [diffusion][Epoch 9748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:54,780 - INFO - [diffusion][Epoch 9749] Epoch 9750/12000
2024-11-05 01:57:57,930 - INFO - [diffusion][Epoch 9749] diffusion training Loss: 0.0681012338027358
2024-11-05 01:57:57,932 - INFO - [diffusion][Epoch 9749] diffusion learning rate: 0.001
2024-11-05 01:57:57,934 - INFO - [diffusion][Epoch 9749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:57,935 - INFO - [diffusion][Epoch 9750] Epoch 9751/12000
2024-11-05 01:58:01,420 - INFO - [diffusion][Epoch 9750] diffusion training Loss: 0.06747374124825001
2024-11-05 01:58:01,422 - INFO - [diffusion][Epoch 9750] diffusion learning rate: 0.001
2024-11-05 01:58:01,424 - INFO - [diffusion][Epoch 9750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:01,425 - INFO - [diffusion][Epoch 9751] Epoch 9752/12000
2024-11-05 01:58:05,348 - INFO - [diffusion][Epoch 9751] diffusion training Loss: 0.06937487050890923
2024-11-05 01:58:05,351 - INFO - [diffusion][Epoch 9751] diffusion learning rate: 0.001
2024-11-05 01:58:05,353 - INFO - [diffusion][Epoch 9751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:05,354 - INFO - [diffusion][Epoch 9752] Epoch 9753/12000
2024-11-05 01:58:08,873 - INFO - [diffusion][Epoch 9752] diffusion training Loss: 0.06616667285561562
2024-11-05 01:58:08,875 - INFO - [diffusion][Epoch 9752] diffusion learning rate: 0.001
2024-11-05 01:58:08,876 - INFO - [diffusion][Epoch 9752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:08,881 - INFO - [diffusion][Epoch 9753] Epoch 9754/12000
2024-11-05 01:58:12,004 - INFO - [diffusion][Epoch 9753] diffusion training Loss: 0.06689888797700405
2024-11-05 01:58:12,006 - INFO - [diffusion][Epoch 9753] diffusion learning rate: 0.001
2024-11-05 01:58:12,007 - INFO - [diffusion][Epoch 9753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:12,009 - INFO - [diffusion][Epoch 9754] Epoch 9755/12000
2024-11-05 01:58:15,191 - INFO - [diffusion][Epoch 9754] diffusion training Loss: 0.06281026732176542
2024-11-05 01:58:15,193 - INFO - [diffusion][Epoch 9754] diffusion learning rate: 0.001
2024-11-05 01:58:15,195 - INFO - [diffusion][Epoch 9754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:15,196 - INFO - [diffusion][Epoch 9755] Epoch 9756/12000
2024-11-05 01:58:18,288 - INFO - [diffusion][Epoch 9755] diffusion training Loss: 0.061938987113535404
2024-11-05 01:58:18,291 - INFO - [diffusion][Epoch 9755] diffusion learning rate: 0.001
2024-11-05 01:58:18,293 - INFO - [diffusion][Epoch 9755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:18,294 - INFO - [diffusion][Epoch 9756] Epoch 9757/12000
2024-11-05 01:58:21,825 - INFO - [diffusion][Epoch 9756] diffusion training Loss: 0.06694065406918526
2024-11-05 01:58:21,826 - INFO - [diffusion][Epoch 9756] diffusion learning rate: 0.001
2024-11-05 01:58:21,828 - INFO - [diffusion][Epoch 9756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:21,830 - INFO - [diffusion][Epoch 9757] Epoch 9758/12000
2024-11-05 01:58:25,359 - INFO - [diffusion][Epoch 9757] diffusion training Loss: 0.06551486067473888
2024-11-05 01:58:25,361 - INFO - [diffusion][Epoch 9757] diffusion learning rate: 0.001
2024-11-05 01:58:25,363 - INFO - [diffusion][Epoch 9757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:25,365 - INFO - [diffusion][Epoch 9758] Epoch 9759/12000
2024-11-05 01:58:28,485 - INFO - [diffusion][Epoch 9758] diffusion training Loss: 0.06407219916582108
2024-11-05 01:58:28,487 - INFO - [diffusion][Epoch 9758] diffusion learning rate: 0.001
2024-11-05 01:58:28,489 - INFO - [diffusion][Epoch 9758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:28,490 - INFO - [diffusion][Epoch 9759] Epoch 9760/12000
2024-11-05 01:58:31,673 - INFO - [diffusion][Epoch 9759] diffusion training Loss: 0.07006700057536364
2024-11-05 01:58:31,675 - INFO - [diffusion][Epoch 9759] diffusion learning rate: 0.001
2024-11-05 01:58:31,677 - INFO - [diffusion][Epoch 9759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:31,678 - INFO - [diffusion][Epoch 9760] Epoch 9761/12000
2024-11-05 01:58:34,870 - INFO - [diffusion][Epoch 9760] diffusion training Loss: 0.0683644562959671
2024-11-05 01:58:34,872 - INFO - [diffusion][Epoch 9760] diffusion learning rate: 0.001
2024-11-05 01:58:34,873 - INFO - [diffusion][Epoch 9760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:34,875 - INFO - [diffusion][Epoch 9761] Epoch 9762/12000
2024-11-05 01:58:38,368 - INFO - [diffusion][Epoch 9761] diffusion training Loss: 0.07095250859856606
2024-11-05 01:58:38,370 - INFO - [diffusion][Epoch 9761] diffusion learning rate: 0.001
2024-11-05 01:58:38,371 - INFO - [diffusion][Epoch 9761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:38,372 - INFO - [diffusion][Epoch 9762] Epoch 9763/12000
2024-11-05 01:58:42,032 - INFO - [diffusion][Epoch 9762] diffusion training Loss: 0.0733517836779356
2024-11-05 01:58:42,034 - INFO - [diffusion][Epoch 9762] diffusion learning rate: 0.001
2024-11-05 01:58:42,035 - INFO - [diffusion][Epoch 9762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:42,037 - INFO - [diffusion][Epoch 9763] Epoch 9764/12000
2024-11-05 01:58:45,155 - INFO - [diffusion][Epoch 9763] diffusion training Loss: 0.06624462828040123
2024-11-05 01:58:45,157 - INFO - [diffusion][Epoch 9763] diffusion learning rate: 0.001
2024-11-05 01:58:45,158 - INFO - [diffusion][Epoch 9763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:45,160 - INFO - [diffusion][Epoch 9764] Epoch 9765/12000
2024-11-05 01:58:48,260 - INFO - [diffusion][Epoch 9764] diffusion training Loss: 0.06915344670414925
2024-11-05 01:58:48,262 - INFO - [diffusion][Epoch 9764] diffusion learning rate: 0.001
2024-11-05 01:58:48,336 - INFO - [diffusion][Epoch 9764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:48,338 - INFO - [diffusion][Epoch 9765] Epoch 9766/12000
2024-11-05 01:58:51,460 - INFO - [diffusion][Epoch 9765] diffusion training Loss: 0.06754621677100658
2024-11-05 01:58:51,461 - INFO - [diffusion][Epoch 9765] diffusion learning rate: 0.001
2024-11-05 01:58:51,463 - INFO - [diffusion][Epoch 9765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:51,464 - INFO - [diffusion][Epoch 9766] Epoch 9767/12000
2024-11-05 01:58:54,974 - INFO - [diffusion][Epoch 9766] diffusion training Loss: 0.06768306437879801
2024-11-05 01:58:54,976 - INFO - [diffusion][Epoch 9766] diffusion learning rate: 0.001
2024-11-05 01:58:54,978 - INFO - [diffusion][Epoch 9766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:54,979 - INFO - [diffusion][Epoch 9767] Epoch 9768/12000
2024-11-05 01:58:58,530 - INFO - [diffusion][Epoch 9767] diffusion training Loss: 0.06929490342736244
2024-11-05 01:58:58,532 - INFO - [diffusion][Epoch 9767] diffusion learning rate: 0.001
2024-11-05 01:58:58,590 - INFO - [diffusion][Epoch 9767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:58,592 - INFO - [diffusion][Epoch 9768] Epoch 9769/12000
2024-11-05 01:59:01,788 - INFO - [diffusion][Epoch 9768] diffusion training Loss: 0.07047463580965996
2024-11-05 01:59:01,790 - INFO - [diffusion][Epoch 9768] diffusion learning rate: 0.001
2024-11-05 01:59:01,791 - INFO - [diffusion][Epoch 9768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:01,793 - INFO - [diffusion][Epoch 9769] Epoch 9770/12000
2024-11-05 01:59:04,892 - INFO - [diffusion][Epoch 9769] diffusion training Loss: 0.0680680125951767
2024-11-05 01:59:04,894 - INFO - [diffusion][Epoch 9769] diffusion learning rate: 0.001
2024-11-05 01:59:04,895 - INFO - [diffusion][Epoch 9769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:04,896 - INFO - [diffusion][Epoch 9770] Epoch 9771/12000
2024-11-05 01:59:08,001 - INFO - [diffusion][Epoch 9770] diffusion training Loss: 0.06710226461291313
2024-11-05 01:59:08,004 - INFO - [diffusion][Epoch 9770] diffusion learning rate: 0.001
2024-11-05 01:59:08,005 - INFO - [diffusion][Epoch 9770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:08,006 - INFO - [diffusion][Epoch 9771] Epoch 9772/12000
2024-11-05 01:59:11,949 - INFO - [diffusion][Epoch 9771] diffusion training Loss: 0.06487038545310497
2024-11-05 01:59:11,950 - INFO - [diffusion][Epoch 9771] diffusion learning rate: 0.001
2024-11-05 01:59:11,952 - INFO - [diffusion][Epoch 9771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:11,953 - INFO - [diffusion][Epoch 9772] Epoch 9773/12000
2024-11-05 01:59:15,575 - INFO - [diffusion][Epoch 9772] diffusion training Loss: 0.06378249637782574
2024-11-05 01:59:15,577 - INFO - [diffusion][Epoch 9772] diffusion learning rate: 0.001
2024-11-05 01:59:15,579 - INFO - [diffusion][Epoch 9772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:15,580 - INFO - [diffusion][Epoch 9773] Epoch 9774/12000
2024-11-05 01:59:18,838 - INFO - [diffusion][Epoch 9773] diffusion training Loss: 0.06727333925664425
2024-11-05 01:59:18,839 - INFO - [diffusion][Epoch 9773] diffusion learning rate: 0.001
2024-11-05 01:59:18,841 - INFO - [diffusion][Epoch 9773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:18,842 - INFO - [diffusion][Epoch 9774] Epoch 9775/12000
2024-11-05 01:59:22,011 - INFO - [diffusion][Epoch 9774] diffusion training Loss: 0.061421796679496765
2024-11-05 01:59:22,013 - INFO - [diffusion][Epoch 9774] diffusion learning rate: 0.001
2024-11-05 01:59:22,015 - INFO - [diffusion][Epoch 9774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:22,016 - INFO - [diffusion][Epoch 9775] Epoch 9776/12000
2024-11-05 01:59:25,162 - INFO - [diffusion][Epoch 9775] diffusion training Loss: 0.06276702601462603
2024-11-05 01:59:25,164 - INFO - [diffusion][Epoch 9775] diffusion learning rate: 0.001
2024-11-05 01:59:25,165 - INFO - [diffusion][Epoch 9775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:25,167 - INFO - [diffusion][Epoch 9776] Epoch 9777/12000
2024-11-05 01:59:28,670 - INFO - [diffusion][Epoch 9776] diffusion training Loss: 0.06808966770768166
2024-11-05 01:59:28,673 - INFO - [diffusion][Epoch 9776] diffusion learning rate: 0.001
2024-11-05 01:59:28,674 - INFO - [diffusion][Epoch 9776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:28,675 - INFO - [diffusion][Epoch 9777] Epoch 9778/12000
2024-11-05 01:59:32,295 - INFO - [diffusion][Epoch 9777] diffusion training Loss: 0.0699622230604291
2024-11-05 01:59:32,297 - INFO - [diffusion][Epoch 9777] diffusion learning rate: 0.001
2024-11-05 01:59:32,299 - INFO - [diffusion][Epoch 9777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:32,300 - INFO - [diffusion][Epoch 9778] Epoch 9779/12000
2024-11-05 01:59:35,389 - INFO - [diffusion][Epoch 9778] diffusion training Loss: 0.06716386880725622
2024-11-05 01:59:35,390 - INFO - [diffusion][Epoch 9778] diffusion learning rate: 0.001
2024-11-05 01:59:35,392 - INFO - [diffusion][Epoch 9778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:35,393 - INFO - [diffusion][Epoch 9779] Epoch 9780/12000
2024-11-05 01:59:38,494 - INFO - [diffusion][Epoch 9779] diffusion training Loss: 0.07366406172513962
2024-11-05 01:59:38,496 - INFO - [diffusion][Epoch 9779] diffusion learning rate: 0.001
2024-11-05 01:59:38,497 - INFO - [diffusion][Epoch 9779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:38,499 - INFO - [diffusion][Epoch 9780] Epoch 9781/12000
2024-11-05 01:59:41,639 - INFO - [diffusion][Epoch 9780] diffusion training Loss: 0.0662747249007225
2024-11-05 01:59:41,641 - INFO - [diffusion][Epoch 9780] diffusion learning rate: 0.001
2024-11-05 01:59:41,643 - INFO - [diffusion][Epoch 9780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:41,644 - INFO - [diffusion][Epoch 9781] Epoch 9782/12000
2024-11-05 01:59:45,173 - INFO - [diffusion][Epoch 9781] diffusion training Loss: 0.0694224163889885
2024-11-05 01:59:45,175 - INFO - [diffusion][Epoch 9781] diffusion learning rate: 0.001
2024-11-05 01:59:45,176 - INFO - [diffusion][Epoch 9781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:45,177 - INFO - [diffusion][Epoch 9782] Epoch 9783/12000
2024-11-05 01:59:48,724 - INFO - [diffusion][Epoch 9782] diffusion training Loss: 0.06839898228645325
2024-11-05 01:59:48,726 - INFO - [diffusion][Epoch 9782] diffusion learning rate: 0.001
2024-11-05 01:59:48,727 - INFO - [diffusion][Epoch 9782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:48,729 - INFO - [diffusion][Epoch 9783] Epoch 9784/12000
2024-11-05 01:59:51,858 - INFO - [diffusion][Epoch 9783] diffusion training Loss: 0.06712754257023335
2024-11-05 01:59:51,877 - INFO - [diffusion][Epoch 9783] diffusion learning rate: 0.001
2024-11-05 01:59:51,879 - INFO - [diffusion][Epoch 9783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:51,880 - INFO - [diffusion][Epoch 9784] Epoch 9785/12000
2024-11-05 01:59:54,960 - INFO - [diffusion][Epoch 9784] diffusion training Loss: 0.06800861284136772
2024-11-05 01:59:54,962 - INFO - [diffusion][Epoch 9784] diffusion learning rate: 0.001
2024-11-05 01:59:54,964 - INFO - [diffusion][Epoch 9784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:54,965 - INFO - [diffusion][Epoch 9785] Epoch 9786/12000
2024-11-05 01:59:58,106 - INFO - [diffusion][Epoch 9785] diffusion training Loss: 0.06414106953889132
2024-11-05 01:59:58,108 - INFO - [diffusion][Epoch 9785] diffusion learning rate: 0.001
2024-11-05 01:59:58,110 - INFO - [diffusion][Epoch 9785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:58,111 - INFO - [diffusion][Epoch 9786] Epoch 9787/12000
2024-11-05 02:00:01,603 - INFO - [diffusion][Epoch 9786] diffusion training Loss: 0.071187699213624
2024-11-05 02:00:01,605 - INFO - [diffusion][Epoch 9786] diffusion learning rate: 0.001
2024-11-05 02:00:01,607 - INFO - [diffusion][Epoch 9786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:01,609 - INFO - [diffusion][Epoch 9787] Epoch 9788/12000
2024-11-05 02:00:05,169 - INFO - [diffusion][Epoch 9787] diffusion training Loss: 0.06698432937264442
2024-11-05 02:00:05,171 - INFO - [diffusion][Epoch 9787] diffusion learning rate: 0.001
2024-11-05 02:00:05,173 - INFO - [diffusion][Epoch 9787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:05,175 - INFO - [diffusion][Epoch 9788] Epoch 9789/12000
2024-11-05 02:00:08,263 - INFO - [diffusion][Epoch 9788] diffusion training Loss: 0.06439926754683256
2024-11-05 02:00:08,266 - INFO - [diffusion][Epoch 9788] diffusion learning rate: 0.001
2024-11-05 02:00:08,268 - INFO - [diffusion][Epoch 9788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:08,269 - INFO - [diffusion][Epoch 9789] Epoch 9790/12000
2024-11-05 02:00:11,367 - INFO - [diffusion][Epoch 9789] diffusion training Loss: 0.07082478422671556
2024-11-05 02:00:11,369 - INFO - [diffusion][Epoch 9789] diffusion learning rate: 0.001
2024-11-05 02:00:11,371 - INFO - [diffusion][Epoch 9789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:11,372 - INFO - [diffusion][Epoch 9790] Epoch 9791/12000
2024-11-05 02:00:14,488 - INFO - [diffusion][Epoch 9790] diffusion training Loss: 0.06422410905361176
2024-11-05 02:00:14,490 - INFO - [diffusion][Epoch 9790] diffusion learning rate: 0.001
2024-11-05 02:00:14,492 - INFO - [diffusion][Epoch 9790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:14,493 - INFO - [diffusion][Epoch 9791] Epoch 9792/12000
2024-11-05 02:00:18,073 - INFO - [diffusion][Epoch 9791] diffusion training Loss: 0.06105594988912344
2024-11-05 02:00:18,075 - INFO - [diffusion][Epoch 9791] diffusion learning rate: 0.001
2024-11-05 02:00:18,077 - INFO - [diffusion][Epoch 9791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:18,079 - INFO - [diffusion][Epoch 9792] Epoch 9793/12000
2024-11-05 02:00:21,634 - INFO - [diffusion][Epoch 9792] diffusion training Loss: 0.06419057864695787
2024-11-05 02:00:21,636 - INFO - [diffusion][Epoch 9792] diffusion learning rate: 0.001
2024-11-05 02:00:21,638 - INFO - [diffusion][Epoch 9792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:21,640 - INFO - [diffusion][Epoch 9793] Epoch 9794/12000
2024-11-05 02:00:25,031 - INFO - [diffusion][Epoch 9793] diffusion training Loss: 0.07001727819442749
2024-11-05 02:00:25,033 - INFO - [diffusion][Epoch 9793] diffusion learning rate: 0.001
2024-11-05 02:00:25,035 - INFO - [diffusion][Epoch 9793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:25,036 - INFO - [diffusion][Epoch 9794] Epoch 9795/12000
2024-11-05 02:00:28,147 - INFO - [diffusion][Epoch 9794] diffusion training Loss: 0.06955661997199059
2024-11-05 02:00:28,149 - INFO - [diffusion][Epoch 9794] diffusion learning rate: 0.001
2024-11-05 02:00:28,151 - INFO - [diffusion][Epoch 9794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:28,152 - INFO - [diffusion][Epoch 9795] Epoch 9796/12000
2024-11-05 02:00:31,303 - INFO - [diffusion][Epoch 9795] diffusion training Loss: 0.06719179078936577
2024-11-05 02:00:31,305 - INFO - [diffusion][Epoch 9795] diffusion learning rate: 0.001
2024-11-05 02:00:31,306 - INFO - [diffusion][Epoch 9795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:31,308 - INFO - [diffusion][Epoch 9796] Epoch 9797/12000
2024-11-05 02:00:34,944 - INFO - [diffusion][Epoch 9796] diffusion training Loss: 0.061932253651320934
2024-11-05 02:00:34,946 - INFO - [diffusion][Epoch 9796] diffusion learning rate: 0.001
2024-11-05 02:00:35,003 - INFO - [diffusion][Epoch 9796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:35,004 - INFO - [diffusion][Epoch 9797] Epoch 9798/12000
2024-11-05 02:00:38,594 - INFO - [diffusion][Epoch 9797] diffusion training Loss: 0.06724961660802364
2024-11-05 02:00:38,596 - INFO - [diffusion][Epoch 9797] diffusion learning rate: 0.001
2024-11-05 02:00:38,597 - INFO - [diffusion][Epoch 9797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:38,599 - INFO - [diffusion][Epoch 9798] Epoch 9799/12000
2024-11-05 02:00:41,825 - INFO - [diffusion][Epoch 9798] diffusion training Loss: 0.0636165076866746
2024-11-05 02:00:41,826 - INFO - [diffusion][Epoch 9798] diffusion learning rate: 0.001
2024-11-05 02:00:41,828 - INFO - [diffusion][Epoch 9798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:41,829 - INFO - [diffusion][Epoch 9799] Epoch 9800/12000
2024-11-05 02:00:44,498 - INFO - [diffusion][Epoch 9799] diffusion training Loss: 0.06485483702272177
2024-11-05 02:00:44,500 - INFO - [diffusion][Epoch 9799] diffusion learning rate: 0.001
2024-11-05 02:00:44,502 - INFO - [diffusion][Epoch 9799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:44,503 - INFO - [diffusion][Epoch 9800] Epoch 9801/12000
2024-11-05 02:00:48,121 - INFO - [diffusion][Epoch 9800] diffusion training Loss: 0.07132374309003353
2024-11-05 02:00:48,122 - INFO - [diffusion][Epoch 9800] diffusion learning rate: 0.001
2024-11-05 02:00:48,124 - INFO - [diffusion][Epoch 9800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:48,125 - INFO - [diffusion][Epoch 9801] Epoch 9802/12000
2024-11-05 02:00:51,449 - INFO - [diffusion][Epoch 9801] diffusion training Loss: 0.07033373229205608
2024-11-05 02:00:51,450 - INFO - [diffusion][Epoch 9801] diffusion learning rate: 0.001
2024-11-05 02:00:51,533 - INFO - [diffusion][Epoch 9801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:51,535 - INFO - [diffusion][Epoch 9802] Epoch 9803/12000
2024-11-05 02:00:54,615 - INFO - [diffusion][Epoch 9802] diffusion training Loss: 0.06380875781178474
2024-11-05 02:00:54,616 - INFO - [diffusion][Epoch 9802] diffusion learning rate: 0.001
2024-11-05 02:00:54,618 - INFO - [diffusion][Epoch 9802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:54,619 - INFO - [diffusion][Epoch 9803] Epoch 9804/12000
2024-11-05 02:00:57,632 - INFO - [diffusion][Epoch 9803] diffusion training Loss: 0.07061056420207024
2024-11-05 02:00:57,634 - INFO - [diffusion][Epoch 9803] diffusion learning rate: 0.001
2024-11-05 02:00:57,636 - INFO - [diffusion][Epoch 9803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:57,637 - INFO - [diffusion][Epoch 9804] Epoch 9805/12000
2024-11-05 02:01:01,203 - INFO - [diffusion][Epoch 9804] diffusion training Loss: 0.06971391476690769
2024-11-05 02:01:01,205 - INFO - [diffusion][Epoch 9804] diffusion learning rate: 0.001
2024-11-05 02:01:01,207 - INFO - [diffusion][Epoch 9804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:01,208 - INFO - [diffusion][Epoch 9805] Epoch 9806/12000
2024-11-05 02:01:04,759 - INFO - [diffusion][Epoch 9805] diffusion training Loss: 0.06906826235353947
2024-11-05 02:01:04,761 - INFO - [diffusion][Epoch 9805] diffusion learning rate: 0.001
2024-11-05 02:01:04,763 - INFO - [diffusion][Epoch 9805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:04,765 - INFO - [diffusion][Epoch 9806] Epoch 9807/12000
2024-11-05 02:01:07,896 - INFO - [diffusion][Epoch 9806] diffusion training Loss: 0.06750940531492233
2024-11-05 02:01:07,898 - INFO - [diffusion][Epoch 9806] diffusion learning rate: 0.001
2024-11-05 02:01:07,900 - INFO - [diffusion][Epoch 9806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:07,901 - INFO - [diffusion][Epoch 9807] Epoch 9808/12000
2024-11-05 02:01:11,032 - INFO - [diffusion][Epoch 9807] diffusion training Loss: 0.06791247241199017
2024-11-05 02:01:11,034 - INFO - [diffusion][Epoch 9807] diffusion learning rate: 0.001
2024-11-05 02:01:11,037 - INFO - [diffusion][Epoch 9807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:11,038 - INFO - [diffusion][Epoch 9808] Epoch 9809/12000
2024-11-05 02:01:14,325 - INFO - [diffusion][Epoch 9808] diffusion training Loss: 0.06886678375303745
2024-11-05 02:01:14,327 - INFO - [diffusion][Epoch 9808] diffusion learning rate: 0.001
2024-11-05 02:01:14,329 - INFO - [diffusion][Epoch 9808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:14,330 - INFO - [diffusion][Epoch 9809] Epoch 9810/12000
2024-11-05 02:01:17,961 - INFO - [diffusion][Epoch 9809] diffusion training Loss: 0.069186438806355
2024-11-05 02:01:17,962 - INFO - [diffusion][Epoch 9809] diffusion learning rate: 0.001
2024-11-05 02:01:17,964 - INFO - [diffusion][Epoch 9809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:17,965 - INFO - [diffusion][Epoch 9810] Epoch 9811/12000
2024-11-05 02:01:21,171 - INFO - [diffusion][Epoch 9810] diffusion training Loss: 0.062192704528570175
2024-11-05 02:01:21,173 - INFO - [diffusion][Epoch 9810] diffusion learning rate: 0.001
2024-11-05 02:01:21,175 - INFO - [diffusion][Epoch 9810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:21,176 - INFO - [diffusion][Epoch 9811] Epoch 9812/12000
2024-11-05 02:01:24,351 - INFO - [diffusion][Epoch 9811] diffusion training Loss: 0.0633188784122467
2024-11-05 02:01:24,353 - INFO - [diffusion][Epoch 9811] diffusion learning rate: 0.001
2024-11-05 02:01:24,401 - INFO - [diffusion][Epoch 9811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:24,402 - INFO - [diffusion][Epoch 9812] Epoch 9813/12000
2024-11-05 02:01:28,068 - INFO - [diffusion][Epoch 9812] diffusion training Loss: 0.0669742189347744
2024-11-05 02:01:28,070 - INFO - [diffusion][Epoch 9812] diffusion learning rate: 0.001
2024-11-05 02:01:28,071 - INFO - [diffusion][Epoch 9812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:28,072 - INFO - [diffusion][Epoch 9813] Epoch 9814/12000
2024-11-05 02:01:31,303 - INFO - [diffusion][Epoch 9813] diffusion training Loss: 0.06464699283242226
2024-11-05 02:01:31,305 - INFO - [diffusion][Epoch 9813] diffusion learning rate: 0.001
2024-11-05 02:01:31,307 - INFO - [diffusion][Epoch 9813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:31,308 - INFO - [diffusion][Epoch 9814] Epoch 9815/12000
2024-11-05 02:01:34,908 - INFO - [diffusion][Epoch 9814] diffusion training Loss: 0.06520574074238539
2024-11-05 02:01:34,909 - INFO - [diffusion][Epoch 9814] diffusion learning rate: 0.001
2024-11-05 02:01:34,911 - INFO - [diffusion][Epoch 9814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:34,912 - INFO - [diffusion][Epoch 9815] Epoch 9816/12000
2024-11-05 02:01:38,328 - INFO - [diffusion][Epoch 9815] diffusion training Loss: 0.0639904011040926
2024-11-05 02:01:38,331 - INFO - [diffusion][Epoch 9815] diffusion learning rate: 0.001
2024-11-05 02:01:38,332 - INFO - [diffusion][Epoch 9815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:38,334 - INFO - [diffusion][Epoch 9816] Epoch 9817/12000
2024-11-05 02:01:41,423 - INFO - [diffusion][Epoch 9816] diffusion training Loss: 0.06689773686230183
2024-11-05 02:01:41,425 - INFO - [diffusion][Epoch 9816] diffusion learning rate: 0.001
2024-11-05 02:01:41,426 - INFO - [diffusion][Epoch 9816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:41,428 - INFO - [diffusion][Epoch 9817] Epoch 9818/12000
2024-11-05 02:01:44,542 - INFO - [diffusion][Epoch 9817] diffusion training Loss: 0.07384664285928011
2024-11-05 02:01:44,544 - INFO - [diffusion][Epoch 9817] diffusion learning rate: 0.001
2024-11-05 02:01:44,546 - INFO - [diffusion][Epoch 9817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:44,547 - INFO - [diffusion][Epoch 9818] Epoch 9819/12000
2024-11-05 02:01:48,014 - INFO - [diffusion][Epoch 9818] diffusion training Loss: 0.06476513389497995
2024-11-05 02:01:48,016 - INFO - [diffusion][Epoch 9818] diffusion learning rate: 0.001
2024-11-05 02:01:48,018 - INFO - [diffusion][Epoch 9818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:48,019 - INFO - [diffusion][Epoch 9819] Epoch 9820/12000
2024-11-05 02:01:51,641 - INFO - [diffusion][Epoch 9819] diffusion training Loss: 0.06425797194242477
2024-11-05 02:01:51,643 - INFO - [diffusion][Epoch 9819] diffusion learning rate: 0.001
2024-11-05 02:01:51,645 - INFO - [diffusion][Epoch 9819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:51,646 - INFO - [diffusion][Epoch 9820] Epoch 9821/12000
2024-11-05 02:01:54,724 - INFO - [diffusion][Epoch 9820] diffusion training Loss: 0.06807462498545647
2024-11-05 02:01:54,726 - INFO - [diffusion][Epoch 9820] diffusion learning rate: 0.001
2024-11-05 02:01:54,727 - INFO - [diffusion][Epoch 9820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:54,729 - INFO - [diffusion][Epoch 9821] Epoch 9822/12000
2024-11-05 02:01:57,791 - INFO - [diffusion][Epoch 9821] diffusion training Loss: 0.06556942127645016
2024-11-05 02:01:57,793 - INFO - [diffusion][Epoch 9821] diffusion learning rate: 0.001
2024-11-05 02:01:57,795 - INFO - [diffusion][Epoch 9821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:57,796 - INFO - [diffusion][Epoch 9822] Epoch 9823/12000
2024-11-05 02:02:00,904 - INFO - [diffusion][Epoch 9822] diffusion training Loss: 0.06586513482034206
2024-11-05 02:02:00,906 - INFO - [diffusion][Epoch 9822] diffusion learning rate: 0.001
2024-11-05 02:02:00,907 - INFO - [diffusion][Epoch 9822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:00,909 - INFO - [diffusion][Epoch 9823] Epoch 9824/12000
2024-11-05 02:02:04,520 - INFO - [diffusion][Epoch 9823] diffusion training Loss: 0.06959995068609715
2024-11-05 02:02:04,522 - INFO - [diffusion][Epoch 9823] diffusion learning rate: 0.001
2024-11-05 02:02:04,524 - INFO - [diffusion][Epoch 9823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:04,525 - INFO - [diffusion][Epoch 9824] Epoch 9825/12000
2024-11-05 02:02:08,004 - INFO - [diffusion][Epoch 9824] diffusion training Loss: 0.06573198549449444
2024-11-05 02:02:08,006 - INFO - [diffusion][Epoch 9824] diffusion learning rate: 0.001
2024-11-05 02:02:08,008 - INFO - [diffusion][Epoch 9824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:08,009 - INFO - [diffusion][Epoch 9825] Epoch 9826/12000
2024-11-05 02:02:11,289 - INFO - [diffusion][Epoch 9825] diffusion training Loss: 0.06469288747757673
2024-11-05 02:02:11,292 - INFO - [diffusion][Epoch 9825] diffusion learning rate: 0.001
2024-11-05 02:02:11,364 - INFO - [diffusion][Epoch 9825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:11,365 - INFO - [diffusion][Epoch 9826] Epoch 9827/12000
2024-11-05 02:02:14,292 - INFO - [diffusion][Epoch 9826] diffusion training Loss: 0.06768736988306046
2024-11-05 02:02:14,294 - INFO - [diffusion][Epoch 9826] diffusion learning rate: 0.001
2024-11-05 02:02:14,295 - INFO - [diffusion][Epoch 9826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:14,296 - INFO - [diffusion][Epoch 9827] Epoch 9828/12000
2024-11-05 02:02:17,365 - INFO - [diffusion][Epoch 9827] diffusion training Loss: 0.0642938818782568
2024-11-05 02:02:17,367 - INFO - [diffusion][Epoch 9827] diffusion learning rate: 0.001
2024-11-05 02:02:17,368 - INFO - [diffusion][Epoch 9827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:17,369 - INFO - [diffusion][Epoch 9828] Epoch 9829/12000
2024-11-05 02:02:20,870 - INFO - [diffusion][Epoch 9828] diffusion training Loss: 0.07091746106743813
2024-11-05 02:02:20,872 - INFO - [diffusion][Epoch 9828] diffusion learning rate: 0.001
2024-11-05 02:02:20,874 - INFO - [diffusion][Epoch 9828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:20,876 - INFO - [diffusion][Epoch 9829] Epoch 9830/12000
2024-11-05 02:02:24,588 - INFO - [diffusion][Epoch 9829] diffusion training Loss: 0.06197372544556856
2024-11-05 02:02:24,589 - INFO - [diffusion][Epoch 9829] diffusion learning rate: 0.001
2024-11-05 02:02:24,591 - INFO - [diffusion][Epoch 9829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:24,593 - INFO - [diffusion][Epoch 9830] Epoch 9831/12000
2024-11-05 02:02:27,821 - INFO - [diffusion][Epoch 9830] diffusion training Loss: 0.06769724935293198
2024-11-05 02:02:27,823 - INFO - [diffusion][Epoch 9830] diffusion learning rate: 0.001
2024-11-05 02:02:27,850 - INFO - [diffusion][Epoch 9830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:27,852 - INFO - [diffusion][Epoch 9831] Epoch 9832/12000
2024-11-05 02:02:31,006 - INFO - [diffusion][Epoch 9831] diffusion training Loss: 0.068183783441782
2024-11-05 02:02:31,008 - INFO - [diffusion][Epoch 9831] diffusion learning rate: 0.001
2024-11-05 02:02:31,010 - INFO - [diffusion][Epoch 9831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:31,011 - INFO - [diffusion][Epoch 9832] Epoch 9833/12000
2024-11-05 02:02:34,097 - INFO - [diffusion][Epoch 9832] diffusion training Loss: 0.06697231903672218
2024-11-05 02:02:34,099 - INFO - [diffusion][Epoch 9832] diffusion learning rate: 0.001
2024-11-05 02:02:34,101 - INFO - [diffusion][Epoch 9832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:34,102 - INFO - [diffusion][Epoch 9833] Epoch 9834/12000
2024-11-05 02:02:37,565 - INFO - [diffusion][Epoch 9833] diffusion training Loss: 0.07001612335443497
2024-11-05 02:02:37,567 - INFO - [diffusion][Epoch 9833] diffusion learning rate: 0.001
2024-11-05 02:02:37,569 - INFO - [diffusion][Epoch 9833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:37,570 - INFO - [diffusion][Epoch 9834] Epoch 9835/12000
2024-11-05 02:02:41,152 - INFO - [diffusion][Epoch 9834] diffusion training Loss: 0.07240571081638336
2024-11-05 02:02:41,153 - INFO - [diffusion][Epoch 9834] diffusion learning rate: 0.001
2024-11-05 02:02:41,155 - INFO - [diffusion][Epoch 9834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:41,156 - INFO - [diffusion][Epoch 9835] Epoch 9836/12000
2024-11-05 02:02:44,517 - INFO - [diffusion][Epoch 9835] diffusion training Loss: 0.06849522143602371
2024-11-05 02:02:44,519 - INFO - [diffusion][Epoch 9835] diffusion learning rate: 0.001
2024-11-05 02:02:44,521 - INFO - [diffusion][Epoch 9835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:44,524 - INFO - [diffusion][Epoch 9836] Epoch 9837/12000
2024-11-05 02:02:47,610 - INFO - [diffusion][Epoch 9836] diffusion training Loss: 0.06300694588571787
2024-11-05 02:02:47,612 - INFO - [diffusion][Epoch 9836] diffusion learning rate: 0.001
2024-11-05 02:02:47,614 - INFO - [diffusion][Epoch 9836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:47,615 - INFO - [diffusion][Epoch 9837] Epoch 9838/12000
2024-11-05 02:02:50,683 - INFO - [diffusion][Epoch 9837] diffusion training Loss: 0.06901740934699774
2024-11-05 02:02:50,685 - INFO - [diffusion][Epoch 9837] diffusion learning rate: 0.001
2024-11-05 02:02:50,687 - INFO - [diffusion][Epoch 9837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:50,688 - INFO - [diffusion][Epoch 9838] Epoch 9839/12000
2024-11-05 02:02:54,230 - INFO - [diffusion][Epoch 9838] diffusion training Loss: 0.06730205286294222
2024-11-05 02:02:54,233 - INFO - [diffusion][Epoch 9838] diffusion learning rate: 0.001
2024-11-05 02:02:54,235 - INFO - [diffusion][Epoch 9838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:54,236 - INFO - [diffusion][Epoch 9839] Epoch 9840/12000
2024-11-05 02:02:57,751 - INFO - [diffusion][Epoch 9839] diffusion training Loss: 0.06565779261291027
2024-11-05 02:02:57,753 - INFO - [diffusion][Epoch 9839] diffusion learning rate: 0.001
2024-11-05 02:02:57,755 - INFO - [diffusion][Epoch 9839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:57,756 - INFO - [diffusion][Epoch 9840] Epoch 9841/12000
2024-11-05 02:03:00,838 - INFO - [diffusion][Epoch 9840] diffusion training Loss: 0.07028336636722088
2024-11-05 02:03:00,840 - INFO - [diffusion][Epoch 9840] diffusion learning rate: 0.001
2024-11-05 02:03:00,905 - INFO - [diffusion][Epoch 9840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:00,907 - INFO - [diffusion][Epoch 9841] Epoch 9842/12000
2024-11-05 02:03:04,013 - INFO - [diffusion][Epoch 9841] diffusion training Loss: 0.06519264541566372
2024-11-05 02:03:04,014 - INFO - [diffusion][Epoch 9841] diffusion learning rate: 0.001
2024-11-05 02:03:04,016 - INFO - [diffusion][Epoch 9841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:04,017 - INFO - [diffusion][Epoch 9842] Epoch 9843/12000
2024-11-05 02:03:07,099 - INFO - [diffusion][Epoch 9842] diffusion training Loss: 0.06672142073512077
2024-11-05 02:03:07,101 - INFO - [diffusion][Epoch 9842] diffusion learning rate: 0.001
2024-11-05 02:03:07,102 - INFO - [diffusion][Epoch 9842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:07,104 - INFO - [diffusion][Epoch 9843] Epoch 9844/12000
2024-11-05 02:03:10,615 - INFO - [diffusion][Epoch 9843] diffusion training Loss: 0.06759772449731827
2024-11-05 02:03:10,617 - INFO - [diffusion][Epoch 9843] diffusion learning rate: 0.001
2024-11-05 02:03:10,619 - INFO - [diffusion][Epoch 9843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:10,620 - INFO - [diffusion][Epoch 9844] Epoch 9845/12000
2024-11-05 02:03:14,101 - INFO - [diffusion][Epoch 9844] diffusion training Loss: 0.06575689278542995
2024-11-05 02:03:14,104 - INFO - [diffusion][Epoch 9844] diffusion learning rate: 0.001
2024-11-05 02:03:14,105 - INFO - [diffusion][Epoch 9844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:14,106 - INFO - [diffusion][Epoch 9845] Epoch 9846/12000
2024-11-05 02:03:17,228 - INFO - [diffusion][Epoch 9845] diffusion training Loss: 0.06857221107929945
2024-11-05 02:03:17,230 - INFO - [diffusion][Epoch 9845] diffusion learning rate: 0.001
2024-11-05 02:03:17,232 - INFO - [diffusion][Epoch 9845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:17,233 - INFO - [diffusion][Epoch 9846] Epoch 9847/12000
2024-11-05 02:03:20,398 - INFO - [diffusion][Epoch 9846] diffusion training Loss: 0.06910961121320724
2024-11-05 02:03:20,400 - INFO - [diffusion][Epoch 9846] diffusion learning rate: 0.001
2024-11-05 02:03:20,402 - INFO - [diffusion][Epoch 9846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:20,403 - INFO - [diffusion][Epoch 9847] Epoch 9848/12000
2024-11-05 02:03:23,690 - INFO - [diffusion][Epoch 9847] diffusion training Loss: 0.06372935324907303
2024-11-05 02:03:23,692 - INFO - [diffusion][Epoch 9847] diffusion learning rate: 0.001
2024-11-05 02:03:23,693 - INFO - [diffusion][Epoch 9847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:23,695 - INFO - [diffusion][Epoch 9848] Epoch 9849/12000
2024-11-05 02:03:27,273 - INFO - [diffusion][Epoch 9848] diffusion training Loss: 0.06380280014127493
2024-11-05 02:03:27,275 - INFO - [diffusion][Epoch 9848] diffusion learning rate: 0.001
2024-11-05 02:03:27,276 - INFO - [diffusion][Epoch 9848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:27,278 - INFO - [diffusion][Epoch 9849] Epoch 9850/12000
2024-11-05 02:03:30,652 - INFO - [diffusion][Epoch 9849] diffusion training Loss: 0.06751494761556387
2024-11-05 02:03:30,654 - INFO - [diffusion][Epoch 9849] diffusion learning rate: 0.001
2024-11-05 02:03:30,656 - INFO - [diffusion][Epoch 9849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:30,657 - INFO - [diffusion][Epoch 9850] Epoch 9851/12000
2024-11-05 02:03:33,756 - INFO - [diffusion][Epoch 9850] diffusion training Loss: 0.06846027541905642
2024-11-05 02:03:33,758 - INFO - [diffusion][Epoch 9850] diffusion learning rate: 0.001
2024-11-05 02:03:33,760 - INFO - [diffusion][Epoch 9850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:33,761 - INFO - [diffusion][Epoch 9851] Epoch 9852/12000
2024-11-05 02:03:36,855 - INFO - [diffusion][Epoch 9851] diffusion training Loss: 0.06668784841895103
2024-11-05 02:03:36,857 - INFO - [diffusion][Epoch 9851] diffusion learning rate: 0.001
2024-11-05 02:03:36,858 - INFO - [diffusion][Epoch 9851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:36,860 - INFO - [diffusion][Epoch 9852] Epoch 9853/12000
2024-11-05 02:03:40,565 - INFO - [diffusion][Epoch 9852] diffusion training Loss: 0.06914559006690979
2024-11-05 02:03:40,566 - INFO - [diffusion][Epoch 9852] diffusion learning rate: 0.001
2024-11-05 02:03:40,568 - INFO - [diffusion][Epoch 9852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:40,569 - INFO - [diffusion][Epoch 9853] Epoch 9854/12000
2024-11-05 02:03:44,182 - INFO - [diffusion][Epoch 9853] diffusion training Loss: 0.06237024441361427
2024-11-05 02:03:44,184 - INFO - [diffusion][Epoch 9853] diffusion learning rate: 0.001
2024-11-05 02:03:44,186 - INFO - [diffusion][Epoch 9853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:44,187 - INFO - [diffusion][Epoch 9854] Epoch 9855/12000
2024-11-05 02:03:47,436 - INFO - [diffusion][Epoch 9854] diffusion training Loss: 0.06527927983552217
2024-11-05 02:03:47,438 - INFO - [diffusion][Epoch 9854] diffusion learning rate: 0.001
2024-11-05 02:03:47,440 - INFO - [diffusion][Epoch 9854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:47,441 - INFO - [diffusion][Epoch 9855] Epoch 9856/12000
2024-11-05 02:03:50,450 - INFO - [diffusion][Epoch 9855] diffusion training Loss: 0.06872123666107655
2024-11-05 02:03:50,452 - INFO - [diffusion][Epoch 9855] diffusion learning rate: 0.001
2024-11-05 02:03:50,479 - INFO - [diffusion][Epoch 9855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:50,481 - INFO - [diffusion][Epoch 9856] Epoch 9857/12000
2024-11-05 02:03:53,523 - INFO - [diffusion][Epoch 9856] diffusion training Loss: 0.06777286343276501
2024-11-05 02:03:53,525 - INFO - [diffusion][Epoch 9856] diffusion learning rate: 0.001
2024-11-05 02:03:53,527 - INFO - [diffusion][Epoch 9856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:53,528 - INFO - [diffusion][Epoch 9857] Epoch 9858/12000
2024-11-05 02:03:57,006 - INFO - [diffusion][Epoch 9857] diffusion training Loss: 0.06632964871823788
2024-11-05 02:03:57,008 - INFO - [diffusion][Epoch 9857] diffusion learning rate: 0.001
2024-11-05 02:03:57,010 - INFO - [diffusion][Epoch 9857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:57,012 - INFO - [diffusion][Epoch 9858] Epoch 9859/12000
2024-11-05 02:04:00,499 - INFO - [diffusion][Epoch 9858] diffusion training Loss: 0.06585318315774202
2024-11-05 02:04:00,501 - INFO - [diffusion][Epoch 9858] diffusion learning rate: 0.001
2024-11-05 02:04:00,503 - INFO - [diffusion][Epoch 9858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:00,504 - INFO - [diffusion][Epoch 9859] Epoch 9860/12000
2024-11-05 02:04:03,627 - INFO - [diffusion][Epoch 9859] diffusion training Loss: 0.06972433440387249
2024-11-05 02:04:03,629 - INFO - [diffusion][Epoch 9859] diffusion learning rate: 0.001
2024-11-05 02:04:03,631 - INFO - [diffusion][Epoch 9859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:03,632 - INFO - [diffusion][Epoch 9860] Epoch 9861/12000
2024-11-05 02:04:06,708 - INFO - [diffusion][Epoch 9860] diffusion training Loss: 0.06925871595740318
2024-11-05 02:04:06,710 - INFO - [diffusion][Epoch 9860] diffusion learning rate: 0.001
2024-11-05 02:04:06,712 - INFO - [diffusion][Epoch 9860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:06,714 - INFO - [diffusion][Epoch 9861] Epoch 9862/12000
2024-11-05 02:04:09,934 - INFO - [diffusion][Epoch 9861] diffusion training Loss: 0.07235621847212315
2024-11-05 02:04:09,936 - INFO - [diffusion][Epoch 9861] diffusion learning rate: 0.001
2024-11-05 02:04:09,938 - INFO - [diffusion][Epoch 9861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:09,939 - INFO - [diffusion][Epoch 9862] Epoch 9863/12000
2024-11-05 02:04:13,371 - INFO - [diffusion][Epoch 9862] diffusion training Loss: 0.06343108788132668
2024-11-05 02:04:13,373 - INFO - [diffusion][Epoch 9862] diffusion learning rate: 0.001
2024-11-05 02:04:13,375 - INFO - [diffusion][Epoch 9862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:13,376 - INFO - [diffusion][Epoch 9863] Epoch 9864/12000
2024-11-05 02:04:16,547 - INFO - [diffusion][Epoch 9863] diffusion training Loss: 0.06670600362122059
2024-11-05 02:04:16,550 - INFO - [diffusion][Epoch 9863] diffusion learning rate: 0.001
2024-11-05 02:04:16,552 - INFO - [diffusion][Epoch 9863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:16,553 - INFO - [diffusion][Epoch 9864] Epoch 9865/12000
2024-11-05 02:04:19,597 - INFO - [diffusion][Epoch 9864] diffusion training Loss: 0.06393555738031864
2024-11-05 02:04:19,599 - INFO - [diffusion][Epoch 9864] diffusion learning rate: 0.001
2024-11-05 02:04:19,601 - INFO - [diffusion][Epoch 9864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:19,602 - INFO - [diffusion][Epoch 9865] Epoch 9866/12000
2024-11-05 02:04:22,921 - INFO - [diffusion][Epoch 9865] diffusion training Loss: 0.06867774948477745
2024-11-05 02:04:22,923 - INFO - [diffusion][Epoch 9865] diffusion learning rate: 0.001
2024-11-05 02:04:22,925 - INFO - [diffusion][Epoch 9865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:22,926 - INFO - [diffusion][Epoch 9866] Epoch 9867/12000
2024-11-05 02:04:26,545 - INFO - [diffusion][Epoch 9866] diffusion training Loss: 0.06604708358645439
2024-11-05 02:04:26,547 - INFO - [diffusion][Epoch 9866] diffusion learning rate: 0.001
2024-11-05 02:04:26,549 - INFO - [diffusion][Epoch 9866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:26,550 - INFO - [diffusion][Epoch 9867] Epoch 9868/12000
2024-11-05 02:04:29,770 - INFO - [diffusion][Epoch 9867] diffusion training Loss: 0.06502589769661427
2024-11-05 02:04:29,772 - INFO - [diffusion][Epoch 9867] diffusion learning rate: 0.001
2024-11-05 02:04:29,774 - INFO - [diffusion][Epoch 9867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:29,775 - INFO - [diffusion][Epoch 9868] Epoch 9869/12000
2024-11-05 02:04:33,030 - INFO - [diffusion][Epoch 9868] diffusion training Loss: 0.06452322006225586
2024-11-05 02:04:33,032 - INFO - [diffusion][Epoch 9868] diffusion learning rate: 0.001
2024-11-05 02:04:33,034 - INFO - [diffusion][Epoch 9868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:33,035 - INFO - [diffusion][Epoch 9869] Epoch 9870/12000
2024-11-05 02:04:36,279 - INFO - [diffusion][Epoch 9869] diffusion training Loss: 0.07177982665598392
2024-11-05 02:04:36,280 - INFO - [diffusion][Epoch 9869] diffusion learning rate: 0.001
2024-11-05 02:04:36,282 - INFO - [diffusion][Epoch 9869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:36,283 - INFO - [diffusion][Epoch 9870] Epoch 9871/12000
2024-11-05 02:04:39,572 - INFO - [diffusion][Epoch 9870] diffusion training Loss: 0.061993950977921486
2024-11-05 02:04:39,574 - INFO - [diffusion][Epoch 9870] diffusion learning rate: 0.001
2024-11-05 02:04:39,575 - INFO - [diffusion][Epoch 9870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:39,576 - INFO - [diffusion][Epoch 9871] Epoch 9872/12000
2024-11-05 02:04:43,276 - INFO - [diffusion][Epoch 9871] diffusion training Loss: 0.07089810632169247
2024-11-05 02:04:43,278 - INFO - [diffusion][Epoch 9871] diffusion learning rate: 0.001
2024-11-05 02:04:43,280 - INFO - [diffusion][Epoch 9871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:43,281 - INFO - [diffusion][Epoch 9872] Epoch 9873/12000
2024-11-05 02:04:46,663 - INFO - [diffusion][Epoch 9872] diffusion training Loss: 0.06619731616228819
2024-11-05 02:04:46,665 - INFO - [diffusion][Epoch 9872] diffusion learning rate: 0.001
2024-11-05 02:04:46,666 - INFO - [diffusion][Epoch 9872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:46,668 - INFO - [diffusion][Epoch 9873] Epoch 9874/12000
2024-11-05 02:04:50,105 - INFO - [diffusion][Epoch 9873] diffusion training Loss: 0.07618502154946327
2024-11-05 02:04:50,107 - INFO - [diffusion][Epoch 9873] diffusion learning rate: 0.001
2024-11-05 02:04:50,108 - INFO - [diffusion][Epoch 9873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:50,110 - INFO - [diffusion][Epoch 9874] Epoch 9875/12000
2024-11-05 02:04:53,167 - INFO - [diffusion][Epoch 9874] diffusion training Loss: 0.06867324747145176
2024-11-05 02:04:53,169 - INFO - [diffusion][Epoch 9874] diffusion learning rate: 0.001
2024-11-05 02:04:53,170 - INFO - [diffusion][Epoch 9874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:53,172 - INFO - [diffusion][Epoch 9875] Epoch 9876/12000
2024-11-05 02:04:56,437 - INFO - [diffusion][Epoch 9875] diffusion training Loss: 0.071470957249403
2024-11-05 02:04:56,439 - INFO - [diffusion][Epoch 9875] diffusion learning rate: 0.001
2024-11-05 02:04:56,441 - INFO - [diffusion][Epoch 9875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:56,442 - INFO - [diffusion][Epoch 9876] Epoch 9877/12000
2024-11-05 02:05:00,152 - INFO - [diffusion][Epoch 9876] diffusion training Loss: 0.06451265048235655
2024-11-05 02:05:00,154 - INFO - [diffusion][Epoch 9876] diffusion learning rate: 0.001
2024-11-05 02:05:00,156 - INFO - [diffusion][Epoch 9876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:00,157 - INFO - [diffusion][Epoch 9877] Epoch 9878/12000
2024-11-05 02:05:03,544 - INFO - [diffusion][Epoch 9877] diffusion training Loss: 0.06789352186024189
2024-11-05 02:05:03,546 - INFO - [diffusion][Epoch 9877] diffusion learning rate: 0.001
2024-11-05 02:05:03,547 - INFO - [diffusion][Epoch 9877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:03,549 - INFO - [diffusion][Epoch 9878] Epoch 9879/12000
2024-11-05 02:05:06,618 - INFO - [diffusion][Epoch 9878] diffusion training Loss: 0.0697507094591856
2024-11-05 02:05:06,620 - INFO - [diffusion][Epoch 9878] diffusion learning rate: 0.001
2024-11-05 02:05:06,622 - INFO - [diffusion][Epoch 9878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:06,624 - INFO - [diffusion][Epoch 9879] Epoch 9880/12000
2024-11-05 02:05:09,628 - INFO - [diffusion][Epoch 9879] diffusion training Loss: 0.07115696743130684
2024-11-05 02:05:09,630 - INFO - [diffusion][Epoch 9879] diffusion learning rate: 0.001
2024-11-05 02:05:09,631 - INFO - [diffusion][Epoch 9879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:09,633 - INFO - [diffusion][Epoch 9880] Epoch 9881/12000
2024-11-05 02:05:13,239 - INFO - [diffusion][Epoch 9880] diffusion training Loss: 0.0611221669241786
2024-11-05 02:05:13,241 - INFO - [diffusion][Epoch 9880] diffusion learning rate: 0.001
2024-11-05 02:05:13,242 - INFO - [diffusion][Epoch 9880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:13,243 - INFO - [diffusion][Epoch 9881] Epoch 9882/12000
2024-11-05 02:05:16,386 - INFO - [diffusion][Epoch 9881] diffusion training Loss: 0.06637133099138737
2024-11-05 02:05:16,388 - INFO - [diffusion][Epoch 9881] diffusion learning rate: 0.001
2024-11-05 02:05:16,390 - INFO - [diffusion][Epoch 9881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:16,391 - INFO - [diffusion][Epoch 9882] Epoch 9883/12000
2024-11-05 02:05:19,511 - INFO - [diffusion][Epoch 9882] diffusion training Loss: 0.06527947075664997
2024-11-05 02:05:19,514 - INFO - [diffusion][Epoch 9882] diffusion learning rate: 0.001
2024-11-05 02:05:19,516 - INFO - [diffusion][Epoch 9882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:19,517 - INFO - [diffusion][Epoch 9883] Epoch 9884/12000
2024-11-05 02:05:22,659 - INFO - [diffusion][Epoch 9883] diffusion training Loss: 0.06987356767058372
2024-11-05 02:05:22,661 - INFO - [diffusion][Epoch 9883] diffusion learning rate: 0.001
2024-11-05 02:05:22,663 - INFO - [diffusion][Epoch 9883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:22,664 - INFO - [diffusion][Epoch 9884] Epoch 9885/12000
2024-11-05 02:05:26,193 - INFO - [diffusion][Epoch 9884] diffusion training Loss: 0.0680046733468771
2024-11-05 02:05:26,195 - INFO - [diffusion][Epoch 9884] diffusion learning rate: 0.001
2024-11-05 02:05:26,196 - INFO - [diffusion][Epoch 9884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:26,198 - INFO - [diffusion][Epoch 9885] Epoch 9886/12000
2024-11-05 02:05:29,700 - INFO - [diffusion][Epoch 9885] diffusion training Loss: 0.06499023549258709
2024-11-05 02:05:29,701 - INFO - [diffusion][Epoch 9885] diffusion learning rate: 0.001
2024-11-05 02:05:29,703 - INFO - [diffusion][Epoch 9885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:29,704 - INFO - [diffusion][Epoch 9886] Epoch 9887/12000
2024-11-05 02:05:32,770 - INFO - [diffusion][Epoch 9886] diffusion training Loss: 0.06292380206286907
2024-11-05 02:05:32,772 - INFO - [diffusion][Epoch 9886] diffusion learning rate: 0.001
2024-11-05 02:05:32,773 - INFO - [diffusion][Epoch 9886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:32,775 - INFO - [diffusion][Epoch 9887] Epoch 9888/12000
2024-11-05 02:05:35,895 - INFO - [diffusion][Epoch 9887] diffusion training Loss: 0.06425617542117834
2024-11-05 02:05:35,897 - INFO - [diffusion][Epoch 9887] diffusion learning rate: 0.001
2024-11-05 02:05:35,899 - INFO - [diffusion][Epoch 9887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:35,900 - INFO - [diffusion][Epoch 9888] Epoch 9889/12000
2024-11-05 02:05:38,997 - INFO - [diffusion][Epoch 9888] diffusion training Loss: 0.06467458605766296
2024-11-05 02:05:38,999 - INFO - [diffusion][Epoch 9888] diffusion learning rate: 0.001
2024-11-05 02:05:39,001 - INFO - [diffusion][Epoch 9888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:39,002 - INFO - [diffusion][Epoch 9889] Epoch 9890/12000
2024-11-05 02:05:42,531 - INFO - [diffusion][Epoch 9889] diffusion training Loss: 0.06424714718014002
2024-11-05 02:05:42,533 - INFO - [diffusion][Epoch 9889] diffusion learning rate: 0.001
2024-11-05 02:05:42,534 - INFO - [diffusion][Epoch 9889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:42,535 - INFO - [diffusion][Epoch 9890] Epoch 9891/12000
2024-11-05 02:05:46,046 - INFO - [diffusion][Epoch 9890] diffusion training Loss: 0.06766414642333984
2024-11-05 02:05:46,047 - INFO - [diffusion][Epoch 9890] diffusion learning rate: 0.001
2024-11-05 02:05:46,049 - INFO - [diffusion][Epoch 9890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:46,050 - INFO - [diffusion][Epoch 9891] Epoch 9892/12000
2024-11-05 02:05:49,216 - INFO - [diffusion][Epoch 9891] diffusion training Loss: 0.06518044322729111
2024-11-05 02:05:49,218 - INFO - [diffusion][Epoch 9891] diffusion learning rate: 0.001
2024-11-05 02:05:49,219 - INFO - [diffusion][Epoch 9891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:49,221 - INFO - [diffusion][Epoch 9892] Epoch 9893/12000
2024-11-05 02:05:52,469 - INFO - [diffusion][Epoch 9892] diffusion training Loss: 0.06837662775069475
2024-11-05 02:05:52,471 - INFO - [diffusion][Epoch 9892] diffusion learning rate: 0.001
2024-11-05 02:05:52,473 - INFO - [diffusion][Epoch 9892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:52,474 - INFO - [diffusion][Epoch 9893] Epoch 9894/12000
2024-11-05 02:05:55,575 - INFO - [diffusion][Epoch 9893] diffusion training Loss: 0.0647524856030941
2024-11-05 02:05:55,577 - INFO - [diffusion][Epoch 9893] diffusion learning rate: 0.001
2024-11-05 02:05:55,579 - INFO - [diffusion][Epoch 9893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:55,580 - INFO - [diffusion][Epoch 9894] Epoch 9895/12000
2024-11-05 02:05:59,001 - INFO - [diffusion][Epoch 9894] diffusion training Loss: 0.06830715015530586
2024-11-05 02:05:59,003 - INFO - [diffusion][Epoch 9894] diffusion learning rate: 0.001
2024-11-05 02:05:59,005 - INFO - [diffusion][Epoch 9894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:59,006 - INFO - [diffusion][Epoch 9895] Epoch 9896/12000
2024-11-05 02:06:02,624 - INFO - [diffusion][Epoch 9895] diffusion training Loss: 0.06430322397500277
2024-11-05 02:06:02,650 - INFO - [diffusion][Epoch 9895] diffusion learning rate: 0.001
2024-11-05 02:06:02,651 - INFO - [diffusion][Epoch 9895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:02,653 - INFO - [diffusion][Epoch 9896] Epoch 9897/12000
2024-11-05 02:06:06,077 - INFO - [diffusion][Epoch 9896] diffusion training Loss: 0.06298765446990728
2024-11-05 02:06:06,079 - INFO - [diffusion][Epoch 9896] diffusion learning rate: 0.001
2024-11-05 02:06:06,081 - INFO - [diffusion][Epoch 9896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:06,082 - INFO - [diffusion][Epoch 9897] Epoch 9898/12000
2024-11-05 02:06:08,957 - INFO - [diffusion][Epoch 9897] diffusion training Loss: 0.06200533825904131
2024-11-05 02:06:08,960 - INFO - [diffusion][Epoch 9897] diffusion learning rate: 0.001
2024-11-05 02:06:08,961 - INFO - [diffusion][Epoch 9897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:08,962 - INFO - [diffusion][Epoch 9898] Epoch 9899/12000
2024-11-05 02:06:12,128 - INFO - [diffusion][Epoch 9898] diffusion training Loss: 0.0656351875513792
2024-11-05 02:06:12,130 - INFO - [diffusion][Epoch 9898] diffusion learning rate: 0.001
2024-11-05 02:06:12,132 - INFO - [diffusion][Epoch 9898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:12,133 - INFO - [diffusion][Epoch 9899] Epoch 9900/12000
2024-11-05 02:06:15,689 - INFO - [diffusion][Epoch 9899] diffusion training Loss: 0.0669096764177084
2024-11-05 02:06:15,691 - INFO - [diffusion][Epoch 9899] diffusion learning rate: 0.001
2024-11-05 02:06:15,693 - INFO - [diffusion][Epoch 9899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:15,695 - INFO - [diffusion][Epoch 9900] Epoch 9901/12000
2024-11-05 02:06:19,192 - INFO - [diffusion][Epoch 9900] diffusion training Loss: 0.0702305231243372
2024-11-05 02:06:19,194 - INFO - [diffusion][Epoch 9900] diffusion learning rate: 0.001
2024-11-05 02:06:19,196 - INFO - [diffusion][Epoch 9900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:19,198 - INFO - [diffusion][Epoch 9901] Epoch 9902/12000
2024-11-05 02:06:22,238 - INFO - [diffusion][Epoch 9901] diffusion training Loss: 0.06852149963378906
2024-11-05 02:06:22,241 - INFO - [diffusion][Epoch 9901] diffusion learning rate: 0.001
2024-11-05 02:06:22,243 - INFO - [diffusion][Epoch 9901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:22,244 - INFO - [diffusion][Epoch 9902] Epoch 9903/12000
2024-11-05 02:06:25,327 - INFO - [diffusion][Epoch 9902] diffusion training Loss: 0.059559877030551434
2024-11-05 02:06:25,329 - INFO - [diffusion][Epoch 9902] diffusion learning rate: 0.001
2024-11-05 02:06:25,331 - INFO - [diffusion][Epoch 9902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:25,332 - INFO - [diffusion][Epoch 9903] Epoch 9904/12000
2024-11-05 02:06:28,619 - INFO - [diffusion][Epoch 9903] diffusion training Loss: 0.0653175637125969
2024-11-05 02:06:28,620 - INFO - [diffusion][Epoch 9903] diffusion learning rate: 0.001
2024-11-05 02:06:28,622 - INFO - [diffusion][Epoch 9903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:28,623 - INFO - [diffusion][Epoch 9904] Epoch 9905/12000
2024-11-05 02:06:32,034 - INFO - [diffusion][Epoch 9904] diffusion training Loss: 0.06814512237906456
2024-11-05 02:06:32,036 - INFO - [diffusion][Epoch 9904] diffusion learning rate: 0.001
2024-11-05 02:06:32,037 - INFO - [diffusion][Epoch 9904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:32,039 - INFO - [diffusion][Epoch 9905] Epoch 9906/12000
2024-11-05 02:06:35,535 - INFO - [diffusion][Epoch 9905] diffusion training Loss: 0.06351776886731386
2024-11-05 02:06:35,537 - INFO - [diffusion][Epoch 9905] diffusion learning rate: 0.001
2024-11-05 02:06:35,539 - INFO - [diffusion][Epoch 9905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:35,540 - INFO - [diffusion][Epoch 9906] Epoch 9907/12000
2024-11-05 02:06:38,659 - INFO - [diffusion][Epoch 9906] diffusion training Loss: 0.06328993942588568
2024-11-05 02:06:38,661 - INFO - [diffusion][Epoch 9906] diffusion learning rate: 0.001
2024-11-05 02:06:38,663 - INFO - [diffusion][Epoch 9906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:38,664 - INFO - [diffusion][Epoch 9907] Epoch 9908/12000
2024-11-05 02:06:41,820 - INFO - [diffusion][Epoch 9907] diffusion training Loss: 0.06510635744780302
2024-11-05 02:06:41,822 - INFO - [diffusion][Epoch 9907] diffusion learning rate: 0.001
2024-11-05 02:06:41,825 - INFO - [diffusion][Epoch 9907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:41,826 - INFO - [diffusion][Epoch 9908] Epoch 9909/12000
2024-11-05 02:06:44,950 - INFO - [diffusion][Epoch 9908] diffusion training Loss: 0.0638917125761509
2024-11-05 02:06:44,952 - INFO - [diffusion][Epoch 9908] diffusion learning rate: 0.001
2024-11-05 02:06:44,954 - INFO - [diffusion][Epoch 9908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:44,955 - INFO - [diffusion][Epoch 9909] Epoch 9910/12000
2024-11-05 02:06:48,455 - INFO - [diffusion][Epoch 9909] diffusion training Loss: 0.06285958737134933
2024-11-05 02:06:48,457 - INFO - [diffusion][Epoch 9909] diffusion learning rate: 0.001
2024-11-05 02:06:48,458 - INFO - [diffusion][Epoch 9909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:48,460 - INFO - [diffusion][Epoch 9910] Epoch 9911/12000
2024-11-05 02:06:51,947 - INFO - [diffusion][Epoch 9910] diffusion training Loss: 0.06401537358760834
2024-11-05 02:06:51,949 - INFO - [diffusion][Epoch 9910] diffusion learning rate: 0.001
2024-11-05 02:06:51,951 - INFO - [diffusion][Epoch 9910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:51,952 - INFO - [diffusion][Epoch 9911] Epoch 9912/12000
2024-11-05 02:06:55,047 - INFO - [diffusion][Epoch 9911] diffusion training Loss: 0.06626056134700775
2024-11-05 02:06:55,048 - INFO - [diffusion][Epoch 9911] diffusion learning rate: 0.001
2024-11-05 02:06:55,050 - INFO - [diffusion][Epoch 9911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:55,052 - INFO - [diffusion][Epoch 9912] Epoch 9913/12000
2024-11-05 02:06:58,208 - INFO - [diffusion][Epoch 9912] diffusion training Loss: 0.07010626420378685
2024-11-05 02:06:58,210 - INFO - [diffusion][Epoch 9912] diffusion learning rate: 0.001
2024-11-05 02:06:58,212 - INFO - [diffusion][Epoch 9912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:58,213 - INFO - [diffusion][Epoch 9913] Epoch 9914/12000
2024-11-05 02:07:01,937 - INFO - [diffusion][Epoch 9913] diffusion training Loss: 0.06267413590103388
2024-11-05 02:07:01,939 - INFO - [diffusion][Epoch 9913] diffusion learning rate: 0.001
2024-11-05 02:07:01,940 - INFO - [diffusion][Epoch 9913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:01,942 - INFO - [diffusion][Epoch 9914] Epoch 9915/12000
2024-11-05 02:07:05,250 - INFO - [diffusion][Epoch 9914] diffusion training Loss: 0.07269036956131458
2024-11-05 02:07:05,252 - INFO - [diffusion][Epoch 9914] diffusion learning rate: 0.001
2024-11-05 02:07:05,254 - INFO - [diffusion][Epoch 9914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:05,255 - INFO - [diffusion][Epoch 9915] Epoch 9916/12000
2024-11-05 02:07:08,727 - INFO - [diffusion][Epoch 9915] diffusion training Loss: 0.05948633514344692
2024-11-05 02:07:08,729 - INFO - [diffusion][Epoch 9915] diffusion learning rate: 0.001
2024-11-05 02:07:08,732 - INFO - [diffusion][Epoch 9915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:08,733 - INFO - [diffusion][Epoch 9916] Epoch 9917/12000
2024-11-05 02:07:11,879 - INFO - [diffusion][Epoch 9916] diffusion training Loss: 0.07226001285016537
2024-11-05 02:07:11,881 - INFO - [diffusion][Epoch 9916] diffusion learning rate: 0.001
2024-11-05 02:07:11,882 - INFO - [diffusion][Epoch 9916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:11,884 - INFO - [diffusion][Epoch 9917] Epoch 9918/12000
2024-11-05 02:07:14,951 - INFO - [diffusion][Epoch 9917] diffusion training Loss: 0.06347689870744944
2024-11-05 02:07:14,953 - INFO - [diffusion][Epoch 9917] diffusion learning rate: 0.001
2024-11-05 02:07:14,954 - INFO - [diffusion][Epoch 9917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:14,955 - INFO - [diffusion][Epoch 9918] Epoch 9919/12000
2024-11-05 02:07:18,057 - INFO - [diffusion][Epoch 9918] diffusion training Loss: 0.06653751619160175
2024-11-05 02:07:18,059 - INFO - [diffusion][Epoch 9918] diffusion learning rate: 0.001
2024-11-05 02:07:18,061 - INFO - [diffusion][Epoch 9918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:18,062 - INFO - [diffusion][Epoch 9919] Epoch 9920/12000
2024-11-05 02:07:21,609 - INFO - [diffusion][Epoch 9919] diffusion training Loss: 0.062001380138099194
2024-11-05 02:07:21,611 - INFO - [diffusion][Epoch 9919] diffusion learning rate: 0.001
2024-11-05 02:07:21,613 - INFO - [diffusion][Epoch 9919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:21,614 - INFO - [diffusion][Epoch 9920] Epoch 9921/12000
2024-11-05 02:07:25,139 - INFO - [diffusion][Epoch 9920] diffusion training Loss: 0.06677846796810627
2024-11-05 02:07:25,180 - INFO - [diffusion][Epoch 9920] diffusion learning rate: 0.001
2024-11-05 02:07:25,182 - INFO - [diffusion][Epoch 9920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:25,183 - INFO - [diffusion][Epoch 9921] Epoch 9922/12000
2024-11-05 02:07:28,334 - INFO - [diffusion][Epoch 9921] diffusion training Loss: 0.06993892230093479
2024-11-05 02:07:28,336 - INFO - [diffusion][Epoch 9921] diffusion learning rate: 0.001
2024-11-05 02:07:28,338 - INFO - [diffusion][Epoch 9921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:28,339 - INFO - [diffusion][Epoch 9922] Epoch 9923/12000
2024-11-05 02:07:31,440 - INFO - [diffusion][Epoch 9922] diffusion training Loss: 0.06926470156759024
2024-11-05 02:07:31,442 - INFO - [diffusion][Epoch 9922] diffusion learning rate: 0.001
2024-11-05 02:07:31,444 - INFO - [diffusion][Epoch 9922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:31,445 - INFO - [diffusion][Epoch 9923] Epoch 9924/12000
2024-11-05 02:07:34,592 - INFO - [diffusion][Epoch 9923] diffusion training Loss: 0.06902794539928436
2024-11-05 02:07:34,595 - INFO - [diffusion][Epoch 9923] diffusion learning rate: 0.001
2024-11-05 02:07:34,598 - INFO - [diffusion][Epoch 9923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:34,599 - INFO - [diffusion][Epoch 9924] Epoch 9925/12000
2024-11-05 02:07:38,133 - INFO - [diffusion][Epoch 9924] diffusion training Loss: 0.06815162673592567
2024-11-05 02:07:38,135 - INFO - [diffusion][Epoch 9924] diffusion learning rate: 0.001
2024-11-05 02:07:38,137 - INFO - [diffusion][Epoch 9924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:38,139 - INFO - [diffusion][Epoch 9925] Epoch 9926/12000
2024-11-05 02:07:41,608 - INFO - [diffusion][Epoch 9925] diffusion training Loss: 0.07081244885921478
2024-11-05 02:07:41,610 - INFO - [diffusion][Epoch 9925] diffusion learning rate: 0.001
2024-11-05 02:07:41,612 - INFO - [diffusion][Epoch 9925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:41,614 - INFO - [diffusion][Epoch 9926] Epoch 9927/12000
2024-11-05 02:07:44,748 - INFO - [diffusion][Epoch 9926] diffusion training Loss: 0.06365989148616791
2024-11-05 02:07:44,750 - INFO - [diffusion][Epoch 9926] diffusion learning rate: 0.001
2024-11-05 02:07:44,752 - INFO - [diffusion][Epoch 9926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:44,753 - INFO - [diffusion][Epoch 9927] Epoch 9928/12000
2024-11-05 02:07:47,900 - INFO - [diffusion][Epoch 9927] diffusion training Loss: 0.06791691295802593
2024-11-05 02:07:47,902 - INFO - [diffusion][Epoch 9927] diffusion learning rate: 0.001
2024-11-05 02:07:47,903 - INFO - [diffusion][Epoch 9927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:47,905 - INFO - [diffusion][Epoch 9928] Epoch 9929/12000
2024-11-05 02:07:51,107 - INFO - [diffusion][Epoch 9928] diffusion training Loss: 0.0672041717916727
2024-11-05 02:07:51,108 - INFO - [diffusion][Epoch 9928] diffusion learning rate: 0.001
2024-11-05 02:07:51,110 - INFO - [diffusion][Epoch 9928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:51,111 - INFO - [diffusion][Epoch 9929] Epoch 9930/12000
2024-11-05 02:07:54,822 - INFO - [diffusion][Epoch 9929] diffusion training Loss: 0.06305836234241724
2024-11-05 02:07:54,824 - INFO - [diffusion][Epoch 9929] diffusion learning rate: 0.001
2024-11-05 02:07:54,826 - INFO - [diffusion][Epoch 9929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:54,827 - INFO - [diffusion][Epoch 9930] Epoch 9931/12000
2024-11-05 02:07:57,953 - INFO - [diffusion][Epoch 9930] diffusion training Loss: 0.06207145005464554
2024-11-05 02:07:57,955 - INFO - [diffusion][Epoch 9930] diffusion learning rate: 0.001
2024-11-05 02:07:57,957 - INFO - [diffusion][Epoch 9930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:57,959 - INFO - [diffusion][Epoch 9931] Epoch 9932/12000
2024-11-05 02:08:01,129 - INFO - [diffusion][Epoch 9931] diffusion training Loss: 0.06681731529533863
2024-11-05 02:08:01,131 - INFO - [diffusion][Epoch 9931] diffusion learning rate: 0.001
2024-11-05 02:08:01,133 - INFO - [diffusion][Epoch 9931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:01,135 - INFO - [diffusion][Epoch 9932] Epoch 9933/12000
2024-11-05 02:08:04,219 - INFO - [diffusion][Epoch 9932] diffusion training Loss: 0.06252072285860777
2024-11-05 02:08:04,221 - INFO - [diffusion][Epoch 9932] diffusion learning rate: 0.001
2024-11-05 02:08:04,222 - INFO - [diffusion][Epoch 9932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:04,224 - INFO - [diffusion][Epoch 9933] Epoch 9934/12000
2024-11-05 02:08:07,718 - INFO - [diffusion][Epoch 9933] diffusion training Loss: 0.06683934759348631
2024-11-05 02:08:07,720 - INFO - [diffusion][Epoch 9933] diffusion learning rate: 0.001
2024-11-05 02:08:07,722 - INFO - [diffusion][Epoch 9933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:07,724 - INFO - [diffusion][Epoch 9934] Epoch 9935/12000
2024-11-05 02:08:11,801 - INFO - [diffusion][Epoch 9934] diffusion training Loss: 0.060821291990578175
2024-11-05 02:08:11,803 - INFO - [diffusion][Epoch 9934] diffusion learning rate: 0.001
2024-11-05 02:08:11,830 - INFO - [diffusion][Epoch 9934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:11,832 - INFO - [diffusion][Epoch 9935] Epoch 9936/12000
2024-11-05 02:08:15,017 - INFO - [diffusion][Epoch 9935] diffusion training Loss: 0.0658515514805913
2024-11-05 02:08:15,020 - INFO - [diffusion][Epoch 9935] diffusion learning rate: 0.001
2024-11-05 02:08:15,022 - INFO - [diffusion][Epoch 9935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:15,023 - INFO - [diffusion][Epoch 9936] Epoch 9937/12000
2024-11-05 02:08:18,138 - INFO - [diffusion][Epoch 9936] diffusion training Loss: 0.07218614779412746
2024-11-05 02:08:18,141 - INFO - [diffusion][Epoch 9936] diffusion learning rate: 0.001
2024-11-05 02:08:18,143 - INFO - [diffusion][Epoch 9936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:18,145 - INFO - [diffusion][Epoch 9937] Epoch 9938/12000
2024-11-05 02:08:21,252 - INFO - [diffusion][Epoch 9937] diffusion training Loss: 0.06696703471243382
2024-11-05 02:08:21,254 - INFO - [diffusion][Epoch 9937] diffusion learning rate: 0.001
2024-11-05 02:08:21,255 - INFO - [diffusion][Epoch 9937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:21,257 - INFO - [diffusion][Epoch 9938] Epoch 9939/12000
2024-11-05 02:08:24,756 - INFO - [diffusion][Epoch 9938] diffusion training Loss: 0.06685722712427378
2024-11-05 02:08:24,758 - INFO - [diffusion][Epoch 9938] diffusion learning rate: 0.001
2024-11-05 02:08:24,760 - INFO - [diffusion][Epoch 9938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:24,761 - INFO - [diffusion][Epoch 9939] Epoch 9940/12000
2024-11-05 02:08:28,304 - INFO - [diffusion][Epoch 9939] diffusion training Loss: 0.06688148155808449
2024-11-05 02:08:28,306 - INFO - [diffusion][Epoch 9939] diffusion learning rate: 0.001
2024-11-05 02:08:28,334 - INFO - [diffusion][Epoch 9939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:28,335 - INFO - [diffusion][Epoch 9940] Epoch 9941/12000
2024-11-05 02:08:31,400 - INFO - [diffusion][Epoch 9940] diffusion training Loss: 0.06840059347450733
2024-11-05 02:08:31,402 - INFO - [diffusion][Epoch 9940] diffusion learning rate: 0.001
2024-11-05 02:08:31,403 - INFO - [diffusion][Epoch 9940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:31,405 - INFO - [diffusion][Epoch 9941] Epoch 9942/12000
2024-11-05 02:08:34,420 - INFO - [diffusion][Epoch 9941] diffusion training Loss: 0.0686922874301672
2024-11-05 02:08:34,422 - INFO - [diffusion][Epoch 9941] diffusion learning rate: 0.001
2024-11-05 02:08:34,424 - INFO - [diffusion][Epoch 9941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:34,425 - INFO - [diffusion][Epoch 9942] Epoch 9943/12000
2024-11-05 02:08:37,854 - INFO - [diffusion][Epoch 9942] diffusion training Loss: 0.07076103612780571
2024-11-05 02:08:37,856 - INFO - [diffusion][Epoch 9942] diffusion learning rate: 0.001
2024-11-05 02:08:37,858 - INFO - [diffusion][Epoch 9942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:37,859 - INFO - [diffusion][Epoch 9943] Epoch 9944/12000
2024-11-05 02:08:41,449 - INFO - [diffusion][Epoch 9943] diffusion training Loss: 0.0631116721779108
2024-11-05 02:08:41,451 - INFO - [diffusion][Epoch 9943] diffusion learning rate: 0.001
2024-11-05 02:08:41,453 - INFO - [diffusion][Epoch 9943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:41,454 - INFO - [diffusion][Epoch 9944] Epoch 9945/12000
2024-11-05 02:08:44,583 - INFO - [diffusion][Epoch 9944] diffusion training Loss: 0.062010228633880615
2024-11-05 02:08:44,585 - INFO - [diffusion][Epoch 9944] diffusion learning rate: 0.001
2024-11-05 02:08:44,586 - INFO - [diffusion][Epoch 9944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:44,588 - INFO - [diffusion][Epoch 9945] Epoch 9946/12000
2024-11-05 02:08:47,674 - INFO - [diffusion][Epoch 9945] diffusion training Loss: 0.06880053039640188
2024-11-05 02:08:47,675 - INFO - [diffusion][Epoch 9945] diffusion learning rate: 0.001
2024-11-05 02:08:47,677 - INFO - [diffusion][Epoch 9945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:47,678 - INFO - [diffusion][Epoch 9946] Epoch 9947/12000
2024-11-05 02:08:50,904 - INFO - [diffusion][Epoch 9946] diffusion training Loss: 0.0717375110834837
2024-11-05 02:08:50,907 - INFO - [diffusion][Epoch 9946] diffusion learning rate: 0.001
2024-11-05 02:08:50,909 - INFO - [diffusion][Epoch 9946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:50,911 - INFO - [diffusion][Epoch 9947] Epoch 9948/12000
2024-11-05 02:08:54,598 - INFO - [diffusion][Epoch 9947] diffusion training Loss: 0.0657016932964325
2024-11-05 02:08:54,600 - INFO - [diffusion][Epoch 9947] diffusion learning rate: 0.001
2024-11-05 02:08:54,619 - INFO - [diffusion][Epoch 9947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:54,620 - INFO - [diffusion][Epoch 9948] Epoch 9949/12000
2024-11-05 02:08:58,068 - INFO - [diffusion][Epoch 9948] diffusion training Loss: 0.06326197646558285
2024-11-05 02:08:58,070 - INFO - [diffusion][Epoch 9948] diffusion learning rate: 0.001
2024-11-05 02:08:58,072 - INFO - [diffusion][Epoch 9948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:58,074 - INFO - [diffusion][Epoch 9949] Epoch 9950/12000
2024-11-05 02:09:01,186 - INFO - [diffusion][Epoch 9949] diffusion training Loss: 0.07207011245191097
2024-11-05 02:09:01,187 - INFO - [diffusion][Epoch 9949] diffusion learning rate: 0.001
2024-11-05 02:09:01,189 - INFO - [diffusion][Epoch 9949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:01,190 - INFO - [diffusion][Epoch 9950] Epoch 9951/12000
2024-11-05 02:09:04,045 - INFO - [diffusion][Epoch 9950] diffusion training Loss: 0.06454137060791254
2024-11-05 02:09:04,047 - INFO - [diffusion][Epoch 9950] diffusion learning rate: 0.001
2024-11-05 02:09:04,048 - INFO - [diffusion][Epoch 9950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:04,050 - INFO - [diffusion][Epoch 9951] Epoch 9952/12000
2024-11-05 02:09:07,512 - INFO - [diffusion][Epoch 9951] diffusion training Loss: 0.07275972329080105
2024-11-05 02:09:07,514 - INFO - [diffusion][Epoch 9951] diffusion learning rate: 0.001
2024-11-05 02:09:07,516 - INFO - [diffusion][Epoch 9951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:07,517 - INFO - [diffusion][Epoch 9952] Epoch 9953/12000
2024-11-05 02:09:10,913 - INFO - [diffusion][Epoch 9952] diffusion training Loss: 0.07173019275069237
2024-11-05 02:09:10,915 - INFO - [diffusion][Epoch 9952] diffusion learning rate: 0.001
2024-11-05 02:09:10,917 - INFO - [diffusion][Epoch 9952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:10,918 - INFO - [diffusion][Epoch 9953] Epoch 9954/12000
2024-11-05 02:09:14,645 - INFO - [diffusion][Epoch 9953] diffusion training Loss: 0.06384115014225245
2024-11-05 02:09:14,647 - INFO - [diffusion][Epoch 9953] diffusion learning rate: 0.001
2024-11-05 02:09:14,648 - INFO - [diffusion][Epoch 9953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:14,650 - INFO - [diffusion][Epoch 9954] Epoch 9955/12000
2024-11-05 02:09:17,755 - INFO - [diffusion][Epoch 9954] diffusion training Loss: 0.06803913414478302
2024-11-05 02:09:17,757 - INFO - [diffusion][Epoch 9954] diffusion learning rate: 0.001
2024-11-05 02:09:17,758 - INFO - [diffusion][Epoch 9954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:17,760 - INFO - [diffusion][Epoch 9955] Epoch 9956/12000
2024-11-05 02:09:20,836 - INFO - [diffusion][Epoch 9955] diffusion training Loss: 0.06298679113388062
2024-11-05 02:09:20,838 - INFO - [diffusion][Epoch 9955] diffusion learning rate: 0.001
2024-11-05 02:09:20,840 - INFO - [diffusion][Epoch 9955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:20,841 - INFO - [diffusion][Epoch 9956] Epoch 9957/12000
2024-11-05 02:09:24,331 - INFO - [diffusion][Epoch 9956] diffusion training Loss: 0.06741968262940645
2024-11-05 02:09:24,332 - INFO - [diffusion][Epoch 9956] diffusion learning rate: 0.001
2024-11-05 02:09:24,334 - INFO - [diffusion][Epoch 9956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:24,336 - INFO - [diffusion][Epoch 9957] Epoch 9958/12000
2024-11-05 02:09:27,890 - INFO - [diffusion][Epoch 9957] diffusion training Loss: 0.07509946264326572
2024-11-05 02:09:27,892 - INFO - [diffusion][Epoch 9957] diffusion learning rate: 0.001
2024-11-05 02:09:27,893 - INFO - [diffusion][Epoch 9957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:27,895 - INFO - [diffusion][Epoch 9958] Epoch 9959/12000
2024-11-05 02:09:30,592 - INFO - [diffusion][Epoch 9958] diffusion training Loss: 0.06971093639731407
2024-11-05 02:09:30,595 - INFO - [diffusion][Epoch 9958] diffusion learning rate: 0.001
2024-11-05 02:09:30,596 - INFO - [diffusion][Epoch 9958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:30,599 - INFO - [diffusion][Epoch 9959] Epoch 9960/12000
2024-11-05 02:09:33,739 - INFO - [diffusion][Epoch 9959] diffusion training Loss: 0.06576662603765726
2024-11-05 02:09:33,741 - INFO - [diffusion][Epoch 9959] diffusion learning rate: 0.001
2024-11-05 02:09:33,742 - INFO - [diffusion][Epoch 9959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:33,744 - INFO - [diffusion][Epoch 9960] Epoch 9961/12000
2024-11-05 02:09:37,284 - INFO - [diffusion][Epoch 9960] diffusion training Loss: 0.07531030662357807
2024-11-05 02:09:37,287 - INFO - [diffusion][Epoch 9960] diffusion learning rate: 0.001
2024-11-05 02:09:37,289 - INFO - [diffusion][Epoch 9960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:37,290 - INFO - [diffusion][Epoch 9961] Epoch 9962/12000
2024-11-05 02:09:40,826 - INFO - [diffusion][Epoch 9961] diffusion training Loss: 0.06558872945606709
2024-11-05 02:09:40,828 - INFO - [diffusion][Epoch 9961] diffusion learning rate: 0.001
2024-11-05 02:09:40,829 - INFO - [diffusion][Epoch 9961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:40,830 - INFO - [diffusion][Epoch 9962] Epoch 9963/12000
2024-11-05 02:09:43,955 - INFO - [diffusion][Epoch 9962] diffusion training Loss: 0.06913266703486443
2024-11-05 02:09:43,957 - INFO - [diffusion][Epoch 9962] diffusion learning rate: 0.001
2024-11-05 02:09:43,959 - INFO - [diffusion][Epoch 9962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:43,960 - INFO - [diffusion][Epoch 9963] Epoch 9964/12000
2024-11-05 02:09:47,094 - INFO - [diffusion][Epoch 9963] diffusion training Loss: 0.06791052594780922
2024-11-05 02:09:47,096 - INFO - [diffusion][Epoch 9963] diffusion learning rate: 0.001
2024-11-05 02:09:47,098 - INFO - [diffusion][Epoch 9963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:47,099 - INFO - [diffusion][Epoch 9964] Epoch 9965/12000
2024-11-05 02:09:50,342 - INFO - [diffusion][Epoch 9964] diffusion training Loss: 0.07024631462991238
2024-11-05 02:09:50,344 - INFO - [diffusion][Epoch 9964] diffusion learning rate: 0.001
2024-11-05 02:09:50,346 - INFO - [diffusion][Epoch 9964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:50,347 - INFO - [diffusion][Epoch 9965] Epoch 9966/12000
2024-11-05 02:09:53,995 - INFO - [diffusion][Epoch 9965] diffusion training Loss: 0.06572698801755905
2024-11-05 02:09:53,997 - INFO - [diffusion][Epoch 9965] diffusion learning rate: 0.001
2024-11-05 02:09:53,999 - INFO - [diffusion][Epoch 9965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:54,000 - INFO - [diffusion][Epoch 9966] Epoch 9967/12000
2024-11-05 02:09:57,351 - INFO - [diffusion][Epoch 9966] diffusion training Loss: 0.06579828262329102
2024-11-05 02:09:57,353 - INFO - [diffusion][Epoch 9966] diffusion learning rate: 0.001
2024-11-05 02:09:57,355 - INFO - [diffusion][Epoch 9966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:57,357 - INFO - [diffusion][Epoch 9967] Epoch 9968/12000
2024-11-05 02:10:00,209 - INFO - [diffusion][Epoch 9967] diffusion training Loss: 0.06475018989294767
2024-11-05 02:10:00,211 - INFO - [diffusion][Epoch 9967] diffusion learning rate: 0.001
2024-11-05 02:10:00,243 - INFO - [diffusion][Epoch 9967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:00,244 - INFO - [diffusion][Epoch 9968] Epoch 9969/12000
2024-11-05 02:10:03,299 - INFO - [diffusion][Epoch 9968] diffusion training Loss: 0.07063363678753376
2024-11-05 02:10:03,302 - INFO - [diffusion][Epoch 9968] diffusion learning rate: 0.001
2024-11-05 02:10:03,304 - INFO - [diffusion][Epoch 9968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:03,305 - INFO - [diffusion][Epoch 9969] Epoch 9970/12000
2024-11-05 02:10:06,768 - INFO - [diffusion][Epoch 9969] diffusion training Loss: 0.062183741480112076
2024-11-05 02:10:06,770 - INFO - [diffusion][Epoch 9969] diffusion learning rate: 0.001
2024-11-05 02:10:06,771 - INFO - [diffusion][Epoch 9969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:06,772 - INFO - [diffusion][Epoch 9970] Epoch 9971/12000
2024-11-05 02:10:09,954 - INFO - [diffusion][Epoch 9970] diffusion training Loss: 0.07166166137903929
2024-11-05 02:10:09,956 - INFO - [diffusion][Epoch 9970] diffusion learning rate: 0.001
2024-11-05 02:10:09,957 - INFO - [diffusion][Epoch 9970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:09,958 - INFO - [diffusion][Epoch 9971] Epoch 9972/12000
2024-11-05 02:10:13,052 - INFO - [diffusion][Epoch 9971] diffusion training Loss: 0.06287534348666668
2024-11-05 02:10:13,054 - INFO - [diffusion][Epoch 9971] diffusion learning rate: 0.001
2024-11-05 02:10:13,056 - INFO - [diffusion][Epoch 9971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:13,057 - INFO - [diffusion][Epoch 9972] Epoch 9973/12000
2024-11-05 02:10:16,204 - INFO - [diffusion][Epoch 9972] diffusion training Loss: 0.06726398225873709
2024-11-05 02:10:16,206 - INFO - [diffusion][Epoch 9972] diffusion learning rate: 0.001
2024-11-05 02:10:16,208 - INFO - [diffusion][Epoch 9972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:16,209 - INFO - [diffusion][Epoch 9973] Epoch 9974/12000
2024-11-05 02:10:19,764 - INFO - [diffusion][Epoch 9973] diffusion training Loss: 0.0670514665544033
2024-11-05 02:10:19,766 - INFO - [diffusion][Epoch 9973] diffusion learning rate: 0.001
2024-11-05 02:10:19,768 - INFO - [diffusion][Epoch 9973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:19,769 - INFO - [diffusion][Epoch 9974] Epoch 9975/12000
2024-11-05 02:10:23,296 - INFO - [diffusion][Epoch 9974] diffusion training Loss: 0.06532527506351471
2024-11-05 02:10:23,298 - INFO - [diffusion][Epoch 9974] diffusion learning rate: 0.001
2024-11-05 02:10:23,299 - INFO - [diffusion][Epoch 9974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:23,300 - INFO - [diffusion][Epoch 9975] Epoch 9976/12000
2024-11-05 02:10:26,212 - INFO - [diffusion][Epoch 9975] diffusion training Loss: 0.06307562999427319
2024-11-05 02:10:26,214 - INFO - [diffusion][Epoch 9975] diffusion learning rate: 0.001
2024-11-05 02:10:26,216 - INFO - [diffusion][Epoch 9975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:26,217 - INFO - [diffusion][Epoch 9976] Epoch 9977/12000
2024-11-05 02:10:29,227 - INFO - [diffusion][Epoch 9976] diffusion training Loss: 0.07343737967312336
2024-11-05 02:10:29,229 - INFO - [diffusion][Epoch 9976] diffusion learning rate: 0.001
2024-11-05 02:10:29,230 - INFO - [diffusion][Epoch 9976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:29,231 - INFO - [diffusion][Epoch 9977] Epoch 9978/12000
2024-11-05 02:10:32,769 - INFO - [diffusion][Epoch 9977] diffusion training Loss: 0.07313113659620285
2024-11-05 02:10:32,772 - INFO - [diffusion][Epoch 9977] diffusion learning rate: 0.001
2024-11-05 02:10:32,774 - INFO - [diffusion][Epoch 9977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:32,775 - INFO - [diffusion][Epoch 9978] Epoch 9979/12000
2024-11-05 02:10:36,333 - INFO - [diffusion][Epoch 9978] diffusion training Loss: 0.06859911605715752
2024-11-05 02:10:36,335 - INFO - [diffusion][Epoch 9978] diffusion learning rate: 0.001
2024-11-05 02:10:36,337 - INFO - [diffusion][Epoch 9978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:36,338 - INFO - [diffusion][Epoch 9979] Epoch 9980/12000
2024-11-05 02:10:39,512 - INFO - [diffusion][Epoch 9979] diffusion training Loss: 0.06554228626191616
2024-11-05 02:10:39,514 - INFO - [diffusion][Epoch 9979] diffusion learning rate: 0.001
2024-11-05 02:10:39,515 - INFO - [diffusion][Epoch 9979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:39,517 - INFO - [diffusion][Epoch 9980] Epoch 9981/12000
2024-11-05 02:10:42,568 - INFO - [diffusion][Epoch 9980] diffusion training Loss: 0.06369486730545759
2024-11-05 02:10:42,570 - INFO - [diffusion][Epoch 9980] diffusion learning rate: 0.001
2024-11-05 02:10:42,572 - INFO - [diffusion][Epoch 9980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:42,573 - INFO - [diffusion][Epoch 9981] Epoch 9982/12000
2024-11-05 02:10:45,684 - INFO - [diffusion][Epoch 9981] diffusion training Loss: 0.069032764993608
2024-11-05 02:10:45,686 - INFO - [diffusion][Epoch 9981] diffusion learning rate: 0.001
2024-11-05 02:10:45,688 - INFO - [diffusion][Epoch 9981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:45,689 - INFO - [diffusion][Epoch 9982] Epoch 9983/12000
2024-11-05 02:10:49,167 - INFO - [diffusion][Epoch 9982] diffusion training Loss: 0.06832141429185867
2024-11-05 02:10:49,169 - INFO - [diffusion][Epoch 9982] diffusion learning rate: 0.001
2024-11-05 02:10:49,171 - INFO - [diffusion][Epoch 9982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:49,172 - INFO - [diffusion][Epoch 9983] Epoch 9984/12000
2024-11-05 02:10:52,429 - INFO - [diffusion][Epoch 9983] diffusion training Loss: 0.0633470993489027
2024-11-05 02:10:52,431 - INFO - [diffusion][Epoch 9983] diffusion learning rate: 0.001
2024-11-05 02:10:52,433 - INFO - [diffusion][Epoch 9983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:52,434 - INFO - [diffusion][Epoch 9984] Epoch 9985/12000
2024-11-05 02:10:55,565 - INFO - [diffusion][Epoch 9984] diffusion training Loss: 0.07106299512088299
2024-11-05 02:10:55,568 - INFO - [diffusion][Epoch 9984] diffusion learning rate: 0.001
2024-11-05 02:10:55,571 - INFO - [diffusion][Epoch 9984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:55,572 - INFO - [diffusion][Epoch 9985] Epoch 9986/12000
2024-11-05 02:10:58,601 - INFO - [diffusion][Epoch 9985] diffusion training Loss: 0.06428728438913822
2024-11-05 02:10:58,603 - INFO - [diffusion][Epoch 9985] diffusion learning rate: 0.001
2024-11-05 02:10:58,605 - INFO - [diffusion][Epoch 9985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:58,606 - INFO - [diffusion][Epoch 9986] Epoch 9987/12000
2024-11-05 02:11:01,959 - INFO - [diffusion][Epoch 9986] diffusion training Loss: 0.06684902869164944
2024-11-05 02:11:01,961 - INFO - [diffusion][Epoch 9986] diffusion learning rate: 0.001
2024-11-05 02:11:01,963 - INFO - [diffusion][Epoch 9986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:01,964 - INFO - [diffusion][Epoch 9987] Epoch 9988/12000
2024-11-05 02:11:05,015 - INFO - [diffusion][Epoch 9987] diffusion training Loss: 0.06560121010988951
2024-11-05 02:11:05,016 - INFO - [diffusion][Epoch 9987] diffusion learning rate: 0.001
2024-11-05 02:11:05,018 - INFO - [diffusion][Epoch 9987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:05,019 - INFO - [diffusion][Epoch 9988] Epoch 9989/12000
2024-11-05 02:11:08,140 - INFO - [diffusion][Epoch 9988] diffusion training Loss: 0.06500961072742939
2024-11-05 02:11:08,142 - INFO - [diffusion][Epoch 9988] diffusion learning rate: 0.001
2024-11-05 02:11:08,144 - INFO - [diffusion][Epoch 9988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:08,145 - INFO - [diffusion][Epoch 9989] Epoch 9990/12000
2024-11-05 02:11:11,396 - INFO - [diffusion][Epoch 9989] diffusion training Loss: 0.06293125078082085
2024-11-05 02:11:11,399 - INFO - [diffusion][Epoch 9989] diffusion learning rate: 0.001
2024-11-05 02:11:11,401 - INFO - [diffusion][Epoch 9989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:11,402 - INFO - [diffusion][Epoch 9990] Epoch 9991/12000
2024-11-05 02:11:15,074 - INFO - [diffusion][Epoch 9990] diffusion training Loss: 0.06652015633881092
2024-11-05 02:11:15,075 - INFO - [diffusion][Epoch 9990] diffusion learning rate: 0.001
2024-11-05 02:11:15,077 - INFO - [diffusion][Epoch 9990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:15,079 - INFO - [diffusion][Epoch 9991] Epoch 9992/12000
2024-11-05 02:11:18,310 - INFO - [diffusion][Epoch 9991] diffusion training Loss: 0.07265766710042953
2024-11-05 02:11:18,312 - INFO - [diffusion][Epoch 9991] diffusion learning rate: 0.001
2024-11-05 02:11:18,314 - INFO - [diffusion][Epoch 9991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:18,316 - INFO - [diffusion][Epoch 9992] Epoch 9993/12000
2024-11-05 02:11:21,222 - INFO - [diffusion][Epoch 9992] diffusion training Loss: 0.06220630742609501
2024-11-05 02:11:21,224 - INFO - [diffusion][Epoch 9992] diffusion learning rate: 0.001
2024-11-05 02:11:21,226 - INFO - [diffusion][Epoch 9992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:21,227 - INFO - [diffusion][Epoch 9993] Epoch 9994/12000
2024-11-05 02:11:24,290 - INFO - [diffusion][Epoch 9993] diffusion training Loss: 0.06449601519852877
2024-11-05 02:11:24,292 - INFO - [diffusion][Epoch 9993] diffusion learning rate: 0.001
2024-11-05 02:11:24,294 - INFO - [diffusion][Epoch 9993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:24,295 - INFO - [diffusion][Epoch 9994] Epoch 9995/12000
2024-11-05 02:11:27,712 - INFO - [diffusion][Epoch 9994] diffusion training Loss: 0.06338901910930872
2024-11-05 02:11:27,714 - INFO - [diffusion][Epoch 9994] diffusion learning rate: 0.001
2024-11-05 02:11:27,715 - INFO - [diffusion][Epoch 9994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:27,717 - INFO - [diffusion][Epoch 9995] Epoch 9996/12000
2024-11-05 02:11:30,752 - INFO - [diffusion][Epoch 9995] diffusion training Loss: 0.0688354391604662
2024-11-05 02:11:30,754 - INFO - [diffusion][Epoch 9995] diffusion learning rate: 0.001
2024-11-05 02:11:30,755 - INFO - [diffusion][Epoch 9995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:30,757 - INFO - [diffusion][Epoch 9996] Epoch 9997/12000
2024-11-05 02:11:33,603 - INFO - [diffusion][Epoch 9996] diffusion training Loss: 0.06401227414608002
2024-11-05 02:11:33,606 - INFO - [diffusion][Epoch 9996] diffusion learning rate: 0.001
2024-11-05 02:11:33,607 - INFO - [diffusion][Epoch 9996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:33,609 - INFO - [diffusion][Epoch 9997] Epoch 9998/12000
2024-11-05 02:11:36,912 - INFO - [diffusion][Epoch 9997] diffusion training Loss: 0.06989975273609161
2024-11-05 02:11:36,914 - INFO - [diffusion][Epoch 9997] diffusion learning rate: 0.001
2024-11-05 02:11:36,916 - INFO - [diffusion][Epoch 9997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:36,917 - INFO - [diffusion][Epoch 9998] Epoch 9999/12000
2024-11-05 02:11:40,499 - INFO - [diffusion][Epoch 9998] diffusion training Loss: 0.07175946608185768
2024-11-05 02:11:40,502 - INFO - [diffusion][Epoch 9998] diffusion learning rate: 0.001
2024-11-05 02:11:40,504 - INFO - [diffusion][Epoch 9998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:40,506 - INFO - [diffusion][Epoch 9999] Epoch 10000/12000
2024-11-05 02:11:43,575 - INFO - [diffusion][Epoch 9999] diffusion training Loss: 0.0650953371077776
2024-11-05 02:11:43,577 - INFO - [diffusion][Epoch 9999] diffusion learning rate: 0.001
2024-11-05 02:11:43,674 - INFO - [diffusion][Epoch 9999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:43,676 - INFO - [diffusion][Epoch 10000] Epoch 10001/12000
2024-11-05 02:11:46,621 - INFO - [diffusion][Epoch 10000] diffusion training Loss: 0.0689474307000637
2024-11-05 02:11:46,623 - INFO - [diffusion][Epoch 10000] diffusion learning rate: 0.001
2024-11-05 02:11:46,625 - INFO - [diffusion][Epoch 10000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:46,626 - INFO - [diffusion][Epoch 10001] Epoch 10002/12000
2024-11-05 02:11:49,883 - INFO - [diffusion][Epoch 10001] diffusion training Loss: 0.07159677520394325
2024-11-05 02:11:49,885 - INFO - [diffusion][Epoch 10001] diffusion learning rate: 0.001
2024-11-05 02:11:49,887 - INFO - [diffusion][Epoch 10001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:49,888 - INFO - [diffusion][Epoch 10002] Epoch 10003/12000
2024-11-05 02:11:53,465 - INFO - [diffusion][Epoch 10002] diffusion training Loss: 0.06319322809576988
2024-11-05 02:11:53,467 - INFO - [diffusion][Epoch 10002] diffusion learning rate: 0.001
2024-11-05 02:11:53,469 - INFO - [diffusion][Epoch 10002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:53,471 - INFO - [diffusion][Epoch 10003] Epoch 10004/12000
2024-11-05 02:11:56,592 - INFO - [diffusion][Epoch 10003] diffusion training Loss: 0.06588158011436462
2024-11-05 02:11:56,594 - INFO - [diffusion][Epoch 10003] diffusion learning rate: 0.001
2024-11-05 02:11:56,596 - INFO - [diffusion][Epoch 10003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:56,597 - INFO - [diffusion][Epoch 10004] Epoch 10005/12000
2024-11-05 02:11:59,673 - INFO - [diffusion][Epoch 10004] diffusion training Loss: 0.06886047311127186
2024-11-05 02:11:59,675 - INFO - [diffusion][Epoch 10004] diffusion learning rate: 0.001
2024-11-05 02:11:59,676 - INFO - [diffusion][Epoch 10004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:59,677 - INFO - [diffusion][Epoch 10005] Epoch 10006/12000
2024-11-05 02:12:02,848 - INFO - [diffusion][Epoch 10005] diffusion training Loss: 0.06662501394748688
2024-11-05 02:12:02,850 - INFO - [diffusion][Epoch 10005] diffusion learning rate: 0.001
2024-11-05 02:12:02,851 - INFO - [diffusion][Epoch 10005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:02,853 - INFO - [diffusion][Epoch 10006] Epoch 10007/12000
2024-11-05 02:12:06,427 - INFO - [diffusion][Epoch 10006] diffusion training Loss: 0.06394797377288342
2024-11-05 02:12:06,429 - INFO - [diffusion][Epoch 10006] diffusion learning rate: 0.001
2024-11-05 02:12:06,430 - INFO - [diffusion][Epoch 10006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:06,432 - INFO - [diffusion][Epoch 10007] Epoch 10008/12000
2024-11-05 02:12:09,755 - INFO - [diffusion][Epoch 10007] diffusion training Loss: 0.06821290031075478
2024-11-05 02:12:09,757 - INFO - [diffusion][Epoch 10007] diffusion learning rate: 0.001
2024-11-05 02:12:09,759 - INFO - [diffusion][Epoch 10007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:09,760 - INFO - [diffusion][Epoch 10008] Epoch 10009/12000
2024-11-05 02:12:13,017 - INFO - [diffusion][Epoch 10008] diffusion training Loss: 0.06619450356811285
2024-11-05 02:12:13,019 - INFO - [diffusion][Epoch 10008] diffusion learning rate: 0.001
2024-11-05 02:12:13,021 - INFO - [diffusion][Epoch 10008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:13,022 - INFO - [diffusion][Epoch 10009] Epoch 10010/12000
2024-11-05 02:12:16,167 - INFO - [diffusion][Epoch 10009] diffusion training Loss: 0.0637939041480422
2024-11-05 02:12:16,170 - INFO - [diffusion][Epoch 10009] diffusion learning rate: 0.001
2024-11-05 02:12:16,171 - INFO - [diffusion][Epoch 10009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:16,173 - INFO - [diffusion][Epoch 10010] Epoch 10011/12000
2024-11-05 02:12:19,336 - INFO - [diffusion][Epoch 10010] diffusion training Loss: 0.06725804414600134
2024-11-05 02:12:19,338 - INFO - [diffusion][Epoch 10010] diffusion learning rate: 0.001
2024-11-05 02:12:19,340 - INFO - [diffusion][Epoch 10010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:19,341 - INFO - [diffusion][Epoch 10011] Epoch 10012/12000
2024-11-05 02:12:22,980 - INFO - [diffusion][Epoch 10011] diffusion training Loss: 0.0682828538119793
2024-11-05 02:12:22,982 - INFO - [diffusion][Epoch 10011] diffusion learning rate: 0.001
2024-11-05 02:12:22,984 - INFO - [diffusion][Epoch 10011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:22,985 - INFO - [diffusion][Epoch 10012] Epoch 10013/12000
2024-11-05 02:12:26,353 - INFO - [diffusion][Epoch 10012] diffusion training Loss: 0.06876765470951796
2024-11-05 02:12:26,355 - INFO - [diffusion][Epoch 10012] diffusion learning rate: 0.001
2024-11-05 02:12:26,358 - INFO - [diffusion][Epoch 10012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:26,360 - INFO - [diffusion][Epoch 10013] Epoch 10014/12000
2024-11-05 02:12:29,518 - INFO - [diffusion][Epoch 10013] diffusion training Loss: 0.06606455892324448
2024-11-05 02:12:29,520 - INFO - [diffusion][Epoch 10013] diffusion learning rate: 0.001
2024-11-05 02:12:29,522 - INFO - [diffusion][Epoch 10013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:29,523 - INFO - [diffusion][Epoch 10014] Epoch 10015/12000
2024-11-05 02:12:32,505 - INFO - [diffusion][Epoch 10014] diffusion training Loss: 0.07172216475009918
2024-11-05 02:12:32,506 - INFO - [diffusion][Epoch 10014] diffusion learning rate: 0.001
2024-11-05 02:12:32,508 - INFO - [diffusion][Epoch 10014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:32,510 - INFO - [diffusion][Epoch 10015] Epoch 10016/12000
2024-11-05 02:12:36,001 - INFO - [diffusion][Epoch 10015] diffusion training Loss: 0.06573304533958435
2024-11-05 02:12:36,004 - INFO - [diffusion][Epoch 10015] diffusion learning rate: 0.001
2024-11-05 02:12:36,005 - INFO - [diffusion][Epoch 10015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:36,006 - INFO - [diffusion][Epoch 10016] Epoch 10017/12000
2024-11-05 02:12:39,600 - INFO - [diffusion][Epoch 10016] diffusion training Loss: 0.0662503195926547
2024-11-05 02:12:39,602 - INFO - [diffusion][Epoch 10016] diffusion learning rate: 0.001
2024-11-05 02:12:39,603 - INFO - [diffusion][Epoch 10016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:39,605 - INFO - [diffusion][Epoch 10017] Epoch 10018/12000
2024-11-05 02:12:42,685 - INFO - [diffusion][Epoch 10017] diffusion training Loss: 0.06503201089799404
2024-11-05 02:12:42,687 - INFO - [diffusion][Epoch 10017] diffusion learning rate: 0.001
2024-11-05 02:12:42,689 - INFO - [diffusion][Epoch 10017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:42,692 - INFO - [diffusion][Epoch 10018] Epoch 10019/12000
2024-11-05 02:12:45,744 - INFO - [diffusion][Epoch 10018] diffusion training Loss: 0.07298197224736214
2024-11-05 02:12:45,746 - INFO - [diffusion][Epoch 10018] diffusion learning rate: 0.001
2024-11-05 02:12:45,748 - INFO - [diffusion][Epoch 10018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:45,749 - INFO - [diffusion][Epoch 10019] Epoch 10020/12000
2024-11-05 02:12:48,893 - INFO - [diffusion][Epoch 10019] diffusion training Loss: 0.06800813227891922
2024-11-05 02:12:48,895 - INFO - [diffusion][Epoch 10019] diffusion learning rate: 0.001
2024-11-05 02:12:48,897 - INFO - [diffusion][Epoch 10019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:48,898 - INFO - [diffusion][Epoch 10020] Epoch 10021/12000
2024-11-05 02:12:52,495 - INFO - [diffusion][Epoch 10020] diffusion training Loss: 0.06217164918780327
2024-11-05 02:12:52,498 - INFO - [diffusion][Epoch 10020] diffusion learning rate: 0.001
2024-11-05 02:12:52,499 - INFO - [diffusion][Epoch 10020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:52,501 - INFO - [diffusion][Epoch 10021] Epoch 10022/12000
2024-11-05 02:12:55,798 - INFO - [diffusion][Epoch 10021] diffusion training Loss: 0.06860919203609228
2024-11-05 02:12:55,800 - INFO - [diffusion][Epoch 10021] diffusion learning rate: 0.001
2024-11-05 02:12:55,801 - INFO - [diffusion][Epoch 10021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:55,803 - INFO - [diffusion][Epoch 10022] Epoch 10023/12000
2024-11-05 02:12:58,911 - INFO - [diffusion][Epoch 10022] diffusion training Loss: 0.06640739925205708
2024-11-05 02:12:58,913 - INFO - [diffusion][Epoch 10022] diffusion learning rate: 0.001
2024-11-05 02:12:58,915 - INFO - [diffusion][Epoch 10022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:58,916 - INFO - [diffusion][Epoch 10023] Epoch 10024/12000
2024-11-05 02:13:02,026 - INFO - [diffusion][Epoch 10023] diffusion training Loss: 0.07084085699170828
2024-11-05 02:13:02,028 - INFO - [diffusion][Epoch 10023] diffusion learning rate: 0.001
2024-11-05 02:13:02,029 - INFO - [diffusion][Epoch 10023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:02,031 - INFO - [diffusion][Epoch 10024] Epoch 10025/12000
2024-11-05 02:13:05,501 - INFO - [diffusion][Epoch 10024] diffusion training Loss: 0.07026908546686172
2024-11-05 02:13:05,503 - INFO - [diffusion][Epoch 10024] diffusion learning rate: 0.001
2024-11-05 02:13:05,504 - INFO - [diffusion][Epoch 10024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:05,506 - INFO - [diffusion][Epoch 10025] Epoch 10026/12000
2024-11-05 02:13:09,133 - INFO - [diffusion][Epoch 10025] diffusion training Loss: 0.06517132557928562
2024-11-05 02:13:09,135 - INFO - [diffusion][Epoch 10025] diffusion learning rate: 0.001
2024-11-05 02:13:09,137 - INFO - [diffusion][Epoch 10025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:09,138 - INFO - [diffusion][Epoch 10026] Epoch 10027/12000
2024-11-05 02:13:12,242 - INFO - [diffusion][Epoch 10026] diffusion training Loss: 0.06271384842693806
2024-11-05 02:13:12,244 - INFO - [diffusion][Epoch 10026] diffusion learning rate: 0.001
2024-11-05 02:13:12,246 - INFO - [diffusion][Epoch 10026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:12,247 - INFO - [diffusion][Epoch 10027] Epoch 10028/12000
2024-11-05 02:13:15,360 - INFO - [diffusion][Epoch 10027] diffusion training Loss: 0.05924057401716709
2024-11-05 02:13:15,362 - INFO - [diffusion][Epoch 10027] diffusion learning rate: 0.001
2024-11-05 02:13:15,364 - INFO - [diffusion][Epoch 10027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:15,365 - INFO - [diffusion][Epoch 10028] Epoch 10029/12000
2024-11-05 02:13:18,413 - INFO - [diffusion][Epoch 10028] diffusion training Loss: 0.06670549605041742
2024-11-05 02:13:18,415 - INFO - [diffusion][Epoch 10028] diffusion learning rate: 0.001
2024-11-05 02:13:18,417 - INFO - [diffusion][Epoch 10028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:18,418 - INFO - [diffusion][Epoch 10029] Epoch 10030/12000
2024-11-05 02:13:22,014 - INFO - [diffusion][Epoch 10029] diffusion training Loss: 0.06350528821349144
2024-11-05 02:13:22,016 - INFO - [diffusion][Epoch 10029] diffusion learning rate: 0.001
2024-11-05 02:13:22,018 - INFO - [diffusion][Epoch 10029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:22,019 - INFO - [diffusion][Epoch 10030] Epoch 10031/12000
2024-11-05 02:13:25,468 - INFO - [diffusion][Epoch 10030] diffusion training Loss: 0.07039408013224602
2024-11-05 02:13:25,470 - INFO - [diffusion][Epoch 10030] diffusion learning rate: 0.001
2024-11-05 02:13:25,471 - INFO - [diffusion][Epoch 10030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:25,472 - INFO - [diffusion][Epoch 10031] Epoch 10032/12000
2024-11-05 02:13:28,507 - INFO - [diffusion][Epoch 10031] diffusion training Loss: 0.06676188856363297
2024-11-05 02:13:28,509 - INFO - [diffusion][Epoch 10031] diffusion learning rate: 0.001
2024-11-05 02:13:28,511 - INFO - [diffusion][Epoch 10031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:28,512 - INFO - [diffusion][Epoch 10032] Epoch 10033/12000
2024-11-05 02:13:31,468 - INFO - [diffusion][Epoch 10032] diffusion training Loss: 0.06470844801515341
2024-11-05 02:13:31,470 - INFO - [diffusion][Epoch 10032] diffusion learning rate: 0.001
2024-11-05 02:13:31,472 - INFO - [diffusion][Epoch 10032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:31,474 - INFO - [diffusion][Epoch 10033] Epoch 10034/12000
2024-11-05 02:13:35,040 - INFO - [diffusion][Epoch 10033] diffusion training Loss: 0.07276202365756035
2024-11-05 02:13:35,043 - INFO - [diffusion][Epoch 10033] diffusion learning rate: 0.001
2024-11-05 02:13:35,045 - INFO - [diffusion][Epoch 10033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:35,047 - INFO - [diffusion][Epoch 10034] Epoch 10035/12000
2024-11-05 02:13:39,019 - INFO - [diffusion][Epoch 10034] diffusion training Loss: 0.057767159305512905
2024-11-05 02:13:39,021 - INFO - [diffusion][Epoch 10034] diffusion learning rate: 0.001
2024-11-05 02:13:39,023 - INFO - [diffusion][Epoch 10034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:39,025 - INFO - [diffusion][Epoch 10035] Epoch 10036/12000
2024-11-05 02:13:42,404 - INFO - [diffusion][Epoch 10035] diffusion training Loss: 0.06511503178626299
2024-11-05 02:13:42,406 - INFO - [diffusion][Epoch 10035] diffusion learning rate: 0.001
2024-11-05 02:13:42,408 - INFO - [diffusion][Epoch 10035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:42,410 - INFO - [diffusion][Epoch 10036] Epoch 10037/12000
2024-11-05 02:13:45,335 - INFO - [diffusion][Epoch 10036] diffusion training Loss: 0.06663859263062477
2024-11-05 02:13:45,337 - INFO - [diffusion][Epoch 10036] diffusion learning rate: 0.001
2024-11-05 02:13:45,339 - INFO - [diffusion][Epoch 10036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:45,340 - INFO - [diffusion][Epoch 10037] Epoch 10038/12000
2024-11-05 02:13:48,380 - INFO - [diffusion][Epoch 10037] diffusion training Loss: 0.0634348951280117
2024-11-05 02:13:48,382 - INFO - [diffusion][Epoch 10037] diffusion learning rate: 0.001
2024-11-05 02:13:48,384 - INFO - [diffusion][Epoch 10037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:48,385 - INFO - [diffusion][Epoch 10038] Epoch 10039/12000
2024-11-05 02:13:51,856 - INFO - [diffusion][Epoch 10038] diffusion training Loss: 0.06614341866225004
2024-11-05 02:13:51,858 - INFO - [diffusion][Epoch 10038] diffusion learning rate: 0.001
2024-11-05 02:13:51,860 - INFO - [diffusion][Epoch 10038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:51,861 - INFO - [diffusion][Epoch 10039] Epoch 10040/12000
2024-11-05 02:13:55,119 - INFO - [diffusion][Epoch 10039] diffusion training Loss: 0.06503505632281303
2024-11-05 02:13:55,121 - INFO - [diffusion][Epoch 10039] diffusion learning rate: 0.001
2024-11-05 02:13:55,123 - INFO - [diffusion][Epoch 10039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:55,124 - INFO - [diffusion][Epoch 10040] Epoch 10041/12000
2024-11-05 02:13:58,140 - INFO - [diffusion][Epoch 10040] diffusion training Loss: 0.06452602986246347
2024-11-05 02:13:58,142 - INFO - [diffusion][Epoch 10040] diffusion learning rate: 0.001
2024-11-05 02:13:58,143 - INFO - [diffusion][Epoch 10040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:58,145 - INFO - [diffusion][Epoch 10041] Epoch 10042/12000
2024-11-05 02:14:01,164 - INFO - [diffusion][Epoch 10041] diffusion training Loss: 0.06345970183610916
2024-11-05 02:14:01,166 - INFO - [diffusion][Epoch 10041] diffusion learning rate: 0.001
2024-11-05 02:14:01,168 - INFO - [diffusion][Epoch 10041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:01,169 - INFO - [diffusion][Epoch 10042] Epoch 10043/12000
2024-11-05 02:14:04,848 - INFO - [diffusion][Epoch 10042] diffusion training Loss: 0.07091879658401012
2024-11-05 02:14:04,850 - INFO - [diffusion][Epoch 10042] diffusion learning rate: 0.001
2024-11-05 02:14:04,852 - INFO - [diffusion][Epoch 10042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:04,853 - INFO - [diffusion][Epoch 10043] Epoch 10044/12000
2024-11-05 02:14:07,983 - INFO - [diffusion][Epoch 10043] diffusion training Loss: 0.06788983754813671
2024-11-05 02:14:07,986 - INFO - [diffusion][Epoch 10043] diffusion learning rate: 0.001
2024-11-05 02:14:07,987 - INFO - [diffusion][Epoch 10043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:07,989 - INFO - [diffusion][Epoch 10044] Epoch 10045/12000
2024-11-05 02:14:11,091 - INFO - [diffusion][Epoch 10044] diffusion training Loss: 0.06471183523535728
2024-11-05 02:14:11,093 - INFO - [diffusion][Epoch 10044] diffusion learning rate: 0.001
2024-11-05 02:14:11,094 - INFO - [diffusion][Epoch 10044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:11,096 - INFO - [diffusion][Epoch 10045] Epoch 10046/12000
2024-11-05 02:14:14,149 - INFO - [diffusion][Epoch 10045] diffusion training Loss: 0.07208000309765339
2024-11-05 02:14:14,151 - INFO - [diffusion][Epoch 10045] diffusion learning rate: 0.001
2024-11-05 02:14:14,153 - INFO - [diffusion][Epoch 10045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:14,154 - INFO - [diffusion][Epoch 10046] Epoch 10047/12000
2024-11-05 02:14:17,722 - INFO - [diffusion][Epoch 10046] diffusion training Loss: 0.06447001826018095
2024-11-05 02:14:17,724 - INFO - [diffusion][Epoch 10046] diffusion learning rate: 0.001
2024-11-05 02:14:17,726 - INFO - [diffusion][Epoch 10046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:17,727 - INFO - [diffusion][Epoch 10047] Epoch 10048/12000
2024-11-05 02:14:21,257 - INFO - [diffusion][Epoch 10047] diffusion training Loss: 0.07152734883129597
2024-11-05 02:14:21,259 - INFO - [diffusion][Epoch 10047] diffusion learning rate: 0.001
2024-11-05 02:14:21,279 - INFO - [diffusion][Epoch 10047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:21,281 - INFO - [diffusion][Epoch 10048] Epoch 10049/12000
2024-11-05 02:14:24,388 - INFO - [diffusion][Epoch 10048] diffusion training Loss: 0.06667452678084373
2024-11-05 02:14:24,390 - INFO - [diffusion][Epoch 10048] diffusion learning rate: 0.001
2024-11-05 02:14:24,392 - INFO - [diffusion][Epoch 10048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:24,393 - INFO - [diffusion][Epoch 10049] Epoch 10050/12000
2024-11-05 02:14:27,488 - INFO - [diffusion][Epoch 10049] diffusion training Loss: 0.06748159229755402
2024-11-05 02:14:27,490 - INFO - [diffusion][Epoch 10049] diffusion learning rate: 0.001
2024-11-05 02:14:27,492 - INFO - [diffusion][Epoch 10049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:27,493 - INFO - [diffusion][Epoch 10050] Epoch 10051/12000
2024-11-05 02:14:30,578 - INFO - [diffusion][Epoch 10050] diffusion training Loss: 0.06275152508169413
2024-11-05 02:14:30,580 - INFO - [diffusion][Epoch 10050] diffusion learning rate: 0.001
2024-11-05 02:14:30,582 - INFO - [diffusion][Epoch 10050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:30,583 - INFO - [diffusion][Epoch 10051] Epoch 10052/12000
2024-11-05 02:14:34,196 - INFO - [diffusion][Epoch 10051] diffusion training Loss: 0.06760489754378796
2024-11-05 02:14:34,198 - INFO - [diffusion][Epoch 10051] diffusion learning rate: 0.001
2024-11-05 02:14:34,200 - INFO - [diffusion][Epoch 10051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:34,201 - INFO - [diffusion][Epoch 10052] Epoch 10053/12000
2024-11-05 02:14:37,514 - INFO - [diffusion][Epoch 10052] diffusion training Loss: 0.06209012493491173
2024-11-05 02:14:37,516 - INFO - [diffusion][Epoch 10052] diffusion learning rate: 0.001
2024-11-05 02:14:37,518 - INFO - [diffusion][Epoch 10052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:37,519 - INFO - [diffusion][Epoch 10053] Epoch 10054/12000
2024-11-05 02:14:40,643 - INFO - [diffusion][Epoch 10053] diffusion training Loss: 0.06275265663862228
2024-11-05 02:14:40,646 - INFO - [diffusion][Epoch 10053] diffusion learning rate: 0.001
2024-11-05 02:14:40,648 - INFO - [diffusion][Epoch 10053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:40,649 - INFO - [diffusion][Epoch 10054] Epoch 10055/12000
2024-11-05 02:14:44,184 - INFO - [diffusion][Epoch 10054] diffusion training Loss: 0.06012377887964249
2024-11-05 02:14:44,234 - INFO - [diffusion][Epoch 10054] diffusion learning rate: 0.001
2024-11-05 02:14:44,236 - INFO - [diffusion][Epoch 10054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:44,237 - INFO - [diffusion][Epoch 10055] Epoch 10056/12000
2024-11-05 02:14:47,414 - INFO - [diffusion][Epoch 10055] diffusion training Loss: 0.06084423325955868
2024-11-05 02:14:47,416 - INFO - [diffusion][Epoch 10055] diffusion learning rate: 0.001
2024-11-05 02:14:47,418 - INFO - [diffusion][Epoch 10055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:47,420 - INFO - [diffusion][Epoch 10056] Epoch 10057/12000
2024-11-05 02:14:50,925 - INFO - [diffusion][Epoch 10056] diffusion training Loss: 0.05616546515375376
2024-11-05 02:14:50,927 - INFO - [diffusion][Epoch 10056] diffusion learning rate: 0.001
2024-11-05 02:14:50,929 - INFO - [diffusion][Epoch 10056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:50,930 - INFO - [diffusion][Epoch 10057] Epoch 10058/12000
2024-11-05 02:14:54,466 - INFO - [diffusion][Epoch 10057] diffusion training Loss: 0.058511972427368164
2024-11-05 02:14:54,642 - INFO - [diffusion][Epoch 10057] diffusion learning rate: 0.001
2024-11-05 02:14:54,644 - INFO - [diffusion][Epoch 10057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:54,645 - INFO - [diffusion][Epoch 10058] Epoch 10059/12000
2024-11-05 02:14:57,955 - INFO - [diffusion][Epoch 10058] diffusion training Loss: 0.0643894150853157
2024-11-05 02:14:57,957 - INFO - [diffusion][Epoch 10058] diffusion learning rate: 0.001
2024-11-05 02:14:57,959 - INFO - [diffusion][Epoch 10058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:57,961 - INFO - [diffusion][Epoch 10059] Epoch 10060/12000
2024-11-05 02:15:01,112 - INFO - [diffusion][Epoch 10059] diffusion training Loss: 0.06864192709326744
2024-11-05 02:15:01,114 - INFO - [diffusion][Epoch 10059] diffusion learning rate: 0.001
2024-11-05 02:15:01,115 - INFO - [diffusion][Epoch 10059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:01,117 - INFO - [diffusion][Epoch 10060] Epoch 10061/12000
2024-11-05 02:15:04,296 - INFO - [diffusion][Epoch 10060] diffusion training Loss: 0.06625049002468586
2024-11-05 02:15:04,298 - INFO - [diffusion][Epoch 10060] diffusion learning rate: 0.001
2024-11-05 02:15:04,299 - INFO - [diffusion][Epoch 10060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:04,301 - INFO - [diffusion][Epoch 10061] Epoch 10062/12000
2024-11-05 02:15:07,753 - INFO - [diffusion][Epoch 10061] diffusion training Loss: 0.06062877085059881
2024-11-05 02:15:07,755 - INFO - [diffusion][Epoch 10061] diffusion learning rate: 0.001
2024-11-05 02:15:07,757 - INFO - [diffusion][Epoch 10061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:07,758 - INFO - [diffusion][Epoch 10062] Epoch 10063/12000
2024-11-05 02:15:11,531 - INFO - [diffusion][Epoch 10062] diffusion training Loss: 0.06842226628214121
2024-11-05 02:15:11,533 - INFO - [diffusion][Epoch 10062] diffusion learning rate: 0.001
2024-11-05 02:15:11,535 - INFO - [diffusion][Epoch 10062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:11,536 - INFO - [diffusion][Epoch 10063] Epoch 10064/12000
2024-11-05 02:15:14,991 - INFO - [diffusion][Epoch 10063] diffusion training Loss: 0.06636708043515682
2024-11-05 02:15:14,992 - INFO - [diffusion][Epoch 10063] diffusion learning rate: 0.001
2024-11-05 02:15:14,994 - INFO - [diffusion][Epoch 10063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:14,995 - INFO - [diffusion][Epoch 10064] Epoch 10065/12000
2024-11-05 02:15:18,095 - INFO - [diffusion][Epoch 10064] diffusion training Loss: 0.0694973785430193
2024-11-05 02:15:18,097 - INFO - [diffusion][Epoch 10064] diffusion learning rate: 0.001
2024-11-05 02:15:18,099 - INFO - [diffusion][Epoch 10064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:18,100 - INFO - [diffusion][Epoch 10065] Epoch 10066/12000
2024-11-05 02:15:21,205 - INFO - [diffusion][Epoch 10065] diffusion training Loss: 0.06735013425350189
2024-11-05 02:15:21,207 - INFO - [diffusion][Epoch 10065] diffusion learning rate: 0.001
2024-11-05 02:15:21,208 - INFO - [diffusion][Epoch 10065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:21,210 - INFO - [diffusion][Epoch 10066] Epoch 10067/12000
2024-11-05 02:15:24,728 - INFO - [diffusion][Epoch 10066] diffusion training Loss: 0.06444927863776684
2024-11-05 02:15:24,730 - INFO - [diffusion][Epoch 10066] diffusion learning rate: 0.001
2024-11-05 02:15:24,732 - INFO - [diffusion][Epoch 10066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:24,733 - INFO - [diffusion][Epoch 10067] Epoch 10068/12000
2024-11-05 02:15:28,548 - INFO - [diffusion][Epoch 10067] diffusion training Loss: 0.06738390866667032
2024-11-05 02:15:28,550 - INFO - [diffusion][Epoch 10067] diffusion learning rate: 0.001
2024-11-05 02:15:28,552 - INFO - [diffusion][Epoch 10067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:28,553 - INFO - [diffusion][Epoch 10068] Epoch 10069/12000
2024-11-05 02:15:31,929 - INFO - [diffusion][Epoch 10068] diffusion training Loss: 0.06276679132133722
2024-11-05 02:15:31,931 - INFO - [diffusion][Epoch 10068] diffusion learning rate: 0.001
2024-11-05 02:15:31,932 - INFO - [diffusion][Epoch 10068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:31,934 - INFO - [diffusion][Epoch 10069] Epoch 10070/12000
2024-11-05 02:15:35,163 - INFO - [diffusion][Epoch 10069] diffusion training Loss: 0.06918815709650517
2024-11-05 02:15:35,165 - INFO - [diffusion][Epoch 10069] diffusion learning rate: 0.001
2024-11-05 02:15:35,166 - INFO - [diffusion][Epoch 10069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:35,168 - INFO - [diffusion][Epoch 10070] Epoch 10071/12000
2024-11-05 02:15:38,364 - INFO - [diffusion][Epoch 10070] diffusion training Loss: 0.06692765094339848
2024-11-05 02:15:38,366 - INFO - [diffusion][Epoch 10070] diffusion learning rate: 0.001
2024-11-05 02:15:38,368 - INFO - [diffusion][Epoch 10070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:38,370 - INFO - [diffusion][Epoch 10071] Epoch 10072/12000
2024-11-05 02:15:41,665 - INFO - [diffusion][Epoch 10071] diffusion training Loss: 0.06256225984543562
2024-11-05 02:15:41,667 - INFO - [diffusion][Epoch 10071] diffusion learning rate: 0.001
2024-11-05 02:15:41,669 - INFO - [diffusion][Epoch 10071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:41,671 - INFO - [diffusion][Epoch 10072] Epoch 10073/12000
2024-11-05 02:15:45,299 - INFO - [diffusion][Epoch 10072] diffusion training Loss: 0.06880859285593033
2024-11-05 02:15:45,301 - INFO - [diffusion][Epoch 10072] diffusion learning rate: 0.001
2024-11-05 02:15:45,303 - INFO - [diffusion][Epoch 10072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:45,304 - INFO - [diffusion][Epoch 10073] Epoch 10074/12000
2024-11-05 02:15:48,685 - INFO - [diffusion][Epoch 10073] diffusion training Loss: 0.07073229365050793
2024-11-05 02:15:48,687 - INFO - [diffusion][Epoch 10073] diffusion learning rate: 0.001
2024-11-05 02:15:48,714 - INFO - [diffusion][Epoch 10073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:48,716 - INFO - [diffusion][Epoch 10074] Epoch 10075/12000
2024-11-05 02:15:51,766 - INFO - [diffusion][Epoch 10074] diffusion training Loss: 0.06745877675712109
2024-11-05 02:15:51,768 - INFO - [diffusion][Epoch 10074] diffusion learning rate: 0.001
2024-11-05 02:15:51,770 - INFO - [diffusion][Epoch 10074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:51,771 - INFO - [diffusion][Epoch 10075] Epoch 10076/12000
2024-11-05 02:15:54,862 - INFO - [diffusion][Epoch 10075] diffusion training Loss: 0.06560334004461765
2024-11-05 02:15:54,863 - INFO - [diffusion][Epoch 10075] diffusion learning rate: 0.001
2024-11-05 02:15:54,865 - INFO - [diffusion][Epoch 10075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:54,866 - INFO - [diffusion][Epoch 10076] Epoch 10077/12000
2024-11-05 02:15:58,367 - INFO - [diffusion][Epoch 10076] diffusion training Loss: 0.06797048635780811
2024-11-05 02:15:58,369 - INFO - [diffusion][Epoch 10076] diffusion learning rate: 0.001
2024-11-05 02:15:58,371 - INFO - [diffusion][Epoch 10076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:58,373 - INFO - [diffusion][Epoch 10077] Epoch 10078/12000
2024-11-05 02:16:02,020 - INFO - [diffusion][Epoch 10077] diffusion training Loss: 0.06306303478777409
2024-11-05 02:16:02,024 - INFO - [diffusion][Epoch 10077] diffusion learning rate: 0.001
2024-11-05 02:16:02,025 - INFO - [diffusion][Epoch 10077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:02,027 - INFO - [diffusion][Epoch 10078] Epoch 10079/12000
2024-11-05 02:16:05,224 - INFO - [diffusion][Epoch 10078] diffusion training Loss: 0.0652145016938448
2024-11-05 02:16:05,227 - INFO - [diffusion][Epoch 10078] diffusion learning rate: 0.001
2024-11-05 02:16:05,254 - INFO - [diffusion][Epoch 10078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:05,255 - INFO - [diffusion][Epoch 10079] Epoch 10080/12000
2024-11-05 02:16:08,361 - INFO - [diffusion][Epoch 10079] diffusion training Loss: 0.06347479671239853
2024-11-05 02:16:08,363 - INFO - [diffusion][Epoch 10079] diffusion learning rate: 0.001
2024-11-05 02:16:08,365 - INFO - [diffusion][Epoch 10079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:08,366 - INFO - [diffusion][Epoch 10080] Epoch 10081/12000
2024-11-05 02:16:11,442 - INFO - [diffusion][Epoch 10080] diffusion training Loss: 0.065245415084064
2024-11-05 02:16:11,444 - INFO - [diffusion][Epoch 10080] diffusion learning rate: 0.001
2024-11-05 02:16:11,447 - INFO - [diffusion][Epoch 10080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:11,448 - INFO - [diffusion][Epoch 10081] Epoch 10082/12000
2024-11-05 02:16:14,976 - INFO - [diffusion][Epoch 10081] diffusion training Loss: 0.06863045506179333
2024-11-05 02:16:14,977 - INFO - [diffusion][Epoch 10081] diffusion learning rate: 0.001
2024-11-05 02:16:14,979 - INFO - [diffusion][Epoch 10081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:14,980 - INFO - [diffusion][Epoch 10082] Epoch 10083/12000
2024-11-05 02:16:18,612 - INFO - [diffusion][Epoch 10082] diffusion training Loss: 0.06334450375288725
2024-11-05 02:16:18,615 - INFO - [diffusion][Epoch 10082] diffusion learning rate: 0.001
2024-11-05 02:16:18,617 - INFO - [diffusion][Epoch 10082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:18,618 - INFO - [diffusion][Epoch 10083] Epoch 10084/12000
2024-11-05 02:16:21,707 - INFO - [diffusion][Epoch 10083] diffusion training Loss: 0.05743979290127754
2024-11-05 02:16:21,709 - INFO - [diffusion][Epoch 10083] diffusion learning rate: 0.001
2024-11-05 02:16:21,711 - INFO - [diffusion][Epoch 10083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:21,712 - INFO - [diffusion][Epoch 10084] Epoch 10085/12000
2024-11-05 02:16:24,801 - INFO - [diffusion][Epoch 10084] diffusion training Loss: 0.06449947506189346
2024-11-05 02:16:24,803 - INFO - [diffusion][Epoch 10084] diffusion learning rate: 0.001
2024-11-05 02:16:24,804 - INFO - [diffusion][Epoch 10084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:24,806 - INFO - [diffusion][Epoch 10085] Epoch 10086/12000
2024-11-05 02:16:27,950 - INFO - [diffusion][Epoch 10085] diffusion training Loss: 0.06869491282850504
2024-11-05 02:16:27,952 - INFO - [diffusion][Epoch 10085] diffusion learning rate: 0.001
2024-11-05 02:16:27,954 - INFO - [diffusion][Epoch 10085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:27,955 - INFO - [diffusion][Epoch 10086] Epoch 10087/12000
2024-11-05 02:16:31,497 - INFO - [diffusion][Epoch 10086] diffusion training Loss: 0.07105762884020805
2024-11-05 02:16:31,499 - INFO - [diffusion][Epoch 10086] diffusion learning rate: 0.001
2024-11-05 02:16:31,502 - INFO - [diffusion][Epoch 10086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:31,503 - INFO - [diffusion][Epoch 10087] Epoch 10088/12000
2024-11-05 02:16:34,869 - INFO - [diffusion][Epoch 10087] diffusion training Loss: 0.06944704055786133
2024-11-05 02:16:34,871 - INFO - [diffusion][Epoch 10087] diffusion learning rate: 0.001
2024-11-05 02:16:34,873 - INFO - [diffusion][Epoch 10087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:34,874 - INFO - [diffusion][Epoch 10088] Epoch 10089/12000
2024-11-05 02:16:37,916 - INFO - [diffusion][Epoch 10088] diffusion training Loss: 0.06698263064026833
2024-11-05 02:16:37,918 - INFO - [diffusion][Epoch 10088] diffusion learning rate: 0.001
2024-11-05 02:16:37,920 - INFO - [diffusion][Epoch 10088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:37,922 - INFO - [diffusion][Epoch 10089] Epoch 10090/12000
2024-11-05 02:16:41,124 - INFO - [diffusion][Epoch 10089] diffusion training Loss: 0.06729849241673946
2024-11-05 02:16:41,126 - INFO - [diffusion][Epoch 10089] diffusion learning rate: 0.001
2024-11-05 02:16:41,128 - INFO - [diffusion][Epoch 10089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:41,129 - INFO - [diffusion][Epoch 10090] Epoch 10091/12000
2024-11-05 02:16:44,550 - INFO - [diffusion][Epoch 10090] diffusion training Loss: 0.06479261815547943
2024-11-05 02:16:44,553 - INFO - [diffusion][Epoch 10090] diffusion learning rate: 0.001
2024-11-05 02:16:44,555 - INFO - [diffusion][Epoch 10090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:44,556 - INFO - [diffusion][Epoch 10091] Epoch 10092/12000
2024-11-05 02:16:48,210 - INFO - [diffusion][Epoch 10091] diffusion training Loss: 0.061540660448372364
2024-11-05 02:16:48,212 - INFO - [diffusion][Epoch 10091] diffusion learning rate: 0.001
2024-11-05 02:16:48,213 - INFO - [diffusion][Epoch 10091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:48,215 - INFO - [diffusion][Epoch 10092] Epoch 10093/12000
2024-11-05 02:16:50,900 - INFO - [diffusion][Epoch 10092] diffusion training Loss: 0.06861280277371407
2024-11-05 02:16:50,902 - INFO - [diffusion][Epoch 10092] diffusion learning rate: 0.001
2024-11-05 02:16:50,904 - INFO - [diffusion][Epoch 10092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:50,906 - INFO - [diffusion][Epoch 10093] Epoch 10094/12000
2024-11-05 02:16:53,996 - INFO - [diffusion][Epoch 10093] diffusion training Loss: 0.0673168869689107
2024-11-05 02:16:53,997 - INFO - [diffusion][Epoch 10093] diffusion learning rate: 0.001
2024-11-05 02:16:53,999 - INFO - [diffusion][Epoch 10093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:54,000 - INFO - [diffusion][Epoch 10094] Epoch 10095/12000
2024-11-05 02:16:57,644 - INFO - [diffusion][Epoch 10094] diffusion training Loss: 0.058006370440125465
2024-11-05 02:16:57,646 - INFO - [diffusion][Epoch 10094] diffusion learning rate: 0.001
2024-11-05 02:16:57,648 - INFO - [diffusion][Epoch 10094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:57,649 - INFO - [diffusion][Epoch 10095] Epoch 10096/12000
2024-11-05 02:17:01,326 - INFO - [diffusion][Epoch 10095] diffusion training Loss: 0.07205763086676598
2024-11-05 02:17:01,328 - INFO - [diffusion][Epoch 10095] diffusion learning rate: 0.001
2024-11-05 02:17:01,330 - INFO - [diffusion][Epoch 10095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:01,331 - INFO - [diffusion][Epoch 10096] Epoch 10097/12000
2024-11-05 02:17:04,554 - INFO - [diffusion][Epoch 10096] diffusion training Loss: 0.06838898546993732
2024-11-05 02:17:04,556 - INFO - [diffusion][Epoch 10096] diffusion learning rate: 0.001
2024-11-05 02:17:04,558 - INFO - [diffusion][Epoch 10096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:04,559 - INFO - [diffusion][Epoch 10097] Epoch 10098/12000
2024-11-05 02:17:07,719 - INFO - [diffusion][Epoch 10097] diffusion training Loss: 0.06708838604390621
2024-11-05 02:17:07,721 - INFO - [diffusion][Epoch 10097] diffusion learning rate: 0.001
2024-11-05 02:17:07,742 - INFO - [diffusion][Epoch 10097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:07,743 - INFO - [diffusion][Epoch 10098] Epoch 10099/12000
2024-11-05 02:17:10,893 - INFO - [diffusion][Epoch 10098] diffusion training Loss: 0.0692077400162816
2024-11-05 02:17:10,895 - INFO - [diffusion][Epoch 10098] diffusion learning rate: 0.001
2024-11-05 02:17:10,897 - INFO - [diffusion][Epoch 10098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:10,898 - INFO - [diffusion][Epoch 10099] Epoch 10100/12000
2024-11-05 02:17:14,319 - INFO - [diffusion][Epoch 10099] diffusion training Loss: 0.06650387309491634
2024-11-05 02:17:14,321 - INFO - [diffusion][Epoch 10099] diffusion learning rate: 0.001
2024-11-05 02:17:14,323 - INFO - [diffusion][Epoch 10099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:14,324 - INFO - [diffusion][Epoch 10100] Epoch 10101/12000
2024-11-05 02:17:17,885 - INFO - [diffusion][Epoch 10100] diffusion training Loss: 0.06329703889787197
2024-11-05 02:17:17,887 - INFO - [diffusion][Epoch 10100] diffusion learning rate: 0.001
2024-11-05 02:17:17,889 - INFO - [diffusion][Epoch 10100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:17,890 - INFO - [diffusion][Epoch 10101] Epoch 10102/12000
2024-11-05 02:17:20,987 - INFO - [diffusion][Epoch 10101] diffusion training Loss: 0.07094597443938255
2024-11-05 02:17:20,989 - INFO - [diffusion][Epoch 10101] diffusion learning rate: 0.001
2024-11-05 02:17:20,991 - INFO - [diffusion][Epoch 10101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:20,993 - INFO - [diffusion][Epoch 10102] Epoch 10103/12000
2024-11-05 02:17:24,132 - INFO - [diffusion][Epoch 10102] diffusion training Loss: 0.0711199901998043
2024-11-05 02:17:24,139 - INFO - [diffusion][Epoch 10102] diffusion learning rate: 0.001
2024-11-05 02:17:24,141 - INFO - [diffusion][Epoch 10102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:24,143 - INFO - [diffusion][Epoch 10103] Epoch 10104/12000
2024-11-05 02:17:27,363 - INFO - [diffusion][Epoch 10103] diffusion training Loss: 0.0673381881788373
2024-11-05 02:17:27,365 - INFO - [diffusion][Epoch 10103] diffusion learning rate: 0.001
2024-11-05 02:17:27,366 - INFO - [diffusion][Epoch 10103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:27,368 - INFO - [diffusion][Epoch 10104] Epoch 10105/12000
2024-11-05 02:17:30,976 - INFO - [diffusion][Epoch 10104] diffusion training Loss: 0.06672636698931456
2024-11-05 02:17:30,979 - INFO - [diffusion][Epoch 10104] diffusion learning rate: 0.001
2024-11-05 02:17:30,980 - INFO - [diffusion][Epoch 10104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:30,982 - INFO - [diffusion][Epoch 10105] Epoch 10106/12000
2024-11-05 02:17:34,462 - INFO - [diffusion][Epoch 10105] diffusion training Loss: 0.06818697601556778
2024-11-05 02:17:34,464 - INFO - [diffusion][Epoch 10105] diffusion learning rate: 0.001
2024-11-05 02:17:34,466 - INFO - [diffusion][Epoch 10105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:34,468 - INFO - [diffusion][Epoch 10106] Epoch 10107/12000
2024-11-05 02:17:37,557 - INFO - [diffusion][Epoch 10106] diffusion training Loss: 0.06631672754883766
2024-11-05 02:17:37,559 - INFO - [diffusion][Epoch 10106] diffusion learning rate: 0.001
2024-11-05 02:17:37,561 - INFO - [diffusion][Epoch 10106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:37,562 - INFO - [diffusion][Epoch 10107] Epoch 10108/12000
2024-11-05 02:17:40,591 - INFO - [diffusion][Epoch 10107] diffusion training Loss: 0.06355079542845488
2024-11-05 02:17:40,593 - INFO - [diffusion][Epoch 10107] diffusion learning rate: 0.001
2024-11-05 02:17:40,595 - INFO - [diffusion][Epoch 10107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:40,596 - INFO - [diffusion][Epoch 10108] Epoch 10109/12000
2024-11-05 02:17:43,938 - INFO - [diffusion][Epoch 10108] diffusion training Loss: 0.06348060816526413
2024-11-05 02:17:43,940 - INFO - [diffusion][Epoch 10108] diffusion learning rate: 0.001
2024-11-05 02:17:43,941 - INFO - [diffusion][Epoch 10108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:43,943 - INFO - [diffusion][Epoch 10109] Epoch 10110/12000
2024-11-05 02:17:47,635 - INFO - [diffusion][Epoch 10109] diffusion training Loss: 0.06958075799047947
2024-11-05 02:17:47,639 - INFO - [diffusion][Epoch 10109] diffusion learning rate: 0.001
2024-11-05 02:17:47,641 - INFO - [diffusion][Epoch 10109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:47,642 - INFO - [diffusion][Epoch 10110] Epoch 10111/12000
2024-11-05 02:17:50,909 - INFO - [diffusion][Epoch 10110] diffusion training Loss: 0.06065599247813225
2024-11-05 02:17:50,911 - INFO - [diffusion][Epoch 10110] diffusion learning rate: 0.001
2024-11-05 02:17:50,931 - INFO - [diffusion][Epoch 10110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:50,932 - INFO - [diffusion][Epoch 10111] Epoch 10112/12000
2024-11-05 02:17:54,166 - INFO - [diffusion][Epoch 10111] diffusion training Loss: 0.061633734963834286
2024-11-05 02:17:54,168 - INFO - [diffusion][Epoch 10111] diffusion learning rate: 0.001
2024-11-05 02:17:54,170 - INFO - [diffusion][Epoch 10111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:54,171 - INFO - [diffusion][Epoch 10112] Epoch 10113/12000
2024-11-05 02:17:57,177 - INFO - [diffusion][Epoch 10112] diffusion training Loss: 0.06934173218905926
2024-11-05 02:17:57,179 - INFO - [diffusion][Epoch 10112] diffusion learning rate: 0.001
2024-11-05 02:17:57,181 - INFO - [diffusion][Epoch 10112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:57,182 - INFO - [diffusion][Epoch 10113] Epoch 10114/12000
2024-11-05 02:18:00,617 - INFO - [diffusion][Epoch 10113] diffusion training Loss: 0.06954080983996391
2024-11-05 02:18:00,619 - INFO - [diffusion][Epoch 10113] diffusion learning rate: 0.001
2024-11-05 02:18:00,621 - INFO - [diffusion][Epoch 10113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:00,622 - INFO - [diffusion][Epoch 10114] Epoch 10115/12000
2024-11-05 02:18:04,169 - INFO - [diffusion][Epoch 10114] diffusion training Loss: 0.06257810443639755
2024-11-05 02:18:04,172 - INFO - [diffusion][Epoch 10114] diffusion learning rate: 0.001
2024-11-05 02:18:04,174 - INFO - [diffusion][Epoch 10114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:04,175 - INFO - [diffusion][Epoch 10115] Epoch 10116/12000
2024-11-05 02:18:07,988 - INFO - [diffusion][Epoch 10115] diffusion training Loss: 0.06847785692662
2024-11-05 02:18:07,990 - INFO - [diffusion][Epoch 10115] diffusion learning rate: 0.001
2024-11-05 02:18:07,992 - INFO - [diffusion][Epoch 10115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:07,993 - INFO - [diffusion][Epoch 10116] Epoch 10117/12000
2024-11-05 02:18:11,118 - INFO - [diffusion][Epoch 10116] diffusion training Loss: 0.06583467684686184
2024-11-05 02:18:11,120 - INFO - [diffusion][Epoch 10116] diffusion learning rate: 0.001
2024-11-05 02:18:11,122 - INFO - [diffusion][Epoch 10116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:11,123 - INFO - [diffusion][Epoch 10117] Epoch 10118/12000
2024-11-05 02:18:14,210 - INFO - [diffusion][Epoch 10117] diffusion training Loss: 0.06592739932239056
2024-11-05 02:18:14,212 - INFO - [diffusion][Epoch 10117] diffusion learning rate: 0.001
2024-11-05 02:18:14,214 - INFO - [diffusion][Epoch 10117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:14,215 - INFO - [diffusion][Epoch 10118] Epoch 10119/12000
2024-11-05 02:18:17,298 - INFO - [diffusion][Epoch 10118] diffusion training Loss: 0.06835100427269936
2024-11-05 02:18:17,300 - INFO - [diffusion][Epoch 10118] diffusion learning rate: 0.001
2024-11-05 02:18:17,302 - INFO - [diffusion][Epoch 10118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:17,303 - INFO - [diffusion][Epoch 10119] Epoch 10120/12000
2024-11-05 02:18:20,836 - INFO - [diffusion][Epoch 10119] diffusion training Loss: 0.06801126897335052
2024-11-05 02:18:20,837 - INFO - [diffusion][Epoch 10119] diffusion learning rate: 0.001
2024-11-05 02:18:20,839 - INFO - [diffusion][Epoch 10119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:20,840 - INFO - [diffusion][Epoch 10120] Epoch 10121/12000
2024-11-05 02:18:23,937 - INFO - [diffusion][Epoch 10120] diffusion training Loss: 0.06574435718357563
2024-11-05 02:18:23,939 - INFO - [diffusion][Epoch 10120] diffusion learning rate: 0.001
2024-11-05 02:18:23,941 - INFO - [diffusion][Epoch 10120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:23,942 - INFO - [diffusion][Epoch 10121] Epoch 10122/12000
2024-11-05 02:18:26,935 - INFO - [diffusion][Epoch 10121] diffusion training Loss: 0.06809031590819359
2024-11-05 02:18:26,937 - INFO - [diffusion][Epoch 10121] diffusion learning rate: 0.001
2024-11-05 02:18:26,939 - INFO - [diffusion][Epoch 10121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:26,940 - INFO - [diffusion][Epoch 10122] Epoch 10123/12000
2024-11-05 02:18:30,119 - INFO - [diffusion][Epoch 10122] diffusion training Loss: 0.06391667667776346
2024-11-05 02:18:30,121 - INFO - [diffusion][Epoch 10122] diffusion learning rate: 0.001
2024-11-05 02:18:30,123 - INFO - [diffusion][Epoch 10122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:30,124 - INFO - [diffusion][Epoch 10123] Epoch 10124/12000
2024-11-05 02:18:33,743 - INFO - [diffusion][Epoch 10123] diffusion training Loss: 0.06137535721063614
2024-11-05 02:18:33,745 - INFO - [diffusion][Epoch 10123] diffusion learning rate: 0.001
2024-11-05 02:18:33,747 - INFO - [diffusion][Epoch 10123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:33,748 - INFO - [diffusion][Epoch 10124] Epoch 10125/12000
2024-11-05 02:18:37,210 - INFO - [diffusion][Epoch 10124] diffusion training Loss: 0.06983831524848938
2024-11-05 02:18:37,212 - INFO - [diffusion][Epoch 10124] diffusion learning rate: 0.001
2024-11-05 02:18:37,214 - INFO - [diffusion][Epoch 10124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:37,215 - INFO - [diffusion][Epoch 10125] Epoch 10126/12000
2024-11-05 02:18:40,402 - INFO - [diffusion][Epoch 10125] diffusion training Loss: 0.06740881595760584
2024-11-05 02:18:40,404 - INFO - [diffusion][Epoch 10125] diffusion learning rate: 0.001
2024-11-05 02:18:40,406 - INFO - [diffusion][Epoch 10125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:40,407 - INFO - [diffusion][Epoch 10126] Epoch 10127/12000
2024-11-05 02:18:43,290 - INFO - [diffusion][Epoch 10126] diffusion training Loss: 0.0673374105244875
2024-11-05 02:18:43,292 - INFO - [diffusion][Epoch 10126] diffusion learning rate: 0.001
2024-11-05 02:18:43,294 - INFO - [diffusion][Epoch 10126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:43,295 - INFO - [diffusion][Epoch 10127] Epoch 10128/12000
2024-11-05 02:18:46,780 - INFO - [diffusion][Epoch 10127] diffusion training Loss: 0.07274101488292217
2024-11-05 02:18:46,782 - INFO - [diffusion][Epoch 10127] diffusion learning rate: 0.001
2024-11-05 02:18:46,783 - INFO - [diffusion][Epoch 10127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:46,785 - INFO - [diffusion][Epoch 10128] Epoch 10129/12000
2024-11-05 02:18:50,449 - INFO - [diffusion][Epoch 10128] diffusion training Loss: 0.06843763589859009
2024-11-05 02:18:50,452 - INFO - [diffusion][Epoch 10128] diffusion learning rate: 0.001
2024-11-05 02:18:50,463 - INFO - [diffusion][Epoch 10128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:50,465 - INFO - [diffusion][Epoch 10129] Epoch 10130/12000
2024-11-05 02:18:53,820 - INFO - [diffusion][Epoch 10129] diffusion training Loss: 0.07232324220240116
2024-11-05 02:18:53,822 - INFO - [diffusion][Epoch 10129] diffusion learning rate: 0.001
2024-11-05 02:18:53,824 - INFO - [diffusion][Epoch 10129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:53,825 - INFO - [diffusion][Epoch 10130] Epoch 10131/12000
2024-11-05 02:18:56,759 - INFO - [diffusion][Epoch 10130] diffusion training Loss: 0.0708478270098567
2024-11-05 02:18:56,761 - INFO - [diffusion][Epoch 10130] diffusion learning rate: 0.001
2024-11-05 02:18:56,762 - INFO - [diffusion][Epoch 10130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:56,763 - INFO - [diffusion][Epoch 10131] Epoch 10132/12000
2024-11-05 02:18:59,946 - INFO - [diffusion][Epoch 10131] diffusion training Loss: 0.062198796309530735
2024-11-05 02:18:59,948 - INFO - [diffusion][Epoch 10131] diffusion learning rate: 0.001
2024-11-05 02:18:59,950 - INFO - [diffusion][Epoch 10131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:59,951 - INFO - [diffusion][Epoch 10132] Epoch 10133/12000
2024-11-05 02:19:03,394 - INFO - [diffusion][Epoch 10132] diffusion training Loss: 0.0684395544230938
2024-11-05 02:19:03,396 - INFO - [diffusion][Epoch 10132] diffusion learning rate: 0.001
2024-11-05 02:19:03,397 - INFO - [diffusion][Epoch 10132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:03,399 - INFO - [diffusion][Epoch 10133] Epoch 10134/12000
2024-11-05 02:19:07,079 - INFO - [diffusion][Epoch 10133] diffusion training Loss: 0.06794306077063084
2024-11-05 02:19:07,081 - INFO - [diffusion][Epoch 10133] diffusion learning rate: 0.001
2024-11-05 02:19:07,083 - INFO - [diffusion][Epoch 10133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:07,084 - INFO - [diffusion][Epoch 10134] Epoch 10135/12000
2024-11-05 02:19:10,474 - INFO - [diffusion][Epoch 10134] diffusion training Loss: 0.06845996528863907
2024-11-05 02:19:10,476 - INFO - [diffusion][Epoch 10134] diffusion learning rate: 0.001
2024-11-05 02:19:10,478 - INFO - [diffusion][Epoch 10134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:10,479 - INFO - [diffusion][Epoch 10135] Epoch 10136/12000
2024-11-05 02:19:13,553 - INFO - [diffusion][Epoch 10135] diffusion training Loss: 0.062046815641224384
2024-11-05 02:19:13,555 - INFO - [diffusion][Epoch 10135] diffusion learning rate: 0.001
2024-11-05 02:19:13,557 - INFO - [diffusion][Epoch 10135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:13,558 - INFO - [diffusion][Epoch 10136] Epoch 10137/12000
2024-11-05 02:19:17,358 - INFO - [diffusion][Epoch 10136] diffusion training Loss: 0.067953547462821
2024-11-05 02:19:17,359 - INFO - [diffusion][Epoch 10136] diffusion learning rate: 0.001
2024-11-05 02:19:17,402 - INFO - [diffusion][Epoch 10136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:17,403 - INFO - [diffusion][Epoch 10137] Epoch 10138/12000
2024-11-05 02:19:20,525 - INFO - [diffusion][Epoch 10137] diffusion training Loss: 0.06470685638487339
2024-11-05 02:19:20,527 - INFO - [diffusion][Epoch 10137] diffusion learning rate: 0.001
2024-11-05 02:19:20,529 - INFO - [diffusion][Epoch 10137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:20,530 - INFO - [diffusion][Epoch 10138] Epoch 10139/12000
2024-11-05 02:19:23,939 - INFO - [diffusion][Epoch 10138] diffusion training Loss: 0.0670539764687419
2024-11-05 02:19:23,941 - INFO - [diffusion][Epoch 10138] diffusion learning rate: 0.001
2024-11-05 02:19:23,942 - INFO - [diffusion][Epoch 10138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:23,943 - INFO - [diffusion][Epoch 10139] Epoch 10140/12000
2024-11-05 02:19:27,619 - INFO - [diffusion][Epoch 10139] diffusion training Loss: 0.0672907019034028
2024-11-05 02:19:27,621 - INFO - [diffusion][Epoch 10139] diffusion learning rate: 0.001
2024-11-05 02:19:27,623 - INFO - [diffusion][Epoch 10139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:27,624 - INFO - [diffusion][Epoch 10140] Epoch 10141/12000
2024-11-05 02:19:30,980 - INFO - [diffusion][Epoch 10140] diffusion training Loss: 0.06443303171545267
2024-11-05 02:19:30,981 - INFO - [diffusion][Epoch 10140] diffusion learning rate: 0.001
2024-11-05 02:19:30,983 - INFO - [diffusion][Epoch 10140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:30,985 - INFO - [diffusion][Epoch 10141] Epoch 10142/12000
2024-11-05 02:19:34,097 - INFO - [diffusion][Epoch 10141] diffusion training Loss: 0.06457050330936909
2024-11-05 02:19:34,099 - INFO - [diffusion][Epoch 10141] diffusion learning rate: 0.001
2024-11-05 02:19:34,101 - INFO - [diffusion][Epoch 10141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:34,102 - INFO - [diffusion][Epoch 10142] Epoch 10143/12000
2024-11-05 02:19:37,155 - INFO - [diffusion][Epoch 10142] diffusion training Loss: 0.06439759023487568
2024-11-05 02:19:37,158 - INFO - [diffusion][Epoch 10142] diffusion learning rate: 0.001
2024-11-05 02:19:37,160 - INFO - [diffusion][Epoch 10142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:37,161 - INFO - [diffusion][Epoch 10143] Epoch 10144/12000
2024-11-05 02:19:40,637 - INFO - [diffusion][Epoch 10143] diffusion training Loss: 0.06366047635674477
2024-11-05 02:19:40,639 - INFO - [diffusion][Epoch 10143] diffusion learning rate: 0.001
2024-11-05 02:19:40,641 - INFO - [diffusion][Epoch 10143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:40,642 - INFO - [diffusion][Epoch 10144] Epoch 10145/12000
2024-11-05 02:19:44,050 - INFO - [diffusion][Epoch 10144] diffusion training Loss: 0.06237342208623886
2024-11-05 02:19:44,051 - INFO - [diffusion][Epoch 10144] diffusion learning rate: 0.001
2024-11-05 02:19:44,053 - INFO - [diffusion][Epoch 10144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:44,055 - INFO - [diffusion][Epoch 10145] Epoch 10146/12000
2024-11-05 02:19:47,087 - INFO - [diffusion][Epoch 10145] diffusion training Loss: 0.07179557252675295
2024-11-05 02:19:47,090 - INFO - [diffusion][Epoch 10145] diffusion learning rate: 0.001
2024-11-05 02:19:47,092 - INFO - [diffusion][Epoch 10145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:47,093 - INFO - [diffusion][Epoch 10146] Epoch 10147/12000
2024-11-05 02:19:50,217 - INFO - [diffusion][Epoch 10146] diffusion training Loss: 0.0667340848594904
2024-11-05 02:19:50,220 - INFO - [diffusion][Epoch 10146] diffusion learning rate: 0.001
2024-11-05 02:19:50,222 - INFO - [diffusion][Epoch 10146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:50,223 - INFO - [diffusion][Epoch 10147] Epoch 10148/12000
2024-11-05 02:19:53,681 - INFO - [diffusion][Epoch 10147] diffusion training Loss: 0.06329250708222389
2024-11-05 02:19:53,684 - INFO - [diffusion][Epoch 10147] diffusion learning rate: 0.001
2024-11-05 02:19:53,685 - INFO - [diffusion][Epoch 10147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:53,687 - INFO - [diffusion][Epoch 10148] Epoch 10149/12000
2024-11-05 02:19:57,402 - INFO - [diffusion][Epoch 10148] diffusion training Loss: 0.06707297824323177
2024-11-05 02:19:57,404 - INFO - [diffusion][Epoch 10148] diffusion learning rate: 0.001
2024-11-05 02:19:57,406 - INFO - [diffusion][Epoch 10148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:57,407 - INFO - [diffusion][Epoch 10149] Epoch 10150/12000
2024-11-05 02:20:00,668 - INFO - [diffusion][Epoch 10149] diffusion training Loss: 0.06502037681639194
2024-11-05 02:20:00,670 - INFO - [diffusion][Epoch 10149] diffusion learning rate: 0.001
2024-11-05 02:20:00,671 - INFO - [diffusion][Epoch 10149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:00,673 - INFO - [diffusion][Epoch 10150] Epoch 10151/12000
2024-11-05 02:20:03,786 - INFO - [diffusion][Epoch 10150] diffusion training Loss: 0.06351949460804462
2024-11-05 02:20:03,788 - INFO - [diffusion][Epoch 10150] diffusion learning rate: 0.001
2024-11-05 02:20:03,790 - INFO - [diffusion][Epoch 10150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:03,792 - INFO - [diffusion][Epoch 10151] Epoch 10152/12000
2024-11-05 02:20:06,987 - INFO - [diffusion][Epoch 10151] diffusion training Loss: 0.0671358136460185
2024-11-05 02:20:06,989 - INFO - [diffusion][Epoch 10151] diffusion learning rate: 0.001
2024-11-05 02:20:06,991 - INFO - [diffusion][Epoch 10151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:06,992 - INFO - [diffusion][Epoch 10152] Epoch 10153/12000
2024-11-05 02:20:10,416 - INFO - [diffusion][Epoch 10152] diffusion training Loss: 0.066276079043746
2024-11-05 02:20:10,418 - INFO - [diffusion][Epoch 10152] diffusion learning rate: 0.001
2024-11-05 02:20:10,420 - INFO - [diffusion][Epoch 10152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:10,421 - INFO - [diffusion][Epoch 10153] Epoch 10154/12000
2024-11-05 02:20:14,193 - INFO - [diffusion][Epoch 10153] diffusion training Loss: 0.06716998107731342
2024-11-05 02:20:14,194 - INFO - [diffusion][Epoch 10153] diffusion learning rate: 0.001
2024-11-05 02:20:14,226 - INFO - [diffusion][Epoch 10153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:14,227 - INFO - [diffusion][Epoch 10154] Epoch 10155/12000
2024-11-05 02:20:17,737 - INFO - [diffusion][Epoch 10154] diffusion training Loss: 0.06392165087163448
2024-11-05 02:20:17,739 - INFO - [diffusion][Epoch 10154] diffusion learning rate: 0.001
2024-11-05 02:20:17,741 - INFO - [diffusion][Epoch 10154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:17,742 - INFO - [diffusion][Epoch 10155] Epoch 10156/12000
2024-11-05 02:20:20,814 - INFO - [diffusion][Epoch 10155] diffusion training Loss: 0.06857944838702679
2024-11-05 02:20:20,817 - INFO - [diffusion][Epoch 10155] diffusion learning rate: 0.001
2024-11-05 02:20:20,819 - INFO - [diffusion][Epoch 10155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:20,820 - INFO - [diffusion][Epoch 10156] Epoch 10157/12000
2024-11-05 02:20:24,226 - INFO - [diffusion][Epoch 10156] diffusion training Loss: 0.06606056913733482
2024-11-05 02:20:24,228 - INFO - [diffusion][Epoch 10156] diffusion learning rate: 0.001
2024-11-05 02:20:24,230 - INFO - [diffusion][Epoch 10156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:24,231 - INFO - [diffusion][Epoch 10157] Epoch 10158/12000
2024-11-05 02:20:27,399 - INFO - [diffusion][Epoch 10157] diffusion training Loss: 0.07194812968373299
2024-11-05 02:20:27,401 - INFO - [diffusion][Epoch 10157] diffusion learning rate: 0.001
2024-11-05 02:20:27,403 - INFO - [diffusion][Epoch 10157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:27,404 - INFO - [diffusion][Epoch 10158] Epoch 10159/12000
2024-11-05 02:20:30,900 - INFO - [diffusion][Epoch 10158] diffusion training Loss: 0.0669873096048832
2024-11-05 02:20:30,902 - INFO - [diffusion][Epoch 10158] diffusion learning rate: 0.001
2024-11-05 02:20:30,904 - INFO - [diffusion][Epoch 10158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:30,905 - INFO - [diffusion][Epoch 10159] Epoch 10160/12000
2024-11-05 02:20:34,440 - INFO - [diffusion][Epoch 10159] diffusion training Loss: 0.06576213985681534
2024-11-05 02:20:34,442 - INFO - [diffusion][Epoch 10159] diffusion learning rate: 0.001
2024-11-05 02:20:34,445 - INFO - [diffusion][Epoch 10159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:34,447 - INFO - [diffusion][Epoch 10160] Epoch 10161/12000
2024-11-05 02:20:37,547 - INFO - [diffusion][Epoch 10160] diffusion training Loss: 0.06479387078434229
2024-11-05 02:20:37,549 - INFO - [diffusion][Epoch 10160] diffusion learning rate: 0.001
2024-11-05 02:20:37,551 - INFO - [diffusion][Epoch 10160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:37,552 - INFO - [diffusion][Epoch 10161] Epoch 10162/12000
2024-11-05 02:20:40,633 - INFO - [diffusion][Epoch 10161] diffusion training Loss: 0.06214468367397785
2024-11-05 02:20:40,635 - INFO - [diffusion][Epoch 10161] diffusion learning rate: 0.001
2024-11-05 02:20:40,666 - INFO - [diffusion][Epoch 10161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:40,668 - INFO - [diffusion][Epoch 10162] Epoch 10163/12000
2024-11-05 02:20:43,785 - INFO - [diffusion][Epoch 10162] diffusion training Loss: 0.06648665852844715
2024-11-05 02:20:43,787 - INFO - [diffusion][Epoch 10162] diffusion learning rate: 0.001
2024-11-05 02:20:43,789 - INFO - [diffusion][Epoch 10162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:43,790 - INFO - [diffusion][Epoch 10163] Epoch 10164/12000
2024-11-05 02:20:47,321 - INFO - [diffusion][Epoch 10163] diffusion training Loss: 0.06796703021973372
2024-11-05 02:20:47,322 - INFO - [diffusion][Epoch 10163] diffusion learning rate: 0.001
2024-11-05 02:20:47,324 - INFO - [diffusion][Epoch 10163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:47,325 - INFO - [diffusion][Epoch 10164] Epoch 10165/12000
2024-11-05 02:20:50,861 - INFO - [diffusion][Epoch 10164] diffusion training Loss: 0.07029899582266808
2024-11-05 02:20:50,863 - INFO - [diffusion][Epoch 10164] diffusion learning rate: 0.001
2024-11-05 02:20:50,894 - INFO - [diffusion][Epoch 10164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:50,895 - INFO - [diffusion][Epoch 10165] Epoch 10166/12000
2024-11-05 02:20:53,985 - INFO - [diffusion][Epoch 10165] diffusion training Loss: 0.06053890660405159
2024-11-05 02:20:53,987 - INFO - [diffusion][Epoch 10165] diffusion learning rate: 0.001
2024-11-05 02:20:53,989 - INFO - [diffusion][Epoch 10165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:53,990 - INFO - [diffusion][Epoch 10166] Epoch 10167/12000
2024-11-05 02:20:57,089 - INFO - [diffusion][Epoch 10166] diffusion training Loss: 0.0655544139444828
2024-11-05 02:20:57,092 - INFO - [diffusion][Epoch 10166] diffusion learning rate: 0.001
2024-11-05 02:20:57,094 - INFO - [diffusion][Epoch 10166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:57,095 - INFO - [diffusion][Epoch 10167] Epoch 10168/12000
2024-11-05 02:21:00,178 - INFO - [diffusion][Epoch 10167] diffusion training Loss: 0.07020762749016285
2024-11-05 02:21:00,181 - INFO - [diffusion][Epoch 10167] diffusion learning rate: 0.001
2024-11-05 02:21:00,182 - INFO - [diffusion][Epoch 10167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:00,184 - INFO - [diffusion][Epoch 10168] Epoch 10169/12000
2024-11-05 02:21:03,719 - INFO - [diffusion][Epoch 10168] diffusion training Loss: 0.06806209310889244
2024-11-05 02:21:03,720 - INFO - [diffusion][Epoch 10168] diffusion learning rate: 0.001
2024-11-05 02:21:03,722 - INFO - [diffusion][Epoch 10168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:03,723 - INFO - [diffusion][Epoch 10169] Epoch 10170/12000
2024-11-05 02:21:07,226 - INFO - [diffusion][Epoch 10169] diffusion training Loss: 0.07464949041604996
2024-11-05 02:21:07,228 - INFO - [diffusion][Epoch 10169] diffusion learning rate: 0.001
2024-11-05 02:21:07,229 - INFO - [diffusion][Epoch 10169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:07,230 - INFO - [diffusion][Epoch 10170] Epoch 10171/12000
2024-11-05 02:21:10,315 - INFO - [diffusion][Epoch 10170] diffusion training Loss: 0.06863431818783283
2024-11-05 02:21:10,318 - INFO - [diffusion][Epoch 10170] diffusion learning rate: 0.001
2024-11-05 02:21:10,320 - INFO - [diffusion][Epoch 10170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:10,321 - INFO - [diffusion][Epoch 10171] Epoch 10172/12000
2024-11-05 02:21:13,434 - INFO - [diffusion][Epoch 10171] diffusion training Loss: 0.07127352524548769
2024-11-05 02:21:13,436 - INFO - [diffusion][Epoch 10171] diffusion learning rate: 0.001
2024-11-05 02:21:13,438 - INFO - [diffusion][Epoch 10171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:13,439 - INFO - [diffusion][Epoch 10172] Epoch 10173/12000
2024-11-05 02:21:16,715 - INFO - [diffusion][Epoch 10172] diffusion training Loss: 0.06604975648224354
2024-11-05 02:21:16,717 - INFO - [diffusion][Epoch 10172] diffusion learning rate: 0.001
2024-11-05 02:21:16,719 - INFO - [diffusion][Epoch 10172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:16,720 - INFO - [diffusion][Epoch 10173] Epoch 10174/12000
2024-11-05 02:21:20,261 - INFO - [diffusion][Epoch 10173] diffusion training Loss: 0.06935541331768036
2024-11-05 02:21:20,263 - INFO - [diffusion][Epoch 10173] diffusion learning rate: 0.001
2024-11-05 02:21:20,265 - INFO - [diffusion][Epoch 10173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:20,266 - INFO - [diffusion][Epoch 10174] Epoch 10175/12000
2024-11-05 02:21:23,455 - INFO - [diffusion][Epoch 10174] diffusion training Loss: 0.06700662896037102
2024-11-05 02:21:23,457 - INFO - [diffusion][Epoch 10174] diffusion learning rate: 0.001
2024-11-05 02:21:23,459 - INFO - [diffusion][Epoch 10174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:23,461 - INFO - [diffusion][Epoch 10175] Epoch 10176/12000
2024-11-05 02:21:26,553 - INFO - [diffusion][Epoch 10175] diffusion training Loss: 0.06754902470856905
2024-11-05 02:21:26,555 - INFO - [diffusion][Epoch 10175] diffusion learning rate: 0.001
2024-11-05 02:21:26,556 - INFO - [diffusion][Epoch 10175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:26,558 - INFO - [diffusion][Epoch 10176] Epoch 10177/12000
2024-11-05 02:21:30,125 - INFO - [diffusion][Epoch 10176] diffusion training Loss: 0.06916626915335655
2024-11-05 02:21:30,127 - INFO - [diffusion][Epoch 10176] diffusion learning rate: 0.001
2024-11-05 02:21:30,128 - INFO - [diffusion][Epoch 10176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:30,129 - INFO - [diffusion][Epoch 10177] Epoch 10178/12000
2024-11-05 02:21:33,520 - INFO - [diffusion][Epoch 10177] diffusion training Loss: 0.06437095254659653
2024-11-05 02:21:33,522 - INFO - [diffusion][Epoch 10177] diffusion learning rate: 0.001
2024-11-05 02:21:33,523 - INFO - [diffusion][Epoch 10177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:33,525 - INFO - [diffusion][Epoch 10178] Epoch 10179/12000
2024-11-05 02:21:37,194 - INFO - [diffusion][Epoch 10178] diffusion training Loss: 0.06788840889930725
2024-11-05 02:21:37,196 - INFO - [diffusion][Epoch 10178] diffusion learning rate: 0.001
2024-11-05 02:21:37,198 - INFO - [diffusion][Epoch 10178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:37,199 - INFO - [diffusion][Epoch 10179] Epoch 10180/12000
2024-11-05 02:21:40,300 - INFO - [diffusion][Epoch 10179] diffusion training Loss: 0.0690960306674242
2024-11-05 02:21:40,301 - INFO - [diffusion][Epoch 10179] diffusion learning rate: 0.001
2024-11-05 02:21:40,303 - INFO - [diffusion][Epoch 10179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:40,304 - INFO - [diffusion][Epoch 10180] Epoch 10181/12000
2024-11-05 02:21:43,349 - INFO - [diffusion][Epoch 10180] diffusion training Loss: 0.07063295319676399
2024-11-05 02:21:43,351 - INFO - [diffusion][Epoch 10180] diffusion learning rate: 0.001
2024-11-05 02:21:43,353 - INFO - [diffusion][Epoch 10180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:43,354 - INFO - [diffusion][Epoch 10181] Epoch 10182/12000
2024-11-05 02:21:46,474 - INFO - [diffusion][Epoch 10181] diffusion training Loss: 0.06570156011730433
2024-11-05 02:21:46,476 - INFO - [diffusion][Epoch 10181] diffusion learning rate: 0.001
2024-11-05 02:21:46,478 - INFO - [diffusion][Epoch 10181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:46,479 - INFO - [diffusion][Epoch 10182] Epoch 10183/12000
2024-11-05 02:21:49,977 - INFO - [diffusion][Epoch 10182] diffusion training Loss: 0.06301074288785458
2024-11-05 02:21:49,980 - INFO - [diffusion][Epoch 10182] diffusion learning rate: 0.001
2024-11-05 02:21:49,982 - INFO - [diffusion][Epoch 10182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:49,984 - INFO - [diffusion][Epoch 10183] Epoch 10184/12000
2024-11-05 02:21:53,251 - INFO - [diffusion][Epoch 10183] diffusion training Loss: 0.06534921284765005
2024-11-05 02:21:53,253 - INFO - [diffusion][Epoch 10183] diffusion learning rate: 0.001
2024-11-05 02:21:53,255 - INFO - [diffusion][Epoch 10183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:53,257 - INFO - [diffusion][Epoch 10184] Epoch 10185/12000
2024-11-05 02:21:56,247 - INFO - [diffusion][Epoch 10184] diffusion training Loss: 0.06430014967918396
2024-11-05 02:21:56,249 - INFO - [diffusion][Epoch 10184] diffusion learning rate: 0.001
2024-11-05 02:21:56,251 - INFO - [diffusion][Epoch 10184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:56,252 - INFO - [diffusion][Epoch 10185] Epoch 10186/12000
2024-11-05 02:21:59,341 - INFO - [diffusion][Epoch 10185] diffusion training Loss: 0.0658828979358077
2024-11-05 02:21:59,343 - INFO - [diffusion][Epoch 10185] diffusion learning rate: 0.001
2024-11-05 02:21:59,345 - INFO - [diffusion][Epoch 10185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:59,346 - INFO - [diffusion][Epoch 10186] Epoch 10187/12000
2024-11-05 02:22:02,908 - INFO - [diffusion][Epoch 10186] diffusion training Loss: 0.07030080072581768
2024-11-05 02:22:02,911 - INFO - [diffusion][Epoch 10186] diffusion learning rate: 0.001
2024-11-05 02:22:02,913 - INFO - [diffusion][Epoch 10186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:02,914 - INFO - [diffusion][Epoch 10187] Epoch 10188/12000
2024-11-05 02:22:06,336 - INFO - [diffusion][Epoch 10187] diffusion training Loss: 0.0700012594461441
2024-11-05 02:22:06,338 - INFO - [diffusion][Epoch 10187] diffusion learning rate: 0.001
2024-11-05 02:22:06,340 - INFO - [diffusion][Epoch 10187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:06,341 - INFO - [diffusion][Epoch 10188] Epoch 10189/12000
2024-11-05 02:22:09,461 - INFO - [diffusion][Epoch 10188] diffusion training Loss: 0.06712682917714119
2024-11-05 02:22:09,463 - INFO - [diffusion][Epoch 10188] diffusion learning rate: 0.001
2024-11-05 02:22:09,465 - INFO - [diffusion][Epoch 10188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:09,466 - INFO - [diffusion][Epoch 10189] Epoch 10190/12000
2024-11-05 02:22:12,410 - INFO - [diffusion][Epoch 10189] diffusion training Loss: 0.060677964240312576
2024-11-05 02:22:12,433 - INFO - [diffusion][Epoch 10189] diffusion learning rate: 0.001
2024-11-05 02:22:12,435 - INFO - [diffusion][Epoch 10189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:12,436 - INFO - [diffusion][Epoch 10190] Epoch 10191/12000
2024-11-05 02:22:15,853 - INFO - [diffusion][Epoch 10190] diffusion training Loss: 0.05975725408643484
2024-11-05 02:22:15,855 - INFO - [diffusion][Epoch 10190] diffusion learning rate: 0.001
2024-11-05 02:22:15,857 - INFO - [diffusion][Epoch 10190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:15,858 - INFO - [diffusion][Epoch 10191] Epoch 10192/12000
2024-11-05 02:22:19,547 - INFO - [diffusion][Epoch 10191] diffusion training Loss: 0.06383763533085585
2024-11-05 02:22:19,549 - INFO - [diffusion][Epoch 10191] diffusion learning rate: 0.001
2024-11-05 02:22:19,551 - INFO - [diffusion][Epoch 10191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:19,552 - INFO - [diffusion][Epoch 10192] Epoch 10193/12000
2024-11-05 02:22:22,771 - INFO - [diffusion][Epoch 10192] diffusion training Loss: 0.07126257568597794
2024-11-05 02:22:22,772 - INFO - [diffusion][Epoch 10192] diffusion learning rate: 0.001
2024-11-05 02:22:22,774 - INFO - [diffusion][Epoch 10192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:22,775 - INFO - [diffusion][Epoch 10193] Epoch 10194/12000
2024-11-05 02:22:25,862 - INFO - [diffusion][Epoch 10193] diffusion training Loss: 0.07049409300088882
2024-11-05 02:22:25,864 - INFO - [diffusion][Epoch 10193] diffusion learning rate: 0.001
2024-11-05 02:22:25,866 - INFO - [diffusion][Epoch 10193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:25,867 - INFO - [diffusion][Epoch 10194] Epoch 10195/12000
2024-11-05 02:22:28,978 - INFO - [diffusion][Epoch 10194] diffusion training Loss: 0.06773747317492962
2024-11-05 02:22:28,980 - INFO - [diffusion][Epoch 10194] diffusion learning rate: 0.001
2024-11-05 02:22:28,982 - INFO - [diffusion][Epoch 10194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:28,983 - INFO - [diffusion][Epoch 10195] Epoch 10196/12000
2024-11-05 02:22:32,518 - INFO - [diffusion][Epoch 10195] diffusion training Loss: 0.06742074992507696
2024-11-05 02:22:32,520 - INFO - [diffusion][Epoch 10195] diffusion learning rate: 0.001
2024-11-05 02:22:32,521 - INFO - [diffusion][Epoch 10195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:32,522 - INFO - [diffusion][Epoch 10196] Epoch 10197/12000
2024-11-05 02:22:35,653 - INFO - [diffusion][Epoch 10196] diffusion training Loss: 0.06370868161320686
2024-11-05 02:22:35,655 - INFO - [diffusion][Epoch 10196] diffusion learning rate: 0.001
2024-11-05 02:22:35,657 - INFO - [diffusion][Epoch 10196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:35,659 - INFO - [diffusion][Epoch 10197] Epoch 10198/12000
2024-11-05 02:22:38,896 - INFO - [diffusion][Epoch 10197] diffusion training Loss: 0.06915564741939306
2024-11-05 02:22:38,898 - INFO - [diffusion][Epoch 10197] diffusion learning rate: 0.001
2024-11-05 02:22:38,900 - INFO - [diffusion][Epoch 10197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:38,901 - INFO - [diffusion][Epoch 10198] Epoch 10199/12000
2024-11-05 02:22:42,095 - INFO - [diffusion][Epoch 10198] diffusion training Loss: 0.07188426330685616
2024-11-05 02:22:42,097 - INFO - [diffusion][Epoch 10198] diffusion learning rate: 0.001
2024-11-05 02:22:42,099 - INFO - [diffusion][Epoch 10198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:42,100 - INFO - [diffusion][Epoch 10199] Epoch 10200/12000
2024-11-05 02:22:45,485 - INFO - [diffusion][Epoch 10199] diffusion training Loss: 0.06092217192053795
2024-11-05 02:22:45,487 - INFO - [diffusion][Epoch 10199] diffusion learning rate: 0.001
2024-11-05 02:22:45,506 - INFO - [diffusion][Epoch 10199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:45,507 - INFO - [diffusion][Epoch 10200] Epoch 10201/12000
2024-11-05 02:22:48,971 - INFO - [diffusion][Epoch 10200] diffusion training Loss: 0.06767748109996319
2024-11-05 02:22:48,973 - INFO - [diffusion][Epoch 10200] diffusion learning rate: 0.001
2024-11-05 02:22:48,975 - INFO - [diffusion][Epoch 10200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:48,976 - INFO - [diffusion][Epoch 10201] Epoch 10202/12000
2024-11-05 02:22:52,091 - INFO - [diffusion][Epoch 10201] diffusion training Loss: 0.06552970316261053
2024-11-05 02:22:52,093 - INFO - [diffusion][Epoch 10201] diffusion learning rate: 0.001
2024-11-05 02:22:52,094 - INFO - [diffusion][Epoch 10201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:52,096 - INFO - [diffusion][Epoch 10202] Epoch 10203/12000
2024-11-05 02:22:55,199 - INFO - [diffusion][Epoch 10202] diffusion training Loss: 0.06517890561372042
2024-11-05 02:22:55,201 - INFO - [diffusion][Epoch 10202] diffusion learning rate: 0.001
2024-11-05 02:22:55,203 - INFO - [diffusion][Epoch 10202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:55,204 - INFO - [diffusion][Epoch 10203] Epoch 10204/12000
2024-11-05 02:22:58,439 - INFO - [diffusion][Epoch 10203] diffusion training Loss: 0.06783295422792435
2024-11-05 02:22:58,442 - INFO - [diffusion][Epoch 10203] diffusion learning rate: 0.001
2024-11-05 02:22:58,444 - INFO - [diffusion][Epoch 10203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:58,445 - INFO - [diffusion][Epoch 10204] Epoch 10205/12000
2024-11-05 02:23:01,977 - INFO - [diffusion][Epoch 10204] diffusion training Loss: 0.06688383594155312
2024-11-05 02:23:01,979 - INFO - [diffusion][Epoch 10204] diffusion learning rate: 0.001
2024-11-05 02:23:01,981 - INFO - [diffusion][Epoch 10204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:01,982 - INFO - [diffusion][Epoch 10205] Epoch 10206/12000
2024-11-05 02:23:05,538 - INFO - [diffusion][Epoch 10205] diffusion training Loss: 0.07000605762004852
2024-11-05 02:23:05,540 - INFO - [diffusion][Epoch 10205] diffusion learning rate: 0.001
2024-11-05 02:23:05,541 - INFO - [diffusion][Epoch 10205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:05,543 - INFO - [diffusion][Epoch 10206] Epoch 10207/12000
2024-11-05 02:23:08,674 - INFO - [diffusion][Epoch 10206] diffusion training Loss: 0.06754068657755852
2024-11-05 02:23:08,676 - INFO - [diffusion][Epoch 10206] diffusion learning rate: 0.001
2024-11-05 02:23:08,678 - INFO - [diffusion][Epoch 10206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:08,679 - INFO - [diffusion][Epoch 10207] Epoch 10208/12000
2024-11-05 02:23:11,793 - INFO - [diffusion][Epoch 10207] diffusion training Loss: 0.07070381566882133
2024-11-05 02:23:11,795 - INFO - [diffusion][Epoch 10207] diffusion learning rate: 0.001
2024-11-05 02:23:11,797 - INFO - [diffusion][Epoch 10207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:11,798 - INFO - [diffusion][Epoch 10208] Epoch 10209/12000
2024-11-05 02:23:14,923 - INFO - [diffusion][Epoch 10208] diffusion training Loss: 0.06937768124043941
2024-11-05 02:23:14,925 - INFO - [diffusion][Epoch 10208] diffusion learning rate: 0.001
2024-11-05 02:23:14,927 - INFO - [diffusion][Epoch 10208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:14,928 - INFO - [diffusion][Epoch 10209] Epoch 10210/12000
2024-11-05 02:23:18,452 - INFO - [diffusion][Epoch 10209] diffusion training Loss: 0.07077105343341827
2024-11-05 02:23:18,454 - INFO - [diffusion][Epoch 10209] diffusion learning rate: 0.001
2024-11-05 02:23:18,456 - INFO - [diffusion][Epoch 10209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:18,457 - INFO - [diffusion][Epoch 10210] Epoch 10211/12000
2024-11-05 02:23:21,944 - INFO - [diffusion][Epoch 10210] diffusion training Loss: 0.06826626881957054
2024-11-05 02:23:21,946 - INFO - [diffusion][Epoch 10210] diffusion learning rate: 0.001
2024-11-05 02:23:21,947 - INFO - [diffusion][Epoch 10210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:21,948 - INFO - [diffusion][Epoch 10211] Epoch 10212/12000
2024-11-05 02:23:25,004 - INFO - [diffusion][Epoch 10211] diffusion training Loss: 0.06704635173082352
2024-11-05 02:23:25,007 - INFO - [diffusion][Epoch 10211] diffusion learning rate: 0.001
2024-11-05 02:23:25,009 - INFO - [diffusion][Epoch 10211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:25,010 - INFO - [diffusion][Epoch 10212] Epoch 10213/12000
2024-11-05 02:23:28,175 - INFO - [diffusion][Epoch 10212] diffusion training Loss: 0.07812815345823765
2024-11-05 02:23:28,177 - INFO - [diffusion][Epoch 10212] diffusion learning rate: 0.001
2024-11-05 02:23:28,179 - INFO - [diffusion][Epoch 10212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:28,180 - INFO - [diffusion][Epoch 10213] Epoch 10214/12000
2024-11-05 02:23:31,387 - INFO - [diffusion][Epoch 10213] diffusion training Loss: 0.06544368341565132
2024-11-05 02:23:31,388 - INFO - [diffusion][Epoch 10213] diffusion learning rate: 0.001
2024-11-05 02:23:31,390 - INFO - [diffusion][Epoch 10213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:31,391 - INFO - [diffusion][Epoch 10214] Epoch 10215/12000
2024-11-05 02:23:35,034 - INFO - [diffusion][Epoch 10214] diffusion training Loss: 0.07033868320286274
2024-11-05 02:23:35,036 - INFO - [diffusion][Epoch 10214] diffusion learning rate: 0.001
2024-11-05 02:23:35,038 - INFO - [diffusion][Epoch 10214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:35,039 - INFO - [diffusion][Epoch 10215] Epoch 10216/12000
2024-11-05 02:23:38,416 - INFO - [diffusion][Epoch 10215] diffusion training Loss: 0.06490490958094597
2024-11-05 02:23:38,418 - INFO - [diffusion][Epoch 10215] diffusion learning rate: 0.001
2024-11-05 02:23:38,420 - INFO - [diffusion][Epoch 10215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:38,421 - INFO - [diffusion][Epoch 10216] Epoch 10217/12000
2024-11-05 02:23:41,370 - INFO - [diffusion][Epoch 10216] diffusion training Loss: 0.06505575682967901
2024-11-05 02:23:41,372 - INFO - [diffusion][Epoch 10216] diffusion learning rate: 0.001
2024-11-05 02:23:41,374 - INFO - [diffusion][Epoch 10216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:41,375 - INFO - [diffusion][Epoch 10217] Epoch 10218/12000
2024-11-05 02:23:44,531 - INFO - [diffusion][Epoch 10217] diffusion training Loss: 0.06756201665848494
2024-11-05 02:23:44,533 - INFO - [diffusion][Epoch 10217] diffusion learning rate: 0.001
2024-11-05 02:23:44,535 - INFO - [diffusion][Epoch 10217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:44,536 - INFO - [diffusion][Epoch 10218] Epoch 10219/12000
2024-11-05 02:23:48,075 - INFO - [diffusion][Epoch 10218] diffusion training Loss: 0.07287565991282463
2024-11-05 02:23:48,077 - INFO - [diffusion][Epoch 10218] diffusion learning rate: 0.001
2024-11-05 02:23:48,079 - INFO - [diffusion][Epoch 10218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:48,081 - INFO - [diffusion][Epoch 10219] Epoch 10220/12000
2024-11-05 02:23:51,583 - INFO - [diffusion][Epoch 10219] diffusion training Loss: 0.06670669838786125
2024-11-05 02:23:51,586 - INFO - [diffusion][Epoch 10219] diffusion learning rate: 0.001
2024-11-05 02:23:51,589 - INFO - [diffusion][Epoch 10219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:51,591 - INFO - [diffusion][Epoch 10220] Epoch 10221/12000
2024-11-05 02:23:55,094 - INFO - [diffusion][Epoch 10220] diffusion training Loss: 0.07180012203752995
2024-11-05 02:23:55,096 - INFO - [diffusion][Epoch 10220] diffusion learning rate: 0.001
2024-11-05 02:23:55,098 - INFO - [diffusion][Epoch 10220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:55,099 - INFO - [diffusion][Epoch 10221] Epoch 10222/12000
2024-11-05 02:23:58,195 - INFO - [diffusion][Epoch 10221] diffusion training Loss: 0.06425807904452085
2024-11-05 02:23:58,197 - INFO - [diffusion][Epoch 10221] diffusion learning rate: 0.001
2024-11-05 02:23:58,198 - INFO - [diffusion][Epoch 10221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:58,200 - INFO - [diffusion][Epoch 10222] Epoch 10223/12000
2024-11-05 02:24:01,268 - INFO - [diffusion][Epoch 10222] diffusion training Loss: 0.06275650672614574
2024-11-05 02:24:01,270 - INFO - [diffusion][Epoch 10222] diffusion learning rate: 0.001
2024-11-05 02:24:01,272 - INFO - [diffusion][Epoch 10222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:01,273 - INFO - [diffusion][Epoch 10223] Epoch 10224/12000
2024-11-05 02:24:04,661 - INFO - [diffusion][Epoch 10223] diffusion training Loss: 0.06284279935061932
2024-11-05 02:24:04,663 - INFO - [diffusion][Epoch 10223] diffusion learning rate: 0.001
2024-11-05 02:24:04,665 - INFO - [diffusion][Epoch 10223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:04,667 - INFO - [diffusion][Epoch 10224] Epoch 10225/12000
2024-11-05 02:24:08,317 - INFO - [diffusion][Epoch 10224] diffusion training Loss: 0.06631877459585667
2024-11-05 02:24:08,319 - INFO - [diffusion][Epoch 10224] diffusion learning rate: 0.001
2024-11-05 02:24:08,320 - INFO - [diffusion][Epoch 10224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:08,322 - INFO - [diffusion][Epoch 10225] Epoch 10226/12000
2024-11-05 02:24:11,519 - INFO - [diffusion][Epoch 10225] diffusion training Loss: 0.07435763999819756
2024-11-05 02:24:11,521 - INFO - [diffusion][Epoch 10225] diffusion learning rate: 0.001
2024-11-05 02:24:11,522 - INFO - [diffusion][Epoch 10225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:11,524 - INFO - [diffusion][Epoch 10226] Epoch 10227/12000
2024-11-05 02:24:14,632 - INFO - [diffusion][Epoch 10226] diffusion training Loss: 0.06795479170978069
2024-11-05 02:24:14,635 - INFO - [diffusion][Epoch 10226] diffusion learning rate: 0.001
2024-11-05 02:24:14,636 - INFO - [diffusion][Epoch 10226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:14,638 - INFO - [diffusion][Epoch 10227] Epoch 10228/12000
2024-11-05 02:24:17,751 - INFO - [diffusion][Epoch 10227] diffusion training Loss: 0.06699215434491634
2024-11-05 02:24:17,753 - INFO - [diffusion][Epoch 10227] diffusion learning rate: 0.001
2024-11-05 02:24:17,755 - INFO - [diffusion][Epoch 10227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:17,756 - INFO - [diffusion][Epoch 10228] Epoch 10229/12000
2024-11-05 02:24:21,308 - INFO - [diffusion][Epoch 10228] diffusion training Loss: 0.0638984739780426
2024-11-05 02:24:21,310 - INFO - [diffusion][Epoch 10228] diffusion learning rate: 0.001
2024-11-05 02:24:21,312 - INFO - [diffusion][Epoch 10228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:21,313 - INFO - [diffusion][Epoch 10229] Epoch 10230/12000
2024-11-05 02:24:24,783 - INFO - [diffusion][Epoch 10229] diffusion training Loss: 0.06345834210515022
2024-11-05 02:24:24,785 - INFO - [diffusion][Epoch 10229] diffusion learning rate: 0.001
2024-11-05 02:24:24,799 - INFO - [diffusion][Epoch 10229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:24,801 - INFO - [diffusion][Epoch 10230] Epoch 10231/12000
2024-11-05 02:24:27,899 - INFO - [diffusion][Epoch 10230] diffusion training Loss: 0.0664614038541913
2024-11-05 02:24:27,901 - INFO - [diffusion][Epoch 10230] diffusion learning rate: 0.001
2024-11-05 02:24:27,903 - INFO - [diffusion][Epoch 10230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:27,904 - INFO - [diffusion][Epoch 10231] Epoch 10232/12000
2024-11-05 02:24:31,038 - INFO - [diffusion][Epoch 10231] diffusion training Loss: 0.05857135076075792
2024-11-05 02:24:31,040 - INFO - [diffusion][Epoch 10231] diffusion learning rate: 0.001
2024-11-05 02:24:31,042 - INFO - [diffusion][Epoch 10231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:31,043 - INFO - [diffusion][Epoch 10232] Epoch 10233/12000
2024-11-05 02:24:34,350 - INFO - [diffusion][Epoch 10232] diffusion training Loss: 0.06553438305854797
2024-11-05 02:24:34,352 - INFO - [diffusion][Epoch 10232] diffusion learning rate: 0.001
2024-11-05 02:24:34,353 - INFO - [diffusion][Epoch 10232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:34,355 - INFO - [diffusion][Epoch 10233] Epoch 10234/12000
2024-11-05 02:24:37,981 - INFO - [diffusion][Epoch 10233] diffusion training Loss: 0.06343307066708803
2024-11-05 02:24:37,983 - INFO - [diffusion][Epoch 10233] diffusion learning rate: 0.001
2024-11-05 02:24:37,985 - INFO - [diffusion][Epoch 10233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:37,986 - INFO - [diffusion][Epoch 10234] Epoch 10235/12000
2024-11-05 02:24:41,090 - INFO - [diffusion][Epoch 10234] diffusion training Loss: 0.05917569436132908
2024-11-05 02:24:41,092 - INFO - [diffusion][Epoch 10234] diffusion learning rate: 0.001
2024-11-05 02:24:41,094 - INFO - [diffusion][Epoch 10234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:41,095 - INFO - [diffusion][Epoch 10235] Epoch 10236/12000
2024-11-05 02:24:44,202 - INFO - [diffusion][Epoch 10235] diffusion training Loss: 0.06052516680210829
2024-11-05 02:24:44,204 - INFO - [diffusion][Epoch 10235] diffusion learning rate: 0.001
2024-11-05 02:24:44,207 - INFO - [diffusion][Epoch 10235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:44,208 - INFO - [diffusion][Epoch 10236] Epoch 10237/12000
2024-11-05 02:24:47,337 - INFO - [diffusion][Epoch 10236] diffusion training Loss: 0.06659565586596727
2024-11-05 02:24:47,339 - INFO - [diffusion][Epoch 10236] diffusion learning rate: 0.001
2024-11-05 02:24:47,341 - INFO - [diffusion][Epoch 10236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:47,342 - INFO - [diffusion][Epoch 10237] Epoch 10238/12000
2024-11-05 02:24:51,260 - INFO - [diffusion][Epoch 10237] diffusion training Loss: 0.06650328263640404
2024-11-05 02:24:51,262 - INFO - [diffusion][Epoch 10237] diffusion learning rate: 0.001
2024-11-05 02:24:51,264 - INFO - [diffusion][Epoch 10237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:51,265 - INFO - [diffusion][Epoch 10238] Epoch 10239/12000
2024-11-05 02:24:54,955 - INFO - [diffusion][Epoch 10238] diffusion training Loss: 0.06168450228869915
2024-11-05 02:24:54,957 - INFO - [diffusion][Epoch 10238] diffusion learning rate: 0.001
2024-11-05 02:24:54,959 - INFO - [diffusion][Epoch 10238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:54,960 - INFO - [diffusion][Epoch 10239] Epoch 10240/12000
2024-11-05 02:24:58,155 - INFO - [diffusion][Epoch 10239] diffusion training Loss: 0.06908566504716873
2024-11-05 02:24:58,157 - INFO - [diffusion][Epoch 10239] diffusion learning rate: 0.001
2024-11-05 02:24:58,159 - INFO - [diffusion][Epoch 10239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:58,160 - INFO - [diffusion][Epoch 10240] Epoch 10241/12000
2024-11-05 02:25:01,272 - INFO - [diffusion][Epoch 10240] diffusion training Loss: 0.06558717507869005
2024-11-05 02:25:01,275 - INFO - [diffusion][Epoch 10240] diffusion learning rate: 0.001
2024-11-05 02:25:01,277 - INFO - [diffusion][Epoch 10240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:01,278 - INFO - [diffusion][Epoch 10241] Epoch 10242/12000
2024-11-05 02:25:04,431 - INFO - [diffusion][Epoch 10241] diffusion training Loss: 0.06951558589935303
2024-11-05 02:25:04,433 - INFO - [diffusion][Epoch 10241] diffusion learning rate: 0.001
2024-11-05 02:25:04,435 - INFO - [diffusion][Epoch 10241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:04,436 - INFO - [diffusion][Epoch 10242] Epoch 10243/12000
2024-11-05 02:25:07,955 - INFO - [diffusion][Epoch 10242] diffusion training Loss: 0.0665696682408452
2024-11-05 02:25:07,957 - INFO - [diffusion][Epoch 10242] diffusion learning rate: 0.001
2024-11-05 02:25:07,958 - INFO - [diffusion][Epoch 10242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:07,960 - INFO - [diffusion][Epoch 10243] Epoch 10244/12000
2024-11-05 02:25:11,477 - INFO - [diffusion][Epoch 10243] diffusion training Loss: 0.06219321023672819
2024-11-05 02:25:11,479 - INFO - [diffusion][Epoch 10243] diffusion learning rate: 0.001
2024-11-05 02:25:11,480 - INFO - [diffusion][Epoch 10243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:11,482 - INFO - [diffusion][Epoch 10244] Epoch 10245/12000
2024-11-05 02:25:14,396 - INFO - [diffusion][Epoch 10244] diffusion training Loss: 0.06411589868366718
2024-11-05 02:25:14,398 - INFO - [diffusion][Epoch 10244] diffusion learning rate: 0.001
2024-11-05 02:25:14,400 - INFO - [diffusion][Epoch 10244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:14,401 - INFO - [diffusion][Epoch 10245] Epoch 10246/12000
2024-11-05 02:25:17,315 - INFO - [diffusion][Epoch 10245] diffusion training Loss: 0.06654572300612926
2024-11-05 02:25:17,318 - INFO - [diffusion][Epoch 10245] diffusion learning rate: 0.001
2024-11-05 02:25:17,320 - INFO - [diffusion][Epoch 10245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:17,321 - INFO - [diffusion][Epoch 10246] Epoch 10247/12000
2024-11-05 02:25:20,788 - INFO - [diffusion][Epoch 10246] diffusion training Loss: 0.06494186352938414
2024-11-05 02:25:20,789 - INFO - [diffusion][Epoch 10246] diffusion learning rate: 0.001
2024-11-05 02:25:20,791 - INFO - [diffusion][Epoch 10246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:20,792 - INFO - [diffusion][Epoch 10247] Epoch 10248/12000
2024-11-05 02:25:24,327 - INFO - [diffusion][Epoch 10247] diffusion training Loss: 0.06998269818723202
2024-11-05 02:25:24,328 - INFO - [diffusion][Epoch 10247] diffusion learning rate: 0.001
2024-11-05 02:25:24,330 - INFO - [diffusion][Epoch 10247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:24,331 - INFO - [diffusion][Epoch 10248] Epoch 10249/12000
2024-11-05 02:25:26,980 - INFO - [diffusion][Epoch 10248] diffusion training Loss: 0.07031525485217571
2024-11-05 02:25:26,983 - INFO - [diffusion][Epoch 10248] diffusion learning rate: 0.001
2024-11-05 02:25:26,984 - INFO - [diffusion][Epoch 10248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:26,986 - INFO - [diffusion][Epoch 10249] Epoch 10250/12000
2024-11-05 02:25:30,181 - INFO - [diffusion][Epoch 10249] diffusion training Loss: 0.06312916055321693
2024-11-05 02:25:30,183 - INFO - [diffusion][Epoch 10249] diffusion learning rate: 0.001
2024-11-05 02:25:30,185 - INFO - [diffusion][Epoch 10249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:30,186 - INFO - [diffusion][Epoch 10250] Epoch 10251/12000
2024-11-05 02:25:33,828 - INFO - [diffusion][Epoch 10250] diffusion training Loss: 0.0599403316155076
2024-11-05 02:25:33,830 - INFO - [diffusion][Epoch 10250] diffusion learning rate: 0.001
2024-11-05 02:25:33,831 - INFO - [diffusion][Epoch 10250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:33,833 - INFO - [diffusion][Epoch 10251] Epoch 10252/12000
2024-11-05 02:25:37,405 - INFO - [diffusion][Epoch 10251] diffusion training Loss: 0.06820627301931381
2024-11-05 02:25:37,406 - INFO - [diffusion][Epoch 10251] diffusion learning rate: 0.001
2024-11-05 02:25:37,408 - INFO - [diffusion][Epoch 10251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:37,409 - INFO - [diffusion][Epoch 10252] Epoch 10253/12000
2024-11-05 02:25:40,530 - INFO - [diffusion][Epoch 10252] diffusion training Loss: 0.07338551431894302
2024-11-05 02:25:40,532 - INFO - [diffusion][Epoch 10252] diffusion learning rate: 0.001
2024-11-05 02:25:40,534 - INFO - [diffusion][Epoch 10252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:40,535 - INFO - [diffusion][Epoch 10253] Epoch 10254/12000
2024-11-05 02:25:43,756 - INFO - [diffusion][Epoch 10253] diffusion training Loss: 0.06717485561966896
2024-11-05 02:25:43,758 - INFO - [diffusion][Epoch 10253] diffusion learning rate: 0.001
2024-11-05 02:25:43,760 - INFO - [diffusion][Epoch 10253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:43,761 - INFO - [diffusion][Epoch 10254] Epoch 10255/12000
2024-11-05 02:25:46,968 - INFO - [diffusion][Epoch 10254] diffusion training Loss: 0.06566819176077843
2024-11-05 02:25:46,969 - INFO - [diffusion][Epoch 10254] diffusion learning rate: 0.001
2024-11-05 02:25:46,971 - INFO - [diffusion][Epoch 10254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:46,972 - INFO - [diffusion][Epoch 10255] Epoch 10256/12000
2024-11-05 02:25:50,704 - INFO - [diffusion][Epoch 10255] diffusion training Loss: 0.06705875974148512
2024-11-05 02:25:50,706 - INFO - [diffusion][Epoch 10255] diffusion learning rate: 0.001
2024-11-05 02:25:50,708 - INFO - [diffusion][Epoch 10255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:50,710 - INFO - [diffusion][Epoch 10256] Epoch 10257/12000
2024-11-05 02:25:54,278 - INFO - [diffusion][Epoch 10256] diffusion training Loss: 0.06916594691574574
2024-11-05 02:25:54,280 - INFO - [diffusion][Epoch 10256] diffusion learning rate: 0.001
2024-11-05 02:25:54,281 - INFO - [diffusion][Epoch 10256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:54,283 - INFO - [diffusion][Epoch 10257] Epoch 10258/12000
2024-11-05 02:25:57,930 - INFO - [diffusion][Epoch 10257] diffusion training Loss: 0.06581748090684414
2024-11-05 02:25:57,932 - INFO - [diffusion][Epoch 10257] diffusion learning rate: 0.001
2024-11-05 02:25:57,933 - INFO - [diffusion][Epoch 10257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:57,934 - INFO - [diffusion][Epoch 10258] Epoch 10259/12000
2024-11-05 02:26:01,056 - INFO - [diffusion][Epoch 10258] diffusion training Loss: 0.0690787024796009
2024-11-05 02:26:01,057 - INFO - [diffusion][Epoch 10258] diffusion learning rate: 0.001
2024-11-05 02:26:01,059 - INFO - [diffusion][Epoch 10258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:01,060 - INFO - [diffusion][Epoch 10259] Epoch 10260/12000
2024-11-05 02:26:04,196 - INFO - [diffusion][Epoch 10259] diffusion training Loss: 0.06705105118453503
2024-11-05 02:26:04,199 - INFO - [diffusion][Epoch 10259] diffusion learning rate: 0.001
2024-11-05 02:26:04,201 - INFO - [diffusion][Epoch 10259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:04,202 - INFO - [diffusion][Epoch 10260] Epoch 10261/12000
2024-11-05 02:26:07,686 - INFO - [diffusion][Epoch 10260] diffusion training Loss: 0.06795993447303772
2024-11-05 02:26:07,689 - INFO - [diffusion][Epoch 10260] diffusion learning rate: 0.001
2024-11-05 02:26:07,691 - INFO - [diffusion][Epoch 10260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:07,693 - INFO - [diffusion][Epoch 10261] Epoch 10262/12000
2024-11-05 02:26:11,227 - INFO - [diffusion][Epoch 10261] diffusion training Loss: 0.06803062185645103
2024-11-05 02:26:11,230 - INFO - [diffusion][Epoch 10261] diffusion learning rate: 0.001
2024-11-05 02:26:11,232 - INFO - [diffusion][Epoch 10261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:11,234 - INFO - [diffusion][Epoch 10262] Epoch 10263/12000
2024-11-05 02:26:14,326 - INFO - [diffusion][Epoch 10262] diffusion training Loss: 0.07043978851288557
2024-11-05 02:26:14,328 - INFO - [diffusion][Epoch 10262] diffusion learning rate: 0.001
2024-11-05 02:26:14,330 - INFO - [diffusion][Epoch 10262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:14,331 - INFO - [diffusion][Epoch 10263] Epoch 10264/12000
2024-11-05 02:26:17,463 - INFO - [diffusion][Epoch 10263] diffusion training Loss: 0.06375493761152029
2024-11-05 02:26:17,465 - INFO - [diffusion][Epoch 10263] diffusion learning rate: 0.001
2024-11-05 02:26:17,467 - INFO - [diffusion][Epoch 10263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:17,468 - INFO - [diffusion][Epoch 10264] Epoch 10265/12000
2024-11-05 02:26:20,634 - INFO - [diffusion][Epoch 10264] diffusion training Loss: 0.07214036956429482
2024-11-05 02:26:20,636 - INFO - [diffusion][Epoch 10264] diffusion learning rate: 0.001
2024-11-05 02:26:20,638 - INFO - [diffusion][Epoch 10264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:20,639 - INFO - [diffusion][Epoch 10265] Epoch 10266/12000
2024-11-05 02:26:24,117 - INFO - [diffusion][Epoch 10265] diffusion training Loss: 0.06991077959537506
2024-11-05 02:26:24,120 - INFO - [diffusion][Epoch 10265] diffusion learning rate: 0.001
2024-11-05 02:26:24,123 - INFO - [diffusion][Epoch 10265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:24,124 - INFO - [diffusion][Epoch 10266] Epoch 10267/12000
2024-11-05 02:26:27,384 - INFO - [diffusion][Epoch 10266] diffusion training Loss: 0.06639813724905252
2024-11-05 02:26:27,387 - INFO - [diffusion][Epoch 10266] diffusion learning rate: 0.001
2024-11-05 02:26:27,389 - INFO - [diffusion][Epoch 10266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:27,390 - INFO - [diffusion][Epoch 10267] Epoch 10268/12000
2024-11-05 02:26:30,505 - INFO - [diffusion][Epoch 10267] diffusion training Loss: 0.06599122937768698
2024-11-05 02:26:30,507 - INFO - [diffusion][Epoch 10267] diffusion learning rate: 0.001
2024-11-05 02:26:30,509 - INFO - [diffusion][Epoch 10267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:30,510 - INFO - [diffusion][Epoch 10268] Epoch 10269/12000
2024-11-05 02:26:33,523 - INFO - [diffusion][Epoch 10268] diffusion training Loss: 0.06995253264904022
2024-11-05 02:26:33,526 - INFO - [diffusion][Epoch 10268] diffusion learning rate: 0.001
2024-11-05 02:26:33,528 - INFO - [diffusion][Epoch 10268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:33,530 - INFO - [diffusion][Epoch 10269] Epoch 10270/12000
2024-11-05 02:26:37,054 - INFO - [diffusion][Epoch 10269] diffusion training Loss: 0.06673908699303865
2024-11-05 02:26:37,055 - INFO - [diffusion][Epoch 10269] diffusion learning rate: 0.001
2024-11-05 02:26:37,057 - INFO - [diffusion][Epoch 10269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:37,058 - INFO - [diffusion][Epoch 10270] Epoch 10271/12000
2024-11-05 02:26:40,583 - INFO - [diffusion][Epoch 10270] diffusion training Loss: 0.0651619778946042
2024-11-05 02:26:40,585 - INFO - [diffusion][Epoch 10270] diffusion learning rate: 0.001
2024-11-05 02:26:40,587 - INFO - [diffusion][Epoch 10270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:40,588 - INFO - [diffusion][Epoch 10271] Epoch 10272/12000
2024-11-05 02:26:43,891 - INFO - [diffusion][Epoch 10271] diffusion training Loss: 0.06913790851831436
2024-11-05 02:26:43,893 - INFO - [diffusion][Epoch 10271] diffusion learning rate: 0.001
2024-11-05 02:26:43,895 - INFO - [diffusion][Epoch 10271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:43,896 - INFO - [diffusion][Epoch 10272] Epoch 10273/12000
2024-11-05 02:26:47,043 - INFO - [diffusion][Epoch 10272] diffusion training Loss: 0.06434931233525276
2024-11-05 02:26:47,046 - INFO - [diffusion][Epoch 10272] diffusion learning rate: 0.001
2024-11-05 02:26:47,049 - INFO - [diffusion][Epoch 10272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:47,050 - INFO - [diffusion][Epoch 10273] Epoch 10274/12000
2024-11-05 02:26:50,190 - INFO - [diffusion][Epoch 10273] diffusion training Loss: 0.07134324498474598
2024-11-05 02:26:50,192 - INFO - [diffusion][Epoch 10273] diffusion learning rate: 0.001
2024-11-05 02:26:50,194 - INFO - [diffusion][Epoch 10273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:50,195 - INFO - [diffusion][Epoch 10274] Epoch 10275/12000
2024-11-05 02:26:53,634 - INFO - [diffusion][Epoch 10274] diffusion training Loss: 0.05971864890307188
2024-11-05 02:26:53,636 - INFO - [diffusion][Epoch 10274] diffusion learning rate: 0.001
2024-11-05 02:26:53,637 - INFO - [diffusion][Epoch 10274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:53,639 - INFO - [diffusion][Epoch 10275] Epoch 10276/12000
2024-11-05 02:26:56,919 - INFO - [diffusion][Epoch 10275] diffusion training Loss: 0.06862861476838589
2024-11-05 02:26:56,921 - INFO - [diffusion][Epoch 10275] diffusion learning rate: 0.001
2024-11-05 02:26:56,923 - INFO - [diffusion][Epoch 10275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:56,924 - INFO - [diffusion][Epoch 10276] Epoch 10277/12000
2024-11-05 02:27:00,081 - INFO - [diffusion][Epoch 10276] diffusion training Loss: 0.06777933053672314
2024-11-05 02:27:00,083 - INFO - [diffusion][Epoch 10276] diffusion learning rate: 0.001
2024-11-05 02:27:00,085 - INFO - [diffusion][Epoch 10276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:00,086 - INFO - [diffusion][Epoch 10277] Epoch 10278/12000
2024-11-05 02:27:03,734 - INFO - [diffusion][Epoch 10277] diffusion training Loss: 0.07149457558989525
2024-11-05 02:27:03,736 - INFO - [diffusion][Epoch 10277] diffusion learning rate: 0.001
2024-11-05 02:27:03,737 - INFO - [diffusion][Epoch 10277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:03,739 - INFO - [diffusion][Epoch 10278] Epoch 10279/12000
2024-11-05 02:27:06,931 - INFO - [diffusion][Epoch 10278] diffusion training Loss: 0.06833178270608187
2024-11-05 02:27:06,934 - INFO - [diffusion][Epoch 10278] diffusion learning rate: 0.001
2024-11-05 02:27:06,982 - INFO - [diffusion][Epoch 10278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:06,983 - INFO - [diffusion][Epoch 10279] Epoch 10280/12000
2024-11-05 02:27:10,342 - INFO - [diffusion][Epoch 10279] diffusion training Loss: 0.05992154777050018
2024-11-05 02:27:10,345 - INFO - [diffusion][Epoch 10279] diffusion learning rate: 0.001
2024-11-05 02:27:10,346 - INFO - [diffusion][Epoch 10279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:10,348 - INFO - [diffusion][Epoch 10280] Epoch 10281/12000
2024-11-05 02:27:13,968 - INFO - [diffusion][Epoch 10280] diffusion training Loss: 0.06118769571185112
2024-11-05 02:27:13,970 - INFO - [diffusion][Epoch 10280] diffusion learning rate: 0.001
2024-11-05 02:27:13,972 - INFO - [diffusion][Epoch 10280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:13,973 - INFO - [diffusion][Epoch 10281] Epoch 10282/12000
2024-11-05 02:27:17,175 - INFO - [diffusion][Epoch 10281] diffusion training Loss: 0.06425475515425205
2024-11-05 02:27:17,177 - INFO - [diffusion][Epoch 10281] diffusion learning rate: 0.001
2024-11-05 02:27:17,179 - INFO - [diffusion][Epoch 10281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:17,180 - INFO - [diffusion][Epoch 10282] Epoch 10283/12000
2024-11-05 02:27:20,331 - INFO - [diffusion][Epoch 10282] diffusion training Loss: 0.06471767742186785
2024-11-05 02:27:20,333 - INFO - [diffusion][Epoch 10282] diffusion learning rate: 0.001
2024-11-05 02:27:20,334 - INFO - [diffusion][Epoch 10282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:20,336 - INFO - [diffusion][Epoch 10283] Epoch 10284/12000
2024-11-05 02:27:23,486 - INFO - [diffusion][Epoch 10283] diffusion training Loss: 0.06693977676331997
2024-11-05 02:27:23,488 - INFO - [diffusion][Epoch 10283] diffusion learning rate: 0.001
2024-11-05 02:27:23,490 - INFO - [diffusion][Epoch 10283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:23,491 - INFO - [diffusion][Epoch 10284] Epoch 10285/12000
2024-11-05 02:27:27,015 - INFO - [diffusion][Epoch 10284] diffusion training Loss: 0.06543085537850857
2024-11-05 02:27:27,017 - INFO - [diffusion][Epoch 10284] diffusion learning rate: 0.001
2024-11-05 02:27:27,019 - INFO - [diffusion][Epoch 10284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:27,020 - INFO - [diffusion][Epoch 10285] Epoch 10286/12000
2024-11-05 02:27:30,818 - INFO - [diffusion][Epoch 10285] diffusion training Loss: 0.06656651385128498
2024-11-05 02:27:30,820 - INFO - [diffusion][Epoch 10285] diffusion learning rate: 0.001
2024-11-05 02:27:30,822 - INFO - [diffusion][Epoch 10285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:30,823 - INFO - [diffusion][Epoch 10286] Epoch 10287/12000
2024-11-05 02:27:34,194 - INFO - [diffusion][Epoch 10286] diffusion training Loss: 0.0662670386955142
2024-11-05 02:27:34,196 - INFO - [diffusion][Epoch 10286] diffusion learning rate: 0.001
2024-11-05 02:27:34,197 - INFO - [diffusion][Epoch 10286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:34,199 - INFO - [diffusion][Epoch 10287] Epoch 10288/12000
2024-11-05 02:27:37,285 - INFO - [diffusion][Epoch 10287] diffusion training Loss: 0.06467882543802261
2024-11-05 02:27:37,287 - INFO - [diffusion][Epoch 10287] diffusion learning rate: 0.001
2024-11-05 02:27:37,288 - INFO - [diffusion][Epoch 10287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:37,290 - INFO - [diffusion][Epoch 10288] Epoch 10289/12000
2024-11-05 02:27:40,455 - INFO - [diffusion][Epoch 10288] diffusion training Loss: 0.06376155465841293
2024-11-05 02:27:40,457 - INFO - [diffusion][Epoch 10288] diffusion learning rate: 0.001
2024-11-05 02:27:40,458 - INFO - [diffusion][Epoch 10288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:40,459 - INFO - [diffusion][Epoch 10289] Epoch 10290/12000
2024-11-05 02:27:43,860 - INFO - [diffusion][Epoch 10289] diffusion training Loss: 0.06264711264520884
2024-11-05 02:27:43,862 - INFO - [diffusion][Epoch 10289] diffusion learning rate: 0.001
2024-11-05 02:27:43,864 - INFO - [diffusion][Epoch 10289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:43,865 - INFO - [diffusion][Epoch 10290] Epoch 10291/12000
2024-11-05 02:27:47,434 - INFO - [diffusion][Epoch 10290] diffusion training Loss: 0.06905136350542307
2024-11-05 02:27:47,435 - INFO - [diffusion][Epoch 10290] diffusion learning rate: 0.001
2024-11-05 02:27:47,437 - INFO - [diffusion][Epoch 10290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:47,438 - INFO - [diffusion][Epoch 10291] Epoch 10292/12000
2024-11-05 02:27:50,537 - INFO - [diffusion][Epoch 10291] diffusion training Loss: 0.06459158100187778
2024-11-05 02:27:50,539 - INFO - [diffusion][Epoch 10291] diffusion learning rate: 0.001
2024-11-05 02:27:50,541 - INFO - [diffusion][Epoch 10291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:50,542 - INFO - [diffusion][Epoch 10292] Epoch 10293/12000
2024-11-05 02:27:53,522 - INFO - [diffusion][Epoch 10292] diffusion training Loss: 0.07289759628474712
2024-11-05 02:27:53,524 - INFO - [diffusion][Epoch 10292] diffusion learning rate: 0.001
2024-11-05 02:27:53,526 - INFO - [diffusion][Epoch 10292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:53,528 - INFO - [diffusion][Epoch 10293] Epoch 10294/12000
2024-11-05 02:27:56,687 - INFO - [diffusion][Epoch 10293] diffusion training Loss: 0.06489306408911943
2024-11-05 02:27:56,689 - INFO - [diffusion][Epoch 10293] diffusion learning rate: 0.001
2024-11-05 02:27:56,690 - INFO - [diffusion][Epoch 10293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:56,692 - INFO - [diffusion][Epoch 10294] Epoch 10295/12000
2024-11-05 02:28:00,383 - INFO - [diffusion][Epoch 10294] diffusion training Loss: 0.06701800785958767
2024-11-05 02:28:00,385 - INFO - [diffusion][Epoch 10294] diffusion learning rate: 0.001
2024-11-05 02:28:00,386 - INFO - [diffusion][Epoch 10294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:00,388 - INFO - [diffusion][Epoch 10295] Epoch 10296/12000
2024-11-05 02:28:03,729 - INFO - [diffusion][Epoch 10295] diffusion training Loss: 0.07269543223083019
2024-11-05 02:28:03,731 - INFO - [diffusion][Epoch 10295] diffusion learning rate: 0.001
2024-11-05 02:28:03,733 - INFO - [diffusion][Epoch 10295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:03,734 - INFO - [diffusion][Epoch 10296] Epoch 10297/12000
2024-11-05 02:28:06,798 - INFO - [diffusion][Epoch 10296] diffusion training Loss: 0.06647630315274
2024-11-05 02:28:06,800 - INFO - [diffusion][Epoch 10296] diffusion learning rate: 0.001
2024-11-05 02:28:06,802 - INFO - [diffusion][Epoch 10296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:06,803 - INFO - [diffusion][Epoch 10297] Epoch 10298/12000
2024-11-05 02:28:09,945 - INFO - [diffusion][Epoch 10297] diffusion training Loss: 0.0694234361872077
2024-11-05 02:28:09,948 - INFO - [diffusion][Epoch 10297] diffusion learning rate: 0.001
2024-11-05 02:28:09,950 - INFO - [diffusion][Epoch 10297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:09,951 - INFO - [diffusion][Epoch 10298] Epoch 10299/12000
2024-11-05 02:28:13,428 - INFO - [diffusion][Epoch 10298] diffusion training Loss: 0.06469383370131254
2024-11-05 02:28:13,430 - INFO - [diffusion][Epoch 10298] diffusion learning rate: 0.001
2024-11-05 02:28:13,431 - INFO - [diffusion][Epoch 10298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:13,432 - INFO - [diffusion][Epoch 10299] Epoch 10300/12000
2024-11-05 02:28:17,062 - INFO - [diffusion][Epoch 10299] diffusion training Loss: 0.0639638639986515
2024-11-05 02:28:17,064 - INFO - [diffusion][Epoch 10299] diffusion learning rate: 0.001
2024-11-05 02:28:17,066 - INFO - [diffusion][Epoch 10299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:17,067 - INFO - [diffusion][Epoch 10300] Epoch 10301/12000
2024-11-05 02:28:20,378 - INFO - [diffusion][Epoch 10300] diffusion training Loss: 0.069011015817523
2024-11-05 02:28:20,380 - INFO - [diffusion][Epoch 10300] diffusion learning rate: 0.001
2024-11-05 02:28:20,382 - INFO - [diffusion][Epoch 10300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:20,383 - INFO - [diffusion][Epoch 10301] Epoch 10302/12000
2024-11-05 02:28:23,483 - INFO - [diffusion][Epoch 10301] diffusion training Loss: 0.06760469265282154
2024-11-05 02:28:23,485 - INFO - [diffusion][Epoch 10301] diffusion learning rate: 0.001
2024-11-05 02:28:23,487 - INFO - [diffusion][Epoch 10301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:23,488 - INFO - [diffusion][Epoch 10302] Epoch 10303/12000
2024-11-05 02:28:26,630 - INFO - [diffusion][Epoch 10302] diffusion training Loss: 0.06562233902513981
2024-11-05 02:28:26,632 - INFO - [diffusion][Epoch 10302] diffusion learning rate: 0.001
2024-11-05 02:28:26,634 - INFO - [diffusion][Epoch 10302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:26,635 - INFO - [diffusion][Epoch 10303] Epoch 10304/12000
2024-11-05 02:28:30,154 - INFO - [diffusion][Epoch 10303] diffusion training Loss: 0.06374443881213665
2024-11-05 02:28:30,156 - INFO - [diffusion][Epoch 10303] diffusion learning rate: 0.001
2024-11-05 02:28:30,158 - INFO - [diffusion][Epoch 10303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:30,159 - INFO - [diffusion][Epoch 10304] Epoch 10305/12000
2024-11-05 02:28:33,796 - INFO - [diffusion][Epoch 10304] diffusion training Loss: 0.0606165686622262
2024-11-05 02:28:33,798 - INFO - [diffusion][Epoch 10304] diffusion learning rate: 0.001
2024-11-05 02:28:33,826 - INFO - [diffusion][Epoch 10304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:33,828 - INFO - [diffusion][Epoch 10305] Epoch 10306/12000
2024-11-05 02:28:37,040 - INFO - [diffusion][Epoch 10305] diffusion training Loss: 0.06752314232289791
2024-11-05 02:28:37,042 - INFO - [diffusion][Epoch 10305] diffusion learning rate: 0.001
2024-11-05 02:28:37,044 - INFO - [diffusion][Epoch 10305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:37,045 - INFO - [diffusion][Epoch 10306] Epoch 10307/12000
2024-11-05 02:28:40,120 - INFO - [diffusion][Epoch 10306] diffusion training Loss: 0.06608861777931452
2024-11-05 02:28:40,122 - INFO - [diffusion][Epoch 10306] diffusion learning rate: 0.001
2024-11-05 02:28:40,124 - INFO - [diffusion][Epoch 10306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:40,125 - INFO - [diffusion][Epoch 10307] Epoch 10308/12000
2024-11-05 02:28:43,246 - INFO - [diffusion][Epoch 10307] diffusion training Loss: 0.06250696908682585
2024-11-05 02:28:43,248 - INFO - [diffusion][Epoch 10307] diffusion learning rate: 0.001
2024-11-05 02:28:43,251 - INFO - [diffusion][Epoch 10307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:43,252 - INFO - [diffusion][Epoch 10308] Epoch 10309/12000
2024-11-05 02:28:46,804 - INFO - [diffusion][Epoch 10308] diffusion training Loss: 0.06427439488470554
2024-11-05 02:28:46,806 - INFO - [diffusion][Epoch 10308] diffusion learning rate: 0.001
2024-11-05 02:28:46,808 - INFO - [diffusion][Epoch 10308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:46,809 - INFO - [diffusion][Epoch 10309] Epoch 10310/12000
2024-11-05 02:28:50,327 - INFO - [diffusion][Epoch 10309] diffusion training Loss: 0.0647939583286643
2024-11-05 02:28:50,330 - INFO - [diffusion][Epoch 10309] diffusion learning rate: 0.001
2024-11-05 02:28:50,332 - INFO - [diffusion][Epoch 10309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:50,333 - INFO - [diffusion][Epoch 10310] Epoch 10311/12000
2024-11-05 02:28:53,438 - INFO - [diffusion][Epoch 10310] diffusion training Loss: 0.06291613914072514
2024-11-05 02:28:53,440 - INFO - [diffusion][Epoch 10310] diffusion learning rate: 0.001
2024-11-05 02:28:53,442 - INFO - [diffusion][Epoch 10310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:53,443 - INFO - [diffusion][Epoch 10311] Epoch 10312/12000
2024-11-05 02:28:56,525 - INFO - [diffusion][Epoch 10311] diffusion training Loss: 0.06866765208542347
2024-11-05 02:28:56,526 - INFO - [diffusion][Epoch 10311] diffusion learning rate: 0.001
2024-11-05 02:28:56,553 - INFO - [diffusion][Epoch 10311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:56,555 - INFO - [diffusion][Epoch 10312] Epoch 10313/12000
2024-11-05 02:28:59,780 - INFO - [diffusion][Epoch 10312] diffusion training Loss: 0.06617183052003384
2024-11-05 02:28:59,782 - INFO - [diffusion][Epoch 10312] diffusion learning rate: 0.001
2024-11-05 02:28:59,784 - INFO - [diffusion][Epoch 10312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:59,785 - INFO - [diffusion][Epoch 10313] Epoch 10314/12000
2024-11-05 02:29:03,520 - INFO - [diffusion][Epoch 10313] diffusion training Loss: 0.06630739942193031
2024-11-05 02:29:03,522 - INFO - [diffusion][Epoch 10313] diffusion learning rate: 0.001
2024-11-05 02:29:03,524 - INFO - [diffusion][Epoch 10313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:03,525 - INFO - [diffusion][Epoch 10314] Epoch 10315/12000
2024-11-05 02:29:06,978 - INFO - [diffusion][Epoch 10314] diffusion training Loss: 0.06863589771091938
2024-11-05 02:29:06,980 - INFO - [diffusion][Epoch 10314] diffusion learning rate: 0.001
2024-11-05 02:29:07,007 - INFO - [diffusion][Epoch 10314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:07,008 - INFO - [diffusion][Epoch 10315] Epoch 10316/12000
2024-11-05 02:29:10,134 - INFO - [diffusion][Epoch 10315] diffusion training Loss: 0.07238784804940224
2024-11-05 02:29:10,136 - INFO - [diffusion][Epoch 10315] diffusion learning rate: 0.001
2024-11-05 02:29:10,138 - INFO - [diffusion][Epoch 10315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:10,139 - INFO - [diffusion][Epoch 10316] Epoch 10317/12000
2024-11-05 02:29:13,310 - INFO - [diffusion][Epoch 10316] diffusion training Loss: 0.06869914755225182
2024-11-05 02:29:13,312 - INFO - [diffusion][Epoch 10316] diffusion learning rate: 0.001
2024-11-05 02:29:13,314 - INFO - [diffusion][Epoch 10316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:13,315 - INFO - [diffusion][Epoch 10317] Epoch 10318/12000
2024-11-05 02:29:16,545 - INFO - [diffusion][Epoch 10317] diffusion training Loss: 0.06744720693677664
2024-11-05 02:29:16,547 - INFO - [diffusion][Epoch 10317] diffusion learning rate: 0.001
2024-11-05 02:29:16,548 - INFO - [diffusion][Epoch 10317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:16,549 - INFO - [diffusion][Epoch 10318] Epoch 10319/12000
2024-11-05 02:29:20,313 - INFO - [diffusion][Epoch 10318] diffusion training Loss: 0.06709773652255535
2024-11-05 02:29:20,315 - INFO - [diffusion][Epoch 10318] diffusion learning rate: 0.001
2024-11-05 02:29:20,316 - INFO - [diffusion][Epoch 10318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:20,317 - INFO - [diffusion][Epoch 10319] Epoch 10320/12000
2024-11-05 02:29:23,839 - INFO - [diffusion][Epoch 10319] diffusion training Loss: 0.06585626117885113
2024-11-05 02:29:23,841 - INFO - [diffusion][Epoch 10319] diffusion learning rate: 0.001
2024-11-05 02:29:23,842 - INFO - [diffusion][Epoch 10319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:23,844 - INFO - [diffusion][Epoch 10320] Epoch 10321/12000
2024-11-05 02:29:26,895 - INFO - [diffusion][Epoch 10320] diffusion training Loss: 0.06191834807395935
2024-11-05 02:29:26,897 - INFO - [diffusion][Epoch 10320] diffusion learning rate: 0.001
2024-11-05 02:29:26,898 - INFO - [diffusion][Epoch 10320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:26,900 - INFO - [diffusion][Epoch 10321] Epoch 10322/12000
2024-11-05 02:29:29,989 - INFO - [diffusion][Epoch 10321] diffusion training Loss: 0.061798359267413616
2024-11-05 02:29:29,991 - INFO - [diffusion][Epoch 10321] diffusion learning rate: 0.001
2024-11-05 02:29:29,993 - INFO - [diffusion][Epoch 10321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:29,994 - INFO - [diffusion][Epoch 10322] Epoch 10323/12000
2024-11-05 02:29:33,210 - INFO - [diffusion][Epoch 10322] diffusion training Loss: 0.06559660006314516
2024-11-05 02:29:33,212 - INFO - [diffusion][Epoch 10322] diffusion learning rate: 0.001
2024-11-05 02:29:33,214 - INFO - [diffusion][Epoch 10322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:33,215 - INFO - [diffusion][Epoch 10323] Epoch 10324/12000
2024-11-05 02:29:36,714 - INFO - [diffusion][Epoch 10323] diffusion training Loss: 0.06632653158158064
2024-11-05 02:29:36,716 - INFO - [diffusion][Epoch 10323] diffusion learning rate: 0.001
2024-11-05 02:29:36,718 - INFO - [diffusion][Epoch 10323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:36,719 - INFO - [diffusion][Epoch 10324] Epoch 10325/12000
2024-11-05 02:29:40,160 - INFO - [diffusion][Epoch 10324] diffusion training Loss: 0.06832873728126287
2024-11-05 02:29:40,163 - INFO - [diffusion][Epoch 10324] diffusion learning rate: 0.001
2024-11-05 02:29:40,188 - INFO - [diffusion][Epoch 10324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:40,190 - INFO - [diffusion][Epoch 10325] Epoch 10326/12000
2024-11-05 02:29:43,052 - INFO - [diffusion][Epoch 10325] diffusion training Loss: 0.06729003228247166
2024-11-05 02:29:43,054 - INFO - [diffusion][Epoch 10325] diffusion learning rate: 0.001
2024-11-05 02:29:43,055 - INFO - [diffusion][Epoch 10325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:43,057 - INFO - [diffusion][Epoch 10326] Epoch 10327/12000
2024-11-05 02:29:45,945 - INFO - [diffusion][Epoch 10326] diffusion training Loss: 0.06879041157662868
2024-11-05 02:29:45,947 - INFO - [diffusion][Epoch 10326] diffusion learning rate: 0.001
2024-11-05 02:29:45,948 - INFO - [diffusion][Epoch 10326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:45,950 - INFO - [diffusion][Epoch 10327] Epoch 10328/12000
2024-11-05 02:29:49,461 - INFO - [diffusion][Epoch 10327] diffusion training Loss: 0.0666816309094429
2024-11-05 02:29:49,464 - INFO - [diffusion][Epoch 10327] diffusion learning rate: 0.001
2024-11-05 02:29:49,465 - INFO - [diffusion][Epoch 10327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:49,467 - INFO - [diffusion][Epoch 10328] Epoch 10329/12000
2024-11-05 02:29:52,537 - INFO - [diffusion][Epoch 10328] diffusion training Loss: 0.07072212360799313
2024-11-05 02:29:52,539 - INFO - [diffusion][Epoch 10328] diffusion learning rate: 0.001
2024-11-05 02:29:52,540 - INFO - [diffusion][Epoch 10328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:52,541 - INFO - [diffusion][Epoch 10329] Epoch 10330/12000
2024-11-05 02:29:55,550 - INFO - [diffusion][Epoch 10329] diffusion training Loss: 0.06477231718599796
2024-11-05 02:29:55,552 - INFO - [diffusion][Epoch 10329] diffusion learning rate: 0.001
2024-11-05 02:29:55,554 - INFO - [diffusion][Epoch 10329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:55,555 - INFO - [diffusion][Epoch 10330] Epoch 10331/12000
2024-11-05 02:29:58,684 - INFO - [diffusion][Epoch 10330] diffusion training Loss: 0.06582480017095804
2024-11-05 02:29:58,686 - INFO - [diffusion][Epoch 10330] diffusion learning rate: 0.001
2024-11-05 02:29:58,687 - INFO - [diffusion][Epoch 10330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:58,688 - INFO - [diffusion][Epoch 10331] Epoch 10332/12000
2024-11-05 02:30:02,201 - INFO - [diffusion][Epoch 10331] diffusion training Loss: 0.06561000272631645
2024-11-05 02:30:02,203 - INFO - [diffusion][Epoch 10331] diffusion learning rate: 0.001
2024-11-05 02:30:02,205 - INFO - [diffusion][Epoch 10331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:02,206 - INFO - [diffusion][Epoch 10332] Epoch 10333/12000
2024-11-05 02:30:05,751 - INFO - [diffusion][Epoch 10332] diffusion training Loss: 0.06428814586251974
2024-11-05 02:30:05,754 - INFO - [diffusion][Epoch 10332] diffusion learning rate: 0.001
2024-11-05 02:30:05,756 - INFO - [diffusion][Epoch 10332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:05,757 - INFO - [diffusion][Epoch 10333] Epoch 10334/12000
2024-11-05 02:30:08,856 - INFO - [diffusion][Epoch 10333] diffusion training Loss: 0.06607418693602085
2024-11-05 02:30:08,858 - INFO - [diffusion][Epoch 10333] diffusion learning rate: 0.001
2024-11-05 02:30:08,860 - INFO - [diffusion][Epoch 10333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:08,862 - INFO - [diffusion][Epoch 10334] Epoch 10335/12000
2024-11-05 02:30:11,968 - INFO - [diffusion][Epoch 10334] diffusion training Loss: 0.06350754015147686
2024-11-05 02:30:11,971 - INFO - [diffusion][Epoch 10334] diffusion learning rate: 0.001
2024-11-05 02:30:11,973 - INFO - [diffusion][Epoch 10334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:11,974 - INFO - [diffusion][Epoch 10335] Epoch 10336/12000
2024-11-05 02:30:15,176 - INFO - [diffusion][Epoch 10335] diffusion training Loss: 0.06755497865378857
2024-11-05 02:30:15,178 - INFO - [diffusion][Epoch 10335] diffusion learning rate: 0.001
2024-11-05 02:30:15,180 - INFO - [diffusion][Epoch 10335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:15,181 - INFO - [diffusion][Epoch 10336] Epoch 10337/12000
2024-11-05 02:30:18,666 - INFO - [diffusion][Epoch 10336] diffusion training Loss: 0.06116806901991367
2024-11-05 02:30:18,668 - INFO - [diffusion][Epoch 10336] diffusion learning rate: 0.001
2024-11-05 02:30:18,670 - INFO - [diffusion][Epoch 10336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:18,671 - INFO - [diffusion][Epoch 10337] Epoch 10338/12000
2024-11-05 02:30:22,130 - INFO - [diffusion][Epoch 10337] diffusion training Loss: 0.07289032265543938
2024-11-05 02:30:22,132 - INFO - [diffusion][Epoch 10337] diffusion learning rate: 0.001
2024-11-05 02:30:22,134 - INFO - [diffusion][Epoch 10337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:22,135 - INFO - [diffusion][Epoch 10338] Epoch 10339/12000
2024-11-05 02:30:25,255 - INFO - [diffusion][Epoch 10338] diffusion training Loss: 0.06508296728134155
2024-11-05 02:30:25,257 - INFO - [diffusion][Epoch 10338] diffusion learning rate: 0.001
2024-11-05 02:30:25,461 - INFO - [diffusion][Epoch 10338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:25,462 - INFO - [diffusion][Epoch 10339] Epoch 10340/12000
2024-11-05 02:30:29,197 - INFO - [diffusion][Epoch 10339] diffusion training Loss: 0.0680746790021658
2024-11-05 02:30:29,198 - INFO - [diffusion][Epoch 10339] diffusion learning rate: 0.001
2024-11-05 02:30:29,200 - INFO - [diffusion][Epoch 10339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:29,201 - INFO - [diffusion][Epoch 10340] Epoch 10341/12000
2024-11-05 02:30:32,328 - INFO - [diffusion][Epoch 10340] diffusion training Loss: 0.06365166325122118
2024-11-05 02:30:32,330 - INFO - [diffusion][Epoch 10340] diffusion learning rate: 0.001
2024-11-05 02:30:32,332 - INFO - [diffusion][Epoch 10340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:32,333 - INFO - [diffusion][Epoch 10341] Epoch 10342/12000
2024-11-05 02:30:35,563 - INFO - [diffusion][Epoch 10341] diffusion training Loss: 0.06431808229535818
2024-11-05 02:30:35,564 - INFO - [diffusion][Epoch 10341] diffusion learning rate: 0.001
2024-11-05 02:30:35,566 - INFO - [diffusion][Epoch 10341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:35,567 - INFO - [diffusion][Epoch 10342] Epoch 10343/12000
2024-11-05 02:30:39,158 - INFO - [diffusion][Epoch 10342] diffusion training Loss: 0.06622028350830078
2024-11-05 02:30:39,160 - INFO - [diffusion][Epoch 10342] diffusion learning rate: 0.001
2024-11-05 02:30:39,161 - INFO - [diffusion][Epoch 10342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:39,163 - INFO - [diffusion][Epoch 10343] Epoch 10344/12000
2024-11-05 02:30:42,627 - INFO - [diffusion][Epoch 10343] diffusion training Loss: 0.05850997008383274
2024-11-05 02:30:42,629 - INFO - [diffusion][Epoch 10343] diffusion learning rate: 0.001
2024-11-05 02:30:42,631 - INFO - [diffusion][Epoch 10343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:42,632 - INFO - [diffusion][Epoch 10344] Epoch 10345/12000
2024-11-05 02:30:45,809 - INFO - [diffusion][Epoch 10344] diffusion training Loss: 0.05663487035781145
2024-11-05 02:30:45,810 - INFO - [diffusion][Epoch 10344] diffusion learning rate: 0.001
2024-11-05 02:30:45,812 - INFO - [diffusion][Epoch 10344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:45,813 - INFO - [diffusion][Epoch 10345] Epoch 10346/12000
2024-11-05 02:30:48,936 - INFO - [diffusion][Epoch 10345] diffusion training Loss: 0.06641846243292093
2024-11-05 02:30:48,938 - INFO - [diffusion][Epoch 10345] diffusion learning rate: 0.001
2024-11-05 02:30:48,940 - INFO - [diffusion][Epoch 10345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:48,941 - INFO - [diffusion][Epoch 10346] Epoch 10347/12000
2024-11-05 02:30:52,094 - INFO - [diffusion][Epoch 10346] diffusion training Loss: 0.06571575254201889
2024-11-05 02:30:52,096 - INFO - [diffusion][Epoch 10346] diffusion learning rate: 0.001
2024-11-05 02:30:52,098 - INFO - [diffusion][Epoch 10346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:52,100 - INFO - [diffusion][Epoch 10347] Epoch 10348/12000
2024-11-05 02:30:55,653 - INFO - [diffusion][Epoch 10347] diffusion training Loss: 0.06656690780073404
2024-11-05 02:30:55,655 - INFO - [diffusion][Epoch 10347] diffusion learning rate: 0.001
2024-11-05 02:30:55,657 - INFO - [diffusion][Epoch 10347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:55,658 - INFO - [diffusion][Epoch 10348] Epoch 10349/12000
2024-11-05 02:30:59,161 - INFO - [diffusion][Epoch 10348] diffusion training Loss: 0.06740374304354191
2024-11-05 02:30:59,163 - INFO - [diffusion][Epoch 10348] diffusion learning rate: 0.001
2024-11-05 02:30:59,165 - INFO - [diffusion][Epoch 10348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:59,166 - INFO - [diffusion][Epoch 10349] Epoch 10350/12000
2024-11-05 02:31:02,273 - INFO - [diffusion][Epoch 10349] diffusion training Loss: 0.06539851799607277
2024-11-05 02:31:02,275 - INFO - [diffusion][Epoch 10349] diffusion learning rate: 0.001
2024-11-05 02:31:02,276 - INFO - [diffusion][Epoch 10349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:02,278 - INFO - [diffusion][Epoch 10350] Epoch 10351/12000
2024-11-05 02:31:05,431 - INFO - [diffusion][Epoch 10350] diffusion training Loss: 0.06094139628112316
2024-11-05 02:31:05,434 - INFO - [diffusion][Epoch 10350] diffusion learning rate: 0.001
2024-11-05 02:31:05,436 - INFO - [diffusion][Epoch 10350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:05,437 - INFO - [diffusion][Epoch 10351] Epoch 10352/12000
2024-11-05 02:31:08,698 - INFO - [diffusion][Epoch 10351] diffusion training Loss: 0.06149349641054869
2024-11-05 02:31:08,699 - INFO - [diffusion][Epoch 10351] diffusion learning rate: 0.001
2024-11-05 02:31:08,701 - INFO - [diffusion][Epoch 10351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:08,702 - INFO - [diffusion][Epoch 10352] Epoch 10353/12000
2024-11-05 02:31:12,335 - INFO - [diffusion][Epoch 10352] diffusion training Loss: 0.06528954580426216
2024-11-05 02:31:12,337 - INFO - [diffusion][Epoch 10352] diffusion learning rate: 0.001
2024-11-05 02:31:12,339 - INFO - [diffusion][Epoch 10352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:12,340 - INFO - [diffusion][Epoch 10353] Epoch 10354/12000
2024-11-05 02:31:15,401 - INFO - [diffusion][Epoch 10353] diffusion training Loss: 0.06786990072578192
2024-11-05 02:31:15,403 - INFO - [diffusion][Epoch 10353] diffusion learning rate: 0.001
2024-11-05 02:31:15,405 - INFO - [diffusion][Epoch 10353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:15,406 - INFO - [diffusion][Epoch 10354] Epoch 10355/12000
2024-11-05 02:31:18,509 - INFO - [diffusion][Epoch 10354] diffusion training Loss: 0.060286104679107666
2024-11-05 02:31:18,511 - INFO - [diffusion][Epoch 10354] diffusion learning rate: 0.001
2024-11-05 02:31:18,513 - INFO - [diffusion][Epoch 10354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:18,514 - INFO - [diffusion][Epoch 10355] Epoch 10356/12000
2024-11-05 02:31:21,726 - INFO - [diffusion][Epoch 10355] diffusion training Loss: 0.07211675308644772
2024-11-05 02:31:21,728 - INFO - [diffusion][Epoch 10355] diffusion learning rate: 0.001
2024-11-05 02:31:21,730 - INFO - [diffusion][Epoch 10355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:21,731 - INFO - [diffusion][Epoch 10356] Epoch 10357/12000
2024-11-05 02:31:25,198 - INFO - [diffusion][Epoch 10356] diffusion training Loss: 0.06990539282560349
2024-11-05 02:31:25,200 - INFO - [diffusion][Epoch 10356] diffusion learning rate: 0.001
2024-11-05 02:31:25,202 - INFO - [diffusion][Epoch 10356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:25,203 - INFO - [diffusion][Epoch 10357] Epoch 10358/12000
2024-11-05 02:31:28,310 - INFO - [diffusion][Epoch 10357] diffusion training Loss: 0.06581198796629906
2024-11-05 02:31:28,311 - INFO - [diffusion][Epoch 10357] diffusion learning rate: 0.001
2024-11-05 02:31:28,313 - INFO - [diffusion][Epoch 10357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:28,314 - INFO - [diffusion][Epoch 10358] Epoch 10359/12000
2024-11-05 02:31:31,876 - INFO - [diffusion][Epoch 10358] diffusion training Loss: 0.06608937121927738
2024-11-05 02:31:31,879 - INFO - [diffusion][Epoch 10358] diffusion learning rate: 0.001
2024-11-05 02:31:31,881 - INFO - [diffusion][Epoch 10358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:31,882 - INFO - [diffusion][Epoch 10359] Epoch 10360/12000
2024-11-05 02:31:34,949 - INFO - [diffusion][Epoch 10359] diffusion training Loss: 0.07095254585146904
2024-11-05 02:31:34,952 - INFO - [diffusion][Epoch 10359] diffusion learning rate: 0.001
2024-11-05 02:31:34,954 - INFO - [diffusion][Epoch 10359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:34,955 - INFO - [diffusion][Epoch 10360] Epoch 10361/12000
2024-11-05 02:31:38,485 - INFO - [diffusion][Epoch 10360] diffusion training Loss: 0.06744228769093752
2024-11-05 02:31:38,487 - INFO - [diffusion][Epoch 10360] diffusion learning rate: 0.001
2024-11-05 02:31:38,489 - INFO - [diffusion][Epoch 10360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:38,490 - INFO - [diffusion][Epoch 10361] Epoch 10362/12000
2024-11-05 02:31:42,027 - INFO - [diffusion][Epoch 10361] diffusion training Loss: 0.0646182419732213
2024-11-05 02:31:42,029 - INFO - [diffusion][Epoch 10361] diffusion learning rate: 0.001
2024-11-05 02:31:42,031 - INFO - [diffusion][Epoch 10361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:42,032 - INFO - [diffusion][Epoch 10362] Epoch 10363/12000
2024-11-05 02:31:44,946 - INFO - [diffusion][Epoch 10362] diffusion training Loss: 0.07018693722784519
2024-11-05 02:31:44,949 - INFO - [diffusion][Epoch 10362] diffusion learning rate: 0.001
2024-11-05 02:31:44,952 - INFO - [diffusion][Epoch 10362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:44,953 - INFO - [diffusion][Epoch 10363] Epoch 10364/12000
2024-11-05 02:31:47,995 - INFO - [diffusion][Epoch 10363] diffusion training Loss: 0.06814047321677208
2024-11-05 02:31:47,997 - INFO - [diffusion][Epoch 10363] diffusion learning rate: 0.001
2024-11-05 02:31:47,999 - INFO - [diffusion][Epoch 10363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:48,000 - INFO - [diffusion][Epoch 10364] Epoch 10365/12000
2024-11-05 02:31:51,424 - INFO - [diffusion][Epoch 10364] diffusion training Loss: 0.06079622730612755
2024-11-05 02:31:51,426 - INFO - [diffusion][Epoch 10364] diffusion learning rate: 0.001
2024-11-05 02:31:51,428 - INFO - [diffusion][Epoch 10364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:51,430 - INFO - [diffusion][Epoch 10365] Epoch 10366/12000
2024-11-05 02:31:54,946 - INFO - [diffusion][Epoch 10365] diffusion training Loss: 0.06585533544421196
2024-11-05 02:31:54,949 - INFO - [diffusion][Epoch 10365] diffusion learning rate: 0.001
2024-11-05 02:31:54,951 - INFO - [diffusion][Epoch 10365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:54,952 - INFO - [diffusion][Epoch 10366] Epoch 10367/12000
2024-11-05 02:31:58,017 - INFO - [diffusion][Epoch 10366] diffusion training Loss: 0.062326207756996155
2024-11-05 02:31:58,019 - INFO - [diffusion][Epoch 10366] diffusion learning rate: 0.001
2024-11-05 02:31:58,021 - INFO - [diffusion][Epoch 10366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:58,022 - INFO - [diffusion][Epoch 10367] Epoch 10368/12000
2024-11-05 02:32:00,949 - INFO - [diffusion][Epoch 10367] diffusion training Loss: 0.07005984336137772
2024-11-05 02:32:00,951 - INFO - [diffusion][Epoch 10367] diffusion learning rate: 0.001
2024-11-05 02:32:00,953 - INFO - [diffusion][Epoch 10367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:00,954 - INFO - [diffusion][Epoch 10368] Epoch 10369/12000
2024-11-05 02:32:04,395 - INFO - [diffusion][Epoch 10368] diffusion training Loss: 0.06721466779708862
2024-11-05 02:32:04,397 - INFO - [diffusion][Epoch 10368] diffusion learning rate: 0.001
2024-11-05 02:32:04,399 - INFO - [diffusion][Epoch 10368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:04,400 - INFO - [diffusion][Epoch 10369] Epoch 10370/12000
2024-11-05 02:32:08,019 - INFO - [diffusion][Epoch 10369] diffusion training Loss: 0.06905362010002136
2024-11-05 02:32:08,021 - INFO - [diffusion][Epoch 10369] diffusion learning rate: 0.001
2024-11-05 02:32:08,022 - INFO - [diffusion][Epoch 10369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:08,023 - INFO - [diffusion][Epoch 10370] Epoch 10371/12000
2024-11-05 02:32:11,278 - INFO - [diffusion][Epoch 10370] diffusion training Loss: 0.06462336890399456
2024-11-05 02:32:11,280 - INFO - [diffusion][Epoch 10370] diffusion learning rate: 0.001
2024-11-05 02:32:11,281 - INFO - [diffusion][Epoch 10370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:11,283 - INFO - [diffusion][Epoch 10371] Epoch 10372/12000
2024-11-05 02:32:14,335 - INFO - [diffusion][Epoch 10371] diffusion training Loss: 0.06607178412377834
2024-11-05 02:32:14,337 - INFO - [diffusion][Epoch 10371] diffusion learning rate: 0.001
2024-11-05 02:32:14,339 - INFO - [diffusion][Epoch 10371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:14,340 - INFO - [diffusion][Epoch 10372] Epoch 10373/12000
2024-11-05 02:32:17,489 - INFO - [diffusion][Epoch 10372] diffusion training Loss: 0.059594291262328625
2024-11-05 02:32:17,491 - INFO - [diffusion][Epoch 10372] diffusion learning rate: 0.001
2024-11-05 02:32:17,492 - INFO - [diffusion][Epoch 10372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:17,494 - INFO - [diffusion][Epoch 10373] Epoch 10374/12000
2024-11-05 02:32:21,047 - INFO - [diffusion][Epoch 10373] diffusion training Loss: 0.06787019781768322
2024-11-05 02:32:21,049 - INFO - [diffusion][Epoch 10373] diffusion learning rate: 0.001
2024-11-05 02:32:21,051 - INFO - [diffusion][Epoch 10373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:21,052 - INFO - [diffusion][Epoch 10374] Epoch 10375/12000
2024-11-05 02:32:24,369 - INFO - [diffusion][Epoch 10374] diffusion training Loss: 0.061402889899909496
2024-11-05 02:32:24,370 - INFO - [diffusion][Epoch 10374] diffusion learning rate: 0.001
2024-11-05 02:32:24,372 - INFO - [diffusion][Epoch 10374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:24,373 - INFO - [diffusion][Epoch 10375] Epoch 10376/12000
2024-11-05 02:32:27,433 - INFO - [diffusion][Epoch 10375] diffusion training Loss: 0.07229465432465076
2024-11-05 02:32:27,435 - INFO - [diffusion][Epoch 10375] diffusion learning rate: 0.001
2024-11-05 02:32:27,437 - INFO - [diffusion][Epoch 10375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:27,438 - INFO - [diffusion][Epoch 10376] Epoch 10377/12000
2024-11-05 02:32:30,563 - INFO - [diffusion][Epoch 10376] diffusion training Loss: 0.06666470039635897
2024-11-05 02:32:30,565 - INFO - [diffusion][Epoch 10376] diffusion learning rate: 0.001
2024-11-05 02:32:30,665 - INFO - [diffusion][Epoch 10376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:30,666 - INFO - [diffusion][Epoch 10377] Epoch 10378/12000
2024-11-05 02:32:34,176 - INFO - [diffusion][Epoch 10377] diffusion training Loss: 0.06757374480366707
2024-11-05 02:32:34,178 - INFO - [diffusion][Epoch 10377] diffusion learning rate: 0.001
2024-11-05 02:32:34,180 - INFO - [diffusion][Epoch 10377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:34,182 - INFO - [diffusion][Epoch 10378] Epoch 10379/12000
2024-11-05 02:32:38,052 - INFO - [diffusion][Epoch 10378] diffusion training Loss: 0.06306282430887222
2024-11-05 02:32:38,054 - INFO - [diffusion][Epoch 10378] diffusion learning rate: 0.001
2024-11-05 02:32:38,056 - INFO - [diffusion][Epoch 10378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:38,057 - INFO - [diffusion][Epoch 10379] Epoch 10380/12000
2024-11-05 02:32:41,288 - INFO - [diffusion][Epoch 10379] diffusion training Loss: 0.06724601425230503
2024-11-05 02:32:41,290 - INFO - [diffusion][Epoch 10379] diffusion learning rate: 0.001
2024-11-05 02:32:41,294 - INFO - [diffusion][Epoch 10379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:41,295 - INFO - [diffusion][Epoch 10380] Epoch 10381/12000
2024-11-05 02:32:44,386 - INFO - [diffusion][Epoch 10380] diffusion training Loss: 0.0645912429317832
2024-11-05 02:32:44,388 - INFO - [diffusion][Epoch 10380] diffusion learning rate: 0.001
2024-11-05 02:32:44,389 - INFO - [diffusion][Epoch 10380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:44,391 - INFO - [diffusion][Epoch 10381] Epoch 10382/12000
2024-11-05 02:32:47,416 - INFO - [diffusion][Epoch 10381] diffusion training Loss: 0.06890387367457151
2024-11-05 02:32:47,418 - INFO - [diffusion][Epoch 10381] diffusion learning rate: 0.001
2024-11-05 02:32:47,420 - INFO - [diffusion][Epoch 10381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:47,421 - INFO - [diffusion][Epoch 10382] Epoch 10383/12000
2024-11-05 02:32:50,988 - INFO - [diffusion][Epoch 10382] diffusion training Loss: 0.06323536671698093
2024-11-05 02:32:50,990 - INFO - [diffusion][Epoch 10382] diffusion learning rate: 0.001
2024-11-05 02:32:50,992 - INFO - [diffusion][Epoch 10382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:50,993 - INFO - [diffusion][Epoch 10383] Epoch 10384/12000
2024-11-05 02:32:54,273 - INFO - [diffusion][Epoch 10383] diffusion training Loss: 0.06554358452558517
2024-11-05 02:32:54,275 - INFO - [diffusion][Epoch 10383] diffusion learning rate: 0.001
2024-11-05 02:32:54,277 - INFO - [diffusion][Epoch 10383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:54,278 - INFO - [diffusion][Epoch 10384] Epoch 10385/12000
2024-11-05 02:32:57,321 - INFO - [diffusion][Epoch 10384] diffusion training Loss: 0.06786802783608437
2024-11-05 02:32:57,323 - INFO - [diffusion][Epoch 10384] diffusion learning rate: 0.001
2024-11-05 02:32:57,324 - INFO - [diffusion][Epoch 10384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:57,326 - INFO - [diffusion][Epoch 10385] Epoch 10386/12000
2024-11-05 02:33:00,369 - INFO - [diffusion][Epoch 10385] diffusion training Loss: 0.06103796046227217
2024-11-05 02:33:00,371 - INFO - [diffusion][Epoch 10385] diffusion learning rate: 0.001
2024-11-05 02:33:00,373 - INFO - [diffusion][Epoch 10385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:00,374 - INFO - [diffusion][Epoch 10386] Epoch 10387/12000
2024-11-05 02:33:03,876 - INFO - [diffusion][Epoch 10386] diffusion training Loss: 0.06711899302899837
2024-11-05 02:33:03,878 - INFO - [diffusion][Epoch 10386] diffusion learning rate: 0.001
2024-11-05 02:33:03,898 - INFO - [diffusion][Epoch 10386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:03,900 - INFO - [diffusion][Epoch 10387] Epoch 10388/12000
2024-11-05 02:33:07,439 - INFO - [diffusion][Epoch 10387] diffusion training Loss: 0.06800579000264406
2024-11-05 02:33:07,442 - INFO - [diffusion][Epoch 10387] diffusion learning rate: 0.001
2024-11-05 02:33:07,444 - INFO - [diffusion][Epoch 10387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:07,445 - INFO - [diffusion][Epoch 10388] Epoch 10389/12000
2024-11-05 02:33:10,483 - INFO - [diffusion][Epoch 10388] diffusion training Loss: 0.07370854634791613
2024-11-05 02:33:10,485 - INFO - [diffusion][Epoch 10388] diffusion learning rate: 0.001
2024-11-05 02:33:10,487 - INFO - [diffusion][Epoch 10388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:10,488 - INFO - [diffusion][Epoch 10389] Epoch 10390/12000
2024-11-05 02:33:13,547 - INFO - [diffusion][Epoch 10389] diffusion training Loss: 0.0683225616812706
2024-11-05 02:33:13,549 - INFO - [diffusion][Epoch 10389] diffusion learning rate: 0.001
2024-11-05 02:33:13,551 - INFO - [diffusion][Epoch 10389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:13,552 - INFO - [diffusion][Epoch 10390] Epoch 10391/12000
2024-11-05 02:33:16,985 - INFO - [diffusion][Epoch 10390] diffusion training Loss: 0.06365857552736998
2024-11-05 02:33:16,988 - INFO - [diffusion][Epoch 10390] diffusion learning rate: 0.001
2024-11-05 02:33:16,989 - INFO - [diffusion][Epoch 10390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:16,991 - INFO - [diffusion][Epoch 10391] Epoch 10392/12000
2024-11-05 02:33:20,546 - INFO - [diffusion][Epoch 10391] diffusion training Loss: 0.0693968441337347
2024-11-05 02:33:20,549 - INFO - [diffusion][Epoch 10391] diffusion learning rate: 0.001
2024-11-05 02:33:20,578 - INFO - [diffusion][Epoch 10391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:20,580 - INFO - [diffusion][Epoch 10392] Epoch 10393/12000
2024-11-05 02:33:23,419 - INFO - [diffusion][Epoch 10392] diffusion training Loss: 0.06877453066408634
2024-11-05 02:33:23,423 - INFO - [diffusion][Epoch 10392] diffusion learning rate: 0.001
2024-11-05 02:33:23,425 - INFO - [diffusion][Epoch 10392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:23,427 - INFO - [diffusion][Epoch 10393] Epoch 10394/12000
2024-11-05 02:33:26,630 - INFO - [diffusion][Epoch 10393] diffusion training Loss: 0.05830536596477032
2024-11-05 02:33:26,632 - INFO - [diffusion][Epoch 10393] diffusion learning rate: 0.001
2024-11-05 02:33:26,633 - INFO - [diffusion][Epoch 10393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:26,635 - INFO - [diffusion][Epoch 10394] Epoch 10395/12000
2024-11-05 02:33:29,846 - INFO - [diffusion][Epoch 10394] diffusion training Loss: 0.07019862625747919
2024-11-05 02:33:29,848 - INFO - [diffusion][Epoch 10394] diffusion learning rate: 0.001
2024-11-05 02:33:29,849 - INFO - [diffusion][Epoch 10394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:29,851 - INFO - [diffusion][Epoch 10395] Epoch 10396/12000
2024-11-05 02:33:33,366 - INFO - [diffusion][Epoch 10395] diffusion training Loss: 0.0642398577183485
2024-11-05 02:33:33,368 - INFO - [diffusion][Epoch 10395] diffusion learning rate: 0.001
2024-11-05 02:33:33,370 - INFO - [diffusion][Epoch 10395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:33,372 - INFO - [diffusion][Epoch 10396] Epoch 10397/12000
2024-11-05 02:33:36,520 - INFO - [diffusion][Epoch 10396] diffusion training Loss: 0.06609117053449154
2024-11-05 02:33:36,522 - INFO - [diffusion][Epoch 10396] diffusion learning rate: 0.001
2024-11-05 02:33:36,524 - INFO - [diffusion][Epoch 10396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:36,525 - INFO - [diffusion][Epoch 10397] Epoch 10398/12000
2024-11-05 02:33:40,272 - INFO - [diffusion][Epoch 10397] diffusion training Loss: 0.07242745161056519
2024-11-05 02:33:40,274 - INFO - [diffusion][Epoch 10397] diffusion learning rate: 0.001
2024-11-05 02:33:40,276 - INFO - [diffusion][Epoch 10397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:40,277 - INFO - [diffusion][Epoch 10398] Epoch 10399/12000
2024-11-05 02:33:43,290 - INFO - [diffusion][Epoch 10398] diffusion training Loss: 0.06326625682413578
2024-11-05 02:33:43,292 - INFO - [diffusion][Epoch 10398] diffusion learning rate: 0.001
2024-11-05 02:33:43,294 - INFO - [diffusion][Epoch 10398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:43,295 - INFO - [diffusion][Epoch 10399] Epoch 10400/12000
2024-11-05 02:33:46,523 - INFO - [diffusion][Epoch 10399] diffusion training Loss: 0.06887917127460241
2024-11-05 02:33:46,525 - INFO - [diffusion][Epoch 10399] diffusion learning rate: 0.001
2024-11-05 02:33:46,527 - INFO - [diffusion][Epoch 10399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:46,528 - INFO - [diffusion][Epoch 10400] Epoch 10401/12000
2024-11-05 02:33:50,043 - INFO - [diffusion][Epoch 10400] diffusion training Loss: 0.06577054504305124
2024-11-05 02:33:50,045 - INFO - [diffusion][Epoch 10400] diffusion learning rate: 0.001
2024-11-05 02:33:50,046 - INFO - [diffusion][Epoch 10400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:50,048 - INFO - [diffusion][Epoch 10401] Epoch 10402/12000
2024-11-05 02:33:53,178 - INFO - [diffusion][Epoch 10401] diffusion training Loss: 0.06896146014332771
2024-11-05 02:33:53,180 - INFO - [diffusion][Epoch 10401] diffusion learning rate: 0.001
2024-11-05 02:33:53,182 - INFO - [diffusion][Epoch 10401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:53,183 - INFO - [diffusion][Epoch 10402] Epoch 10403/12000
2024-11-05 02:33:56,279 - INFO - [diffusion][Epoch 10402] diffusion training Loss: 0.07017869129776955
2024-11-05 02:33:56,281 - INFO - [diffusion][Epoch 10402] diffusion learning rate: 0.001
2024-11-05 02:33:56,283 - INFO - [diffusion][Epoch 10402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:56,284 - INFO - [diffusion][Epoch 10403] Epoch 10404/12000
2024-11-05 02:33:59,412 - INFO - [diffusion][Epoch 10403] diffusion training Loss: 0.06643355824053288
2024-11-05 02:33:59,414 - INFO - [diffusion][Epoch 10403] diffusion learning rate: 0.001
2024-11-05 02:33:59,416 - INFO - [diffusion][Epoch 10403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:59,417 - INFO - [diffusion][Epoch 10404] Epoch 10405/12000
2024-11-05 02:34:03,055 - INFO - [diffusion][Epoch 10404] diffusion training Loss: 0.06471098773181438
2024-11-05 02:34:03,057 - INFO - [diffusion][Epoch 10404] diffusion learning rate: 0.001
2024-11-05 02:34:03,059 - INFO - [diffusion][Epoch 10404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:03,060 - INFO - [diffusion][Epoch 10405] Epoch 10406/12000
2024-11-05 02:34:06,548 - INFO - [diffusion][Epoch 10405] diffusion training Loss: 0.06120915152132511
2024-11-05 02:34:06,550 - INFO - [diffusion][Epoch 10405] diffusion learning rate: 0.001
2024-11-05 02:34:06,551 - INFO - [diffusion][Epoch 10405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:06,553 - INFO - [diffusion][Epoch 10406] Epoch 10407/12000
2024-11-05 02:34:09,275 - INFO - [diffusion][Epoch 10406] diffusion training Loss: 0.06784968636929989
2024-11-05 02:34:09,277 - INFO - [diffusion][Epoch 10406] diffusion learning rate: 0.001
2024-11-05 02:34:09,303 - INFO - [diffusion][Epoch 10406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:09,305 - INFO - [diffusion][Epoch 10407] Epoch 10408/12000
2024-11-05 02:34:12,513 - INFO - [diffusion][Epoch 10407] diffusion training Loss: 0.0691738361492753
2024-11-05 02:34:12,514 - INFO - [diffusion][Epoch 10407] diffusion learning rate: 0.001
2024-11-05 02:34:12,516 - INFO - [diffusion][Epoch 10407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:12,517 - INFO - [diffusion][Epoch 10408] Epoch 10409/12000
2024-11-05 02:34:16,089 - INFO - [diffusion][Epoch 10408] diffusion training Loss: 0.06581887975335121
2024-11-05 02:34:16,091 - INFO - [diffusion][Epoch 10408] diffusion learning rate: 0.001
2024-11-05 02:34:16,093 - INFO - [diffusion][Epoch 10408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:16,094 - INFO - [diffusion][Epoch 10409] Epoch 10410/12000
2024-11-05 02:34:19,123 - INFO - [diffusion][Epoch 10409] diffusion training Loss: 0.06865872256457806
2024-11-05 02:34:19,125 - INFO - [diffusion][Epoch 10409] diffusion learning rate: 0.001
2024-11-05 02:34:19,127 - INFO - [diffusion][Epoch 10409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:19,128 - INFO - [diffusion][Epoch 10410] Epoch 10411/12000
2024-11-05 02:34:22,203 - INFO - [diffusion][Epoch 10410] diffusion training Loss: 0.06117944419384003
2024-11-05 02:34:22,205 - INFO - [diffusion][Epoch 10410] diffusion learning rate: 0.001
2024-11-05 02:34:22,206 - INFO - [diffusion][Epoch 10410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:22,208 - INFO - [diffusion][Epoch 10411] Epoch 10412/12000
2024-11-05 02:34:25,398 - INFO - [diffusion][Epoch 10411] diffusion training Loss: 0.06564661115407944
2024-11-05 02:34:25,400 - INFO - [diffusion][Epoch 10411] diffusion learning rate: 0.001
2024-11-05 02:34:25,402 - INFO - [diffusion][Epoch 10411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:25,403 - INFO - [diffusion][Epoch 10412] Epoch 10413/12000
2024-11-05 02:34:29,044 - INFO - [diffusion][Epoch 10412] diffusion training Loss: 0.06930776312947273
2024-11-05 02:34:29,045 - INFO - [diffusion][Epoch 10412] diffusion learning rate: 0.001
2024-11-05 02:34:29,047 - INFO - [diffusion][Epoch 10412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:29,048 - INFO - [diffusion][Epoch 10413] Epoch 10414/12000
2024-11-05 02:34:32,390 - INFO - [diffusion][Epoch 10413] diffusion training Loss: 0.06396114733070135
2024-11-05 02:34:32,392 - INFO - [diffusion][Epoch 10413] diffusion learning rate: 0.001
2024-11-05 02:34:32,394 - INFO - [diffusion][Epoch 10413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:32,395 - INFO - [diffusion][Epoch 10414] Epoch 10415/12000
2024-11-05 02:34:35,580 - INFO - [diffusion][Epoch 10414] diffusion training Loss: 0.06658248044550419
2024-11-05 02:34:35,582 - INFO - [diffusion][Epoch 10414] diffusion learning rate: 0.001
2024-11-05 02:34:35,583 - INFO - [diffusion][Epoch 10414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:35,584 - INFO - [diffusion][Epoch 10415] Epoch 10416/12000
2024-11-05 02:34:38,630 - INFO - [diffusion][Epoch 10415] diffusion training Loss: 0.06792539916932583
2024-11-05 02:34:38,632 - INFO - [diffusion][Epoch 10415] diffusion learning rate: 0.001
2024-11-05 02:34:38,634 - INFO - [diffusion][Epoch 10415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:38,635 - INFO - [diffusion][Epoch 10416] Epoch 10417/12000
2024-11-05 02:34:42,134 - INFO - [diffusion][Epoch 10416] diffusion training Loss: 0.06279286276549101
2024-11-05 02:34:42,136 - INFO - [diffusion][Epoch 10416] diffusion learning rate: 0.001
2024-11-05 02:34:42,138 - INFO - [diffusion][Epoch 10416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:42,139 - INFO - [diffusion][Epoch 10417] Epoch 10418/12000
2024-11-05 02:34:45,724 - INFO - [diffusion][Epoch 10417] diffusion training Loss: 0.0650063855573535
2024-11-05 02:34:45,726 - INFO - [diffusion][Epoch 10417] diffusion learning rate: 0.001
2024-11-05 02:34:45,727 - INFO - [diffusion][Epoch 10417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:45,729 - INFO - [diffusion][Epoch 10418] Epoch 10419/12000
2024-11-05 02:34:48,930 - INFO - [diffusion][Epoch 10418] diffusion training Loss: 0.06496099941432476
2024-11-05 02:34:48,932 - INFO - [diffusion][Epoch 10418] diffusion learning rate: 0.001
2024-11-05 02:34:48,933 - INFO - [diffusion][Epoch 10418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:48,935 - INFO - [diffusion][Epoch 10419] Epoch 10420/12000
2024-11-05 02:34:52,246 - INFO - [diffusion][Epoch 10419] diffusion training Loss: 0.06776637956500053
2024-11-05 02:34:52,248 - INFO - [diffusion][Epoch 10419] diffusion learning rate: 0.001
2024-11-05 02:34:52,289 - INFO - [diffusion][Epoch 10419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:52,290 - INFO - [diffusion][Epoch 10420] Epoch 10421/12000
2024-11-05 02:34:55,455 - INFO - [diffusion][Epoch 10420] diffusion training Loss: 0.06717254035174847
2024-11-05 02:34:55,457 - INFO - [diffusion][Epoch 10420] diffusion learning rate: 0.001
2024-11-05 02:34:55,459 - INFO - [diffusion][Epoch 10420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:55,460 - INFO - [diffusion][Epoch 10421] Epoch 10422/12000
2024-11-05 02:34:58,795 - INFO - [diffusion][Epoch 10421] diffusion training Loss: 0.06102301366627216
2024-11-05 02:34:58,797 - INFO - [diffusion][Epoch 10421] diffusion learning rate: 0.001
2024-11-05 02:34:58,798 - INFO - [diffusion][Epoch 10421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:58,800 - INFO - [diffusion][Epoch 10422] Epoch 10423/12000
2024-11-05 02:35:02,458 - INFO - [diffusion][Epoch 10422] diffusion training Loss: 0.06553758215159178
2024-11-05 02:35:02,459 - INFO - [diffusion][Epoch 10422] diffusion learning rate: 0.001
2024-11-05 02:35:02,499 - INFO - [diffusion][Epoch 10422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:02,500 - INFO - [diffusion][Epoch 10423] Epoch 10424/12000
2024-11-05 02:35:05,838 - INFO - [diffusion][Epoch 10423] diffusion training Loss: 0.06932078115642071
2024-11-05 02:35:05,840 - INFO - [diffusion][Epoch 10423] diffusion learning rate: 0.001
2024-11-05 02:35:05,842 - INFO - [diffusion][Epoch 10423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:05,843 - INFO - [diffusion][Epoch 10424] Epoch 10425/12000
2024-11-05 02:35:08,980 - INFO - [diffusion][Epoch 10424] diffusion training Loss: 0.07027635350823402
2024-11-05 02:35:08,981 - INFO - [diffusion][Epoch 10424] diffusion learning rate: 0.001
2024-11-05 02:35:08,983 - INFO - [diffusion][Epoch 10424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:08,984 - INFO - [diffusion][Epoch 10425] Epoch 10426/12000
2024-11-05 02:35:11,990 - INFO - [diffusion][Epoch 10425] diffusion training Loss: 0.06673213467001915
2024-11-05 02:35:11,992 - INFO - [diffusion][Epoch 10425] diffusion learning rate: 0.001
2024-11-05 02:35:11,994 - INFO - [diffusion][Epoch 10425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:11,995 - INFO - [diffusion][Epoch 10426] Epoch 10427/12000
2024-11-05 02:35:15,501 - INFO - [diffusion][Epoch 10426] diffusion training Loss: 0.06315616052597761
2024-11-05 02:35:15,503 - INFO - [diffusion][Epoch 10426] diffusion learning rate: 0.001
2024-11-05 02:35:15,505 - INFO - [diffusion][Epoch 10426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:15,506 - INFO - [diffusion][Epoch 10427] Epoch 10428/12000
2024-11-05 02:35:18,788 - INFO - [diffusion][Epoch 10427] diffusion training Loss: 0.06643561366945505
2024-11-05 02:35:18,790 - INFO - [diffusion][Epoch 10427] diffusion learning rate: 0.001
2024-11-05 02:35:18,818 - INFO - [diffusion][Epoch 10427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:18,819 - INFO - [diffusion][Epoch 10428] Epoch 10429/12000
2024-11-05 02:35:21,914 - INFO - [diffusion][Epoch 10428] diffusion training Loss: 0.06576149724423885
2024-11-05 02:35:21,917 - INFO - [diffusion][Epoch 10428] diffusion learning rate: 0.001
2024-11-05 02:35:21,919 - INFO - [diffusion][Epoch 10428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:21,920 - INFO - [diffusion][Epoch 10429] Epoch 10430/12000
2024-11-05 02:35:24,959 - INFO - [diffusion][Epoch 10429] diffusion training Loss: 0.06534222606569529
2024-11-05 02:35:24,961 - INFO - [diffusion][Epoch 10429] diffusion learning rate: 0.001
2024-11-05 02:35:24,962 - INFO - [diffusion][Epoch 10429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:24,964 - INFO - [diffusion][Epoch 10430] Epoch 10431/12000
2024-11-05 02:35:28,469 - INFO - [diffusion][Epoch 10430] diffusion training Loss: 0.060939605347812176
2024-11-05 02:35:28,472 - INFO - [diffusion][Epoch 10430] diffusion learning rate: 0.001
2024-11-05 02:35:28,473 - INFO - [diffusion][Epoch 10430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:28,475 - INFO - [diffusion][Epoch 10431] Epoch 10432/12000
2024-11-05 02:35:31,997 - INFO - [diffusion][Epoch 10431] diffusion training Loss: 0.06068640388548374
2024-11-05 02:35:31,998 - INFO - [diffusion][Epoch 10431] diffusion learning rate: 0.001
2024-11-05 02:35:32,000 - INFO - [diffusion][Epoch 10431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:32,002 - INFO - [diffusion][Epoch 10432] Epoch 10433/12000
2024-11-05 02:35:34,663 - INFO - [diffusion][Epoch 10432] diffusion training Loss: 0.0668539460748434
2024-11-05 02:35:34,665 - INFO - [diffusion][Epoch 10432] diffusion learning rate: 0.001
2024-11-05 02:35:34,666 - INFO - [diffusion][Epoch 10432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:34,668 - INFO - [diffusion][Epoch 10433] Epoch 10434/12000
2024-11-05 02:35:37,778 - INFO - [diffusion][Epoch 10433] diffusion training Loss: 0.0648520477116108
2024-11-05 02:35:37,780 - INFO - [diffusion][Epoch 10433] diffusion learning rate: 0.001
2024-11-05 02:35:37,783 - INFO - [diffusion][Epoch 10433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:37,784 - INFO - [diffusion][Epoch 10434] Epoch 10435/12000
2024-11-05 02:35:41,286 - INFO - [diffusion][Epoch 10434] diffusion training Loss: 0.06731265410780907
2024-11-05 02:35:41,288 - INFO - [diffusion][Epoch 10434] diffusion learning rate: 0.001
2024-11-05 02:35:41,290 - INFO - [diffusion][Epoch 10434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:41,294 - INFO - [diffusion][Epoch 10435] Epoch 10436/12000
2024-11-05 02:35:44,770 - INFO - [diffusion][Epoch 10435] diffusion training Loss: 0.06483994983136654
2024-11-05 02:35:44,772 - INFO - [diffusion][Epoch 10435] diffusion learning rate: 0.001
2024-11-05 02:35:44,774 - INFO - [diffusion][Epoch 10435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:44,775 - INFO - [diffusion][Epoch 10436] Epoch 10437/12000
2024-11-05 02:35:47,770 - INFO - [diffusion][Epoch 10436] diffusion training Loss: 0.0665548425167799
2024-11-05 02:35:47,772 - INFO - [diffusion][Epoch 10436] diffusion learning rate: 0.001
2024-11-05 02:35:47,774 - INFO - [diffusion][Epoch 10436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:47,775 - INFO - [diffusion][Epoch 10437] Epoch 10438/12000
2024-11-05 02:35:50,885 - INFO - [diffusion][Epoch 10437] diffusion training Loss: 0.07031071837991476
2024-11-05 02:35:50,887 - INFO - [diffusion][Epoch 10437] diffusion learning rate: 0.001
2024-11-05 02:35:50,888 - INFO - [diffusion][Epoch 10437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:50,890 - INFO - [diffusion][Epoch 10438] Epoch 10439/12000
2024-11-05 02:35:54,399 - INFO - [diffusion][Epoch 10438] diffusion training Loss: 0.0708360206335783
2024-11-05 02:35:54,401 - INFO - [diffusion][Epoch 10438] diffusion learning rate: 0.001
2024-11-05 02:35:54,403 - INFO - [diffusion][Epoch 10438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:54,404 - INFO - [diffusion][Epoch 10439] Epoch 10440/12000
2024-11-05 02:35:58,397 - INFO - [diffusion][Epoch 10439] diffusion training Loss: 0.06855558417737484
2024-11-05 02:35:58,399 - INFO - [diffusion][Epoch 10439] diffusion learning rate: 0.001
2024-11-05 02:35:58,401 - INFO - [diffusion][Epoch 10439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:58,402 - INFO - [diffusion][Epoch 10440] Epoch 10441/12000
2024-11-05 02:36:01,922 - INFO - [diffusion][Epoch 10440] diffusion training Loss: 0.06881824601441622
2024-11-05 02:36:01,924 - INFO - [diffusion][Epoch 10440] diffusion learning rate: 0.001
2024-11-05 02:36:01,926 - INFO - [diffusion][Epoch 10440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:01,927 - INFO - [diffusion][Epoch 10441] Epoch 10442/12000
2024-11-05 02:36:04,993 - INFO - [diffusion][Epoch 10441] diffusion training Loss: 0.06694383174180984
2024-11-05 02:36:04,995 - INFO - [diffusion][Epoch 10441] diffusion learning rate: 0.001
2024-11-05 02:36:04,997 - INFO - [diffusion][Epoch 10441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:04,998 - INFO - [diffusion][Epoch 10442] Epoch 10443/12000
2024-11-05 02:36:07,806 - INFO - [diffusion][Epoch 10442] diffusion training Loss: 0.06516181211918592
2024-11-05 02:36:07,808 - INFO - [diffusion][Epoch 10442] diffusion learning rate: 0.001
2024-11-05 02:36:07,810 - INFO - [diffusion][Epoch 10442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:07,811 - INFO - [diffusion][Epoch 10443] Epoch 10444/12000
2024-11-05 02:36:11,368 - INFO - [diffusion][Epoch 10443] diffusion training Loss: 0.06859449669718742
2024-11-05 02:36:11,370 - INFO - [diffusion][Epoch 10443] diffusion learning rate: 0.001
2024-11-05 02:36:11,372 - INFO - [diffusion][Epoch 10443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:11,373 - INFO - [diffusion][Epoch 10444] Epoch 10445/12000
2024-11-05 02:36:15,052 - INFO - [diffusion][Epoch 10444] diffusion training Loss: 0.06662930734455585
2024-11-05 02:36:15,055 - INFO - [diffusion][Epoch 10444] diffusion learning rate: 0.001
2024-11-05 02:36:15,056 - INFO - [diffusion][Epoch 10444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:15,057 - INFO - [diffusion][Epoch 10445] Epoch 10446/12000
2024-11-05 02:36:18,181 - INFO - [diffusion][Epoch 10445] diffusion training Loss: 0.06678549759089947
2024-11-05 02:36:18,183 - INFO - [diffusion][Epoch 10445] diffusion learning rate: 0.001
2024-11-05 02:36:18,185 - INFO - [diffusion][Epoch 10445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:18,186 - INFO - [diffusion][Epoch 10446] Epoch 10447/12000
2024-11-05 02:36:21,226 - INFO - [diffusion][Epoch 10446] diffusion training Loss: 0.06940486840903759
2024-11-05 02:36:21,227 - INFO - [diffusion][Epoch 10446] diffusion learning rate: 0.001
2024-11-05 02:36:21,229 - INFO - [diffusion][Epoch 10446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:21,230 - INFO - [diffusion][Epoch 10447] Epoch 10448/12000
2024-11-05 02:36:24,296 - INFO - [diffusion][Epoch 10447] diffusion training Loss: 0.07005739770829678
2024-11-05 02:36:24,299 - INFO - [diffusion][Epoch 10447] diffusion learning rate: 0.001
2024-11-05 02:36:24,301 - INFO - [diffusion][Epoch 10447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:24,302 - INFO - [diffusion][Epoch 10448] Epoch 10449/12000
2024-11-05 02:36:27,760 - INFO - [diffusion][Epoch 10448] diffusion training Loss: 0.06577518302947283
2024-11-05 02:36:27,763 - INFO - [diffusion][Epoch 10448] diffusion learning rate: 0.001
2024-11-05 02:36:27,764 - INFO - [diffusion][Epoch 10448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:27,766 - INFO - [diffusion][Epoch 10449] Epoch 10450/12000
2024-11-05 02:36:30,892 - INFO - [diffusion][Epoch 10449] diffusion training Loss: 0.06197560764849186
2024-11-05 02:36:30,894 - INFO - [diffusion][Epoch 10449] diffusion learning rate: 0.001
2024-11-05 02:36:30,897 - INFO - [diffusion][Epoch 10449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:30,898 - INFO - [diffusion][Epoch 10450] Epoch 10451/12000
2024-11-05 02:36:33,831 - INFO - [diffusion][Epoch 10450] diffusion training Loss: 0.06818214803934097
2024-11-05 02:36:33,833 - INFO - [diffusion][Epoch 10450] diffusion learning rate: 0.001
2024-11-05 02:36:33,835 - INFO - [diffusion][Epoch 10450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:33,836 - INFO - [diffusion][Epoch 10451] Epoch 10452/12000
2024-11-05 02:36:37,121 - INFO - [diffusion][Epoch 10451] diffusion training Loss: 0.06944228522479534
2024-11-05 02:36:37,123 - INFO - [diffusion][Epoch 10451] diffusion learning rate: 0.001
2024-11-05 02:36:37,125 - INFO - [diffusion][Epoch 10451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:37,126 - INFO - [diffusion][Epoch 10452] Epoch 10453/12000
2024-11-05 02:36:40,859 - INFO - [diffusion][Epoch 10452] diffusion training Loss: 0.06002696231007576
2024-11-05 02:36:40,861 - INFO - [diffusion][Epoch 10452] diffusion learning rate: 0.001
2024-11-05 02:36:40,863 - INFO - [diffusion][Epoch 10452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:40,864 - INFO - [diffusion][Epoch 10453] Epoch 10454/12000
2024-11-05 02:36:44,183 - INFO - [diffusion][Epoch 10453] diffusion training Loss: 0.0639332365244627
2024-11-05 02:36:44,185 - INFO - [diffusion][Epoch 10453] diffusion learning rate: 0.001
2024-11-05 02:36:44,187 - INFO - [diffusion][Epoch 10453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:44,188 - INFO - [diffusion][Epoch 10454] Epoch 10455/12000
2024-11-05 02:36:47,309 - INFO - [diffusion][Epoch 10454] diffusion training Loss: 0.05712504032999277
2024-11-05 02:36:47,311 - INFO - [diffusion][Epoch 10454] diffusion learning rate: 0.001
2024-11-05 02:36:47,313 - INFO - [diffusion][Epoch 10454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:47,314 - INFO - [diffusion][Epoch 10455] Epoch 10456/12000
2024-11-05 02:36:50,393 - INFO - [diffusion][Epoch 10455] diffusion training Loss: 0.06048632226884365
2024-11-05 02:36:50,395 - INFO - [diffusion][Epoch 10455] diffusion learning rate: 0.001
2024-11-05 02:36:50,396 - INFO - [diffusion][Epoch 10455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:50,398 - INFO - [diffusion][Epoch 10456] Epoch 10457/12000
2024-11-05 02:36:53,921 - INFO - [diffusion][Epoch 10456] diffusion training Loss: 0.06186551507562399
2024-11-05 02:36:53,922 - INFO - [diffusion][Epoch 10456] diffusion learning rate: 0.001
2024-11-05 02:36:53,924 - INFO - [diffusion][Epoch 10456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:53,925 - INFO - [diffusion][Epoch 10457] Epoch 10458/12000
2024-11-05 02:36:57,486 - INFO - [diffusion][Epoch 10457] diffusion training Loss: 0.06475836411118507
2024-11-05 02:36:57,487 - INFO - [diffusion][Epoch 10457] diffusion learning rate: 0.001
2024-11-05 02:36:57,489 - INFO - [diffusion][Epoch 10457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:57,490 - INFO - [diffusion][Epoch 10458] Epoch 10459/12000
2024-11-05 02:37:00,558 - INFO - [diffusion][Epoch 10458] diffusion training Loss: 0.06538816541433334
2024-11-05 02:37:00,560 - INFO - [diffusion][Epoch 10458] diffusion learning rate: 0.001
2024-11-05 02:37:00,562 - INFO - [diffusion][Epoch 10458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:00,563 - INFO - [diffusion][Epoch 10459] Epoch 10460/12000
2024-11-05 02:37:03,642 - INFO - [diffusion][Epoch 10459] diffusion training Loss: 0.06269938312470913
2024-11-05 02:37:03,643 - INFO - [diffusion][Epoch 10459] diffusion learning rate: 0.001
2024-11-05 02:37:03,645 - INFO - [diffusion][Epoch 10459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:03,646 - INFO - [diffusion][Epoch 10460] Epoch 10461/12000
2024-11-05 02:37:07,221 - INFO - [diffusion][Epoch 10460] diffusion training Loss: 0.06699429452419281
2024-11-05 02:37:07,223 - INFO - [diffusion][Epoch 10460] diffusion learning rate: 0.001
2024-11-05 02:37:07,224 - INFO - [diffusion][Epoch 10460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:07,226 - INFO - [diffusion][Epoch 10461] Epoch 10462/12000
2024-11-05 02:37:10,800 - INFO - [diffusion][Epoch 10461] diffusion training Loss: 0.06750036776065826
2024-11-05 02:37:10,802 - INFO - [diffusion][Epoch 10461] diffusion learning rate: 0.001
2024-11-05 02:37:10,804 - INFO - [diffusion][Epoch 10461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:10,805 - INFO - [diffusion][Epoch 10462] Epoch 10463/12000
2024-11-05 02:37:14,214 - INFO - [diffusion][Epoch 10462] diffusion training Loss: 0.06180788204073906
2024-11-05 02:37:14,216 - INFO - [diffusion][Epoch 10462] diffusion learning rate: 0.001
2024-11-05 02:37:14,218 - INFO - [diffusion][Epoch 10462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:14,219 - INFO - [diffusion][Epoch 10463] Epoch 10464/12000
2024-11-05 02:37:17,327 - INFO - [diffusion][Epoch 10463] diffusion training Loss: 0.059334833174943924
2024-11-05 02:37:17,329 - INFO - [diffusion][Epoch 10463] diffusion learning rate: 0.001
2024-11-05 02:37:17,331 - INFO - [diffusion][Epoch 10463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:17,332 - INFO - [diffusion][Epoch 10464] Epoch 10465/12000
2024-11-05 02:37:20,451 - INFO - [diffusion][Epoch 10464] diffusion training Loss: 0.06582220736891031
2024-11-05 02:37:20,453 - INFO - [diffusion][Epoch 10464] diffusion learning rate: 0.001
2024-11-05 02:37:20,455 - INFO - [diffusion][Epoch 10464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:20,456 - INFO - [diffusion][Epoch 10465] Epoch 10466/12000
2024-11-05 02:37:23,876 - INFO - [diffusion][Epoch 10465] diffusion training Loss: 0.06211991794407368
2024-11-05 02:37:23,877 - INFO - [diffusion][Epoch 10465] diffusion learning rate: 0.001
2024-11-05 02:37:23,879 - INFO - [diffusion][Epoch 10465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:23,880 - INFO - [diffusion][Epoch 10466] Epoch 10467/12000
2024-11-05 02:37:27,402 - INFO - [diffusion][Epoch 10466] diffusion training Loss: 0.06938663683831692
2024-11-05 02:37:27,404 - INFO - [diffusion][Epoch 10466] diffusion learning rate: 0.001
2024-11-05 02:37:27,406 - INFO - [diffusion][Epoch 10466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:27,407 - INFO - [diffusion][Epoch 10467] Epoch 10468/12000
2024-11-05 02:37:30,509 - INFO - [diffusion][Epoch 10467] diffusion training Loss: 0.06504522450268269
2024-11-05 02:37:30,511 - INFO - [diffusion][Epoch 10467] diffusion learning rate: 0.001
2024-11-05 02:37:30,513 - INFO - [diffusion][Epoch 10467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:30,514 - INFO - [diffusion][Epoch 10468] Epoch 10469/12000
2024-11-05 02:37:33,590 - INFO - [diffusion][Epoch 10468] diffusion training Loss: 0.0598707664757967
2024-11-05 02:37:33,592 - INFO - [diffusion][Epoch 10468] diffusion learning rate: 0.001
2024-11-05 02:37:33,594 - INFO - [diffusion][Epoch 10468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:33,595 - INFO - [diffusion][Epoch 10469] Epoch 10470/12000
2024-11-05 02:37:36,761 - INFO - [diffusion][Epoch 10469] diffusion training Loss: 0.05898733530193567
2024-11-05 02:37:36,763 - INFO - [diffusion][Epoch 10469] diffusion learning rate: 0.001
2024-11-05 02:37:36,765 - INFO - [diffusion][Epoch 10469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:36,766 - INFO - [diffusion][Epoch 10470] Epoch 10471/12000
2024-11-05 02:37:40,390 - INFO - [diffusion][Epoch 10470] diffusion training Loss: 0.06854771450161934
2024-11-05 02:37:40,393 - INFO - [diffusion][Epoch 10470] diffusion learning rate: 0.001
2024-11-05 02:37:40,420 - INFO - [diffusion][Epoch 10470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:40,422 - INFO - [diffusion][Epoch 10471] Epoch 10472/12000
2024-11-05 02:37:43,780 - INFO - [diffusion][Epoch 10471] diffusion training Loss: 0.06437387876212597
2024-11-05 02:37:43,782 - INFO - [diffusion][Epoch 10471] diffusion learning rate: 0.001
2024-11-05 02:37:43,783 - INFO - [diffusion][Epoch 10471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:43,785 - INFO - [diffusion][Epoch 10472] Epoch 10473/12000
2024-11-05 02:37:46,859 - INFO - [diffusion][Epoch 10472] diffusion training Loss: 0.06299346406012774
2024-11-05 02:37:46,860 - INFO - [diffusion][Epoch 10472] diffusion learning rate: 0.001
2024-11-05 02:37:46,862 - INFO - [diffusion][Epoch 10472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:46,863 - INFO - [diffusion][Epoch 10473] Epoch 10474/12000
2024-11-05 02:37:49,877 - INFO - [diffusion][Epoch 10473] diffusion training Loss: 0.06920194998383522
2024-11-05 02:37:49,880 - INFO - [diffusion][Epoch 10473] diffusion learning rate: 0.001
2024-11-05 02:37:49,881 - INFO - [diffusion][Epoch 10473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:49,883 - INFO - [diffusion][Epoch 10474] Epoch 10475/12000
2024-11-05 02:37:53,386 - INFO - [diffusion][Epoch 10474] diffusion training Loss: 0.06193970423191786
2024-11-05 02:37:53,388 - INFO - [diffusion][Epoch 10474] diffusion learning rate: 0.001
2024-11-05 02:37:53,390 - INFO - [diffusion][Epoch 10474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:53,391 - INFO - [diffusion][Epoch 10475] Epoch 10476/12000
2024-11-05 02:37:56,932 - INFO - [diffusion][Epoch 10475] diffusion training Loss: 0.05914186965674162
2024-11-05 02:37:56,934 - INFO - [diffusion][Epoch 10475] diffusion learning rate: 0.001
2024-11-05 02:37:56,936 - INFO - [diffusion][Epoch 10475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:56,937 - INFO - [diffusion][Epoch 10476] Epoch 10477/12000
2024-11-05 02:37:59,849 - INFO - [diffusion][Epoch 10476] diffusion training Loss: 0.06508737988770008
2024-11-05 02:37:59,853 - INFO - [diffusion][Epoch 10476] diffusion learning rate: 0.001
2024-11-05 02:37:59,854 - INFO - [diffusion][Epoch 10476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:59,856 - INFO - [diffusion][Epoch 10477] Epoch 10478/12000
2024-11-05 02:38:02,895 - INFO - [diffusion][Epoch 10477] diffusion training Loss: 0.06717791222035885
2024-11-05 02:38:02,897 - INFO - [diffusion][Epoch 10477] diffusion learning rate: 0.001
2024-11-05 02:38:02,899 - INFO - [diffusion][Epoch 10477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:02,900 - INFO - [diffusion][Epoch 10478] Epoch 10479/12000
2024-11-05 02:38:06,352 - INFO - [diffusion][Epoch 10478] diffusion training Loss: 0.0642131706699729
2024-11-05 02:38:06,354 - INFO - [diffusion][Epoch 10478] diffusion learning rate: 0.001
2024-11-05 02:38:06,356 - INFO - [diffusion][Epoch 10478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:06,357 - INFO - [diffusion][Epoch 10479] Epoch 10480/12000
2024-11-05 02:38:10,419 - INFO - [diffusion][Epoch 10479] diffusion training Loss: 0.06303891725838184
2024-11-05 02:38:10,422 - INFO - [diffusion][Epoch 10479] diffusion learning rate: 0.001
2024-11-05 02:38:10,424 - INFO - [diffusion][Epoch 10479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:10,426 - INFO - [diffusion][Epoch 10480] Epoch 10481/12000
2024-11-05 02:38:13,976 - INFO - [diffusion][Epoch 10480] diffusion training Loss: 0.0714796781539917
2024-11-05 02:38:14,031 - INFO - [diffusion][Epoch 10480] diffusion learning rate: 0.001
2024-11-05 02:38:14,034 - INFO - [diffusion][Epoch 10480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:14,035 - INFO - [diffusion][Epoch 10481] Epoch 10482/12000
2024-11-05 02:38:17,134 - INFO - [diffusion][Epoch 10481] diffusion training Loss: 0.06320444215089083
2024-11-05 02:38:17,137 - INFO - [diffusion][Epoch 10481] diffusion learning rate: 0.001
2024-11-05 02:38:17,138 - INFO - [diffusion][Epoch 10481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:17,140 - INFO - [diffusion][Epoch 10482] Epoch 10483/12000
2024-11-05 02:38:20,279 - INFO - [diffusion][Epoch 10482] diffusion training Loss: 0.06512490008026361
2024-11-05 02:38:20,282 - INFO - [diffusion][Epoch 10482] diffusion learning rate: 0.001
2024-11-05 02:38:20,283 - INFO - [diffusion][Epoch 10482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:20,284 - INFO - [diffusion][Epoch 10483] Epoch 10484/12000
2024-11-05 02:38:23,460 - INFO - [diffusion][Epoch 10483] diffusion training Loss: 0.06435230933129787
2024-11-05 02:38:23,462 - INFO - [diffusion][Epoch 10483] diffusion learning rate: 0.001
2024-11-05 02:38:23,464 - INFO - [diffusion][Epoch 10483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:23,465 - INFO - [diffusion][Epoch 10484] Epoch 10485/12000
2024-11-05 02:38:27,107 - INFO - [diffusion][Epoch 10484] diffusion training Loss: 0.06453981529921293
2024-11-05 02:38:27,109 - INFO - [diffusion][Epoch 10484] diffusion learning rate: 0.001
2024-11-05 02:38:27,111 - INFO - [diffusion][Epoch 10484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:27,112 - INFO - [diffusion][Epoch 10485] Epoch 10486/12000
2024-11-05 02:38:30,419 - INFO - [diffusion][Epoch 10485] diffusion training Loss: 0.06914207153022289
2024-11-05 02:38:30,421 - INFO - [diffusion][Epoch 10485] diffusion learning rate: 0.001
2024-11-05 02:38:30,422 - INFO - [diffusion][Epoch 10485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:30,424 - INFO - [diffusion][Epoch 10486] Epoch 10487/12000
2024-11-05 02:38:33,495 - INFO - [diffusion][Epoch 10486] diffusion training Loss: 0.07160364650189877
2024-11-05 02:38:33,496 - INFO - [diffusion][Epoch 10486] diffusion learning rate: 0.001
2024-11-05 02:38:33,498 - INFO - [diffusion][Epoch 10486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:33,499 - INFO - [diffusion][Epoch 10487] Epoch 10488/12000
2024-11-05 02:38:36,625 - INFO - [diffusion][Epoch 10487] diffusion training Loss: 0.0652810875326395
2024-11-05 02:38:36,626 - INFO - [diffusion][Epoch 10487] diffusion learning rate: 0.001
2024-11-05 02:38:36,628 - INFO - [diffusion][Epoch 10487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:36,629 - INFO - [diffusion][Epoch 10488] Epoch 10489/12000
2024-11-05 02:38:40,165 - INFO - [diffusion][Epoch 10488] diffusion training Loss: 0.0699270823970437
2024-11-05 02:38:40,167 - INFO - [diffusion][Epoch 10488] diffusion learning rate: 0.001
2024-11-05 02:38:40,169 - INFO - [diffusion][Epoch 10488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:40,170 - INFO - [diffusion][Epoch 10489] Epoch 10490/12000
2024-11-05 02:38:43,684 - INFO - [diffusion][Epoch 10489] diffusion training Loss: 0.07635372504591942
2024-11-05 02:38:43,694 - INFO - [diffusion][Epoch 10489] diffusion learning rate: 0.001
2024-11-05 02:38:43,696 - INFO - [diffusion][Epoch 10489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:43,697 - INFO - [diffusion][Epoch 10490] Epoch 10491/12000
2024-11-05 02:38:46,690 - INFO - [diffusion][Epoch 10490] diffusion training Loss: 0.06640377268195152
2024-11-05 02:38:46,692 - INFO - [diffusion][Epoch 10490] diffusion learning rate: 0.001
2024-11-05 02:38:46,693 - INFO - [diffusion][Epoch 10490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:46,695 - INFO - [diffusion][Epoch 10491] Epoch 10492/12000
2024-11-05 02:38:49,763 - INFO - [diffusion][Epoch 10491] diffusion training Loss: 0.06991604436188936
2024-11-05 02:38:49,765 - INFO - [diffusion][Epoch 10491] diffusion learning rate: 0.001
2024-11-05 02:38:49,767 - INFO - [diffusion][Epoch 10491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:49,768 - INFO - [diffusion][Epoch 10492] Epoch 10493/12000
2024-11-05 02:38:53,271 - INFO - [diffusion][Epoch 10492] diffusion training Loss: 0.06492342427372932
2024-11-05 02:38:53,273 - INFO - [diffusion][Epoch 10492] diffusion learning rate: 0.001
2024-11-05 02:38:53,275 - INFO - [diffusion][Epoch 10492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:53,276 - INFO - [diffusion][Epoch 10493] Epoch 10494/12000
2024-11-05 02:38:56,762 - INFO - [diffusion][Epoch 10493] diffusion training Loss: 0.06382399704307318
2024-11-05 02:38:56,764 - INFO - [diffusion][Epoch 10493] diffusion learning rate: 0.001
2024-11-05 02:38:56,766 - INFO - [diffusion][Epoch 10493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:56,768 - INFO - [diffusion][Epoch 10494] Epoch 10495/12000
2024-11-05 02:38:59,611 - INFO - [diffusion][Epoch 10494] diffusion training Loss: 0.06562008522450924
2024-11-05 02:38:59,613 - INFO - [diffusion][Epoch 10494] diffusion learning rate: 0.001
2024-11-05 02:38:59,615 - INFO - [diffusion][Epoch 10494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:59,616 - INFO - [diffusion][Epoch 10495] Epoch 10496/12000
2024-11-05 02:39:02,693 - INFO - [diffusion][Epoch 10495] diffusion training Loss: 0.06823640875518322
2024-11-05 02:39:02,695 - INFO - [diffusion][Epoch 10495] diffusion learning rate: 0.001
2024-11-05 02:39:02,697 - INFO - [diffusion][Epoch 10495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:02,698 - INFO - [diffusion][Epoch 10496] Epoch 10497/12000
2024-11-05 02:39:06,225 - INFO - [diffusion][Epoch 10496] diffusion training Loss: 0.06836463883519173
2024-11-05 02:39:06,227 - INFO - [diffusion][Epoch 10496] diffusion learning rate: 0.001
2024-11-05 02:39:06,254 - INFO - [diffusion][Epoch 10496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:06,255 - INFO - [diffusion][Epoch 10497] Epoch 10498/12000
2024-11-05 02:39:09,819 - INFO - [diffusion][Epoch 10497] diffusion training Loss: 0.06559823546558619
2024-11-05 02:39:09,821 - INFO - [diffusion][Epoch 10497] diffusion learning rate: 0.001
2024-11-05 02:39:09,822 - INFO - [diffusion][Epoch 10497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:09,824 - INFO - [diffusion][Epoch 10498] Epoch 10499/12000
2024-11-05 02:39:12,959 - INFO - [diffusion][Epoch 10498] diffusion training Loss: 0.06517172232270241
2024-11-05 02:39:12,961 - INFO - [diffusion][Epoch 10498] diffusion learning rate: 0.001
2024-11-05 02:39:12,963 - INFO - [diffusion][Epoch 10498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:12,964 - INFO - [diffusion][Epoch 10499] Epoch 10500/12000
2024-11-05 02:39:16,041 - INFO - [diffusion][Epoch 10499] diffusion training Loss: 0.07265875674784184
2024-11-05 02:39:16,049 - INFO - [diffusion][Epoch 10499] diffusion learning rate: 0.001
2024-11-05 02:39:16,050 - INFO - [diffusion][Epoch 10499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:16,052 - INFO - [diffusion][Epoch 10500] Epoch 10501/12000
2024-11-05 02:39:19,592 - INFO - [diffusion][Epoch 10500] diffusion training Loss: 0.061909159645438194
2024-11-05 02:39:19,594 - INFO - [diffusion][Epoch 10500] diffusion learning rate: 0.001
2024-11-05 02:39:19,596 - INFO - [diffusion][Epoch 10500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:19,597 - INFO - [diffusion][Epoch 10501] Epoch 10502/12000
2024-11-05 02:39:22,807 - INFO - [diffusion][Epoch 10501] diffusion training Loss: 0.0635236594825983
2024-11-05 02:39:22,809 - INFO - [diffusion][Epoch 10501] diffusion learning rate: 0.001
2024-11-05 02:39:22,811 - INFO - [diffusion][Epoch 10501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:22,812 - INFO - [diffusion][Epoch 10502] Epoch 10503/12000
2024-11-05 02:39:26,412 - INFO - [diffusion][Epoch 10502] diffusion training Loss: 0.06424080580472946
2024-11-05 02:39:26,414 - INFO - [diffusion][Epoch 10502] diffusion learning rate: 0.001
2024-11-05 02:39:26,415 - INFO - [diffusion][Epoch 10502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:26,417 - INFO - [diffusion][Epoch 10503] Epoch 10504/12000
2024-11-05 02:39:29,967 - INFO - [diffusion][Epoch 10503] diffusion training Loss: 0.06303990911692381
2024-11-05 02:39:29,969 - INFO - [diffusion][Epoch 10503] diffusion learning rate: 0.001
2024-11-05 02:39:29,971 - INFO - [diffusion][Epoch 10503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:29,972 - INFO - [diffusion][Epoch 10504] Epoch 10505/12000
2024-11-05 02:39:33,196 - INFO - [diffusion][Epoch 10504] diffusion training Loss: 0.06387091148644686
2024-11-05 02:39:33,198 - INFO - [diffusion][Epoch 10504] diffusion learning rate: 0.001
2024-11-05 02:39:33,200 - INFO - [diffusion][Epoch 10504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:33,201 - INFO - [diffusion][Epoch 10505] Epoch 10506/12000
2024-11-05 02:39:36,377 - INFO - [diffusion][Epoch 10505] diffusion training Loss: 0.06480482406914234
2024-11-05 02:39:36,378 - INFO - [diffusion][Epoch 10505] diffusion learning rate: 0.001
2024-11-05 02:39:36,380 - INFO - [diffusion][Epoch 10505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:36,381 - INFO - [diffusion][Epoch 10506] Epoch 10507/12000
2024-11-05 02:39:39,457 - INFO - [diffusion][Epoch 10506] diffusion training Loss: 0.06271812878549099
2024-11-05 02:39:39,459 - INFO - [diffusion][Epoch 10506] diffusion learning rate: 0.001
2024-11-05 02:39:39,461 - INFO - [diffusion][Epoch 10506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:39,465 - INFO - [diffusion][Epoch 10507] Epoch 10508/12000
2024-11-05 02:39:43,003 - INFO - [diffusion][Epoch 10507] diffusion training Loss: 0.06645093020051718
2024-11-05 02:39:43,005 - INFO - [diffusion][Epoch 10507] diffusion learning rate: 0.001
2024-11-05 02:39:43,045 - INFO - [diffusion][Epoch 10507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:43,047 - INFO - [diffusion][Epoch 10508] Epoch 10509/12000
2024-11-05 02:39:46,702 - INFO - [diffusion][Epoch 10508] diffusion training Loss: 0.06502393074333668
2024-11-05 02:39:46,704 - INFO - [diffusion][Epoch 10508] diffusion learning rate: 0.001
2024-11-05 02:39:46,706 - INFO - [diffusion][Epoch 10508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:46,707 - INFO - [diffusion][Epoch 10509] Epoch 10510/12000
2024-11-05 02:39:49,949 - INFO - [diffusion][Epoch 10509] diffusion training Loss: 0.06372370477765799
2024-11-05 02:39:49,951 - INFO - [diffusion][Epoch 10509] diffusion learning rate: 0.001
2024-11-05 02:39:49,953 - INFO - [diffusion][Epoch 10509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:49,954 - INFO - [diffusion][Epoch 10510] Epoch 10511/12000
2024-11-05 02:39:53,298 - INFO - [diffusion][Epoch 10510] diffusion training Loss: 0.06272951327264309
2024-11-05 02:39:53,300 - INFO - [diffusion][Epoch 10510] diffusion learning rate: 0.001
2024-11-05 02:39:53,350 - INFO - [diffusion][Epoch 10510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:53,351 - INFO - [diffusion][Epoch 10511] Epoch 10512/12000
2024-11-05 02:39:56,497 - INFO - [diffusion][Epoch 10511] diffusion training Loss: 0.06193544156849384
2024-11-05 02:39:56,499 - INFO - [diffusion][Epoch 10511] diffusion learning rate: 0.001
2024-11-05 02:39:56,501 - INFO - [diffusion][Epoch 10511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:56,502 - INFO - [diffusion][Epoch 10512] Epoch 10513/12000
2024-11-05 02:39:59,611 - INFO - [diffusion][Epoch 10512] diffusion training Loss: 0.06257685273885727
2024-11-05 02:39:59,614 - INFO - [diffusion][Epoch 10512] diffusion learning rate: 0.001
2024-11-05 02:39:59,616 - INFO - [diffusion][Epoch 10512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:59,618 - INFO - [diffusion][Epoch 10513] Epoch 10514/12000
2024-11-05 02:40:03,416 - INFO - [diffusion][Epoch 10513] diffusion training Loss: 0.06136011052876711
2024-11-05 02:40:03,418 - INFO - [diffusion][Epoch 10513] diffusion learning rate: 0.001
2024-11-05 02:40:03,420 - INFO - [diffusion][Epoch 10513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:03,421 - INFO - [diffusion][Epoch 10514] Epoch 10515/12000
2024-11-05 02:40:06,843 - INFO - [diffusion][Epoch 10514] diffusion training Loss: 0.062049439176917076
2024-11-05 02:40:06,845 - INFO - [diffusion][Epoch 10514] diffusion learning rate: 0.001
2024-11-05 02:40:06,847 - INFO - [diffusion][Epoch 10514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:06,848 - INFO - [diffusion][Epoch 10515] Epoch 10516/12000
2024-11-05 02:40:10,069 - INFO - [diffusion][Epoch 10515] diffusion training Loss: 0.0752139762043953
2024-11-05 02:40:10,071 - INFO - [diffusion][Epoch 10515] diffusion learning rate: 0.001
2024-11-05 02:40:10,073 - INFO - [diffusion][Epoch 10515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:10,074 - INFO - [diffusion][Epoch 10516] Epoch 10517/12000
2024-11-05 02:40:13,277 - INFO - [diffusion][Epoch 10516] diffusion training Loss: 0.06521677691489458
2024-11-05 02:40:13,279 - INFO - [diffusion][Epoch 10516] diffusion learning rate: 0.001
2024-11-05 02:40:13,280 - INFO - [diffusion][Epoch 10516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:13,282 - INFO - [diffusion][Epoch 10517] Epoch 10518/12000
2024-11-05 02:40:16,421 - INFO - [diffusion][Epoch 10517] diffusion training Loss: 0.06287004798650742
2024-11-05 02:40:16,423 - INFO - [diffusion][Epoch 10517] diffusion learning rate: 0.001
2024-11-05 02:40:16,425 - INFO - [diffusion][Epoch 10517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:16,426 - INFO - [diffusion][Epoch 10518] Epoch 10519/12000
2024-11-05 02:40:20,025 - INFO - [diffusion][Epoch 10518] diffusion training Loss: 0.062012046575546265
2024-11-05 02:40:20,026 - INFO - [diffusion][Epoch 10518] diffusion learning rate: 0.001
2024-11-05 02:40:20,028 - INFO - [diffusion][Epoch 10518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:20,030 - INFO - [diffusion][Epoch 10519] Epoch 10520/12000
2024-11-05 02:40:23,553 - INFO - [diffusion][Epoch 10519] diffusion training Loss: 0.06591655872762203
2024-11-05 02:40:23,555 - INFO - [diffusion][Epoch 10519] diffusion learning rate: 0.001
2024-11-05 02:40:23,557 - INFO - [diffusion][Epoch 10519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:23,558 - INFO - [diffusion][Epoch 10520] Epoch 10521/12000
2024-11-05 02:40:26,542 - INFO - [diffusion][Epoch 10520] diffusion training Loss: 0.06709922943264246
2024-11-05 02:40:26,544 - INFO - [diffusion][Epoch 10520] diffusion learning rate: 0.001
2024-11-05 02:40:26,545 - INFO - [diffusion][Epoch 10520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:26,547 - INFO - [diffusion][Epoch 10521] Epoch 10522/12000
2024-11-05 02:40:29,663 - INFO - [diffusion][Epoch 10521] diffusion training Loss: 0.06840716674923897
2024-11-05 02:40:29,665 - INFO - [diffusion][Epoch 10521] diffusion learning rate: 0.001
2024-11-05 02:40:29,692 - INFO - [diffusion][Epoch 10521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:29,693 - INFO - [diffusion][Epoch 10522] Epoch 10523/12000
2024-11-05 02:40:33,177 - INFO - [diffusion][Epoch 10522] diffusion training Loss: 0.07180827762931585
2024-11-05 02:40:33,178 - INFO - [diffusion][Epoch 10522] diffusion learning rate: 0.001
2024-11-05 02:40:33,180 - INFO - [diffusion][Epoch 10522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:33,181 - INFO - [diffusion][Epoch 10523] Epoch 10524/12000
2024-11-05 02:40:36,692 - INFO - [diffusion][Epoch 10523] diffusion training Loss: 0.06556028686463833
2024-11-05 02:40:36,695 - INFO - [diffusion][Epoch 10523] diffusion learning rate: 0.001
2024-11-05 02:40:36,696 - INFO - [diffusion][Epoch 10523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:36,698 - INFO - [diffusion][Epoch 10524] Epoch 10525/12000
2024-11-05 02:40:40,178 - INFO - [diffusion][Epoch 10524] diffusion training Loss: 0.06514929421246052
2024-11-05 02:40:40,181 - INFO - [diffusion][Epoch 10524] diffusion learning rate: 0.001
2024-11-05 02:40:40,184 - INFO - [diffusion][Epoch 10524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:40,185 - INFO - [diffusion][Epoch 10525] Epoch 10526/12000
2024-11-05 02:40:43,273 - INFO - [diffusion][Epoch 10525] diffusion training Loss: 0.06436264887452126
2024-11-05 02:40:43,275 - INFO - [diffusion][Epoch 10525] diffusion learning rate: 0.001
2024-11-05 02:40:43,277 - INFO - [diffusion][Epoch 10525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:43,278 - INFO - [diffusion][Epoch 10526] Epoch 10527/12000
2024-11-05 02:40:46,528 - INFO - [diffusion][Epoch 10526] diffusion training Loss: 0.06685972400009632
2024-11-05 02:40:46,529 - INFO - [diffusion][Epoch 10526] diffusion learning rate: 0.001
2024-11-05 02:40:46,557 - INFO - [diffusion][Epoch 10526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:46,558 - INFO - [diffusion][Epoch 10527] Epoch 10528/12000
2024-11-05 02:40:49,780 - INFO - [diffusion][Epoch 10527] diffusion training Loss: 0.06785722635686398
2024-11-05 02:40:49,782 - INFO - [diffusion][Epoch 10527] diffusion learning rate: 0.001
2024-11-05 02:40:49,784 - INFO - [diffusion][Epoch 10527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:49,785 - INFO - [diffusion][Epoch 10528] Epoch 10529/12000
2024-11-05 02:40:53,499 - INFO - [diffusion][Epoch 10528] diffusion training Loss: 0.06799247674643993
2024-11-05 02:40:53,501 - INFO - [diffusion][Epoch 10528] diffusion learning rate: 0.001
2024-11-05 02:40:53,503 - INFO - [diffusion][Epoch 10528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:53,504 - INFO - [diffusion][Epoch 10529] Epoch 10530/12000
2024-11-05 02:40:56,916 - INFO - [diffusion][Epoch 10529] diffusion training Loss: 0.06469255033880472
2024-11-05 02:40:56,918 - INFO - [diffusion][Epoch 10529] diffusion learning rate: 0.001
2024-11-05 02:40:56,960 - INFO - [diffusion][Epoch 10529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:56,961 - INFO - [diffusion][Epoch 10530] Epoch 10531/12000
2024-11-05 02:41:00,046 - INFO - [diffusion][Epoch 10530] diffusion training Loss: 0.0628260113298893
2024-11-05 02:41:00,048 - INFO - [diffusion][Epoch 10530] diffusion learning rate: 0.001
2024-11-05 02:41:00,050 - INFO - [diffusion][Epoch 10530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:00,051 - INFO - [diffusion][Epoch 10531] Epoch 10532/12000
2024-11-05 02:41:03,099 - INFO - [diffusion][Epoch 10531] diffusion training Loss: 0.06741840951144695
2024-11-05 02:41:03,101 - INFO - [diffusion][Epoch 10531] diffusion learning rate: 0.001
2024-11-05 02:41:03,103 - INFO - [diffusion][Epoch 10531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:03,104 - INFO - [diffusion][Epoch 10532] Epoch 10533/12000
2024-11-05 02:41:06,491 - INFO - [diffusion][Epoch 10532] diffusion training Loss: 0.06393382791429758
2024-11-05 02:41:06,493 - INFO - [diffusion][Epoch 10532] diffusion learning rate: 0.001
2024-11-05 02:41:06,494 - INFO - [diffusion][Epoch 10532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:06,496 - INFO - [diffusion][Epoch 10533] Epoch 10534/12000
2024-11-05 02:41:10,102 - INFO - [diffusion][Epoch 10533] diffusion training Loss: 0.06274973507970572
2024-11-05 02:41:10,105 - INFO - [diffusion][Epoch 10533] diffusion learning rate: 0.001
2024-11-05 02:41:10,107 - INFO - [diffusion][Epoch 10533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:10,108 - INFO - [diffusion][Epoch 10534] Epoch 10535/12000
2024-11-05 02:41:13,313 - INFO - [diffusion][Epoch 10534] diffusion training Loss: 0.061976486817002296
2024-11-05 02:41:13,315 - INFO - [diffusion][Epoch 10534] diffusion learning rate: 0.001
2024-11-05 02:41:13,317 - INFO - [diffusion][Epoch 10534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:13,318 - INFO - [diffusion][Epoch 10535] Epoch 10536/12000
2024-11-05 02:41:16,429 - INFO - [diffusion][Epoch 10535] diffusion training Loss: 0.06941931415349245
2024-11-05 02:41:16,431 - INFO - [diffusion][Epoch 10535] diffusion learning rate: 0.001
2024-11-05 02:41:16,433 - INFO - [diffusion][Epoch 10535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:16,435 - INFO - [diffusion][Epoch 10536] Epoch 10537/12000
2024-11-05 02:41:19,576 - INFO - [diffusion][Epoch 10536] diffusion training Loss: 0.06854995712637901
2024-11-05 02:41:19,578 - INFO - [diffusion][Epoch 10536] diffusion learning rate: 0.001
2024-11-05 02:41:19,580 - INFO - [diffusion][Epoch 10536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:19,581 - INFO - [diffusion][Epoch 10537] Epoch 10538/12000
2024-11-05 02:41:23,156 - INFO - [diffusion][Epoch 10537] diffusion training Loss: 0.06930265203118324
2024-11-05 02:41:23,158 - INFO - [diffusion][Epoch 10537] diffusion learning rate: 0.001
2024-11-05 02:41:23,160 - INFO - [diffusion][Epoch 10537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:23,161 - INFO - [diffusion][Epoch 10538] Epoch 10539/12000
2024-11-05 02:41:26,689 - INFO - [diffusion][Epoch 10538] diffusion training Loss: 0.06966830603778362
2024-11-05 02:41:26,691 - INFO - [diffusion][Epoch 10538] diffusion learning rate: 0.001
2024-11-05 02:41:26,693 - INFO - [diffusion][Epoch 10538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:26,694 - INFO - [diffusion][Epoch 10539] Epoch 10540/12000
2024-11-05 02:41:29,767 - INFO - [diffusion][Epoch 10539] diffusion training Loss: 0.06502480618655682
2024-11-05 02:41:29,769 - INFO - [diffusion][Epoch 10539] diffusion learning rate: 0.001
2024-11-05 02:41:29,771 - INFO - [diffusion][Epoch 10539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:29,772 - INFO - [diffusion][Epoch 10540] Epoch 10541/12000
2024-11-05 02:41:32,864 - INFO - [diffusion][Epoch 10540] diffusion training Loss: 0.06357365846633911
2024-11-05 02:41:32,866 - INFO - [diffusion][Epoch 10540] diffusion learning rate: 0.001
2024-11-05 02:41:32,867 - INFO - [diffusion][Epoch 10540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:32,869 - INFO - [diffusion][Epoch 10541] Epoch 10542/12000
2024-11-05 02:41:36,339 - INFO - [diffusion][Epoch 10541] diffusion training Loss: 0.0682580266147852
2024-11-05 02:41:36,341 - INFO - [diffusion][Epoch 10541] diffusion learning rate: 0.001
2024-11-05 02:41:36,343 - INFO - [diffusion][Epoch 10541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:36,344 - INFO - [diffusion][Epoch 10542] Epoch 10543/12000
2024-11-05 02:41:39,825 - INFO - [diffusion][Epoch 10542] diffusion training Loss: 0.0633551049977541
2024-11-05 02:41:39,827 - INFO - [diffusion][Epoch 10542] diffusion learning rate: 0.001
2024-11-05 02:41:39,829 - INFO - [diffusion][Epoch 10542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:39,830 - INFO - [diffusion][Epoch 10543] Epoch 10544/12000
2024-11-05 02:41:43,502 - INFO - [diffusion][Epoch 10543] diffusion training Loss: 0.06867345981299877
2024-11-05 02:41:43,504 - INFO - [diffusion][Epoch 10543] diffusion learning rate: 0.001
2024-11-05 02:41:43,506 - INFO - [diffusion][Epoch 10543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:43,507 - INFO - [diffusion][Epoch 10544] Epoch 10545/12000
2024-11-05 02:41:46,618 - INFO - [diffusion][Epoch 10544] diffusion training Loss: 0.06566364504396915
2024-11-05 02:41:46,621 - INFO - [diffusion][Epoch 10544] diffusion learning rate: 0.001
2024-11-05 02:41:46,622 - INFO - [diffusion][Epoch 10544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:46,624 - INFO - [diffusion][Epoch 10545] Epoch 10546/12000
2024-11-05 02:41:49,702 - INFO - [diffusion][Epoch 10545] diffusion training Loss: 0.06376873049885035
2024-11-05 02:41:49,704 - INFO - [diffusion][Epoch 10545] diffusion learning rate: 0.001
2024-11-05 02:41:49,706 - INFO - [diffusion][Epoch 10545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:49,707 - INFO - [diffusion][Epoch 10546] Epoch 10547/12000
2024-11-05 02:41:52,859 - INFO - [diffusion][Epoch 10546] diffusion training Loss: 0.06309291813522577
2024-11-05 02:41:52,861 - INFO - [diffusion][Epoch 10546] diffusion learning rate: 0.001
2024-11-05 02:41:52,862 - INFO - [diffusion][Epoch 10546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:52,864 - INFO - [diffusion][Epoch 10547] Epoch 10548/12000
2024-11-05 02:41:56,481 - INFO - [diffusion][Epoch 10547] diffusion training Loss: 0.06124900933355093
2024-11-05 02:41:56,483 - INFO - [diffusion][Epoch 10547] diffusion learning rate: 0.001
2024-11-05 02:41:56,485 - INFO - [diffusion][Epoch 10547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:56,486 - INFO - [diffusion][Epoch 10548] Epoch 10549/12000
2024-11-05 02:41:59,552 - INFO - [diffusion][Epoch 10548] diffusion training Loss: 0.06615725066512823
2024-11-05 02:41:59,554 - INFO - [diffusion][Epoch 10548] diffusion learning rate: 0.001
2024-11-05 02:41:59,555 - INFO - [diffusion][Epoch 10548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:59,557 - INFO - [diffusion][Epoch 10549] Epoch 10550/12000
2024-11-05 02:42:02,651 - INFO - [diffusion][Epoch 10549] diffusion training Loss: 0.06907463632524014
2024-11-05 02:42:02,653 - INFO - [diffusion][Epoch 10549] diffusion learning rate: 0.001
2024-11-05 02:42:02,654 - INFO - [diffusion][Epoch 10549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:02,656 - INFO - [diffusion][Epoch 10550] Epoch 10551/12000
2024-11-05 02:42:05,737 - INFO - [diffusion][Epoch 10550] diffusion training Loss: 0.06479685287922621
2024-11-05 02:42:05,739 - INFO - [diffusion][Epoch 10550] diffusion learning rate: 0.001
2024-11-05 02:42:05,741 - INFO - [diffusion][Epoch 10550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:05,742 - INFO - [diffusion][Epoch 10551] Epoch 10552/12000
2024-11-05 02:42:09,367 - INFO - [diffusion][Epoch 10551] diffusion training Loss: 0.06560593284666538
2024-11-05 02:42:09,369 - INFO - [diffusion][Epoch 10551] diffusion learning rate: 0.001
2024-11-05 02:42:09,371 - INFO - [diffusion][Epoch 10551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:09,372 - INFO - [diffusion][Epoch 10552] Epoch 10553/12000
2024-11-05 02:42:12,917 - INFO - [diffusion][Epoch 10552] diffusion training Loss: 0.06260073557496071
2024-11-05 02:42:12,919 - INFO - [diffusion][Epoch 10552] diffusion learning rate: 0.001
2024-11-05 02:42:12,961 - INFO - [diffusion][Epoch 10552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:12,962 - INFO - [diffusion][Epoch 10553] Epoch 10554/12000
2024-11-05 02:42:16,073 - INFO - [diffusion][Epoch 10553] diffusion training Loss: 0.06674143858253956
2024-11-05 02:42:16,076 - INFO - [diffusion][Epoch 10553] diffusion learning rate: 0.001
2024-11-05 02:42:16,078 - INFO - [diffusion][Epoch 10553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:16,079 - INFO - [diffusion][Epoch 10554] Epoch 10555/12000
2024-11-05 02:42:19,028 - INFO - [diffusion][Epoch 10554] diffusion training Loss: 0.06015620846301317
2024-11-05 02:42:19,030 - INFO - [diffusion][Epoch 10554] diffusion learning rate: 0.001
2024-11-05 02:42:19,032 - INFO - [diffusion][Epoch 10554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:19,033 - INFO - [diffusion][Epoch 10555] Epoch 10556/12000
2024-11-05 02:42:22,214 - INFO - [diffusion][Epoch 10555] diffusion training Loss: 0.06784513406455517
2024-11-05 02:42:22,217 - INFO - [diffusion][Epoch 10555] diffusion learning rate: 0.001
2024-11-05 02:42:22,218 - INFO - [diffusion][Epoch 10555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:22,219 - INFO - [diffusion][Epoch 10556] Epoch 10557/12000
2024-11-05 02:42:25,837 - INFO - [diffusion][Epoch 10556] diffusion training Loss: 0.06615100242197514
2024-11-05 02:42:25,839 - INFO - [diffusion][Epoch 10556] diffusion learning rate: 0.001
2024-11-05 02:42:25,841 - INFO - [diffusion][Epoch 10556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:25,842 - INFO - [diffusion][Epoch 10557] Epoch 10558/12000
2024-11-05 02:42:29,348 - INFO - [diffusion][Epoch 10557] diffusion training Loss: 0.06273490004241467
2024-11-05 02:42:29,351 - INFO - [diffusion][Epoch 10557] diffusion learning rate: 0.001
2024-11-05 02:42:29,352 - INFO - [diffusion][Epoch 10557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:29,354 - INFO - [diffusion][Epoch 10558] Epoch 10559/12000
2024-11-05 02:42:32,333 - INFO - [diffusion][Epoch 10558] diffusion training Loss: 0.06646621972322464
2024-11-05 02:42:32,335 - INFO - [diffusion][Epoch 10558] diffusion learning rate: 0.001
2024-11-05 02:42:32,337 - INFO - [diffusion][Epoch 10558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:32,338 - INFO - [diffusion][Epoch 10559] Epoch 10560/12000
2024-11-05 02:42:35,273 - INFO - [diffusion][Epoch 10559] diffusion training Loss: 0.06215544883161783
2024-11-05 02:42:35,275 - INFO - [diffusion][Epoch 10559] diffusion learning rate: 0.001
2024-11-05 02:42:35,305 - INFO - [diffusion][Epoch 10559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:35,306 - INFO - [diffusion][Epoch 10560] Epoch 10561/12000
2024-11-05 02:42:38,592 - INFO - [diffusion][Epoch 10560] diffusion training Loss: 0.058605476282536983
2024-11-05 02:42:38,595 - INFO - [diffusion][Epoch 10560] diffusion learning rate: 0.001
2024-11-05 02:42:38,598 - INFO - [diffusion][Epoch 10560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:38,599 - INFO - [diffusion][Epoch 10561] Epoch 10562/12000
2024-11-05 02:42:42,407 - INFO - [diffusion][Epoch 10561] diffusion training Loss: 0.0643168818205595
2024-11-05 02:42:42,410 - INFO - [diffusion][Epoch 10561] diffusion learning rate: 0.001
2024-11-05 02:42:42,412 - INFO - [diffusion][Epoch 10561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:42,413 - INFO - [diffusion][Epoch 10562] Epoch 10563/12000
2024-11-05 02:42:45,529 - INFO - [diffusion][Epoch 10562] diffusion training Loss: 0.06349323224276304
2024-11-05 02:42:45,531 - INFO - [diffusion][Epoch 10562] diffusion learning rate: 0.001
2024-11-05 02:42:45,532 - INFO - [diffusion][Epoch 10562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:45,534 - INFO - [diffusion][Epoch 10563] Epoch 10564/12000
2024-11-05 02:42:48,456 - INFO - [diffusion][Epoch 10563] diffusion training Loss: 0.06300215236842632
2024-11-05 02:42:48,458 - INFO - [diffusion][Epoch 10563] diffusion learning rate: 0.001
2024-11-05 02:42:48,460 - INFO - [diffusion][Epoch 10563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:48,461 - INFO - [diffusion][Epoch 10564] Epoch 10565/12000
2024-11-05 02:42:51,916 - INFO - [diffusion][Epoch 10564] diffusion training Loss: 0.06113085150718689
2024-11-05 02:42:51,918 - INFO - [diffusion][Epoch 10564] diffusion learning rate: 0.001
2024-11-05 02:42:51,919 - INFO - [diffusion][Epoch 10564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:51,921 - INFO - [diffusion][Epoch 10565] Epoch 10566/12000
2024-11-05 02:42:55,458 - INFO - [diffusion][Epoch 10565] diffusion training Loss: 0.06130837555974722
2024-11-05 02:42:55,460 - INFO - [diffusion][Epoch 10565] diffusion learning rate: 0.001
2024-11-05 02:42:55,462 - INFO - [diffusion][Epoch 10565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:55,463 - INFO - [diffusion][Epoch 10566] Epoch 10567/12000
2024-11-05 02:42:58,559 - INFO - [diffusion][Epoch 10566] diffusion training Loss: 0.07083268463611603
2024-11-05 02:42:58,561 - INFO - [diffusion][Epoch 10566] diffusion learning rate: 0.001
2024-11-05 02:42:58,563 - INFO - [diffusion][Epoch 10566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:58,564 - INFO - [diffusion][Epoch 10567] Epoch 10568/12000
2024-11-05 02:43:01,653 - INFO - [diffusion][Epoch 10567] diffusion training Loss: 0.06233507767319679
2024-11-05 02:43:01,655 - INFO - [diffusion][Epoch 10567] diffusion learning rate: 0.001
2024-11-05 02:43:01,657 - INFO - [diffusion][Epoch 10567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:01,658 - INFO - [diffusion][Epoch 10568] Epoch 10569/12000
2024-11-05 02:43:04,743 - INFO - [diffusion][Epoch 10568] diffusion training Loss: 0.06631401274353266
2024-11-05 02:43:04,745 - INFO - [diffusion][Epoch 10568] diffusion learning rate: 0.001
2024-11-05 02:43:04,746 - INFO - [diffusion][Epoch 10568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:04,748 - INFO - [diffusion][Epoch 10569] Epoch 10570/12000
2024-11-05 02:43:08,303 - INFO - [diffusion][Epoch 10569] diffusion training Loss: 0.06866081617772579
2024-11-05 02:43:08,305 - INFO - [diffusion][Epoch 10569] diffusion learning rate: 0.001
2024-11-05 02:43:08,307 - INFO - [diffusion][Epoch 10569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:08,308 - INFO - [diffusion][Epoch 10570] Epoch 10571/12000
2024-11-05 02:43:11,729 - INFO - [diffusion][Epoch 10570] diffusion training Loss: 0.06829795241355896
2024-11-05 02:43:11,731 - INFO - [diffusion][Epoch 10570] diffusion learning rate: 0.001
2024-11-05 02:43:11,732 - INFO - [diffusion][Epoch 10570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:11,734 - INFO - [diffusion][Epoch 10571] Epoch 10572/12000
2024-11-05 02:43:14,851 - INFO - [diffusion][Epoch 10571] diffusion training Loss: 0.07284572534263134
2024-11-05 02:43:14,853 - INFO - [diffusion][Epoch 10571] diffusion learning rate: 0.001
2024-11-05 02:43:14,855 - INFO - [diffusion][Epoch 10571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:14,856 - INFO - [diffusion][Epoch 10572] Epoch 10573/12000
2024-11-05 02:43:17,747 - INFO - [diffusion][Epoch 10572] diffusion training Loss: 0.06652197800576687
2024-11-05 02:43:17,750 - INFO - [diffusion][Epoch 10572] diffusion learning rate: 0.001
2024-11-05 02:43:17,751 - INFO - [diffusion][Epoch 10572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:17,753 - INFO - [diffusion][Epoch 10573] Epoch 10574/12000
2024-11-05 02:43:21,262 - INFO - [diffusion][Epoch 10573] diffusion training Loss: 0.0658548753708601
2024-11-05 02:43:21,264 - INFO - [diffusion][Epoch 10573] diffusion learning rate: 0.001
2024-11-05 02:43:21,267 - INFO - [diffusion][Epoch 10573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:21,268 - INFO - [diffusion][Epoch 10574] Epoch 10575/12000
2024-11-05 02:43:24,686 - INFO - [diffusion][Epoch 10574] diffusion training Loss: 0.06707237102091312
2024-11-05 02:43:24,688 - INFO - [diffusion][Epoch 10574] diffusion learning rate: 0.001
2024-11-05 02:43:24,689 - INFO - [diffusion][Epoch 10574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:24,691 - INFO - [diffusion][Epoch 10575] Epoch 10576/12000
2024-11-05 02:43:27,721 - INFO - [diffusion][Epoch 10575] diffusion training Loss: 0.06360964849591255
2024-11-05 02:43:27,723 - INFO - [diffusion][Epoch 10575] diffusion learning rate: 0.001
2024-11-05 02:43:27,725 - INFO - [diffusion][Epoch 10575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:27,726 - INFO - [diffusion][Epoch 10576] Epoch 10577/12000
2024-11-05 02:43:30,635 - INFO - [diffusion][Epoch 10576] diffusion training Loss: 0.0637841708958149
2024-11-05 02:43:30,637 - INFO - [diffusion][Epoch 10576] diffusion learning rate: 0.001
2024-11-05 02:43:30,638 - INFO - [diffusion][Epoch 10576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:30,639 - INFO - [diffusion][Epoch 10577] Epoch 10578/12000
2024-11-05 02:43:34,167 - INFO - [diffusion][Epoch 10577] diffusion training Loss: 0.06496584042906761
2024-11-05 02:43:34,169 - INFO - [diffusion][Epoch 10577] diffusion learning rate: 0.001
2024-11-05 02:43:34,171 - INFO - [diffusion][Epoch 10577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:34,173 - INFO - [diffusion][Epoch 10578] Epoch 10579/12000
2024-11-05 02:43:37,697 - INFO - [diffusion][Epoch 10578] diffusion training Loss: 0.06413341779261827
2024-11-05 02:43:37,699 - INFO - [diffusion][Epoch 10578] diffusion learning rate: 0.001
2024-11-05 02:43:37,700 - INFO - [diffusion][Epoch 10578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:37,702 - INFO - [diffusion][Epoch 10579] Epoch 10580/12000
2024-11-05 02:43:40,799 - INFO - [diffusion][Epoch 10579] diffusion training Loss: 0.06484765093773603
2024-11-05 02:43:40,801 - INFO - [diffusion][Epoch 10579] diffusion learning rate: 0.001
2024-11-05 02:43:40,803 - INFO - [diffusion][Epoch 10579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:40,804 - INFO - [diffusion][Epoch 10580] Epoch 10581/12000
2024-11-05 02:43:44,366 - INFO - [diffusion][Epoch 10580] diffusion training Loss: 0.07007648423314095
2024-11-05 02:43:44,369 - INFO - [diffusion][Epoch 10580] diffusion learning rate: 0.001
2024-11-05 02:43:44,371 - INFO - [diffusion][Epoch 10580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:44,373 - INFO - [diffusion][Epoch 10581] Epoch 10582/12000
2024-11-05 02:43:47,467 - INFO - [diffusion][Epoch 10581] diffusion training Loss: 0.06084455922245979
2024-11-05 02:43:47,469 - INFO - [diffusion][Epoch 10581] diffusion learning rate: 0.001
2024-11-05 02:43:47,471 - INFO - [diffusion][Epoch 10581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:47,472 - INFO - [diffusion][Epoch 10582] Epoch 10583/12000
2024-11-05 02:43:50,834 - INFO - [diffusion][Epoch 10582] diffusion training Loss: 0.06392820924520493
2024-11-05 02:43:50,836 - INFO - [diffusion][Epoch 10582] diffusion learning rate: 0.001
2024-11-05 02:43:50,837 - INFO - [diffusion][Epoch 10582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:50,839 - INFO - [diffusion][Epoch 10583] Epoch 10584/12000
2024-11-05 02:43:54,283 - INFO - [diffusion][Epoch 10583] diffusion training Loss: 0.0676623648032546
2024-11-05 02:43:54,285 - INFO - [diffusion][Epoch 10583] diffusion learning rate: 0.001
2024-11-05 02:43:54,287 - INFO - [diffusion][Epoch 10583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:54,289 - INFO - [diffusion][Epoch 10584] Epoch 10585/12000
2024-11-05 02:43:57,308 - INFO - [diffusion][Epoch 10584] diffusion training Loss: 0.06059721764177084
2024-11-05 02:43:57,310 - INFO - [diffusion][Epoch 10584] diffusion learning rate: 0.001
2024-11-05 02:43:57,312 - INFO - [diffusion][Epoch 10584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:57,313 - INFO - [diffusion][Epoch 10585] Epoch 10586/12000
2024-11-05 02:44:00,394 - INFO - [diffusion][Epoch 10585] diffusion training Loss: 0.07130192965269089
2024-11-05 02:44:00,397 - INFO - [diffusion][Epoch 10585] diffusion learning rate: 0.001
2024-11-05 02:44:00,399 - INFO - [diffusion][Epoch 10585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:00,400 - INFO - [diffusion][Epoch 10586] Epoch 10587/12000
2024-11-05 02:44:03,629 - INFO - [diffusion][Epoch 10586] diffusion training Loss: 0.06340001989156008
2024-11-05 02:44:03,631 - INFO - [diffusion][Epoch 10586] diffusion learning rate: 0.001
2024-11-05 02:44:03,633 - INFO - [diffusion][Epoch 10586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:03,634 - INFO - [diffusion][Epoch 10587] Epoch 10588/12000
2024-11-05 02:44:07,316 - INFO - [diffusion][Epoch 10587] diffusion training Loss: 0.06694643199443817
2024-11-05 02:44:07,318 - INFO - [diffusion][Epoch 10587] diffusion learning rate: 0.001
2024-11-05 02:44:07,320 - INFO - [diffusion][Epoch 10587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:07,321 - INFO - [diffusion][Epoch 10588] Epoch 10589/12000
2024-11-05 02:44:10,671 - INFO - [diffusion][Epoch 10588] diffusion training Loss: 0.06618048436939716
2024-11-05 02:44:10,673 - INFO - [diffusion][Epoch 10588] diffusion learning rate: 0.001
2024-11-05 02:44:10,675 - INFO - [diffusion][Epoch 10588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:10,677 - INFO - [diffusion][Epoch 10589] Epoch 10590/12000
2024-11-05 02:44:13,750 - INFO - [diffusion][Epoch 10589] diffusion training Loss: 0.06599998846650124
2024-11-05 02:44:13,752 - INFO - [diffusion][Epoch 10589] diffusion learning rate: 0.001
2024-11-05 02:44:13,782 - INFO - [diffusion][Epoch 10589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:13,784 - INFO - [diffusion][Epoch 10590] Epoch 10591/12000
2024-11-05 02:44:16,895 - INFO - [diffusion][Epoch 10590] diffusion training Loss: 0.06844491325318813
2024-11-05 02:44:16,897 - INFO - [diffusion][Epoch 10590] diffusion learning rate: 0.001
2024-11-05 02:44:16,899 - INFO - [diffusion][Epoch 10590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:16,900 - INFO - [diffusion][Epoch 10591] Epoch 10592/12000
2024-11-05 02:44:20,419 - INFO - [diffusion][Epoch 10591] diffusion training Loss: 0.06841932609677315
2024-11-05 02:44:20,421 - INFO - [diffusion][Epoch 10591] diffusion learning rate: 0.001
2024-11-05 02:44:20,422 - INFO - [diffusion][Epoch 10591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:20,424 - INFO - [diffusion][Epoch 10592] Epoch 10593/12000
2024-11-05 02:44:24,007 - INFO - [diffusion][Epoch 10592] diffusion training Loss: 0.0678960382938385
2024-11-05 02:44:24,009 - INFO - [diffusion][Epoch 10592] diffusion learning rate: 0.001
2024-11-05 02:44:24,010 - INFO - [diffusion][Epoch 10592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:24,012 - INFO - [diffusion][Epoch 10593] Epoch 10594/12000
2024-11-05 02:44:27,096 - INFO - [diffusion][Epoch 10593] diffusion training Loss: 0.06492982618510723
2024-11-05 02:44:27,098 - INFO - [diffusion][Epoch 10593] diffusion learning rate: 0.001
2024-11-05 02:44:27,099 - INFO - [diffusion][Epoch 10593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:27,101 - INFO - [diffusion][Epoch 10594] Epoch 10595/12000
2024-11-05 02:44:30,226 - INFO - [diffusion][Epoch 10594] diffusion training Loss: 0.06650761049240828
2024-11-05 02:44:30,228 - INFO - [diffusion][Epoch 10594] diffusion learning rate: 0.001
2024-11-05 02:44:30,229 - INFO - [diffusion][Epoch 10594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:30,231 - INFO - [diffusion][Epoch 10595] Epoch 10596/12000
2024-11-05 02:44:33,461 - INFO - [diffusion][Epoch 10595] diffusion training Loss: 0.06290267407894135
2024-11-05 02:44:33,463 - INFO - [diffusion][Epoch 10595] diffusion learning rate: 0.001
2024-11-05 02:44:33,464 - INFO - [diffusion][Epoch 10595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:33,466 - INFO - [diffusion][Epoch 10596] Epoch 10597/12000
2024-11-05 02:44:37,032 - INFO - [diffusion][Epoch 10596] diffusion training Loss: 0.068296043202281
2024-11-05 02:44:37,034 - INFO - [diffusion][Epoch 10596] diffusion learning rate: 0.001
2024-11-05 02:44:37,035 - INFO - [diffusion][Epoch 10596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:37,037 - INFO - [diffusion][Epoch 10597] Epoch 10598/12000
2024-11-05 02:44:40,147 - INFO - [diffusion][Epoch 10597] diffusion training Loss: 0.06510213669389486
2024-11-05 02:44:40,149 - INFO - [diffusion][Epoch 10597] diffusion learning rate: 0.001
2024-11-05 02:44:40,151 - INFO - [diffusion][Epoch 10597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:40,152 - INFO - [diffusion][Epoch 10598] Epoch 10599/12000
2024-11-05 02:44:43,261 - INFO - [diffusion][Epoch 10598] diffusion training Loss: 0.06572388112545013
2024-11-05 02:44:43,263 - INFO - [diffusion][Epoch 10598] diffusion learning rate: 0.001
2024-11-05 02:44:43,265 - INFO - [diffusion][Epoch 10598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:43,266 - INFO - [diffusion][Epoch 10599] Epoch 10600/12000
2024-11-05 02:44:46,344 - INFO - [diffusion][Epoch 10599] diffusion training Loss: 0.06078485958278179
2024-11-05 02:44:46,346 - INFO - [diffusion][Epoch 10599] diffusion learning rate: 0.001
2024-11-05 02:44:46,348 - INFO - [diffusion][Epoch 10599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:46,349 - INFO - [diffusion][Epoch 10600] Epoch 10601/12000
2024-11-05 02:44:50,212 - INFO - [diffusion][Epoch 10600] diffusion training Loss: 0.06383328512310982
2024-11-05 02:44:50,214 - INFO - [diffusion][Epoch 10600] diffusion learning rate: 0.001
2024-11-05 02:44:50,216 - INFO - [diffusion][Epoch 10600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:50,217 - INFO - [diffusion][Epoch 10601] Epoch 10602/12000
2024-11-05 02:44:53,708 - INFO - [diffusion][Epoch 10601] diffusion training Loss: 0.06145001482218504
2024-11-05 02:44:53,710 - INFO - [diffusion][Epoch 10601] diffusion learning rate: 0.001
2024-11-05 02:44:53,712 - INFO - [diffusion][Epoch 10601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:53,713 - INFO - [diffusion][Epoch 10602] Epoch 10603/12000
2024-11-05 02:44:56,826 - INFO - [diffusion][Epoch 10602] diffusion training Loss: 0.07376972399652004
2024-11-05 02:44:56,828 - INFO - [diffusion][Epoch 10602] diffusion learning rate: 0.001
2024-11-05 02:44:56,830 - INFO - [diffusion][Epoch 10602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:56,831 - INFO - [diffusion][Epoch 10603] Epoch 10604/12000
2024-11-05 02:44:59,914 - INFO - [diffusion][Epoch 10603] diffusion training Loss: 0.06971754506230354
2024-11-05 02:44:59,916 - INFO - [diffusion][Epoch 10603] diffusion learning rate: 0.001
2024-11-05 02:44:59,918 - INFO - [diffusion][Epoch 10603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:59,919 - INFO - [diffusion][Epoch 10604] Epoch 10605/12000
2024-11-05 02:45:03,103 - INFO - [diffusion][Epoch 10604] diffusion training Loss: 0.06606162060052156
2024-11-05 02:45:03,105 - INFO - [diffusion][Epoch 10604] diffusion learning rate: 0.001
2024-11-05 02:45:03,107 - INFO - [diffusion][Epoch 10604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:03,108 - INFO - [diffusion][Epoch 10605] Epoch 10606/12000
2024-11-05 02:45:06,668 - INFO - [diffusion][Epoch 10605] diffusion training Loss: 0.06891745887696743
2024-11-05 02:45:06,671 - INFO - [diffusion][Epoch 10605] diffusion learning rate: 0.001
2024-11-05 02:45:06,673 - INFO - [diffusion][Epoch 10605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:06,674 - INFO - [diffusion][Epoch 10606] Epoch 10607/12000
2024-11-05 02:45:10,082 - INFO - [diffusion][Epoch 10606] diffusion training Loss: 0.06748615205287933
2024-11-05 02:45:10,084 - INFO - [diffusion][Epoch 10606] diffusion learning rate: 0.001
2024-11-05 02:45:10,085 - INFO - [diffusion][Epoch 10606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:10,087 - INFO - [diffusion][Epoch 10607] Epoch 10608/12000
2024-11-05 02:45:13,228 - INFO - [diffusion][Epoch 10607] diffusion training Loss: 0.06492605805397034
2024-11-05 02:45:13,230 - INFO - [diffusion][Epoch 10607] diffusion learning rate: 0.001
2024-11-05 02:45:13,232 - INFO - [diffusion][Epoch 10607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:13,233 - INFO - [diffusion][Epoch 10608] Epoch 10609/12000
2024-11-05 02:45:16,332 - INFO - [diffusion][Epoch 10608] diffusion training Loss: 0.06760966964066029
2024-11-05 02:45:16,334 - INFO - [diffusion][Epoch 10608] diffusion learning rate: 0.001
2024-11-05 02:45:16,336 - INFO - [diffusion][Epoch 10608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:16,337 - INFO - [diffusion][Epoch 10609] Epoch 10610/12000
2024-11-05 02:45:19,740 - INFO - [diffusion][Epoch 10609] diffusion training Loss: 0.06748910062015057
2024-11-05 02:45:19,741 - INFO - [diffusion][Epoch 10609] diffusion learning rate: 0.001
2024-11-05 02:45:19,743 - INFO - [diffusion][Epoch 10609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:19,744 - INFO - [diffusion][Epoch 10610] Epoch 10611/12000
2024-11-05 02:45:23,224 - INFO - [diffusion][Epoch 10610] diffusion training Loss: 0.06513337418437004
2024-11-05 02:45:23,226 - INFO - [diffusion][Epoch 10610] diffusion learning rate: 0.001
2024-11-05 02:45:23,278 - INFO - [diffusion][Epoch 10610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:23,280 - INFO - [diffusion][Epoch 10611] Epoch 10612/12000
2024-11-05 02:45:26,338 - INFO - [diffusion][Epoch 10611] diffusion training Loss: 0.06516490876674652
2024-11-05 02:45:26,340 - INFO - [diffusion][Epoch 10611] diffusion learning rate: 0.001
2024-11-05 02:45:26,341 - INFO - [diffusion][Epoch 10611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:26,343 - INFO - [diffusion][Epoch 10612] Epoch 10613/12000
2024-11-05 02:45:29,488 - INFO - [diffusion][Epoch 10612] diffusion training Loss: 0.0681625884026289
2024-11-05 02:45:29,490 - INFO - [diffusion][Epoch 10612] diffusion learning rate: 0.001
2024-11-05 02:45:29,491 - INFO - [diffusion][Epoch 10612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:29,492 - INFO - [diffusion][Epoch 10613] Epoch 10614/12000
2024-11-05 02:45:32,777 - INFO - [diffusion][Epoch 10613] diffusion training Loss: 0.0667045209556818
2024-11-05 02:45:32,779 - INFO - [diffusion][Epoch 10613] diffusion learning rate: 0.001
2024-11-05 02:45:32,781 - INFO - [diffusion][Epoch 10613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:32,782 - INFO - [diffusion][Epoch 10614] Epoch 10615/12000
2024-11-05 02:45:36,420 - INFO - [diffusion][Epoch 10614] diffusion training Loss: 0.06878533586859703
2024-11-05 02:45:36,422 - INFO - [diffusion][Epoch 10614] diffusion learning rate: 0.001
2024-11-05 02:45:36,424 - INFO - [diffusion][Epoch 10614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:36,425 - INFO - [diffusion][Epoch 10615] Epoch 10616/12000
2024-11-05 02:45:39,711 - INFO - [diffusion][Epoch 10615] diffusion training Loss: 0.06623378209769726
2024-11-05 02:45:39,713 - INFO - [diffusion][Epoch 10615] diffusion learning rate: 0.001
2024-11-05 02:45:39,714 - INFO - [diffusion][Epoch 10615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:39,716 - INFO - [diffusion][Epoch 10616] Epoch 10617/12000
2024-11-05 02:45:42,812 - INFO - [diffusion][Epoch 10616] diffusion training Loss: 0.07030919007956982
2024-11-05 02:45:42,814 - INFO - [diffusion][Epoch 10616] diffusion learning rate: 0.001
2024-11-05 02:45:42,816 - INFO - [diffusion][Epoch 10616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:42,817 - INFO - [diffusion][Epoch 10617] Epoch 10618/12000
2024-11-05 02:45:45,915 - INFO - [diffusion][Epoch 10617] diffusion training Loss: 0.07314882799983025
2024-11-05 02:45:45,917 - INFO - [diffusion][Epoch 10617] diffusion learning rate: 0.001
2024-11-05 02:45:45,919 - INFO - [diffusion][Epoch 10617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:45,921 - INFO - [diffusion][Epoch 10618] Epoch 10619/12000
2024-11-05 02:45:49,443 - INFO - [diffusion][Epoch 10618] diffusion training Loss: 0.0606433330103755
2024-11-05 02:45:49,446 - INFO - [diffusion][Epoch 10618] diffusion learning rate: 0.001
2024-11-05 02:45:49,447 - INFO - [diffusion][Epoch 10618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:49,449 - INFO - [diffusion][Epoch 10619] Epoch 10620/12000
2024-11-05 02:45:52,724 - INFO - [diffusion][Epoch 10619] diffusion training Loss: 0.06885704398155212
2024-11-05 02:45:52,727 - INFO - [diffusion][Epoch 10619] diffusion learning rate: 0.001
2024-11-05 02:45:52,729 - INFO - [diffusion][Epoch 10619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:52,730 - INFO - [diffusion][Epoch 10620] Epoch 10621/12000
2024-11-05 02:45:55,757 - INFO - [diffusion][Epoch 10620] diffusion training Loss: 0.06425322126597166
2024-11-05 02:45:55,759 - INFO - [diffusion][Epoch 10620] diffusion learning rate: 0.001
2024-11-05 02:45:55,820 - INFO - [diffusion][Epoch 10620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:55,821 - INFO - [diffusion][Epoch 10621] Epoch 10622/12000
2024-11-05 02:45:59,466 - INFO - [diffusion][Epoch 10621] diffusion training Loss: 0.06374302878975868
2024-11-05 02:45:59,468 - INFO - [diffusion][Epoch 10621] diffusion learning rate: 0.001
2024-11-05 02:45:59,471 - INFO - [diffusion][Epoch 10621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:59,472 - INFO - [diffusion][Epoch 10622] Epoch 10623/12000
2024-11-05 02:46:02,572 - INFO - [diffusion][Epoch 10622] diffusion training Loss: 0.0648424830287695
2024-11-05 02:46:02,574 - INFO - [diffusion][Epoch 10622] diffusion learning rate: 0.001
2024-11-05 02:46:02,576 - INFO - [diffusion][Epoch 10622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:02,577 - INFO - [diffusion][Epoch 10623] Epoch 10624/12000
2024-11-05 02:46:06,095 - INFO - [diffusion][Epoch 10623] diffusion training Loss: 0.0607658950611949
2024-11-05 02:46:06,097 - INFO - [diffusion][Epoch 10623] diffusion learning rate: 0.001
2024-11-05 02:46:06,099 - INFO - [diffusion][Epoch 10623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:06,100 - INFO - [diffusion][Epoch 10624] Epoch 10625/12000
2024-11-05 02:46:09,632 - INFO - [diffusion][Epoch 10624] diffusion training Loss: 0.07106814906001091
2024-11-05 02:46:09,634 - INFO - [diffusion][Epoch 10624] diffusion learning rate: 0.001
2024-11-05 02:46:09,636 - INFO - [diffusion][Epoch 10624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:09,637 - INFO - [diffusion][Epoch 10625] Epoch 10626/12000
2024-11-05 02:46:12,787 - INFO - [diffusion][Epoch 10625] diffusion training Loss: 0.06037667579948902
2024-11-05 02:46:12,789 - INFO - [diffusion][Epoch 10625] diffusion learning rate: 0.001
2024-11-05 02:46:12,791 - INFO - [diffusion][Epoch 10625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:12,792 - INFO - [diffusion][Epoch 10626] Epoch 10627/12000
2024-11-05 02:46:15,658 - INFO - [diffusion][Epoch 10626] diffusion training Loss: 0.07122849300503731
2024-11-05 02:46:15,660 - INFO - [diffusion][Epoch 10626] diffusion learning rate: 0.001
2024-11-05 02:46:15,661 - INFO - [diffusion][Epoch 10626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:15,663 - INFO - [diffusion][Epoch 10627] Epoch 10628/12000
2024-11-05 02:46:18,989 - INFO - [diffusion][Epoch 10627] diffusion training Loss: 0.06066818907856941
2024-11-05 02:46:18,991 - INFO - [diffusion][Epoch 10627] diffusion learning rate: 0.001
2024-11-05 02:46:18,993 - INFO - [diffusion][Epoch 10627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:18,994 - INFO - [diffusion][Epoch 10628] Epoch 10629/12000
2024-11-05 02:46:22,602 - INFO - [diffusion][Epoch 10628] diffusion training Loss: 0.06714270729571581
2024-11-05 02:46:22,604 - INFO - [diffusion][Epoch 10628] diffusion learning rate: 0.001
2024-11-05 02:46:22,606 - INFO - [diffusion][Epoch 10628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:22,607 - INFO - [diffusion][Epoch 10629] Epoch 10630/12000
2024-11-05 02:46:25,861 - INFO - [diffusion][Epoch 10629] diffusion training Loss: 0.06441298685967922
2024-11-05 02:46:25,863 - INFO - [diffusion][Epoch 10629] diffusion learning rate: 0.001
2024-11-05 02:46:25,864 - INFO - [diffusion][Epoch 10629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:25,866 - INFO - [diffusion][Epoch 10630] Epoch 10631/12000
2024-11-05 02:46:28,989 - INFO - [diffusion][Epoch 10630] diffusion training Loss: 0.06618934869766235
2024-11-05 02:46:28,991 - INFO - [diffusion][Epoch 10630] diffusion learning rate: 0.001
2024-11-05 02:46:28,993 - INFO - [diffusion][Epoch 10630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:28,994 - INFO - [diffusion][Epoch 10631] Epoch 10632/12000
2024-11-05 02:46:32,169 - INFO - [diffusion][Epoch 10631] diffusion training Loss: 0.0687213446944952
2024-11-05 02:46:32,171 - INFO - [diffusion][Epoch 10631] diffusion learning rate: 0.001
2024-11-05 02:46:32,172 - INFO - [diffusion][Epoch 10631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:32,173 - INFO - [diffusion][Epoch 10632] Epoch 10633/12000
2024-11-05 02:46:35,697 - INFO - [diffusion][Epoch 10632] diffusion training Loss: 0.06126812659204006
2024-11-05 02:46:35,698 - INFO - [diffusion][Epoch 10632] diffusion learning rate: 0.001
2024-11-05 02:46:35,700 - INFO - [diffusion][Epoch 10632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:35,701 - INFO - [diffusion][Epoch 10633] Epoch 10634/12000
2024-11-05 02:46:39,258 - INFO - [diffusion][Epoch 10633] diffusion training Loss: 0.06766838952898979
2024-11-05 02:46:39,260 - INFO - [diffusion][Epoch 10633] diffusion learning rate: 0.001
2024-11-05 02:46:39,262 - INFO - [diffusion][Epoch 10633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:39,263 - INFO - [diffusion][Epoch 10634] Epoch 10635/12000
2024-11-05 02:46:42,470 - INFO - [diffusion][Epoch 10634] diffusion training Loss: 0.06621871795505285
2024-11-05 02:46:42,472 - INFO - [diffusion][Epoch 10634] diffusion learning rate: 0.001
2024-11-05 02:46:42,474 - INFO - [diffusion][Epoch 10634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:42,475 - INFO - [diffusion][Epoch 10635] Epoch 10636/12000
2024-11-05 02:46:45,609 - INFO - [diffusion][Epoch 10635] diffusion training Loss: 0.06307071261107922
2024-11-05 02:46:45,611 - INFO - [diffusion][Epoch 10635] diffusion learning rate: 0.001
2024-11-05 02:46:45,613 - INFO - [diffusion][Epoch 10635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:45,614 - INFO - [diffusion][Epoch 10636] Epoch 10637/12000
2024-11-05 02:46:48,729 - INFO - [diffusion][Epoch 10636] diffusion training Loss: 0.06325005553662777
2024-11-05 02:46:48,731 - INFO - [diffusion][Epoch 10636] diffusion learning rate: 0.001
2024-11-05 02:46:48,733 - INFO - [diffusion][Epoch 10636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:48,734 - INFO - [diffusion][Epoch 10637] Epoch 10638/12000
2024-11-05 02:46:52,289 - INFO - [diffusion][Epoch 10637] diffusion training Loss: 0.059454429894685745
2024-11-05 02:46:52,292 - INFO - [diffusion][Epoch 10637] diffusion learning rate: 0.001
2024-11-05 02:46:52,294 - INFO - [diffusion][Epoch 10637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:52,295 - INFO - [diffusion][Epoch 10638] Epoch 10639/12000
2024-11-05 02:46:55,680 - INFO - [diffusion][Epoch 10638] diffusion training Loss: 0.06952922604978085
2024-11-05 02:46:55,683 - INFO - [diffusion][Epoch 10638] diffusion learning rate: 0.001
2024-11-05 02:46:55,684 - INFO - [diffusion][Epoch 10638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:55,686 - INFO - [diffusion][Epoch 10639] Epoch 10640/12000
2024-11-05 02:46:58,862 - INFO - [diffusion][Epoch 10639] diffusion training Loss: 0.06767085753381252
2024-11-05 02:46:58,863 - INFO - [diffusion][Epoch 10639] diffusion learning rate: 0.001
2024-11-05 02:46:58,865 - INFO - [diffusion][Epoch 10639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:58,866 - INFO - [diffusion][Epoch 10640] Epoch 10641/12000
2024-11-05 02:47:01,938 - INFO - [diffusion][Epoch 10640] diffusion training Loss: 0.0620827479287982
2024-11-05 02:47:01,940 - INFO - [diffusion][Epoch 10640] diffusion learning rate: 0.001
2024-11-05 02:47:01,942 - INFO - [diffusion][Epoch 10640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:01,943 - INFO - [diffusion][Epoch 10641] Epoch 10642/12000
2024-11-05 02:47:05,314 - INFO - [diffusion][Epoch 10641] diffusion training Loss: 0.06953472085297108
2024-11-05 02:47:05,316 - INFO - [diffusion][Epoch 10641] diffusion learning rate: 0.001
2024-11-05 02:47:05,318 - INFO - [diffusion][Epoch 10641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:05,319 - INFO - [diffusion][Epoch 10642] Epoch 10643/12000
2024-11-05 02:47:08,577 - INFO - [diffusion][Epoch 10642] diffusion training Loss: 0.06491126399487257
2024-11-05 02:47:08,579 - INFO - [diffusion][Epoch 10642] diffusion learning rate: 0.001
2024-11-05 02:47:08,581 - INFO - [diffusion][Epoch 10642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:08,583 - INFO - [diffusion][Epoch 10643] Epoch 10644/12000
2024-11-05 02:47:11,549 - INFO - [diffusion][Epoch 10643] diffusion training Loss: 0.07055679894983768
2024-11-05 02:47:11,551 - INFO - [diffusion][Epoch 10643] diffusion learning rate: 0.001
2024-11-05 02:47:11,552 - INFO - [diffusion][Epoch 10643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:11,554 - INFO - [diffusion][Epoch 10644] Epoch 10645/12000
2024-11-05 02:47:14,661 - INFO - [diffusion][Epoch 10644] diffusion training Loss: 0.06705387867987156
2024-11-05 02:47:14,663 - INFO - [diffusion][Epoch 10644] diffusion learning rate: 0.001
2024-11-05 02:47:14,665 - INFO - [diffusion][Epoch 10644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:14,666 - INFO - [diffusion][Epoch 10645] Epoch 10646/12000
2024-11-05 02:47:18,096 - INFO - [diffusion][Epoch 10645] diffusion training Loss: 0.06190789118409157
2024-11-05 02:47:18,098 - INFO - [diffusion][Epoch 10645] diffusion learning rate: 0.001
2024-11-05 02:47:18,100 - INFO - [diffusion][Epoch 10645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:18,101 - INFO - [diffusion][Epoch 10646] Epoch 10647/12000
2024-11-05 02:47:21,691 - INFO - [diffusion][Epoch 10646] diffusion training Loss: 0.0706738568842411
2024-11-05 02:47:21,692 - INFO - [diffusion][Epoch 10646] diffusion learning rate: 0.001
2024-11-05 02:47:21,694 - INFO - [diffusion][Epoch 10646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:21,695 - INFO - [diffusion][Epoch 10647] Epoch 10648/12000
2024-11-05 02:47:24,921 - INFO - [diffusion][Epoch 10647] diffusion training Loss: 0.0672067329287529
2024-11-05 02:47:24,924 - INFO - [diffusion][Epoch 10647] diffusion learning rate: 0.001
2024-11-05 02:47:24,925 - INFO - [diffusion][Epoch 10647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:24,927 - INFO - [diffusion][Epoch 10648] Epoch 10649/12000
2024-11-05 02:47:28,105 - INFO - [diffusion][Epoch 10648] diffusion training Loss: 0.0641025397926569
2024-11-05 02:47:28,107 - INFO - [diffusion][Epoch 10648] diffusion learning rate: 0.001
2024-11-05 02:47:28,108 - INFO - [diffusion][Epoch 10648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:28,109 - INFO - [diffusion][Epoch 10649] Epoch 10650/12000
2024-11-05 02:47:31,204 - INFO - [diffusion][Epoch 10649] diffusion training Loss: 0.0708879828453064
2024-11-05 02:47:31,207 - INFO - [diffusion][Epoch 10649] diffusion learning rate: 0.001
2024-11-05 02:47:31,209 - INFO - [diffusion][Epoch 10649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:31,210 - INFO - [diffusion][Epoch 10650] Epoch 10651/12000
2024-11-05 02:47:34,705 - INFO - [diffusion][Epoch 10650] diffusion training Loss: 0.06673167459666729
2024-11-05 02:47:34,707 - INFO - [diffusion][Epoch 10650] diffusion learning rate: 0.001
2024-11-05 02:47:34,709 - INFO - [diffusion][Epoch 10650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:34,711 - INFO - [diffusion][Epoch 10651] Epoch 10652/12000
2024-11-05 02:47:38,233 - INFO - [diffusion][Epoch 10651] diffusion training Loss: 0.06589592434465885
2024-11-05 02:47:38,235 - INFO - [diffusion][Epoch 10651] diffusion learning rate: 0.001
2024-11-05 02:47:38,237 - INFO - [diffusion][Epoch 10651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:38,238 - INFO - [diffusion][Epoch 10652] Epoch 10653/12000
2024-11-05 02:47:41,354 - INFO - [diffusion][Epoch 10652] diffusion training Loss: 0.0643545351922512
2024-11-05 02:47:41,357 - INFO - [diffusion][Epoch 10652] diffusion learning rate: 0.001
2024-11-05 02:47:41,358 - INFO - [diffusion][Epoch 10652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:41,360 - INFO - [diffusion][Epoch 10653] Epoch 10654/12000
2024-11-05 02:47:44,468 - INFO - [diffusion][Epoch 10653] diffusion training Loss: 0.06554335448890924
2024-11-05 02:47:44,470 - INFO - [diffusion][Epoch 10653] diffusion learning rate: 0.001
2024-11-05 02:47:44,472 - INFO - [diffusion][Epoch 10653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:44,473 - INFO - [diffusion][Epoch 10654] Epoch 10655/12000
2024-11-05 02:47:47,656 - INFO - [diffusion][Epoch 10654] diffusion training Loss: 0.06476790457963943
2024-11-05 02:47:47,658 - INFO - [diffusion][Epoch 10654] diffusion learning rate: 0.001
2024-11-05 02:47:47,660 - INFO - [diffusion][Epoch 10654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:47,661 - INFO - [diffusion][Epoch 10655] Epoch 10656/12000
2024-11-05 02:47:51,086 - INFO - [diffusion][Epoch 10655] diffusion training Loss: 0.060987453907728195
2024-11-05 02:47:51,089 - INFO - [diffusion][Epoch 10655] diffusion learning rate: 0.001
2024-11-05 02:47:51,091 - INFO - [diffusion][Epoch 10655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:51,092 - INFO - [diffusion][Epoch 10656] Epoch 10657/12000
2024-11-05 02:47:54,440 - INFO - [diffusion][Epoch 10656] diffusion training Loss: 0.06549193896353245
2024-11-05 02:47:54,442 - INFO - [diffusion][Epoch 10656] diffusion learning rate: 0.001
2024-11-05 02:47:54,444 - INFO - [diffusion][Epoch 10656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:54,446 - INFO - [diffusion][Epoch 10657] Epoch 10658/12000
2024-11-05 02:47:57,506 - INFO - [diffusion][Epoch 10657] diffusion training Loss: 0.06769600696861744
2024-11-05 02:47:57,508 - INFO - [diffusion][Epoch 10657] diffusion learning rate: 0.001
2024-11-05 02:47:57,510 - INFO - [diffusion][Epoch 10657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:57,511 - INFO - [diffusion][Epoch 10658] Epoch 10659/12000
2024-11-05 02:48:00,627 - INFO - [diffusion][Epoch 10658] diffusion training Loss: 0.0654691644012928
2024-11-05 02:48:00,629 - INFO - [diffusion][Epoch 10658] diffusion learning rate: 0.001
2024-11-05 02:48:00,630 - INFO - [diffusion][Epoch 10658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:00,632 - INFO - [diffusion][Epoch 10659] Epoch 10660/12000
2024-11-05 02:48:04,160 - INFO - [diffusion][Epoch 10659] diffusion training Loss: 0.06487252842634916
2024-11-05 02:48:04,162 - INFO - [diffusion][Epoch 10659] diffusion learning rate: 0.001
2024-11-05 02:48:04,164 - INFO - [diffusion][Epoch 10659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:04,165 - INFO - [diffusion][Epoch 10660] Epoch 10661/12000
2024-11-05 02:48:08,222 - INFO - [diffusion][Epoch 10660] diffusion training Loss: 0.06526159960776567
2024-11-05 02:48:08,225 - INFO - [diffusion][Epoch 10660] diffusion learning rate: 0.001
2024-11-05 02:48:08,228 - INFO - [diffusion][Epoch 10660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:08,230 - INFO - [diffusion][Epoch 10661] Epoch 10662/12000
2024-11-05 02:48:11,778 - INFO - [diffusion][Epoch 10661] diffusion training Loss: 0.06641706265509129
2024-11-05 02:48:11,780 - INFO - [diffusion][Epoch 10661] diffusion learning rate: 0.001
2024-11-05 02:48:11,783 - INFO - [diffusion][Epoch 10661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:11,785 - INFO - [diffusion][Epoch 10662] Epoch 10663/12000
2024-11-05 02:48:14,674 - INFO - [diffusion][Epoch 10662] diffusion training Loss: 0.06667403131723404
2024-11-05 02:48:14,676 - INFO - [diffusion][Epoch 10662] diffusion learning rate: 0.001
2024-11-05 02:48:14,678 - INFO - [diffusion][Epoch 10662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:14,679 - INFO - [diffusion][Epoch 10663] Epoch 10664/12000
2024-11-05 02:48:17,842 - INFO - [diffusion][Epoch 10663] diffusion training Loss: 0.06464569084346294
2024-11-05 02:48:17,844 - INFO - [diffusion][Epoch 10663] diffusion learning rate: 0.001
2024-11-05 02:48:17,846 - INFO - [diffusion][Epoch 10663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:17,847 - INFO - [diffusion][Epoch 10664] Epoch 10665/12000
2024-11-05 02:48:21,341 - INFO - [diffusion][Epoch 10664] diffusion training Loss: 0.06099092774093151
2024-11-05 02:48:21,343 - INFO - [diffusion][Epoch 10664] diffusion learning rate: 0.001
2024-11-05 02:48:21,345 - INFO - [diffusion][Epoch 10664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:21,346 - INFO - [diffusion][Epoch 10665] Epoch 10666/12000
2024-11-05 02:48:24,902 - INFO - [diffusion][Epoch 10665] diffusion training Loss: 0.06782224774360657
2024-11-05 02:48:24,905 - INFO - [diffusion][Epoch 10665] diffusion learning rate: 0.001
2024-11-05 02:48:24,907 - INFO - [diffusion][Epoch 10665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:24,908 - INFO - [diffusion][Epoch 10666] Epoch 10667/12000
2024-11-05 02:48:27,982 - INFO - [diffusion][Epoch 10666] diffusion training Loss: 0.06652235146611929
2024-11-05 02:48:27,984 - INFO - [diffusion][Epoch 10666] diffusion learning rate: 0.001
2024-11-05 02:48:27,986 - INFO - [diffusion][Epoch 10666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:27,987 - INFO - [diffusion][Epoch 10667] Epoch 10668/12000
2024-11-05 02:48:31,096 - INFO - [diffusion][Epoch 10667] diffusion training Loss: 0.0594625947996974
2024-11-05 02:48:31,098 - INFO - [diffusion][Epoch 10667] diffusion learning rate: 0.001
2024-11-05 02:48:31,099 - INFO - [diffusion][Epoch 10667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:31,101 - INFO - [diffusion][Epoch 10668] Epoch 10669/12000
2024-11-05 02:48:34,247 - INFO - [diffusion][Epoch 10668] diffusion training Loss: 0.06709358841180801
2024-11-05 02:48:34,249 - INFO - [diffusion][Epoch 10668] diffusion learning rate: 0.001
2024-11-05 02:48:34,251 - INFO - [diffusion][Epoch 10668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:34,253 - INFO - [diffusion][Epoch 10669] Epoch 10670/12000
2024-11-05 02:48:37,592 - INFO - [diffusion][Epoch 10669] diffusion training Loss: 0.06328802742063999
2024-11-05 02:48:37,594 - INFO - [diffusion][Epoch 10669] diffusion learning rate: 0.001
2024-11-05 02:48:37,614 - INFO - [diffusion][Epoch 10669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:37,615 - INFO - [diffusion][Epoch 10670] Epoch 10671/12000
2024-11-05 02:48:40,701 - INFO - [diffusion][Epoch 10670] diffusion training Loss: 0.07079691346734762
2024-11-05 02:48:40,703 - INFO - [diffusion][Epoch 10670] diffusion learning rate: 0.001
2024-11-05 02:48:40,705 - INFO - [diffusion][Epoch 10670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:40,707 - INFO - [diffusion][Epoch 10671] Epoch 10672/12000
2024-11-05 02:48:43,798 - INFO - [diffusion][Epoch 10671] diffusion training Loss: 0.06508635357022285
2024-11-05 02:48:43,800 - INFO - [diffusion][Epoch 10671] diffusion learning rate: 0.001
2024-11-05 02:48:43,802 - INFO - [diffusion][Epoch 10671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:43,803 - INFO - [diffusion][Epoch 10672] Epoch 10673/12000
2024-11-05 02:48:47,323 - INFO - [diffusion][Epoch 10672] diffusion training Loss: 0.06480131763964891
2024-11-05 02:48:47,324 - INFO - [diffusion][Epoch 10672] diffusion learning rate: 0.001
2024-11-05 02:48:47,326 - INFO - [diffusion][Epoch 10672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:47,327 - INFO - [diffusion][Epoch 10673] Epoch 10674/12000
2024-11-05 02:48:50,992 - INFO - [diffusion][Epoch 10673] diffusion training Loss: 0.06810390204191208
2024-11-05 02:48:50,993 - INFO - [diffusion][Epoch 10673] diffusion learning rate: 0.001
2024-11-05 02:48:50,995 - INFO - [diffusion][Epoch 10673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:50,996 - INFO - [diffusion][Epoch 10674] Epoch 10675/12000
2024-11-05 02:48:54,208 - INFO - [diffusion][Epoch 10674] diffusion training Loss: 0.06701724790036678
2024-11-05 02:48:54,210 - INFO - [diffusion][Epoch 10674] diffusion learning rate: 0.001
2024-11-05 02:48:54,212 - INFO - [diffusion][Epoch 10674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:54,213 - INFO - [diffusion][Epoch 10675] Epoch 10676/12000
2024-11-05 02:48:57,329 - INFO - [diffusion][Epoch 10675] diffusion training Loss: 0.0652614776045084
2024-11-05 02:48:57,332 - INFO - [diffusion][Epoch 10675] diffusion learning rate: 0.001
2024-11-05 02:48:57,334 - INFO - [diffusion][Epoch 10675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:57,335 - INFO - [diffusion][Epoch 10676] Epoch 10677/12000
2024-11-05 02:49:00,381 - INFO - [diffusion][Epoch 10676] diffusion training Loss: 0.060762036591768265
2024-11-05 02:49:00,383 - INFO - [diffusion][Epoch 10676] diffusion learning rate: 0.001
2024-11-05 02:49:00,385 - INFO - [diffusion][Epoch 10676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:00,386 - INFO - [diffusion][Epoch 10677] Epoch 10678/12000
2024-11-05 02:49:03,920 - INFO - [diffusion][Epoch 10677] diffusion training Loss: 0.06274294201284647
2024-11-05 02:49:03,922 - INFO - [diffusion][Epoch 10677] diffusion learning rate: 0.001
2024-11-05 02:49:03,925 - INFO - [diffusion][Epoch 10677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:03,926 - INFO - [diffusion][Epoch 10678] Epoch 10679/12000
2024-11-05 02:49:07,405 - INFO - [diffusion][Epoch 10678] diffusion training Loss: 0.06294276751577854
2024-11-05 02:49:07,407 - INFO - [diffusion][Epoch 10678] diffusion learning rate: 0.001
2024-11-05 02:49:07,409 - INFO - [diffusion][Epoch 10678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:07,410 - INFO - [diffusion][Epoch 10679] Epoch 10680/12000
2024-11-05 02:49:11,285 - INFO - [diffusion][Epoch 10679] diffusion training Loss: 0.06514652632176876
2024-11-05 02:49:11,287 - INFO - [diffusion][Epoch 10679] diffusion learning rate: 0.001
2024-11-05 02:49:11,288 - INFO - [diffusion][Epoch 10679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:11,290 - INFO - [diffusion][Epoch 10680] Epoch 10681/12000
2024-11-05 02:49:14,461 - INFO - [diffusion][Epoch 10680] diffusion training Loss: 0.0640167873352766
2024-11-05 02:49:14,463 - INFO - [diffusion][Epoch 10680] diffusion learning rate: 0.001
2024-11-05 02:49:14,465 - INFO - [diffusion][Epoch 10680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:14,467 - INFO - [diffusion][Epoch 10681] Epoch 10682/12000
2024-11-05 02:49:17,508 - INFO - [diffusion][Epoch 10681] diffusion training Loss: 0.06472035869956017
2024-11-05 02:49:17,511 - INFO - [diffusion][Epoch 10681] diffusion learning rate: 0.001
2024-11-05 02:49:17,513 - INFO - [diffusion][Epoch 10681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:17,514 - INFO - [diffusion][Epoch 10682] Epoch 10683/12000
2024-11-05 02:49:21,015 - INFO - [diffusion][Epoch 10682] diffusion training Loss: 0.06235422845929861
2024-11-05 02:49:21,017 - INFO - [diffusion][Epoch 10682] diffusion learning rate: 0.001
2024-11-05 02:49:21,019 - INFO - [diffusion][Epoch 10682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:21,021 - INFO - [diffusion][Epoch 10683] Epoch 10684/12000
2024-11-05 02:49:24,570 - INFO - [diffusion][Epoch 10683] diffusion training Loss: 0.06566780246794224
2024-11-05 02:49:24,572 - INFO - [diffusion][Epoch 10683] diffusion learning rate: 0.001
2024-11-05 02:49:24,574 - INFO - [diffusion][Epoch 10683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:24,575 - INFO - [diffusion][Epoch 10684] Epoch 10685/12000
2024-11-05 02:49:27,691 - INFO - [diffusion][Epoch 10684] diffusion training Loss: 0.061716387048363686
2024-11-05 02:49:27,693 - INFO - [diffusion][Epoch 10684] diffusion learning rate: 0.001
2024-11-05 02:49:27,695 - INFO - [diffusion][Epoch 10684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:27,696 - INFO - [diffusion][Epoch 10685] Epoch 10686/12000
2024-11-05 02:49:30,868 - INFO - [diffusion][Epoch 10685] diffusion training Loss: 0.06327346060425043
2024-11-05 02:49:30,870 - INFO - [diffusion][Epoch 10685] diffusion learning rate: 0.001
2024-11-05 02:49:30,872 - INFO - [diffusion][Epoch 10685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:30,874 - INFO - [diffusion][Epoch 10686] Epoch 10687/12000
2024-11-05 02:49:33,953 - INFO - [diffusion][Epoch 10686] diffusion training Loss: 0.06811718456447124
2024-11-05 02:49:33,954 - INFO - [diffusion][Epoch 10686] diffusion learning rate: 0.001
2024-11-05 02:49:33,956 - INFO - [diffusion][Epoch 10686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:33,957 - INFO - [diffusion][Epoch 10687] Epoch 10688/12000
2024-11-05 02:49:37,564 - INFO - [diffusion][Epoch 10687] diffusion training Loss: 0.061275163665413857
2024-11-05 02:49:37,566 - INFO - [diffusion][Epoch 10687] diffusion learning rate: 0.001
2024-11-05 02:49:37,568 - INFO - [diffusion][Epoch 10687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:37,569 - INFO - [diffusion][Epoch 10688] Epoch 10689/12000
2024-11-05 02:49:40,703 - INFO - [diffusion][Epoch 10688] diffusion training Loss: 0.06590487062931061
2024-11-05 02:49:40,705 - INFO - [diffusion][Epoch 10688] diffusion learning rate: 0.001
2024-11-05 02:49:40,707 - INFO - [diffusion][Epoch 10688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:40,708 - INFO - [diffusion][Epoch 10689] Epoch 10690/12000
2024-11-05 02:49:43,829 - INFO - [diffusion][Epoch 10689] diffusion training Loss: 0.06395190581679344
2024-11-05 02:49:43,831 - INFO - [diffusion][Epoch 10689] diffusion learning rate: 0.001
2024-11-05 02:49:43,832 - INFO - [diffusion][Epoch 10689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:43,834 - INFO - [diffusion][Epoch 10690] Epoch 10691/12000
2024-11-05 02:49:46,884 - INFO - [diffusion][Epoch 10690] diffusion training Loss: 0.06291045434772968
2024-11-05 02:49:46,886 - INFO - [diffusion][Epoch 10690] diffusion learning rate: 0.001
2024-11-05 02:49:46,888 - INFO - [diffusion][Epoch 10690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:46,889 - INFO - [diffusion][Epoch 10691] Epoch 10692/12000
2024-11-05 02:49:50,514 - INFO - [diffusion][Epoch 10691] diffusion training Loss: 0.06407634727656841
2024-11-05 02:49:50,516 - INFO - [diffusion][Epoch 10691] diffusion learning rate: 0.001
2024-11-05 02:49:50,517 - INFO - [diffusion][Epoch 10691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:50,519 - INFO - [diffusion][Epoch 10692] Epoch 10693/12000
2024-11-05 02:49:53,868 - INFO - [diffusion][Epoch 10692] diffusion training Loss: 0.06498976051807404
2024-11-05 02:49:53,870 - INFO - [diffusion][Epoch 10692] diffusion learning rate: 0.001
2024-11-05 02:49:53,872 - INFO - [diffusion][Epoch 10692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:53,874 - INFO - [diffusion][Epoch 10693] Epoch 10694/12000
2024-11-05 02:49:56,901 - INFO - [diffusion][Epoch 10693] diffusion training Loss: 0.06618187949061394
2024-11-05 02:49:56,903 - INFO - [diffusion][Epoch 10693] diffusion learning rate: 0.001
2024-11-05 02:49:56,905 - INFO - [diffusion][Epoch 10693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:56,906 - INFO - [diffusion][Epoch 10694] Epoch 10695/12000
2024-11-05 02:49:59,903 - INFO - [diffusion][Epoch 10694] diffusion training Loss: 0.06394409015774727
2024-11-05 02:49:59,906 - INFO - [diffusion][Epoch 10694] diffusion learning rate: 0.001
2024-11-05 02:49:59,908 - INFO - [diffusion][Epoch 10694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:59,909 - INFO - [diffusion][Epoch 10695] Epoch 10696/12000
2024-11-05 02:50:03,143 - INFO - [diffusion][Epoch 10695] diffusion training Loss: 0.05921622645109892
2024-11-05 02:50:03,145 - INFO - [diffusion][Epoch 10695] diffusion learning rate: 0.001
2024-11-05 02:50:03,147 - INFO - [diffusion][Epoch 10695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:03,148 - INFO - [diffusion][Epoch 10696] Epoch 10697/12000
2024-11-05 02:50:06,386 - INFO - [diffusion][Epoch 10696] diffusion training Loss: 0.06407638546079397
2024-11-05 02:50:06,388 - INFO - [diffusion][Epoch 10696] diffusion learning rate: 0.001
2024-11-05 02:50:06,390 - INFO - [diffusion][Epoch 10696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:06,391 - INFO - [diffusion][Epoch 10697] Epoch 10698/12000
2024-11-05 02:50:09,503 - INFO - [diffusion][Epoch 10697] diffusion training Loss: 0.06191903539001942
2024-11-05 02:50:09,505 - INFO - [diffusion][Epoch 10697] diffusion learning rate: 0.001
2024-11-05 02:50:09,507 - INFO - [diffusion][Epoch 10697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:09,508 - INFO - [diffusion][Epoch 10698] Epoch 10699/12000
2024-11-05 02:50:13,024 - INFO - [diffusion][Epoch 10698] diffusion training Loss: 0.06049564108252525
2024-11-05 02:50:13,026 - INFO - [diffusion][Epoch 10698] diffusion learning rate: 0.001
2024-11-05 02:50:13,028 - INFO - [diffusion][Epoch 10698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:13,029 - INFO - [diffusion][Epoch 10699] Epoch 10700/12000
2024-11-05 02:50:16,370 - INFO - [diffusion][Epoch 10699] diffusion training Loss: 0.0632496029138565
2024-11-05 02:50:16,373 - INFO - [diffusion][Epoch 10699] diffusion learning rate: 0.001
2024-11-05 02:50:16,375 - INFO - [diffusion][Epoch 10699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:16,377 - INFO - [diffusion][Epoch 10700] Epoch 10701/12000
2024-11-05 02:50:19,845 - INFO - [diffusion][Epoch 10700] diffusion training Loss: 0.06496226042509079
2024-11-05 02:50:19,847 - INFO - [diffusion][Epoch 10700] diffusion learning rate: 0.001
2024-11-05 02:50:19,849 - INFO - [diffusion][Epoch 10700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:19,850 - INFO - [diffusion][Epoch 10701] Epoch 10702/12000
2024-11-05 02:50:22,935 - INFO - [diffusion][Epoch 10701] diffusion training Loss: 0.06142211612313986
2024-11-05 02:50:22,936 - INFO - [diffusion][Epoch 10701] diffusion learning rate: 0.001
2024-11-05 02:50:22,938 - INFO - [diffusion][Epoch 10701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:22,939 - INFO - [diffusion][Epoch 10702] Epoch 10703/12000
2024-11-05 02:50:26,028 - INFO - [diffusion][Epoch 10702] diffusion training Loss: 0.0661616800352931
2024-11-05 02:50:26,030 - INFO - [diffusion][Epoch 10702] diffusion learning rate: 0.001
2024-11-05 02:50:26,032 - INFO - [diffusion][Epoch 10702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:26,033 - INFO - [diffusion][Epoch 10703] Epoch 10704/12000
2024-11-05 02:50:29,229 - INFO - [diffusion][Epoch 10703] diffusion training Loss: 0.06971432268619537
2024-11-05 02:50:29,231 - INFO - [diffusion][Epoch 10703] diffusion learning rate: 0.001
2024-11-05 02:50:29,233 - INFO - [diffusion][Epoch 10703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:29,234 - INFO - [diffusion][Epoch 10704] Epoch 10705/12000
2024-11-05 02:50:32,854 - INFO - [diffusion][Epoch 10704] diffusion training Loss: 0.06058982852846384
2024-11-05 02:50:32,856 - INFO - [diffusion][Epoch 10704] diffusion learning rate: 0.001
2024-11-05 02:50:32,857 - INFO - [diffusion][Epoch 10704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:32,859 - INFO - [diffusion][Epoch 10705] Epoch 10706/12000
2024-11-05 02:50:36,052 - INFO - [diffusion][Epoch 10705] diffusion training Loss: 0.06381765007972717
2024-11-05 02:50:36,054 - INFO - [diffusion][Epoch 10705] diffusion learning rate: 0.001
2024-11-05 02:50:36,056 - INFO - [diffusion][Epoch 10705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:36,057 - INFO - [diffusion][Epoch 10706] Epoch 10707/12000
2024-11-05 02:50:38,933 - INFO - [diffusion][Epoch 10706] diffusion training Loss: 0.0643400214612484
2024-11-05 02:50:38,935 - INFO - [diffusion][Epoch 10706] diffusion learning rate: 0.001
2024-11-05 02:50:38,937 - INFO - [diffusion][Epoch 10706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:38,938 - INFO - [diffusion][Epoch 10707] Epoch 10708/12000
2024-11-05 02:50:42,156 - INFO - [diffusion][Epoch 10707] diffusion training Loss: 0.06914300844073296
2024-11-05 02:50:42,158 - INFO - [diffusion][Epoch 10707] diffusion learning rate: 0.001
2024-11-05 02:50:42,160 - INFO - [diffusion][Epoch 10707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:42,161 - INFO - [diffusion][Epoch 10708] Epoch 10709/12000
2024-11-05 02:50:45,695 - INFO - [diffusion][Epoch 10708] diffusion training Loss: 0.07282822206616402
2024-11-05 02:50:45,697 - INFO - [diffusion][Epoch 10708] diffusion learning rate: 0.001
2024-11-05 02:50:45,699 - INFO - [diffusion][Epoch 10708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:45,700 - INFO - [diffusion][Epoch 10709] Epoch 10710/12000
2024-11-05 02:50:49,141 - INFO - [diffusion][Epoch 10709] diffusion training Loss: 0.06320212874561548
2024-11-05 02:50:49,142 - INFO - [diffusion][Epoch 10709] diffusion learning rate: 0.001
2024-11-05 02:50:49,144 - INFO - [diffusion][Epoch 10709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:49,146 - INFO - [diffusion][Epoch 10710] Epoch 10711/12000
2024-11-05 02:50:52,270 - INFO - [diffusion][Epoch 10710] diffusion training Loss: 0.07179334759712219
2024-11-05 02:50:52,272 - INFO - [diffusion][Epoch 10710] diffusion learning rate: 0.001
2024-11-05 02:50:52,273 - INFO - [diffusion][Epoch 10710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:52,275 - INFO - [diffusion][Epoch 10711] Epoch 10712/12000
2024-11-05 02:50:55,376 - INFO - [diffusion][Epoch 10711] diffusion training Loss: 0.06597902812063694
2024-11-05 02:50:55,379 - INFO - [diffusion][Epoch 10711] diffusion learning rate: 0.001
2024-11-05 02:50:55,381 - INFO - [diffusion][Epoch 10711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:55,383 - INFO - [diffusion][Epoch 10712] Epoch 10713/12000
2024-11-05 02:50:58,891 - INFO - [diffusion][Epoch 10712] diffusion training Loss: 0.06646205484867096
2024-11-05 02:50:58,893 - INFO - [diffusion][Epoch 10712] diffusion learning rate: 0.001
2024-11-05 02:50:58,894 - INFO - [diffusion][Epoch 10712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:58,896 - INFO - [diffusion][Epoch 10713] Epoch 10714/12000
2024-11-05 02:51:02,451 - INFO - [diffusion][Epoch 10713] diffusion training Loss: 0.0648260023444891
2024-11-05 02:51:02,454 - INFO - [diffusion][Epoch 10713] diffusion learning rate: 0.001
2024-11-05 02:51:02,456 - INFO - [diffusion][Epoch 10713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:02,457 - INFO - [diffusion][Epoch 10714] Epoch 10715/12000
2024-11-05 02:51:05,560 - INFO - [diffusion][Epoch 10714] diffusion training Loss: 0.06567665189504623
2024-11-05 02:51:05,562 - INFO - [diffusion][Epoch 10714] diffusion learning rate: 0.001
2024-11-05 02:51:05,590 - INFO - [diffusion][Epoch 10714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:05,591 - INFO - [diffusion][Epoch 10715] Epoch 10716/12000
2024-11-05 02:51:08,680 - INFO - [diffusion][Epoch 10715] diffusion training Loss: 0.057898049242794514
2024-11-05 02:51:08,682 - INFO - [diffusion][Epoch 10715] diffusion learning rate: 0.001
2024-11-05 02:51:08,684 - INFO - [diffusion][Epoch 10715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:08,685 - INFO - [diffusion][Epoch 10716] Epoch 10717/12000
2024-11-05 02:51:11,822 - INFO - [diffusion][Epoch 10716] diffusion training Loss: 0.05864537786692381
2024-11-05 02:51:11,824 - INFO - [diffusion][Epoch 10716] diffusion learning rate: 0.001
2024-11-05 02:51:11,826 - INFO - [diffusion][Epoch 10716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:11,827 - INFO - [diffusion][Epoch 10717] Epoch 10718/12000
2024-11-05 02:51:15,419 - INFO - [diffusion][Epoch 10717] diffusion training Loss: 0.06352958735078573
2024-11-05 02:51:15,421 - INFO - [diffusion][Epoch 10717] diffusion learning rate: 0.001
2024-11-05 02:51:15,423 - INFO - [diffusion][Epoch 10717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:15,424 - INFO - [diffusion][Epoch 10718] Epoch 10719/12000
2024-11-05 02:51:18,951 - INFO - [diffusion][Epoch 10718] diffusion training Loss: 0.06438799761235714
2024-11-05 02:51:18,953 - INFO - [diffusion][Epoch 10718] diffusion learning rate: 0.001
2024-11-05 02:51:18,955 - INFO - [diffusion][Epoch 10718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:18,956 - INFO - [diffusion][Epoch 10719] Epoch 10720/12000
2024-11-05 02:51:22,611 - INFO - [diffusion][Epoch 10719] diffusion training Loss: 0.06403181049972773
2024-11-05 02:51:22,613 - INFO - [diffusion][Epoch 10719] diffusion learning rate: 0.001
2024-11-05 02:51:22,614 - INFO - [diffusion][Epoch 10719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:22,616 - INFO - [diffusion][Epoch 10720] Epoch 10721/12000
2024-11-05 02:51:25,649 - INFO - [diffusion][Epoch 10720] diffusion training Loss: 0.07107718102633953
2024-11-05 02:51:25,651 - INFO - [diffusion][Epoch 10720] diffusion learning rate: 0.001
2024-11-05 02:51:25,652 - INFO - [diffusion][Epoch 10720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:25,654 - INFO - [diffusion][Epoch 10721] Epoch 10722/12000
2024-11-05 02:51:28,680 - INFO - [diffusion][Epoch 10721] diffusion training Loss: 0.06758426688611507
2024-11-05 02:51:28,682 - INFO - [diffusion][Epoch 10721] diffusion learning rate: 0.001
2024-11-05 02:51:28,684 - INFO - [diffusion][Epoch 10721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:28,685 - INFO - [diffusion][Epoch 10722] Epoch 10723/12000
2024-11-05 02:51:32,213 - INFO - [diffusion][Epoch 10722] diffusion training Loss: 0.056297590024769306
2024-11-05 02:51:32,216 - INFO - [diffusion][Epoch 10722] diffusion learning rate: 0.001
2024-11-05 02:51:32,217 - INFO - [diffusion][Epoch 10722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:32,219 - INFO - [diffusion][Epoch 10723] Epoch 10724/12000
2024-11-05 02:51:35,576 - INFO - [diffusion][Epoch 10723] diffusion training Loss: 0.06536714360117912
2024-11-05 02:51:35,579 - INFO - [diffusion][Epoch 10723] diffusion learning rate: 0.001
2024-11-05 02:51:35,580 - INFO - [diffusion][Epoch 10723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:35,582 - INFO - [diffusion][Epoch 10724] Epoch 10725/12000
2024-11-05 02:51:38,525 - INFO - [diffusion][Epoch 10724] diffusion training Loss: 0.0658619748428464
2024-11-05 02:51:38,527 - INFO - [diffusion][Epoch 10724] diffusion learning rate: 0.001
2024-11-05 02:51:38,529 - INFO - [diffusion][Epoch 10724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:38,530 - INFO - [diffusion][Epoch 10725] Epoch 10726/12000
2024-11-05 02:51:41,586 - INFO - [diffusion][Epoch 10725] diffusion training Loss: 0.06368689704686403
2024-11-05 02:51:41,588 - INFO - [diffusion][Epoch 10725] diffusion learning rate: 0.001
2024-11-05 02:51:41,589 - INFO - [diffusion][Epoch 10725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:41,591 - INFO - [diffusion][Epoch 10726] Epoch 10727/12000
2024-11-05 02:51:45,052 - INFO - [diffusion][Epoch 10726] diffusion training Loss: 0.07038131915032864
2024-11-05 02:51:45,054 - INFO - [diffusion][Epoch 10726] diffusion learning rate: 0.001
2024-11-05 02:51:45,056 - INFO - [diffusion][Epoch 10726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:45,057 - INFO - [diffusion][Epoch 10727] Epoch 10728/12000
2024-11-05 02:51:48,577 - INFO - [diffusion][Epoch 10727] diffusion training Loss: 0.0690594706684351
2024-11-05 02:51:48,580 - INFO - [diffusion][Epoch 10727] diffusion learning rate: 0.001
2024-11-05 02:51:48,582 - INFO - [diffusion][Epoch 10727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:48,583 - INFO - [diffusion][Epoch 10728] Epoch 10729/12000
2024-11-05 02:51:51,727 - INFO - [diffusion][Epoch 10728] diffusion training Loss: 0.063023142516613
2024-11-05 02:51:51,729 - INFO - [diffusion][Epoch 10728] diffusion learning rate: 0.001
2024-11-05 02:51:51,731 - INFO - [diffusion][Epoch 10728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:51,732 - INFO - [diffusion][Epoch 10729] Epoch 10730/12000
2024-11-05 02:51:54,907 - INFO - [diffusion][Epoch 10729] diffusion training Loss: 0.0672327820211649
2024-11-05 02:51:54,909 - INFO - [diffusion][Epoch 10729] diffusion learning rate: 0.001
2024-11-05 02:51:54,911 - INFO - [diffusion][Epoch 10729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:54,912 - INFO - [diffusion][Epoch 10730] Epoch 10731/12000
2024-11-05 02:51:58,048 - INFO - [diffusion][Epoch 10730] diffusion training Loss: 0.06645877566188574
2024-11-05 02:51:58,050 - INFO - [diffusion][Epoch 10730] diffusion learning rate: 0.001
2024-11-05 02:51:58,052 - INFO - [diffusion][Epoch 10730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:58,053 - INFO - [diffusion][Epoch 10731] Epoch 10732/12000
2024-11-05 02:52:01,647 - INFO - [diffusion][Epoch 10731] diffusion training Loss: 0.06908148899674416
2024-11-05 02:52:01,649 - INFO - [diffusion][Epoch 10731] diffusion learning rate: 0.001
2024-11-05 02:52:01,650 - INFO - [diffusion][Epoch 10731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:01,652 - INFO - [diffusion][Epoch 10732] Epoch 10733/12000
2024-11-05 02:52:05,118 - INFO - [diffusion][Epoch 10732] diffusion training Loss: 0.06428718566894531
2024-11-05 02:52:05,121 - INFO - [diffusion][Epoch 10732] diffusion learning rate: 0.001
2024-11-05 02:52:05,122 - INFO - [diffusion][Epoch 10732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:05,124 - INFO - [diffusion][Epoch 10733] Epoch 10734/12000
2024-11-05 02:52:08,109 - INFO - [diffusion][Epoch 10733] diffusion training Loss: 0.06287902873009443
2024-11-05 02:52:08,111 - INFO - [diffusion][Epoch 10733] diffusion learning rate: 0.001
2024-11-05 02:52:08,113 - INFO - [diffusion][Epoch 10733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:08,114 - INFO - [diffusion][Epoch 10734] Epoch 10735/12000
2024-11-05 02:52:11,381 - INFO - [diffusion][Epoch 10734] diffusion training Loss: 0.06841624155640602
2024-11-05 02:52:11,383 - INFO - [diffusion][Epoch 10734] diffusion learning rate: 0.001
2024-11-05 02:52:11,385 - INFO - [diffusion][Epoch 10734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:11,386 - INFO - [diffusion][Epoch 10735] Epoch 10736/12000
2024-11-05 02:52:14,617 - INFO - [diffusion][Epoch 10735] diffusion training Loss: 0.05887738615274429
2024-11-05 02:52:14,620 - INFO - [diffusion][Epoch 10735] diffusion learning rate: 0.001
2024-11-05 02:52:14,622 - INFO - [diffusion][Epoch 10735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:14,623 - INFO - [diffusion][Epoch 10736] Epoch 10737/12000
2024-11-05 02:52:18,365 - INFO - [diffusion][Epoch 10736] diffusion training Loss: 0.062058279290795326
2024-11-05 02:52:18,367 - INFO - [diffusion][Epoch 10736] diffusion learning rate: 0.001
2024-11-05 02:52:18,369 - INFO - [diffusion][Epoch 10736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:18,370 - INFO - [diffusion][Epoch 10737] Epoch 10738/12000
2024-11-05 02:52:21,870 - INFO - [diffusion][Epoch 10737] diffusion training Loss: 0.062114790081977844
2024-11-05 02:52:21,872 - INFO - [diffusion][Epoch 10737] diffusion learning rate: 0.001
2024-11-05 02:52:21,873 - INFO - [diffusion][Epoch 10737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:21,874 - INFO - [diffusion][Epoch 10738] Epoch 10739/12000
2024-11-05 02:52:25,005 - INFO - [diffusion][Epoch 10738] diffusion training Loss: 0.07066348567605019
2024-11-05 02:52:25,007 - INFO - [diffusion][Epoch 10738] diffusion learning rate: 0.001
2024-11-05 02:52:25,008 - INFO - [diffusion][Epoch 10738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:25,010 - INFO - [diffusion][Epoch 10739] Epoch 10740/12000
2024-11-05 02:52:28,132 - INFO - [diffusion][Epoch 10739] diffusion training Loss: 0.06851236708462238
2024-11-05 02:52:28,134 - INFO - [diffusion][Epoch 10739] diffusion learning rate: 0.001
2024-11-05 02:52:28,136 - INFO - [diffusion][Epoch 10739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:28,137 - INFO - [diffusion][Epoch 10740] Epoch 10741/12000
2024-11-05 02:52:31,495 - INFO - [diffusion][Epoch 10740] diffusion training Loss: 0.06539986748248339
2024-11-05 02:52:31,497 - INFO - [diffusion][Epoch 10740] diffusion learning rate: 0.001
2024-11-05 02:52:31,499 - INFO - [diffusion][Epoch 10740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:31,500 - INFO - [diffusion][Epoch 10741] Epoch 10742/12000
2024-11-05 02:52:35,117 - INFO - [diffusion][Epoch 10741] diffusion training Loss: 0.06287098117172718
2024-11-05 02:52:35,119 - INFO - [diffusion][Epoch 10741] diffusion learning rate: 0.001
2024-11-05 02:52:35,120 - INFO - [diffusion][Epoch 10741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:35,122 - INFO - [diffusion][Epoch 10742] Epoch 10743/12000
2024-11-05 02:52:38,572 - INFO - [diffusion][Epoch 10742] diffusion training Loss: 0.06474035978317261
2024-11-05 02:52:38,574 - INFO - [diffusion][Epoch 10742] diffusion learning rate: 0.001
2024-11-05 02:52:38,576 - INFO - [diffusion][Epoch 10742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:38,577 - INFO - [diffusion][Epoch 10743] Epoch 10744/12000
2024-11-05 02:52:41,697 - INFO - [diffusion][Epoch 10743] diffusion training Loss: 0.06753736548125744
2024-11-05 02:52:41,699 - INFO - [diffusion][Epoch 10743] diffusion learning rate: 0.001
2024-11-05 02:52:41,700 - INFO - [diffusion][Epoch 10743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:41,702 - INFO - [diffusion][Epoch 10744] Epoch 10745/12000
2024-11-05 02:52:44,666 - INFO - [diffusion][Epoch 10744] diffusion training Loss: 0.06557982973754406
2024-11-05 02:52:44,668 - INFO - [diffusion][Epoch 10744] diffusion learning rate: 0.001
2024-11-05 02:52:44,670 - INFO - [diffusion][Epoch 10744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:44,671 - INFO - [diffusion][Epoch 10745] Epoch 10746/12000
2024-11-05 02:52:48,135 - INFO - [diffusion][Epoch 10745] diffusion training Loss: 0.06403231248259544
2024-11-05 02:52:48,138 - INFO - [diffusion][Epoch 10745] diffusion learning rate: 0.001
2024-11-05 02:52:48,140 - INFO - [diffusion][Epoch 10745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:48,141 - INFO - [diffusion][Epoch 10746] Epoch 10747/12000
2024-11-05 02:52:51,701 - INFO - [diffusion][Epoch 10746] diffusion training Loss: 0.0627751462161541
2024-11-05 02:52:51,703 - INFO - [diffusion][Epoch 10746] diffusion learning rate: 0.001
2024-11-05 02:52:51,705 - INFO - [diffusion][Epoch 10746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:51,706 - INFO - [diffusion][Epoch 10747] Epoch 10748/12000
2024-11-05 02:52:54,814 - INFO - [diffusion][Epoch 10747] diffusion training Loss: 0.06546328961849213
2024-11-05 02:52:54,816 - INFO - [diffusion][Epoch 10747] diffusion learning rate: 0.001
2024-11-05 02:52:54,818 - INFO - [diffusion][Epoch 10747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:54,819 - INFO - [diffusion][Epoch 10748] Epoch 10749/12000
2024-11-05 02:52:57,630 - INFO - [diffusion][Epoch 10748] diffusion training Loss: 0.06892807222902775
2024-11-05 02:52:57,632 - INFO - [diffusion][Epoch 10748] diffusion learning rate: 0.001
2024-11-05 02:52:57,633 - INFO - [diffusion][Epoch 10748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:57,635 - INFO - [diffusion][Epoch 10749] Epoch 10750/12000
2024-11-05 02:53:01,057 - INFO - [diffusion][Epoch 10749] diffusion training Loss: 0.06238054018467665
2024-11-05 02:53:01,059 - INFO - [diffusion][Epoch 10749] diffusion learning rate: 0.001
2024-11-05 02:53:01,061 - INFO - [diffusion][Epoch 10749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:01,062 - INFO - [diffusion][Epoch 10750] Epoch 10751/12000
2024-11-05 02:53:04,596 - INFO - [diffusion][Epoch 10750] diffusion training Loss: 0.06425361149013042
2024-11-05 02:53:04,599 - INFO - [diffusion][Epoch 10750] diffusion learning rate: 0.001
2024-11-05 02:53:04,602 - INFO - [diffusion][Epoch 10750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:04,604 - INFO - [diffusion][Epoch 10751] Epoch 10752/12000
2024-11-05 02:53:07,686 - INFO - [diffusion][Epoch 10751] diffusion training Loss: 0.06314453855156898
2024-11-05 02:53:07,688 - INFO - [diffusion][Epoch 10751] diffusion learning rate: 0.001
2024-11-05 02:53:07,690 - INFO - [diffusion][Epoch 10751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:07,691 - INFO - [diffusion][Epoch 10752] Epoch 10753/12000
2024-11-05 02:53:10,789 - INFO - [diffusion][Epoch 10752] diffusion training Loss: 0.06374193169176579
2024-11-05 02:53:10,792 - INFO - [diffusion][Epoch 10752] diffusion learning rate: 0.001
2024-11-05 02:53:10,837 - INFO - [diffusion][Epoch 10752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:10,839 - INFO - [diffusion][Epoch 10753] Epoch 10754/12000
2024-11-05 02:53:13,970 - INFO - [diffusion][Epoch 10753] diffusion training Loss: 0.06285408232361078
2024-11-05 02:53:13,972 - INFO - [diffusion][Epoch 10753] diffusion learning rate: 0.001
2024-11-05 02:53:13,974 - INFO - [diffusion][Epoch 10753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:13,975 - INFO - [diffusion][Epoch 10754] Epoch 10755/12000
2024-11-05 02:53:17,573 - INFO - [diffusion][Epoch 10754] diffusion training Loss: 0.06358321197330952
2024-11-05 02:53:17,575 - INFO - [diffusion][Epoch 10754] diffusion learning rate: 0.001
2024-11-05 02:53:17,577 - INFO - [diffusion][Epoch 10754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:17,579 - INFO - [diffusion][Epoch 10755] Epoch 10756/12000
2024-11-05 02:53:21,106 - INFO - [diffusion][Epoch 10755] diffusion training Loss: 0.0664813369512558
2024-11-05 02:53:21,107 - INFO - [diffusion][Epoch 10755] diffusion learning rate: 0.001
2024-11-05 02:53:21,109 - INFO - [diffusion][Epoch 10755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:21,111 - INFO - [diffusion][Epoch 10756] Epoch 10757/12000
2024-11-05 02:53:24,202 - INFO - [diffusion][Epoch 10756] diffusion training Loss: 0.06282770819962025
2024-11-05 02:53:24,203 - INFO - [diffusion][Epoch 10756] diffusion learning rate: 0.001
2024-11-05 02:53:24,205 - INFO - [diffusion][Epoch 10756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:24,206 - INFO - [diffusion][Epoch 10757] Epoch 10758/12000
2024-11-05 02:53:27,288 - INFO - [diffusion][Epoch 10757] diffusion training Loss: 0.05529479868710041
2024-11-05 02:53:27,290 - INFO - [diffusion][Epoch 10757] diffusion learning rate: 0.001
2024-11-05 02:53:27,292 - INFO - [diffusion][Epoch 10757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:27,294 - INFO - [diffusion][Epoch 10758] Epoch 10759/12000
2024-11-05 02:53:30,618 - INFO - [diffusion][Epoch 10758] diffusion training Loss: 0.06412736233323812
2024-11-05 02:53:30,620 - INFO - [diffusion][Epoch 10758] diffusion learning rate: 0.001
2024-11-05 02:53:30,622 - INFO - [diffusion][Epoch 10758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:30,623 - INFO - [diffusion][Epoch 10759] Epoch 10760/12000
2024-11-05 02:53:34,264 - INFO - [diffusion][Epoch 10759] diffusion training Loss: 0.06540760584175587
2024-11-05 02:53:34,266 - INFO - [diffusion][Epoch 10759] diffusion learning rate: 0.001
2024-11-05 02:53:34,268 - INFO - [diffusion][Epoch 10759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:34,269 - INFO - [diffusion][Epoch 10760] Epoch 10761/12000
2024-11-05 02:53:37,605 - INFO - [diffusion][Epoch 10760] diffusion training Loss: 0.06134816259145737
2024-11-05 02:53:37,607 - INFO - [diffusion][Epoch 10760] diffusion learning rate: 0.001
2024-11-05 02:53:37,608 - INFO - [diffusion][Epoch 10760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:37,610 - INFO - [diffusion][Epoch 10761] Epoch 10762/12000
2024-11-05 02:53:40,676 - INFO - [diffusion][Epoch 10761] diffusion training Loss: 0.0635436475276947
2024-11-05 02:53:40,678 - INFO - [diffusion][Epoch 10761] diffusion learning rate: 0.001
2024-11-05 02:53:40,680 - INFO - [diffusion][Epoch 10761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:40,681 - INFO - [diffusion][Epoch 10762] Epoch 10763/12000
2024-11-05 02:53:43,790 - INFO - [diffusion][Epoch 10762] diffusion training Loss: 0.06046978197991848
2024-11-05 02:53:43,792 - INFO - [diffusion][Epoch 10762] diffusion learning rate: 0.001
2024-11-05 02:53:43,793 - INFO - [diffusion][Epoch 10762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:43,795 - INFO - [diffusion][Epoch 10763] Epoch 10764/12000
2024-11-05 02:53:47,411 - INFO - [diffusion][Epoch 10763] diffusion training Loss: 0.06337897665798664
2024-11-05 02:53:47,413 - INFO - [diffusion][Epoch 10763] diffusion learning rate: 0.001
2024-11-05 02:53:47,415 - INFO - [diffusion][Epoch 10763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:47,416 - INFO - [diffusion][Epoch 10764] Epoch 10765/12000
2024-11-05 02:53:50,969 - INFO - [diffusion][Epoch 10764] diffusion training Loss: 0.06515896692872047
2024-11-05 02:53:50,971 - INFO - [diffusion][Epoch 10764] diffusion learning rate: 0.001
2024-11-05 02:53:50,973 - INFO - [diffusion][Epoch 10764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:50,974 - INFO - [diffusion][Epoch 10765] Epoch 10766/12000
2024-11-05 02:53:54,437 - INFO - [diffusion][Epoch 10765] diffusion training Loss: 0.0637868819758296
2024-11-05 02:53:54,439 - INFO - [diffusion][Epoch 10765] diffusion learning rate: 0.001
2024-11-05 02:53:54,440 - INFO - [diffusion][Epoch 10765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:54,442 - INFO - [diffusion][Epoch 10766] Epoch 10767/12000
2024-11-05 02:53:57,594 - INFO - [diffusion][Epoch 10766] diffusion training Loss: 0.062632592394948
2024-11-05 02:53:57,596 - INFO - [diffusion][Epoch 10766] diffusion learning rate: 0.001
2024-11-05 02:53:57,597 - INFO - [diffusion][Epoch 10766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:57,599 - INFO - [diffusion][Epoch 10767] Epoch 10768/12000
2024-11-05 02:54:00,862 - INFO - [diffusion][Epoch 10767] diffusion training Loss: 0.06804046966135502
2024-11-05 02:54:00,864 - INFO - [diffusion][Epoch 10767] diffusion learning rate: 0.001
2024-11-05 02:54:00,891 - INFO - [diffusion][Epoch 10767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:00,893 - INFO - [diffusion][Epoch 10768] Epoch 10769/12000
2024-11-05 02:54:04,080 - INFO - [diffusion][Epoch 10768] diffusion training Loss: 0.06723806075751781
2024-11-05 02:54:04,082 - INFO - [diffusion][Epoch 10768] diffusion learning rate: 0.001
2024-11-05 02:54:04,084 - INFO - [diffusion][Epoch 10768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:04,085 - INFO - [diffusion][Epoch 10769] Epoch 10770/12000
2024-11-05 02:54:07,477 - INFO - [diffusion][Epoch 10769] diffusion training Loss: 0.06305976118892431
2024-11-05 02:54:07,479 - INFO - [diffusion][Epoch 10769] diffusion learning rate: 0.001
2024-11-05 02:54:07,481 - INFO - [diffusion][Epoch 10769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:07,482 - INFO - [diffusion][Epoch 10770] Epoch 10771/12000
2024-11-05 02:54:10,746 - INFO - [diffusion][Epoch 10770] diffusion training Loss: 0.06933489628136158
2024-11-05 02:54:10,748 - INFO - [diffusion][Epoch 10770] diffusion learning rate: 0.001
2024-11-05 02:54:10,750 - INFO - [diffusion][Epoch 10770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:10,752 - INFO - [diffusion][Epoch 10771] Epoch 10772/12000
2024-11-05 02:54:13,827 - INFO - [diffusion][Epoch 10771] diffusion training Loss: 0.06648282147943974
2024-11-05 02:54:13,830 - INFO - [diffusion][Epoch 10771] diffusion learning rate: 0.001
2024-11-05 02:54:13,832 - INFO - [diffusion][Epoch 10771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:13,833 - INFO - [diffusion][Epoch 10772] Epoch 10773/12000
2024-11-05 02:54:16,959 - INFO - [diffusion][Epoch 10772] diffusion training Loss: 0.06552030984312296
2024-11-05 02:54:16,961 - INFO - [diffusion][Epoch 10772] diffusion learning rate: 0.001
2024-11-05 02:54:16,963 - INFO - [diffusion][Epoch 10772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:16,964 - INFO - [diffusion][Epoch 10773] Epoch 10774/12000
2024-11-05 02:54:20,481 - INFO - [diffusion][Epoch 10773] diffusion training Loss: 0.07351373508572578
2024-11-05 02:54:20,483 - INFO - [diffusion][Epoch 10773] diffusion learning rate: 0.001
2024-11-05 02:54:20,485 - INFO - [diffusion][Epoch 10773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:20,486 - INFO - [diffusion][Epoch 10774] Epoch 10775/12000
2024-11-05 02:54:23,985 - INFO - [diffusion][Epoch 10774] diffusion training Loss: 0.06738221645355225
2024-11-05 02:54:23,987 - INFO - [diffusion][Epoch 10774] diffusion learning rate: 0.001
2024-11-05 02:54:23,989 - INFO - [diffusion][Epoch 10774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:23,990 - INFO - [diffusion][Epoch 10775] Epoch 10776/12000
2024-11-05 02:54:26,695 - INFO - [diffusion][Epoch 10775] diffusion training Loss: 0.07228800095617771
2024-11-05 02:54:26,697 - INFO - [diffusion][Epoch 10775] diffusion learning rate: 0.001
2024-11-05 02:54:26,699 - INFO - [diffusion][Epoch 10775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:26,700 - INFO - [diffusion][Epoch 10776] Epoch 10777/12000
2024-11-05 02:54:29,791 - INFO - [diffusion][Epoch 10776] diffusion training Loss: 0.06283938139677048
2024-11-05 02:54:29,793 - INFO - [diffusion][Epoch 10776] diffusion learning rate: 0.001
2024-11-05 02:54:29,795 - INFO - [diffusion][Epoch 10776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:29,796 - INFO - [diffusion][Epoch 10777] Epoch 10778/12000
2024-11-05 02:54:33,314 - INFO - [diffusion][Epoch 10777] diffusion training Loss: 0.06386137567460537
2024-11-05 02:54:33,316 - INFO - [diffusion][Epoch 10777] diffusion learning rate: 0.001
2024-11-05 02:54:33,317 - INFO - [diffusion][Epoch 10777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:33,319 - INFO - [diffusion][Epoch 10778] Epoch 10779/12000
2024-11-05 02:54:36,834 - INFO - [diffusion][Epoch 10778] diffusion training Loss: 0.06461124867200851
2024-11-05 02:54:36,836 - INFO - [diffusion][Epoch 10778] diffusion learning rate: 0.001
2024-11-05 02:54:36,838 - INFO - [diffusion][Epoch 10778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:36,839 - INFO - [diffusion][Epoch 10779] Epoch 10780/12000
2024-11-05 02:54:39,935 - INFO - [diffusion][Epoch 10779] diffusion training Loss: 0.06348551623523235
2024-11-05 02:54:39,937 - INFO - [diffusion][Epoch 10779] diffusion learning rate: 0.001
2024-11-05 02:54:39,939 - INFO - [diffusion][Epoch 10779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:39,940 - INFO - [diffusion][Epoch 10780] Epoch 10781/12000
2024-11-05 02:54:43,084 - INFO - [diffusion][Epoch 10780] diffusion training Loss: 0.06642105896025896
2024-11-05 02:54:43,086 - INFO - [diffusion][Epoch 10780] diffusion learning rate: 0.001
2024-11-05 02:54:43,088 - INFO - [diffusion][Epoch 10780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:43,089 - INFO - [diffusion][Epoch 10781] Epoch 10782/12000
2024-11-05 02:54:46,378 - INFO - [diffusion][Epoch 10781] diffusion training Loss: 0.06669767294079065
2024-11-05 02:54:46,380 - INFO - [diffusion][Epoch 10781] diffusion learning rate: 0.001
2024-11-05 02:54:46,382 - INFO - [diffusion][Epoch 10781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:46,383 - INFO - [diffusion][Epoch 10782] Epoch 10783/12000
2024-11-05 02:54:50,055 - INFO - [diffusion][Epoch 10782] diffusion training Loss: 0.067532978951931
2024-11-05 02:54:50,057 - INFO - [diffusion][Epoch 10782] diffusion learning rate: 0.001
2024-11-05 02:54:50,059 - INFO - [diffusion][Epoch 10782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:50,060 - INFO - [diffusion][Epoch 10783] Epoch 10784/12000
2024-11-05 02:54:54,107 - INFO - [diffusion][Epoch 10783] diffusion training Loss: 0.06801763083785772
2024-11-05 02:54:54,109 - INFO - [diffusion][Epoch 10783] diffusion learning rate: 0.001
2024-11-05 02:54:54,110 - INFO - [diffusion][Epoch 10783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:54,112 - INFO - [diffusion][Epoch 10784] Epoch 10785/12000
2024-11-05 02:54:57,270 - INFO - [diffusion][Epoch 10784] diffusion training Loss: 0.06749911978840828
2024-11-05 02:54:57,271 - INFO - [diffusion][Epoch 10784] diffusion learning rate: 0.001
2024-11-05 02:54:57,273 - INFO - [diffusion][Epoch 10784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:57,274 - INFO - [diffusion][Epoch 10785] Epoch 10786/12000
2024-11-05 02:55:00,426 - INFO - [diffusion][Epoch 10785] diffusion training Loss: 0.06451878510415554
2024-11-05 02:55:00,428 - INFO - [diffusion][Epoch 10785] diffusion learning rate: 0.001
2024-11-05 02:55:00,448 - INFO - [diffusion][Epoch 10785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:00,450 - INFO - [diffusion][Epoch 10786] Epoch 10787/12000
2024-11-05 02:55:03,570 - INFO - [diffusion][Epoch 10786] diffusion training Loss: 0.06692450400441885
2024-11-05 02:55:03,571 - INFO - [diffusion][Epoch 10786] diffusion learning rate: 0.001
2024-11-05 02:55:03,573 - INFO - [diffusion][Epoch 10786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:03,574 - INFO - [diffusion][Epoch 10787] Epoch 10788/12000
2024-11-05 02:55:07,127 - INFO - [diffusion][Epoch 10787] diffusion training Loss: 0.06737373396754265
2024-11-05 02:55:07,128 - INFO - [diffusion][Epoch 10787] diffusion learning rate: 0.001
2024-11-05 02:55:07,130 - INFO - [diffusion][Epoch 10787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:07,131 - INFO - [diffusion][Epoch 10788] Epoch 10789/12000
2024-11-05 02:55:10,658 - INFO - [diffusion][Epoch 10788] diffusion training Loss: 0.062350355088710785
2024-11-05 02:55:10,660 - INFO - [diffusion][Epoch 10788] diffusion learning rate: 0.001
2024-11-05 02:55:10,700 - INFO - [diffusion][Epoch 10788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:10,701 - INFO - [diffusion][Epoch 10789] Epoch 10790/12000
2024-11-05 02:55:13,821 - INFO - [diffusion][Epoch 10789] diffusion training Loss: 0.06991600804030895
2024-11-05 02:55:13,823 - INFO - [diffusion][Epoch 10789] diffusion learning rate: 0.001
2024-11-05 02:55:13,825 - INFO - [diffusion][Epoch 10789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:13,827 - INFO - [diffusion][Epoch 10790] Epoch 10791/12000
2024-11-05 02:55:16,850 - INFO - [diffusion][Epoch 10790] diffusion training Loss: 0.07047568913549185
2024-11-05 02:55:16,852 - INFO - [diffusion][Epoch 10790] diffusion learning rate: 0.001
2024-11-05 02:55:16,854 - INFO - [diffusion][Epoch 10790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:16,855 - INFO - [diffusion][Epoch 10791] Epoch 10792/12000
2024-11-05 02:55:20,058 - INFO - [diffusion][Epoch 10791] diffusion training Loss: 0.06866386998444796
2024-11-05 02:55:20,060 - INFO - [diffusion][Epoch 10791] diffusion learning rate: 0.001
2024-11-05 02:55:20,062 - INFO - [diffusion][Epoch 10791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:20,063 - INFO - [diffusion][Epoch 10792] Epoch 10793/12000
2024-11-05 02:55:23,711 - INFO - [diffusion][Epoch 10792] diffusion training Loss: 0.06611790228635073
2024-11-05 02:55:23,714 - INFO - [diffusion][Epoch 10792] diffusion learning rate: 0.001
2024-11-05 02:55:23,716 - INFO - [diffusion][Epoch 10792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:23,717 - INFO - [diffusion][Epoch 10793] Epoch 10794/12000
2024-11-05 02:55:26,988 - INFO - [diffusion][Epoch 10793] diffusion training Loss: 0.07231488078832626
2024-11-05 02:55:26,990 - INFO - [diffusion][Epoch 10793] diffusion learning rate: 0.001
2024-11-05 02:55:27,011 - INFO - [diffusion][Epoch 10793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:27,012 - INFO - [diffusion][Epoch 10794] Epoch 10795/12000
2024-11-05 02:55:30,101 - INFO - [diffusion][Epoch 10794] diffusion training Loss: 0.06544942781329155
2024-11-05 02:55:30,103 - INFO - [diffusion][Epoch 10794] diffusion learning rate: 0.001
2024-11-05 02:55:30,104 - INFO - [diffusion][Epoch 10794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:30,106 - INFO - [diffusion][Epoch 10795] Epoch 10796/12000
2024-11-05 02:55:33,151 - INFO - [diffusion][Epoch 10795] diffusion training Loss: 0.06704098172485828
2024-11-05 02:55:33,153 - INFO - [diffusion][Epoch 10795] diffusion learning rate: 0.001
2024-11-05 02:55:33,154 - INFO - [diffusion][Epoch 10795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:33,155 - INFO - [diffusion][Epoch 10796] Epoch 10797/12000
2024-11-05 02:55:36,680 - INFO - [diffusion][Epoch 10796] diffusion training Loss: 0.06413876172155142
2024-11-05 02:55:36,682 - INFO - [diffusion][Epoch 10796] diffusion learning rate: 0.001
2024-11-05 02:55:36,683 - INFO - [diffusion][Epoch 10796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:36,685 - INFO - [diffusion][Epoch 10797] Epoch 10798/12000
2024-11-05 02:55:40,301 - INFO - [diffusion][Epoch 10797] diffusion training Loss: 0.06646492332220078
2024-11-05 02:55:40,303 - INFO - [diffusion][Epoch 10797] diffusion learning rate: 0.001
2024-11-05 02:55:40,305 - INFO - [diffusion][Epoch 10797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:40,306 - INFO - [diffusion][Epoch 10798] Epoch 10799/12000
2024-11-05 02:55:43,412 - INFO - [diffusion][Epoch 10798] diffusion training Loss: 0.06700314953923225
2024-11-05 02:55:43,414 - INFO - [diffusion][Epoch 10798] diffusion learning rate: 0.001
2024-11-05 02:55:43,434 - INFO - [diffusion][Epoch 10798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:43,436 - INFO - [diffusion][Epoch 10799] Epoch 10800/12000
2024-11-05 02:55:46,511 - INFO - [diffusion][Epoch 10799] diffusion training Loss: 0.07009531371295452
2024-11-05 02:55:46,512 - INFO - [diffusion][Epoch 10799] diffusion learning rate: 0.001
2024-11-05 02:55:46,514 - INFO - [diffusion][Epoch 10799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:46,516 - INFO - [diffusion][Epoch 10800] Epoch 10801/12000
2024-11-05 02:55:49,843 - INFO - [diffusion][Epoch 10800] diffusion training Loss: 0.06698804162442684
2024-11-05 02:55:49,845 - INFO - [diffusion][Epoch 10800] diffusion learning rate: 0.001
2024-11-05 02:55:49,847 - INFO - [diffusion][Epoch 10800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:49,848 - INFO - [diffusion][Epoch 10801] Epoch 10802/12000
2024-11-05 02:55:53,533 - INFO - [diffusion][Epoch 10801] diffusion training Loss: 0.06325908936560154
2024-11-05 02:55:53,535 - INFO - [diffusion][Epoch 10801] diffusion learning rate: 0.001
2024-11-05 02:55:53,537 - INFO - [diffusion][Epoch 10801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:53,538 - INFO - [diffusion][Epoch 10802] Epoch 10803/12000
2024-11-05 02:55:56,936 - INFO - [diffusion][Epoch 10802] diffusion training Loss: 0.06415501423180103
2024-11-05 02:55:56,938 - INFO - [diffusion][Epoch 10802] diffusion learning rate: 0.001
2024-11-05 02:55:56,940 - INFO - [diffusion][Epoch 10802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:56,941 - INFO - [diffusion][Epoch 10803] Epoch 10804/12000
2024-11-05 02:55:59,969 - INFO - [diffusion][Epoch 10803] diffusion training Loss: 0.06028197333216667
2024-11-05 02:55:59,971 - INFO - [diffusion][Epoch 10803] diffusion learning rate: 0.001
2024-11-05 02:55:59,972 - INFO - [diffusion][Epoch 10803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:59,974 - INFO - [diffusion][Epoch 10804] Epoch 10805/12000
2024-11-05 02:56:03,008 - INFO - [diffusion][Epoch 10804] diffusion training Loss: 0.05445815064013004
2024-11-05 02:56:03,010 - INFO - [diffusion][Epoch 10804] diffusion learning rate: 0.001
2024-11-05 02:56:03,011 - INFO - [diffusion][Epoch 10804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:03,013 - INFO - [diffusion][Epoch 10805] Epoch 10806/12000
2024-11-05 02:56:06,592 - INFO - [diffusion][Epoch 10805] diffusion training Loss: 0.0631332453340292
2024-11-05 02:56:06,594 - INFO - [diffusion][Epoch 10805] diffusion learning rate: 0.001
2024-11-05 02:56:06,596 - INFO - [diffusion][Epoch 10805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:06,598 - INFO - [diffusion][Epoch 10806] Epoch 10807/12000
2024-11-05 02:56:10,155 - INFO - [diffusion][Epoch 10806] diffusion training Loss: 0.0637276154011488
2024-11-05 02:56:10,157 - INFO - [diffusion][Epoch 10806] diffusion learning rate: 0.001
2024-11-05 02:56:10,177 - INFO - [diffusion][Epoch 10806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:10,178 - INFO - [diffusion][Epoch 10807] Epoch 10808/12000
2024-11-05 02:56:13,282 - INFO - [diffusion][Epoch 10807] diffusion training Loss: 0.06485102325677872
2024-11-05 02:56:13,284 - INFO - [diffusion][Epoch 10807] diffusion learning rate: 0.001
2024-11-05 02:56:13,286 - INFO - [diffusion][Epoch 10807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:13,287 - INFO - [diffusion][Epoch 10808] Epoch 10809/12000
2024-11-05 02:56:16,363 - INFO - [diffusion][Epoch 10808] diffusion training Loss: 0.06342555955052376
2024-11-05 02:56:16,365 - INFO - [diffusion][Epoch 10808] diffusion learning rate: 0.001
2024-11-05 02:56:16,368 - INFO - [diffusion][Epoch 10808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:16,369 - INFO - [diffusion][Epoch 10809] Epoch 10810/12000
2024-11-05 02:56:19,573 - INFO - [diffusion][Epoch 10809] diffusion training Loss: 0.06428091507405043
2024-11-05 02:56:19,574 - INFO - [diffusion][Epoch 10809] diffusion learning rate: 0.001
2024-11-05 02:56:19,576 - INFO - [diffusion][Epoch 10809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:19,577 - INFO - [diffusion][Epoch 10810] Epoch 10811/12000
2024-11-05 02:56:23,213 - INFO - [diffusion][Epoch 10810] diffusion training Loss: 0.06622063182294369
2024-11-05 02:56:23,215 - INFO - [diffusion][Epoch 10810] diffusion learning rate: 0.001
2024-11-05 02:56:23,216 - INFO - [diffusion][Epoch 10810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:23,218 - INFO - [diffusion][Epoch 10811] Epoch 10812/12000
2024-11-05 02:56:26,648 - INFO - [diffusion][Epoch 10811] diffusion training Loss: 0.0631859777495265
2024-11-05 02:56:26,649 - INFO - [diffusion][Epoch 10811] diffusion learning rate: 0.001
2024-11-05 02:56:26,668 - INFO - [diffusion][Epoch 10811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:26,669 - INFO - [diffusion][Epoch 10812] Epoch 10813/12000
2024-11-05 02:56:29,749 - INFO - [diffusion][Epoch 10812] diffusion training Loss: 0.06400665920227766
2024-11-05 02:56:29,750 - INFO - [diffusion][Epoch 10812] diffusion learning rate: 0.001
2024-11-05 02:56:29,752 - INFO - [diffusion][Epoch 10812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:29,754 - INFO - [diffusion][Epoch 10813] Epoch 10814/12000
2024-11-05 02:56:32,840 - INFO - [diffusion][Epoch 10813] diffusion training Loss: 0.06763447262346745
2024-11-05 02:56:32,842 - INFO - [diffusion][Epoch 10813] diffusion learning rate: 0.001
2024-11-05 02:56:32,844 - INFO - [diffusion][Epoch 10813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:32,845 - INFO - [diffusion][Epoch 10814] Epoch 10815/12000
2024-11-05 02:56:36,345 - INFO - [diffusion][Epoch 10814] diffusion training Loss: 0.06414243578910828
2024-11-05 02:56:36,347 - INFO - [diffusion][Epoch 10814] diffusion learning rate: 0.001
2024-11-05 02:56:36,349 - INFO - [diffusion][Epoch 10814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:36,350 - INFO - [diffusion][Epoch 10815] Epoch 10816/12000
2024-11-05 02:56:39,898 - INFO - [diffusion][Epoch 10815] diffusion training Loss: 0.06651348806917667
2024-11-05 02:56:39,900 - INFO - [diffusion][Epoch 10815] diffusion learning rate: 0.001
2024-11-05 02:56:39,902 - INFO - [diffusion][Epoch 10815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:39,903 - INFO - [diffusion][Epoch 10816] Epoch 10817/12000
2024-11-05 02:56:43,028 - INFO - [diffusion][Epoch 10816] diffusion training Loss: 0.0652104839682579
2024-11-05 02:56:43,029 - INFO - [diffusion][Epoch 10816] diffusion learning rate: 0.001
2024-11-05 02:56:43,031 - INFO - [diffusion][Epoch 10816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:43,032 - INFO - [diffusion][Epoch 10817] Epoch 10818/12000
2024-11-05 02:56:46,195 - INFO - [diffusion][Epoch 10817] diffusion training Loss: 0.06326576881110668
2024-11-05 02:56:46,197 - INFO - [diffusion][Epoch 10817] diffusion learning rate: 0.001
2024-11-05 02:56:46,199 - INFO - [diffusion][Epoch 10817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:46,200 - INFO - [diffusion][Epoch 10818] Epoch 10819/12000
2024-11-05 02:56:49,334 - INFO - [diffusion][Epoch 10818] diffusion training Loss: 0.06734524946659803
2024-11-05 02:56:49,336 - INFO - [diffusion][Epoch 10818] diffusion learning rate: 0.001
2024-11-05 02:56:49,337 - INFO - [diffusion][Epoch 10818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:49,339 - INFO - [diffusion][Epoch 10819] Epoch 10820/12000
2024-11-05 02:56:52,889 - INFO - [diffusion][Epoch 10819] diffusion training Loss: 0.06220107711851597
2024-11-05 02:56:52,891 - INFO - [diffusion][Epoch 10819] diffusion learning rate: 0.001
2024-11-05 02:56:52,894 - INFO - [diffusion][Epoch 10819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:52,895 - INFO - [diffusion][Epoch 10820] Epoch 10821/12000
2024-11-05 02:56:56,448 - INFO - [diffusion][Epoch 10820] diffusion training Loss: 0.06481767259538174
2024-11-05 02:56:56,457 - INFO - [diffusion][Epoch 10820] diffusion learning rate: 0.001
2024-11-05 02:56:56,459 - INFO - [diffusion][Epoch 10820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:56,460 - INFO - [diffusion][Epoch 10821] Epoch 10822/12000
2024-11-05 02:56:59,624 - INFO - [diffusion][Epoch 10821] diffusion training Loss: 0.06121937558054924
2024-11-05 02:56:59,626 - INFO - [diffusion][Epoch 10821] diffusion learning rate: 0.001
2024-11-05 02:56:59,628 - INFO - [diffusion][Epoch 10821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:59,629 - INFO - [diffusion][Epoch 10822] Epoch 10823/12000
2024-11-05 02:57:02,915 - INFO - [diffusion][Epoch 10822] diffusion training Loss: 0.06487936154007912
2024-11-05 02:57:02,916 - INFO - [diffusion][Epoch 10822] diffusion learning rate: 0.001
2024-11-05 02:57:02,918 - INFO - [diffusion][Epoch 10822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:02,919 - INFO - [diffusion][Epoch 10823] Epoch 10824/12000
2024-11-05 02:57:06,092 - INFO - [diffusion][Epoch 10823] diffusion training Loss: 0.06298159062862396
2024-11-05 02:57:06,095 - INFO - [diffusion][Epoch 10823] diffusion learning rate: 0.001
2024-11-05 02:57:06,097 - INFO - [diffusion][Epoch 10823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:06,098 - INFO - [diffusion][Epoch 10824] Epoch 10825/12000
2024-11-05 02:57:09,656 - INFO - [diffusion][Epoch 10824] diffusion training Loss: 0.06265608966350555
2024-11-05 02:57:09,658 - INFO - [diffusion][Epoch 10824] diffusion learning rate: 0.001
2024-11-05 02:57:09,659 - INFO - [diffusion][Epoch 10824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:09,661 - INFO - [diffusion][Epoch 10825] Epoch 10826/12000
2024-11-05 02:57:13,191 - INFO - [diffusion][Epoch 10825] diffusion training Loss: 0.0641943784430623
2024-11-05 02:57:13,193 - INFO - [diffusion][Epoch 10825] diffusion learning rate: 0.001
2024-11-05 02:57:13,195 - INFO - [diffusion][Epoch 10825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:13,196 - INFO - [diffusion][Epoch 10826] Epoch 10827/12000
2024-11-05 02:57:16,284 - INFO - [diffusion][Epoch 10826] diffusion training Loss: 0.06381898932158947
2024-11-05 02:57:16,286 - INFO - [diffusion][Epoch 10826] diffusion learning rate: 0.001
2024-11-05 02:57:16,288 - INFO - [diffusion][Epoch 10826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:16,289 - INFO - [diffusion][Epoch 10827] Epoch 10828/12000
2024-11-05 02:57:19,380 - INFO - [diffusion][Epoch 10827] diffusion training Loss: 0.06373161636292934
2024-11-05 02:57:19,382 - INFO - [diffusion][Epoch 10827] diffusion learning rate: 0.001
2024-11-05 02:57:19,384 - INFO - [diffusion][Epoch 10827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:19,385 - INFO - [diffusion][Epoch 10828] Epoch 10829/12000
2024-11-05 02:57:22,500 - INFO - [diffusion][Epoch 10828] diffusion training Loss: 0.06374404579401016
2024-11-05 02:57:22,503 - INFO - [diffusion][Epoch 10828] diffusion learning rate: 0.001
2024-11-05 02:57:22,505 - INFO - [diffusion][Epoch 10828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:22,506 - INFO - [diffusion][Epoch 10829] Epoch 10830/12000
2024-11-05 02:57:26,072 - INFO - [diffusion][Epoch 10829] diffusion training Loss: 0.06547295302152634
2024-11-05 02:57:26,074 - INFO - [diffusion][Epoch 10829] diffusion learning rate: 0.001
2024-11-05 02:57:26,076 - INFO - [diffusion][Epoch 10829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:26,077 - INFO - [diffusion][Epoch 10830] Epoch 10831/12000
2024-11-05 02:57:29,189 - INFO - [diffusion][Epoch 10830] diffusion training Loss: 0.06425774656236172
2024-11-05 02:57:29,191 - INFO - [diffusion][Epoch 10830] diffusion learning rate: 0.001
2024-11-05 02:57:29,193 - INFO - [diffusion][Epoch 10830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:29,194 - INFO - [diffusion][Epoch 10831] Epoch 10832/12000
2024-11-05 02:57:32,328 - INFO - [diffusion][Epoch 10831] diffusion training Loss: 0.06245512515306473
2024-11-05 02:57:32,330 - INFO - [diffusion][Epoch 10831] diffusion learning rate: 0.001
2024-11-05 02:57:32,332 - INFO - [diffusion][Epoch 10831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:32,333 - INFO - [diffusion][Epoch 10832] Epoch 10833/12000
2024-11-05 02:57:35,418 - INFO - [diffusion][Epoch 10832] diffusion training Loss: 0.06098437402397394
2024-11-05 02:57:35,546 - INFO - [diffusion][Epoch 10832] diffusion learning rate: 0.001
2024-11-05 02:57:35,621 - INFO - [diffusion][Epoch 10832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:35,633 - INFO - [diffusion][Epoch 10833] Epoch 10834/12000
2024-11-05 02:57:39,143 - INFO - [diffusion][Epoch 10833] diffusion training Loss: 0.06850163638591766
2024-11-05 02:57:39,145 - INFO - [diffusion][Epoch 10833] diffusion learning rate: 0.001
2024-11-05 02:57:39,147 - INFO - [diffusion][Epoch 10833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:39,148 - INFO - [diffusion][Epoch 10834] Epoch 10835/12000
2024-11-05 02:57:42,643 - INFO - [diffusion][Epoch 10834] diffusion training Loss: 0.06581174582242966
2024-11-05 02:57:42,645 - INFO - [diffusion][Epoch 10834] diffusion learning rate: 0.001
2024-11-05 02:57:42,647 - INFO - [diffusion][Epoch 10834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:42,648 - INFO - [diffusion][Epoch 10835] Epoch 10836/12000
2024-11-05 02:57:45,655 - INFO - [diffusion][Epoch 10835] diffusion training Loss: 0.0623224014416337
2024-11-05 02:57:45,657 - INFO - [diffusion][Epoch 10835] diffusion learning rate: 0.001
2024-11-05 02:57:45,659 - INFO - [diffusion][Epoch 10835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:45,660 - INFO - [diffusion][Epoch 10836] Epoch 10837/12000
2024-11-05 02:57:48,826 - INFO - [diffusion][Epoch 10836] diffusion training Loss: 0.06205300986766815
2024-11-05 02:57:48,827 - INFO - [diffusion][Epoch 10836] diffusion learning rate: 0.001
2024-11-05 02:57:48,829 - INFO - [diffusion][Epoch 10836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:48,830 - INFO - [diffusion][Epoch 10837] Epoch 10838/12000
2024-11-05 02:57:52,029 - INFO - [diffusion][Epoch 10837] diffusion training Loss: 0.06936328671872616
2024-11-05 02:57:52,030 - INFO - [diffusion][Epoch 10837] diffusion learning rate: 0.001
2024-11-05 02:57:52,032 - INFO - [diffusion][Epoch 10837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:52,033 - INFO - [diffusion][Epoch 10838] Epoch 10839/12000
2024-11-05 02:57:55,617 - INFO - [diffusion][Epoch 10838] diffusion training Loss: 0.06582750380039215
2024-11-05 02:57:55,619 - INFO - [diffusion][Epoch 10838] diffusion learning rate: 0.001
2024-11-05 02:57:55,621 - INFO - [diffusion][Epoch 10838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:55,622 - INFO - [diffusion][Epoch 10839] Epoch 10840/12000
2024-11-05 02:57:58,951 - INFO - [diffusion][Epoch 10839] diffusion training Loss: 0.06306708790361881
2024-11-05 02:57:58,953 - INFO - [diffusion][Epoch 10839] diffusion learning rate: 0.001
2024-11-05 02:57:58,955 - INFO - [diffusion][Epoch 10839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:58,957 - INFO - [diffusion][Epoch 10840] Epoch 10841/12000
2024-11-05 02:58:01,998 - INFO - [diffusion][Epoch 10840] diffusion training Loss: 0.07189391925930977
2024-11-05 02:58:02,000 - INFO - [diffusion][Epoch 10840] diffusion learning rate: 0.001
2024-11-05 02:58:02,003 - INFO - [diffusion][Epoch 10840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:02,004 - INFO - [diffusion][Epoch 10841] Epoch 10842/12000
2024-11-05 02:58:05,049 - INFO - [diffusion][Epoch 10841] diffusion training Loss: 0.0671493336558342
2024-11-05 02:58:05,051 - INFO - [diffusion][Epoch 10841] diffusion learning rate: 0.001
2024-11-05 02:58:05,053 - INFO - [diffusion][Epoch 10841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:05,054 - INFO - [diffusion][Epoch 10842] Epoch 10843/12000
2024-11-05 02:58:08,629 - INFO - [diffusion][Epoch 10842] diffusion training Loss: 0.06950896792113781
2024-11-05 02:58:08,631 - INFO - [diffusion][Epoch 10842] diffusion learning rate: 0.001
2024-11-05 02:58:08,632 - INFO - [diffusion][Epoch 10842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:08,634 - INFO - [diffusion][Epoch 10843] Epoch 10844/12000
2024-11-05 02:58:12,687 - INFO - [diffusion][Epoch 10843] diffusion training Loss: 0.06590967625379562
2024-11-05 02:58:12,689 - INFO - [diffusion][Epoch 10843] diffusion learning rate: 0.001
2024-11-05 02:58:12,690 - INFO - [diffusion][Epoch 10843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:12,692 - INFO - [diffusion][Epoch 10844] Epoch 10845/12000
2024-11-05 02:58:16,014 - INFO - [diffusion][Epoch 10844] diffusion training Loss: 0.06373131182044744
2024-11-05 02:58:16,016 - INFO - [diffusion][Epoch 10844] diffusion learning rate: 0.001
2024-11-05 02:58:16,017 - INFO - [diffusion][Epoch 10844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:16,019 - INFO - [diffusion][Epoch 10845] Epoch 10846/12000
2024-11-05 02:58:19,146 - INFO - [diffusion][Epoch 10845] diffusion training Loss: 0.06525817885994911
2024-11-05 02:58:19,147 - INFO - [diffusion][Epoch 10845] diffusion learning rate: 0.001
2024-11-05 02:58:19,149 - INFO - [diffusion][Epoch 10845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:19,150 - INFO - [diffusion][Epoch 10846] Epoch 10847/12000
2024-11-05 02:58:22,290 - INFO - [diffusion][Epoch 10846] diffusion training Loss: 0.0639211367815733
2024-11-05 02:58:22,293 - INFO - [diffusion][Epoch 10846] diffusion learning rate: 0.001
2024-11-05 02:58:22,294 - INFO - [diffusion][Epoch 10846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:22,296 - INFO - [diffusion][Epoch 10847] Epoch 10848/12000
2024-11-05 02:58:25,818 - INFO - [diffusion][Epoch 10847] diffusion training Loss: 0.06890791654586792
2024-11-05 02:58:25,820 - INFO - [diffusion][Epoch 10847] diffusion learning rate: 0.001
2024-11-05 02:58:25,822 - INFO - [diffusion][Epoch 10847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:25,823 - INFO - [diffusion][Epoch 10848] Epoch 10849/12000
2024-11-05 02:58:29,389 - INFO - [diffusion][Epoch 10848] diffusion training Loss: 0.06753400340676308
2024-11-05 02:58:29,391 - INFO - [diffusion][Epoch 10848] diffusion learning rate: 0.001
2024-11-05 02:58:29,440 - INFO - [diffusion][Epoch 10848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:29,441 - INFO - [diffusion][Epoch 10849] Epoch 10850/12000
2024-11-05 02:58:32,567 - INFO - [diffusion][Epoch 10849] diffusion training Loss: 0.06619304325431585
2024-11-05 02:58:32,569 - INFO - [diffusion][Epoch 10849] diffusion learning rate: 0.001
2024-11-05 02:58:32,571 - INFO - [diffusion][Epoch 10849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:32,572 - INFO - [diffusion][Epoch 10850] Epoch 10851/12000
2024-11-05 02:58:35,676 - INFO - [diffusion][Epoch 10850] diffusion training Loss: 0.06367866136133671
2024-11-05 02:58:35,678 - INFO - [diffusion][Epoch 10850] diffusion learning rate: 0.001
2024-11-05 02:58:35,680 - INFO - [diffusion][Epoch 10850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:35,681 - INFO - [diffusion][Epoch 10851] Epoch 10852/12000
2024-11-05 02:58:38,895 - INFO - [diffusion][Epoch 10851] diffusion training Loss: 0.06593974027782679
2024-11-05 02:58:38,897 - INFO - [diffusion][Epoch 10851] diffusion learning rate: 0.001
2024-11-05 02:58:38,899 - INFO - [diffusion][Epoch 10851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:38,901 - INFO - [diffusion][Epoch 10852] Epoch 10853/12000
2024-11-05 02:58:42,562 - INFO - [diffusion][Epoch 10852] diffusion training Loss: 0.06887087691575289
2024-11-05 02:58:42,564 - INFO - [diffusion][Epoch 10852] diffusion learning rate: 0.001
2024-11-05 02:58:42,566 - INFO - [diffusion][Epoch 10852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:42,567 - INFO - [diffusion][Epoch 10853] Epoch 10854/12000
2024-11-05 02:58:45,569 - INFO - [diffusion][Epoch 10853] diffusion training Loss: 0.059960328973829746
2024-11-05 02:58:45,571 - INFO - [diffusion][Epoch 10853] diffusion learning rate: 0.001
2024-11-05 02:58:45,589 - INFO - [diffusion][Epoch 10853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:45,591 - INFO - [diffusion][Epoch 10854] Epoch 10855/12000
2024-11-05 02:58:48,654 - INFO - [diffusion][Epoch 10854] diffusion training Loss: 0.06230479013174772
2024-11-05 02:58:48,657 - INFO - [diffusion][Epoch 10854] diffusion learning rate: 0.001
2024-11-05 02:58:48,659 - INFO - [diffusion][Epoch 10854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:48,660 - INFO - [diffusion][Epoch 10855] Epoch 10856/12000
2024-11-05 02:58:52,041 - INFO - [diffusion][Epoch 10855] diffusion training Loss: 0.06398615147918463
2024-11-05 02:58:52,043 - INFO - [diffusion][Epoch 10855] diffusion learning rate: 0.001
2024-11-05 02:58:52,044 - INFO - [diffusion][Epoch 10855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:52,045 - INFO - [diffusion][Epoch 10856] Epoch 10857/12000
2024-11-05 02:58:55,704 - INFO - [diffusion][Epoch 10856] diffusion training Loss: 0.06790615431964397
2024-11-05 02:58:55,706 - INFO - [diffusion][Epoch 10856] diffusion learning rate: 0.001
2024-11-05 02:58:55,708 - INFO - [diffusion][Epoch 10856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:55,709 - INFO - [diffusion][Epoch 10857] Epoch 10858/12000
2024-11-05 02:58:59,155 - INFO - [diffusion][Epoch 10857] diffusion training Loss: 0.07274197414517403
2024-11-05 02:58:59,156 - INFO - [diffusion][Epoch 10857] diffusion learning rate: 0.001
2024-11-05 02:58:59,158 - INFO - [diffusion][Epoch 10857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:59,160 - INFO - [diffusion][Epoch 10858] Epoch 10859/12000
2024-11-05 02:59:02,297 - INFO - [diffusion][Epoch 10858] diffusion training Loss: 0.06680874899029732
2024-11-05 02:59:02,299 - INFO - [diffusion][Epoch 10858] diffusion learning rate: 0.001
2024-11-05 02:59:02,318 - INFO - [diffusion][Epoch 10858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:02,319 - INFO - [diffusion][Epoch 10859] Epoch 10860/12000
2024-11-05 02:59:05,401 - INFO - [diffusion][Epoch 10859] diffusion training Loss: 0.06504575163125992
2024-11-05 02:59:05,403 - INFO - [diffusion][Epoch 10859] diffusion learning rate: 0.001
2024-11-05 02:59:05,405 - INFO - [diffusion][Epoch 10859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:05,406 - INFO - [diffusion][Epoch 10860] Epoch 10861/12000
2024-11-05 02:59:08,729 - INFO - [diffusion][Epoch 10860] diffusion training Loss: 0.0684351846575737
2024-11-05 02:59:08,731 - INFO - [diffusion][Epoch 10860] diffusion learning rate: 0.001
2024-11-05 02:59:08,733 - INFO - [diffusion][Epoch 10860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:08,734 - INFO - [diffusion][Epoch 10861] Epoch 10862/12000
2024-11-05 02:59:12,381 - INFO - [diffusion][Epoch 10861] diffusion training Loss: 0.056860185228288174
2024-11-05 02:59:12,389 - INFO - [diffusion][Epoch 10861] diffusion learning rate: 0.001
2024-11-05 02:59:12,391 - INFO - [diffusion][Epoch 10861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:12,392 - INFO - [diffusion][Epoch 10862] Epoch 10863/12000
2024-11-05 02:59:15,760 - INFO - [diffusion][Epoch 10862] diffusion training Loss: 0.07183579541742802
2024-11-05 02:59:15,762 - INFO - [diffusion][Epoch 10862] diffusion learning rate: 0.001
2024-11-05 02:59:15,764 - INFO - [diffusion][Epoch 10862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:15,765 - INFO - [diffusion][Epoch 10863] Epoch 10864/12000
2024-11-05 02:59:19,810 - INFO - [diffusion][Epoch 10863] diffusion training Loss: 0.0640205629169941
2024-11-05 02:59:19,812 - INFO - [diffusion][Epoch 10863] diffusion learning rate: 0.001
2024-11-05 02:59:19,814 - INFO - [diffusion][Epoch 10863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:19,815 - INFO - [diffusion][Epoch 10864] Epoch 10865/12000
2024-11-05 02:59:23,010 - INFO - [diffusion][Epoch 10864] diffusion training Loss: 0.06974109821021557
2024-11-05 02:59:23,013 - INFO - [diffusion][Epoch 10864] diffusion learning rate: 0.001
2024-11-05 02:59:23,015 - INFO - [diffusion][Epoch 10864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:23,016 - INFO - [diffusion][Epoch 10865] Epoch 10866/12000
2024-11-05 02:59:26,186 - INFO - [diffusion][Epoch 10865] diffusion training Loss: 0.06109892018139362
2024-11-05 02:59:26,189 - INFO - [diffusion][Epoch 10865] diffusion learning rate: 0.001
2024-11-05 02:59:26,191 - INFO - [diffusion][Epoch 10865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:26,193 - INFO - [diffusion][Epoch 10866] Epoch 10867/12000
2024-11-05 02:59:29,327 - INFO - [diffusion][Epoch 10866] diffusion training Loss: 0.07064251229166985
2024-11-05 02:59:29,330 - INFO - [diffusion][Epoch 10866] diffusion learning rate: 0.001
2024-11-05 02:59:29,333 - INFO - [diffusion][Epoch 10866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:29,335 - INFO - [diffusion][Epoch 10867] Epoch 10868/12000
2024-11-05 02:59:32,961 - INFO - [diffusion][Epoch 10867] diffusion training Loss: 0.06736456602811813
2024-11-05 02:59:32,963 - INFO - [diffusion][Epoch 10867] diffusion learning rate: 0.001
2024-11-05 02:59:32,965 - INFO - [diffusion][Epoch 10867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:32,966 - INFO - [diffusion][Epoch 10868] Epoch 10869/12000
2024-11-05 02:59:36,464 - INFO - [diffusion][Epoch 10868] diffusion training Loss: 0.06482835672795773
2024-11-05 02:59:36,466 - INFO - [diffusion][Epoch 10868] diffusion learning rate: 0.001
2024-11-05 02:59:36,468 - INFO - [diffusion][Epoch 10868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:36,469 - INFO - [diffusion][Epoch 10869] Epoch 10870/12000
2024-11-05 02:59:39,526 - INFO - [diffusion][Epoch 10869] diffusion training Loss: 0.06861428171396255
2024-11-05 02:59:39,528 - INFO - [diffusion][Epoch 10869] diffusion learning rate: 0.001
2024-11-05 02:59:39,530 - INFO - [diffusion][Epoch 10869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:39,531 - INFO - [diffusion][Epoch 10870] Epoch 10871/12000
2024-11-05 02:59:42,697 - INFO - [diffusion][Epoch 10870] diffusion training Loss: 0.059952570125460625
2024-11-05 02:59:42,699 - INFO - [diffusion][Epoch 10870] diffusion learning rate: 0.001
2024-11-05 02:59:42,701 - INFO - [diffusion][Epoch 10870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:42,703 - INFO - [diffusion][Epoch 10871] Epoch 10872/12000
2024-11-05 02:59:45,849 - INFO - [diffusion][Epoch 10871] diffusion training Loss: 0.05952398292720318
2024-11-05 02:59:45,851 - INFO - [diffusion][Epoch 10871] diffusion learning rate: 0.001
2024-11-05 02:59:45,852 - INFO - [diffusion][Epoch 10871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:45,854 - INFO - [diffusion][Epoch 10872] Epoch 10873/12000
2024-11-05 02:59:49,393 - INFO - [diffusion][Epoch 10872] diffusion training Loss: 0.06144129205495119
2024-11-05 02:59:49,395 - INFO - [diffusion][Epoch 10872] diffusion learning rate: 0.001
2024-11-05 02:59:49,396 - INFO - [diffusion][Epoch 10872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:49,398 - INFO - [diffusion][Epoch 10873] Epoch 10874/12000
2024-11-05 02:59:53,011 - INFO - [diffusion][Epoch 10873] diffusion training Loss: 0.06002466008067131
2024-11-05 02:59:53,013 - INFO - [diffusion][Epoch 10873] diffusion learning rate: 0.001
2024-11-05 02:59:53,015 - INFO - [diffusion][Epoch 10873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:53,016 - INFO - [diffusion][Epoch 10874] Epoch 10875/12000
2024-11-05 02:59:56,086 - INFO - [diffusion][Epoch 10874] diffusion training Loss: 0.06628741696476936
2024-11-05 02:59:56,088 - INFO - [diffusion][Epoch 10874] diffusion learning rate: 0.001
2024-11-05 02:59:56,090 - INFO - [diffusion][Epoch 10874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:56,091 - INFO - [diffusion][Epoch 10875] Epoch 10876/12000
2024-11-05 02:59:59,171 - INFO - [diffusion][Epoch 10875] diffusion training Loss: 0.06663761846721172
2024-11-05 02:59:59,173 - INFO - [diffusion][Epoch 10875] diffusion learning rate: 0.001
2024-11-05 02:59:59,174 - INFO - [diffusion][Epoch 10875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:59,176 - INFO - [diffusion][Epoch 10876] Epoch 10877/12000
2024-11-05 03:00:02,346 - INFO - [diffusion][Epoch 10876] diffusion training Loss: 0.06780510768294334
2024-11-05 03:00:02,348 - INFO - [diffusion][Epoch 10876] diffusion learning rate: 0.001
2024-11-05 03:00:02,350 - INFO - [diffusion][Epoch 10876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:02,351 - INFO - [diffusion][Epoch 10877] Epoch 10878/12000
2024-11-05 03:00:05,841 - INFO - [diffusion][Epoch 10877] diffusion training Loss: 0.06428304687142372
2024-11-05 03:00:05,843 - INFO - [diffusion][Epoch 10877] diffusion learning rate: 0.001
2024-11-05 03:00:05,844 - INFO - [diffusion][Epoch 10877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:05,846 - INFO - [diffusion][Epoch 10878] Epoch 10879/12000
2024-11-05 03:00:09,117 - INFO - [diffusion][Epoch 10878] diffusion training Loss: 0.06513050850480795
2024-11-05 03:00:09,118 - INFO - [diffusion][Epoch 10878] diffusion learning rate: 0.001
2024-11-05 03:00:09,120 - INFO - [diffusion][Epoch 10878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:09,121 - INFO - [diffusion][Epoch 10879] Epoch 10880/12000
2024-11-05 03:00:12,184 - INFO - [diffusion][Epoch 10879] diffusion training Loss: 0.06492065358906984
2024-11-05 03:00:12,410 - INFO - [diffusion][Epoch 10879] diffusion learning rate: 0.001
2024-11-05 03:00:12,411 - INFO - [diffusion][Epoch 10879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:12,413 - INFO - [diffusion][Epoch 10880] Epoch 10881/12000
2024-11-05 03:00:15,507 - INFO - [diffusion][Epoch 10880] diffusion training Loss: 0.06623534392565489
2024-11-05 03:00:15,509 - INFO - [diffusion][Epoch 10880] diffusion learning rate: 0.001
2024-11-05 03:00:15,510 - INFO - [diffusion][Epoch 10880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:15,512 - INFO - [diffusion][Epoch 10881] Epoch 10882/12000
2024-11-05 03:00:18,589 - INFO - [diffusion][Epoch 10881] diffusion training Loss: 0.06099643837660551
2024-11-05 03:00:18,591 - INFO - [diffusion][Epoch 10881] diffusion learning rate: 0.001
2024-11-05 03:00:18,593 - INFO - [diffusion][Epoch 10881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:18,595 - INFO - [diffusion][Epoch 10882] Epoch 10883/12000
2024-11-05 03:00:22,135 - INFO - [diffusion][Epoch 10882] diffusion training Loss: 0.06967864744365215
2024-11-05 03:00:22,137 - INFO - [diffusion][Epoch 10882] diffusion learning rate: 0.001
2024-11-05 03:00:22,138 - INFO - [diffusion][Epoch 10882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:22,139 - INFO - [diffusion][Epoch 10883] Epoch 10884/12000
2024-11-05 03:00:25,898 - INFO - [diffusion][Epoch 10883] diffusion training Loss: 0.06622404139488935
2024-11-05 03:00:25,901 - INFO - [diffusion][Epoch 10883] diffusion learning rate: 0.001
2024-11-05 03:00:25,903 - INFO - [diffusion][Epoch 10883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:25,904 - INFO - [diffusion][Epoch 10884] Epoch 10885/12000
2024-11-05 03:00:29,172 - INFO - [diffusion][Epoch 10884] diffusion training Loss: 0.06764563266187906
2024-11-05 03:00:29,174 - INFO - [diffusion][Epoch 10884] diffusion learning rate: 0.001
2024-11-05 03:00:29,176 - INFO - [diffusion][Epoch 10884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:29,177 - INFO - [diffusion][Epoch 10885] Epoch 10886/12000
2024-11-05 03:00:32,363 - INFO - [diffusion][Epoch 10885] diffusion training Loss: 0.06465429905802011
2024-11-05 03:00:32,365 - INFO - [diffusion][Epoch 10885] diffusion learning rate: 0.001
2024-11-05 03:00:32,367 - INFO - [diffusion][Epoch 10885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:32,368 - INFO - [diffusion][Epoch 10886] Epoch 10887/12000
2024-11-05 03:00:35,661 - INFO - [diffusion][Epoch 10886] diffusion training Loss: 0.05985220894217491
2024-11-05 03:00:35,663 - INFO - [diffusion][Epoch 10886] diffusion learning rate: 0.001
2024-11-05 03:00:35,665 - INFO - [diffusion][Epoch 10886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:35,666 - INFO - [diffusion][Epoch 10887] Epoch 10888/12000
2024-11-05 03:00:38,891 - INFO - [diffusion][Epoch 10887] diffusion training Loss: 0.06077633146196604
2024-11-05 03:00:38,893 - INFO - [diffusion][Epoch 10887] diffusion learning rate: 0.001
2024-11-05 03:00:38,894 - INFO - [diffusion][Epoch 10887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:38,895 - INFO - [diffusion][Epoch 10888] Epoch 10889/12000
2024-11-05 03:00:42,479 - INFO - [diffusion][Epoch 10888] diffusion training Loss: 0.06362273078411818
2024-11-05 03:00:42,481 - INFO - [diffusion][Epoch 10888] diffusion learning rate: 0.001
2024-11-05 03:00:42,501 - INFO - [diffusion][Epoch 10888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:42,503 - INFO - [diffusion][Epoch 10889] Epoch 10890/12000
2024-11-05 03:00:45,859 - INFO - [diffusion][Epoch 10889] diffusion training Loss: 0.06816496793180704
2024-11-05 03:00:45,861 - INFO - [diffusion][Epoch 10889] diffusion learning rate: 0.001
2024-11-05 03:00:45,863 - INFO - [diffusion][Epoch 10889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:45,864 - INFO - [diffusion][Epoch 10890] Epoch 10891/12000
2024-11-05 03:00:48,770 - INFO - [diffusion][Epoch 10890] diffusion training Loss: 0.067302405834198
2024-11-05 03:00:48,772 - INFO - [diffusion][Epoch 10890] diffusion learning rate: 0.001
2024-11-05 03:00:48,773 - INFO - [diffusion][Epoch 10890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:48,774 - INFO - [diffusion][Epoch 10891] Epoch 10892/12000
2024-11-05 03:00:51,895 - INFO - [diffusion][Epoch 10891] diffusion training Loss: 0.0752471499145031
2024-11-05 03:00:51,897 - INFO - [diffusion][Epoch 10891] diffusion learning rate: 0.001
2024-11-05 03:00:51,899 - INFO - [diffusion][Epoch 10891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:51,900 - INFO - [diffusion][Epoch 10892] Epoch 10893/12000
2024-11-05 03:00:55,435 - INFO - [diffusion][Epoch 10892] diffusion training Loss: 0.06155320443212986
2024-11-05 03:00:55,437 - INFO - [diffusion][Epoch 10892] diffusion learning rate: 0.001
2024-11-05 03:00:55,438 - INFO - [diffusion][Epoch 10892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:55,439 - INFO - [diffusion][Epoch 10893] Epoch 10894/12000
2024-11-05 03:00:59,272 - INFO - [diffusion][Epoch 10893] diffusion training Loss: 0.06700230203568935
2024-11-05 03:00:59,274 - INFO - [diffusion][Epoch 10893] diffusion learning rate: 0.001
2024-11-05 03:00:59,276 - INFO - [diffusion][Epoch 10893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:59,277 - INFO - [diffusion][Epoch 10894] Epoch 10895/12000
2024-11-05 03:01:02,705 - INFO - [diffusion][Epoch 10894] diffusion training Loss: 0.06469394639134407
2024-11-05 03:01:02,706 - INFO - [diffusion][Epoch 10894] diffusion learning rate: 0.001
2024-11-05 03:01:02,708 - INFO - [diffusion][Epoch 10894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:02,709 - INFO - [diffusion][Epoch 10895] Epoch 10896/12000
2024-11-05 03:01:05,869 - INFO - [diffusion][Epoch 10895] diffusion training Loss: 0.06442027352750301
2024-11-05 03:01:05,871 - INFO - [diffusion][Epoch 10895] diffusion learning rate: 0.001
2024-11-05 03:01:05,873 - INFO - [diffusion][Epoch 10895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:05,874 - INFO - [diffusion][Epoch 10896] Epoch 10897/12000
2024-11-05 03:01:09,066 - INFO - [diffusion][Epoch 10896] diffusion training Loss: 0.06305165588855743
2024-11-05 03:01:09,068 - INFO - [diffusion][Epoch 10896] diffusion learning rate: 0.001
2024-11-05 03:01:09,069 - INFO - [diffusion][Epoch 10896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:09,071 - INFO - [diffusion][Epoch 10897] Epoch 10898/12000
2024-11-05 03:01:12,361 - INFO - [diffusion][Epoch 10897] diffusion training Loss: 0.07106956839561462
2024-11-05 03:01:12,363 - INFO - [diffusion][Epoch 10897] diffusion learning rate: 0.001
2024-11-05 03:01:12,365 - INFO - [diffusion][Epoch 10897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:12,366 - INFO - [diffusion][Epoch 10898] Epoch 10899/12000
2024-11-05 03:01:16,028 - INFO - [diffusion][Epoch 10898] diffusion training Loss: 0.06404562667012215
2024-11-05 03:01:16,031 - INFO - [diffusion][Epoch 10898] diffusion learning rate: 0.001
2024-11-05 03:01:16,033 - INFO - [diffusion][Epoch 10898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:16,034 - INFO - [diffusion][Epoch 10899] Epoch 10900/12000
2024-11-05 03:01:19,633 - INFO - [diffusion][Epoch 10899] diffusion training Loss: 0.06651736237108707
2024-11-05 03:01:19,635 - INFO - [diffusion][Epoch 10899] diffusion learning rate: 0.001
2024-11-05 03:01:19,636 - INFO - [diffusion][Epoch 10899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:19,637 - INFO - [diffusion][Epoch 10900] Epoch 10901/12000
2024-11-05 03:01:22,827 - INFO - [diffusion][Epoch 10900] diffusion training Loss: 0.06324025057256222
2024-11-05 03:01:22,829 - INFO - [diffusion][Epoch 10900] diffusion learning rate: 0.001
2024-11-05 03:01:22,831 - INFO - [diffusion][Epoch 10900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:22,832 - INFO - [diffusion][Epoch 10901] Epoch 10902/12000
2024-11-05 03:01:25,901 - INFO - [diffusion][Epoch 10901] diffusion training Loss: 0.05956981796771288
2024-11-05 03:01:25,904 - INFO - [diffusion][Epoch 10901] diffusion learning rate: 0.001
2024-11-05 03:01:25,906 - INFO - [diffusion][Epoch 10901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:25,907 - INFO - [diffusion][Epoch 10902] Epoch 10903/12000
2024-11-05 03:01:29,543 - INFO - [diffusion][Epoch 10902] diffusion training Loss: 0.06517213024199009
2024-11-05 03:01:29,545 - INFO - [diffusion][Epoch 10902] diffusion learning rate: 0.001
2024-11-05 03:01:29,547 - INFO - [diffusion][Epoch 10902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:29,548 - INFO - [diffusion][Epoch 10903] Epoch 10904/12000
2024-11-05 03:01:32,691 - INFO - [diffusion][Epoch 10903] diffusion training Loss: 0.0645140241831541
2024-11-05 03:01:32,693 - INFO - [diffusion][Epoch 10903] diffusion learning rate: 0.001
2024-11-05 03:01:32,695 - INFO - [diffusion][Epoch 10903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:32,696 - INFO - [diffusion][Epoch 10904] Epoch 10905/12000
2024-11-05 03:01:36,085 - INFO - [diffusion][Epoch 10904] diffusion training Loss: 0.056195260025560856
2024-11-05 03:01:36,087 - INFO - [diffusion][Epoch 10904] diffusion learning rate: 0.001
2024-11-05 03:01:36,089 - INFO - [diffusion][Epoch 10904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:36,090 - INFO - [diffusion][Epoch 10905] Epoch 10906/12000
2024-11-05 03:01:39,380 - INFO - [diffusion][Epoch 10905] diffusion training Loss: 0.06682327389717102
2024-11-05 03:01:39,382 - INFO - [diffusion][Epoch 10905] diffusion learning rate: 0.001
2024-11-05 03:01:39,409 - INFO - [diffusion][Epoch 10905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:39,410 - INFO - [diffusion][Epoch 10906] Epoch 10907/12000
2024-11-05 03:01:42,511 - INFO - [diffusion][Epoch 10906] diffusion training Loss: 0.0654450785368681
2024-11-05 03:01:42,513 - INFO - [diffusion][Epoch 10906] diffusion learning rate: 0.001
2024-11-05 03:01:42,515 - INFO - [diffusion][Epoch 10906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:42,516 - INFO - [diffusion][Epoch 10907] Epoch 10908/12000
2024-11-05 03:01:45,581 - INFO - [diffusion][Epoch 10907] diffusion training Loss: 0.06373245362192392
2024-11-05 03:01:45,583 - INFO - [diffusion][Epoch 10907] diffusion learning rate: 0.001
2024-11-05 03:01:45,585 - INFO - [diffusion][Epoch 10907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:45,586 - INFO - [diffusion][Epoch 10908] Epoch 10909/12000
2024-11-05 03:01:49,114 - INFO - [diffusion][Epoch 10908] diffusion training Loss: 0.06434154603630304
2024-11-05 03:01:49,116 - INFO - [diffusion][Epoch 10908] diffusion learning rate: 0.001
2024-11-05 03:01:49,118 - INFO - [diffusion][Epoch 10908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:49,119 - INFO - [diffusion][Epoch 10909] Epoch 10910/12000
2024-11-05 03:01:52,565 - INFO - [diffusion][Epoch 10909] diffusion training Loss: 0.06782164145261049
2024-11-05 03:01:52,567 - INFO - [diffusion][Epoch 10909] diffusion learning rate: 0.001
2024-11-05 03:01:52,569 - INFO - [diffusion][Epoch 10909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:52,570 - INFO - [diffusion][Epoch 10910] Epoch 10911/12000
2024-11-05 03:01:55,481 - INFO - [diffusion][Epoch 10910] diffusion training Loss: 0.06312425807118416
2024-11-05 03:01:55,483 - INFO - [diffusion][Epoch 10910] diffusion learning rate: 0.001
2024-11-05 03:01:55,485 - INFO - [diffusion][Epoch 10910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:55,487 - INFO - [diffusion][Epoch 10911] Epoch 10912/12000
2024-11-05 03:01:58,550 - INFO - [diffusion][Epoch 10911] diffusion training Loss: 0.06466682255268097
2024-11-05 03:01:58,552 - INFO - [diffusion][Epoch 10911] diffusion learning rate: 0.001
2024-11-05 03:01:58,554 - INFO - [diffusion][Epoch 10911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:58,555 - INFO - [diffusion][Epoch 10912] Epoch 10913/12000
2024-11-05 03:02:02,070 - INFO - [diffusion][Epoch 10912] diffusion training Loss: 0.061316574923694134
2024-11-05 03:02:02,073 - INFO - [diffusion][Epoch 10912] diffusion learning rate: 0.001
2024-11-05 03:02:02,076 - INFO - [diffusion][Epoch 10912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:02,077 - INFO - [diffusion][Epoch 10913] Epoch 10914/12000
2024-11-05 03:02:05,536 - INFO - [diffusion][Epoch 10913] diffusion training Loss: 0.06779814511537552
2024-11-05 03:02:05,537 - INFO - [diffusion][Epoch 10913] diffusion learning rate: 0.001
2024-11-05 03:02:05,539 - INFO - [diffusion][Epoch 10913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:05,540 - INFO - [diffusion][Epoch 10914] Epoch 10915/12000
2024-11-05 03:02:08,561 - INFO - [diffusion][Epoch 10914] diffusion training Loss: 0.06540226470679045
2024-11-05 03:02:08,563 - INFO - [diffusion][Epoch 10914] diffusion learning rate: 0.001
2024-11-05 03:02:08,565 - INFO - [diffusion][Epoch 10914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:08,566 - INFO - [diffusion][Epoch 10915] Epoch 10916/12000
2024-11-05 03:02:11,363 - INFO - [diffusion][Epoch 10915] diffusion training Loss: 0.06495797354727983
2024-11-05 03:02:11,365 - INFO - [diffusion][Epoch 10915] diffusion learning rate: 0.001
2024-11-05 03:02:11,366 - INFO - [diffusion][Epoch 10915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:11,368 - INFO - [diffusion][Epoch 10916] Epoch 10917/12000
2024-11-05 03:02:14,873 - INFO - [diffusion][Epoch 10916] diffusion training Loss: 0.0631544329226017
2024-11-05 03:02:14,875 - INFO - [diffusion][Epoch 10916] diffusion learning rate: 0.001
2024-11-05 03:02:14,877 - INFO - [diffusion][Epoch 10916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:14,878 - INFO - [diffusion][Epoch 10917] Epoch 10918/12000
2024-11-05 03:02:18,468 - INFO - [diffusion][Epoch 10917] diffusion training Loss: 0.06496515404433012
2024-11-05 03:02:18,470 - INFO - [diffusion][Epoch 10917] diffusion learning rate: 0.001
2024-11-05 03:02:18,521 - INFO - [diffusion][Epoch 10917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:18,522 - INFO - [diffusion][Epoch 10918] Epoch 10919/12000
2024-11-05 03:02:21,614 - INFO - [diffusion][Epoch 10918] diffusion training Loss: 0.06623714976012707
2024-11-05 03:02:21,616 - INFO - [diffusion][Epoch 10918] diffusion learning rate: 0.001
2024-11-05 03:02:21,618 - INFO - [diffusion][Epoch 10918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:21,619 - INFO - [diffusion][Epoch 10919] Epoch 10920/12000
2024-11-05 03:02:24,741 - INFO - [diffusion][Epoch 10919] diffusion training Loss: 0.0629696985706687
2024-11-05 03:02:24,743 - INFO - [diffusion][Epoch 10919] diffusion learning rate: 0.001
2024-11-05 03:02:24,745 - INFO - [diffusion][Epoch 10919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:24,747 - INFO - [diffusion][Epoch 10920] Epoch 10921/12000
2024-11-05 03:02:27,843 - INFO - [diffusion][Epoch 10920] diffusion training Loss: 0.06873688660562038
2024-11-05 03:02:27,845 - INFO - [diffusion][Epoch 10920] diffusion learning rate: 0.001
2024-11-05 03:02:27,847 - INFO - [diffusion][Epoch 10920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:27,848 - INFO - [diffusion][Epoch 10921] Epoch 10922/12000
2024-11-05 03:02:31,436 - INFO - [diffusion][Epoch 10921] diffusion training Loss: 0.06713740713894367
2024-11-05 03:02:31,438 - INFO - [diffusion][Epoch 10921] diffusion learning rate: 0.001
2024-11-05 03:02:31,440 - INFO - [diffusion][Epoch 10921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:31,441 - INFO - [diffusion][Epoch 10922] Epoch 10923/12000
2024-11-05 03:02:35,332 - INFO - [diffusion][Epoch 10922] diffusion training Loss: 0.0637517012655735
2024-11-05 03:02:35,334 - INFO - [diffusion][Epoch 10922] diffusion learning rate: 0.001
2024-11-05 03:02:35,336 - INFO - [diffusion][Epoch 10922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:35,337 - INFO - [diffusion][Epoch 10923] Epoch 10924/12000
2024-11-05 03:02:39,370 - INFO - [diffusion][Epoch 10923] diffusion training Loss: 0.06096608657389879
2024-11-05 03:02:39,372 - INFO - [diffusion][Epoch 10923] diffusion learning rate: 0.001
2024-11-05 03:02:39,373 - INFO - [diffusion][Epoch 10923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:39,375 - INFO - [diffusion][Epoch 10924] Epoch 10925/12000
2024-11-05 03:02:42,472 - INFO - [diffusion][Epoch 10924] diffusion training Loss: 0.0643309485167265
2024-11-05 03:02:42,474 - INFO - [diffusion][Epoch 10924] diffusion learning rate: 0.001
2024-11-05 03:02:42,475 - INFO - [diffusion][Epoch 10924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:42,476 - INFO - [diffusion][Epoch 10925] Epoch 10926/12000
2024-11-05 03:02:45,554 - INFO - [diffusion][Epoch 10925] diffusion training Loss: 0.05944640003144741
2024-11-05 03:02:45,556 - INFO - [diffusion][Epoch 10925] diffusion learning rate: 0.001
2024-11-05 03:02:45,558 - INFO - [diffusion][Epoch 10925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:45,560 - INFO - [diffusion][Epoch 10926] Epoch 10927/12000
2024-11-05 03:02:48,681 - INFO - [diffusion][Epoch 10926] diffusion training Loss: 0.06528700049966574
2024-11-05 03:02:48,684 - INFO - [diffusion][Epoch 10926] diffusion learning rate: 0.001
2024-11-05 03:02:48,686 - INFO - [diffusion][Epoch 10926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:48,687 - INFO - [diffusion][Epoch 10927] Epoch 10928/12000
2024-11-05 03:02:52,224 - INFO - [diffusion][Epoch 10927] diffusion training Loss: 0.07020465843379498
2024-11-05 03:02:52,226 - INFO - [diffusion][Epoch 10927] diffusion learning rate: 0.001
2024-11-05 03:02:52,227 - INFO - [diffusion][Epoch 10927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:52,229 - INFO - [diffusion][Epoch 10928] Epoch 10929/12000
2024-11-05 03:02:55,727 - INFO - [diffusion][Epoch 10928] diffusion training Loss: 0.06201335694640875
2024-11-05 03:02:55,729 - INFO - [diffusion][Epoch 10928] diffusion learning rate: 0.001
2024-11-05 03:02:55,731 - INFO - [diffusion][Epoch 10928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:55,732 - INFO - [diffusion][Epoch 10929] Epoch 10930/12000
2024-11-05 03:02:58,509 - INFO - [diffusion][Epoch 10929] diffusion training Loss: 0.06582765001803637
2024-11-05 03:02:58,511 - INFO - [diffusion][Epoch 10929] diffusion learning rate: 0.001
2024-11-05 03:02:58,513 - INFO - [diffusion][Epoch 10929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:58,514 - INFO - [diffusion][Epoch 10930] Epoch 10931/12000
2024-11-05 03:03:01,447 - INFO - [diffusion][Epoch 10930] diffusion training Loss: 0.06567868404090405
2024-11-05 03:03:01,449 - INFO - [diffusion][Epoch 10930] diffusion learning rate: 0.001
2024-11-05 03:03:01,451 - INFO - [diffusion][Epoch 10930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:01,452 - INFO - [diffusion][Epoch 10931] Epoch 10932/12000
2024-11-05 03:03:04,972 - INFO - [diffusion][Epoch 10931] diffusion training Loss: 0.06110359728336334
2024-11-05 03:03:04,974 - INFO - [diffusion][Epoch 10931] diffusion learning rate: 0.001
2024-11-05 03:03:04,976 - INFO - [diffusion][Epoch 10931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:04,977 - INFO - [diffusion][Epoch 10932] Epoch 10933/12000
2024-11-05 03:03:08,408 - INFO - [diffusion][Epoch 10932] diffusion training Loss: 0.0662486832588911
2024-11-05 03:03:08,410 - INFO - [diffusion][Epoch 10932] diffusion learning rate: 0.001
2024-11-05 03:03:08,412 - INFO - [diffusion][Epoch 10932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:08,413 - INFO - [diffusion][Epoch 10933] Epoch 10934/12000
2024-11-05 03:03:11,480 - INFO - [diffusion][Epoch 10933] diffusion training Loss: 0.0631565498188138
2024-11-05 03:03:11,482 - INFO - [diffusion][Epoch 10933] diffusion learning rate: 0.001
2024-11-05 03:03:11,484 - INFO - [diffusion][Epoch 10933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:11,485 - INFO - [diffusion][Epoch 10934] Epoch 10935/12000
2024-11-05 03:03:14,744 - INFO - [diffusion][Epoch 10934] diffusion training Loss: 0.06759948655962944
2024-11-05 03:03:14,746 - INFO - [diffusion][Epoch 10934] diffusion learning rate: 0.001
2024-11-05 03:03:14,748 - INFO - [diffusion][Epoch 10934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:14,749 - INFO - [diffusion][Epoch 10935] Epoch 10936/12000
2024-11-05 03:03:18,010 - INFO - [diffusion][Epoch 10935] diffusion training Loss: 0.06014293432235718
2024-11-05 03:03:18,012 - INFO - [diffusion][Epoch 10935] diffusion learning rate: 0.001
2024-11-05 03:03:18,014 - INFO - [diffusion][Epoch 10935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:18,015 - INFO - [diffusion][Epoch 10936] Epoch 10937/12000
2024-11-05 03:03:21,643 - INFO - [diffusion][Epoch 10936] diffusion training Loss: 0.06744984164834023
2024-11-05 03:03:21,645 - INFO - [diffusion][Epoch 10936] diffusion learning rate: 0.001
2024-11-05 03:03:21,647 - INFO - [diffusion][Epoch 10936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:21,648 - INFO - [diffusion][Epoch 10937] Epoch 10938/12000
2024-11-05 03:03:24,932 - INFO - [diffusion][Epoch 10937] diffusion training Loss: 0.06208243779838085
2024-11-05 03:03:24,934 - INFO - [diffusion][Epoch 10937] diffusion learning rate: 0.001
2024-11-05 03:03:24,936 - INFO - [diffusion][Epoch 10937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:24,937 - INFO - [diffusion][Epoch 10938] Epoch 10939/12000
2024-11-05 03:03:28,123 - INFO - [diffusion][Epoch 10938] diffusion training Loss: 0.06541378330439329
2024-11-05 03:03:28,125 - INFO - [diffusion][Epoch 10938] diffusion learning rate: 0.001
2024-11-05 03:03:28,127 - INFO - [diffusion][Epoch 10938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:28,129 - INFO - [diffusion][Epoch 10939] Epoch 10940/12000
2024-11-05 03:03:31,271 - INFO - [diffusion][Epoch 10939] diffusion training Loss: 0.06695341970771551
2024-11-05 03:03:31,273 - INFO - [diffusion][Epoch 10939] diffusion learning rate: 0.001
2024-11-05 03:03:31,274 - INFO - [diffusion][Epoch 10939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:31,276 - INFO - [diffusion][Epoch 10940] Epoch 10941/12000
2024-11-05 03:03:34,678 - INFO - [diffusion][Epoch 10940] diffusion training Loss: 0.06681105494499207
2024-11-05 03:03:34,680 - INFO - [diffusion][Epoch 10940] diffusion learning rate: 0.001
2024-11-05 03:03:34,682 - INFO - [diffusion][Epoch 10940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:34,683 - INFO - [diffusion][Epoch 10941] Epoch 10942/12000
2024-11-05 03:03:38,492 - INFO - [diffusion][Epoch 10941] diffusion training Loss: 0.06483448669314384
2024-11-05 03:03:38,495 - INFO - [diffusion][Epoch 10941] diffusion learning rate: 0.001
2024-11-05 03:03:38,497 - INFO - [diffusion][Epoch 10941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:38,498 - INFO - [diffusion][Epoch 10942] Epoch 10943/12000
2024-11-05 03:03:42,775 - INFO - [diffusion][Epoch 10942] diffusion training Loss: 0.06361926533281803
2024-11-05 03:03:42,777 - INFO - [diffusion][Epoch 10942] diffusion learning rate: 0.001
2024-11-05 03:03:42,779 - INFO - [diffusion][Epoch 10942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:42,780 - INFO - [diffusion][Epoch 10943] Epoch 10944/12000
2024-11-05 03:03:46,257 - INFO - [diffusion][Epoch 10943] diffusion training Loss: 0.06840099394321442
2024-11-05 03:03:46,260 - INFO - [diffusion][Epoch 10943] diffusion learning rate: 0.001
2024-11-05 03:03:46,261 - INFO - [diffusion][Epoch 10943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:46,263 - INFO - [diffusion][Epoch 10944] Epoch 10945/12000
2024-11-05 03:03:49,377 - INFO - [diffusion][Epoch 10944] diffusion training Loss: 0.06118337903171778
2024-11-05 03:03:49,378 - INFO - [diffusion][Epoch 10944] diffusion learning rate: 0.001
2024-11-05 03:03:49,380 - INFO - [diffusion][Epoch 10944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:49,381 - INFO - [diffusion][Epoch 10945] Epoch 10946/12000
2024-11-05 03:03:52,578 - INFO - [diffusion][Epoch 10945] diffusion training Loss: 0.05999132804572582
2024-11-05 03:03:52,580 - INFO - [diffusion][Epoch 10945] diffusion learning rate: 0.001
2024-11-05 03:03:52,582 - INFO - [diffusion][Epoch 10945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:52,583 - INFO - [diffusion][Epoch 10946] Epoch 10947/12000
2024-11-05 03:03:55,752 - INFO - [diffusion][Epoch 10946] diffusion training Loss: 0.06693518627434969
2024-11-05 03:03:55,754 - INFO - [diffusion][Epoch 10946] diffusion learning rate: 0.001
2024-11-05 03:03:55,756 - INFO - [diffusion][Epoch 10946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:55,757 - INFO - [diffusion][Epoch 10947] Epoch 10948/12000
2024-11-05 03:03:59,334 - INFO - [diffusion][Epoch 10947] diffusion training Loss: 0.05985707603394985
2024-11-05 03:03:59,336 - INFO - [diffusion][Epoch 10947] diffusion learning rate: 0.001
2024-11-05 03:03:59,338 - INFO - [diffusion][Epoch 10947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:59,339 - INFO - [diffusion][Epoch 10948] Epoch 10949/12000
2024-11-05 03:04:02,886 - INFO - [diffusion][Epoch 10948] diffusion training Loss: 0.06597121246159077
2024-11-05 03:04:02,887 - INFO - [diffusion][Epoch 10948] diffusion learning rate: 0.001
2024-11-05 03:04:02,929 - INFO - [diffusion][Epoch 10948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:02,931 - INFO - [diffusion][Epoch 10949] Epoch 10950/12000
2024-11-05 03:04:06,078 - INFO - [diffusion][Epoch 10949] diffusion training Loss: 0.060892642475664616
2024-11-05 03:04:06,080 - INFO - [diffusion][Epoch 10949] diffusion learning rate: 0.001
2024-11-05 03:04:06,083 - INFO - [diffusion][Epoch 10949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:06,084 - INFO - [diffusion][Epoch 10950] Epoch 10951/12000
2024-11-05 03:04:09,166 - INFO - [diffusion][Epoch 10950] diffusion training Loss: 0.06833069771528244
2024-11-05 03:04:09,168 - INFO - [diffusion][Epoch 10950] diffusion learning rate: 0.001
2024-11-05 03:04:09,170 - INFO - [diffusion][Epoch 10950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:09,172 - INFO - [diffusion][Epoch 10951] Epoch 10952/12000
2024-11-05 03:04:12,318 - INFO - [diffusion][Epoch 10951] diffusion training Loss: 0.06250622030347586
2024-11-05 03:04:12,320 - INFO - [diffusion][Epoch 10951] diffusion learning rate: 0.001
2024-11-05 03:04:12,322 - INFO - [diffusion][Epoch 10951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:12,323 - INFO - [diffusion][Epoch 10952] Epoch 10953/12000
2024-11-05 03:04:15,899 - INFO - [diffusion][Epoch 10952] diffusion training Loss: 0.05894895363599062
2024-11-05 03:04:15,901 - INFO - [diffusion][Epoch 10952] diffusion learning rate: 0.001
2024-11-05 03:04:15,903 - INFO - [diffusion][Epoch 10952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:15,904 - INFO - [diffusion][Epoch 10953] Epoch 10954/12000
2024-11-05 03:04:19,429 - INFO - [diffusion][Epoch 10953] diffusion training Loss: 0.06275647319853306
2024-11-05 03:04:19,432 - INFO - [diffusion][Epoch 10953] diffusion learning rate: 0.001
2024-11-05 03:04:19,434 - INFO - [diffusion][Epoch 10953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:19,435 - INFO - [diffusion][Epoch 10954] Epoch 10955/12000
2024-11-05 03:04:22,596 - INFO - [diffusion][Epoch 10954] diffusion training Loss: 0.05793273635208607
2024-11-05 03:04:22,599 - INFO - [diffusion][Epoch 10954] diffusion learning rate: 0.001
2024-11-05 03:04:22,600 - INFO - [diffusion][Epoch 10954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:22,602 - INFO - [diffusion][Epoch 10955] Epoch 10956/12000
2024-11-05 03:04:25,776 - INFO - [diffusion][Epoch 10955] diffusion training Loss: 0.06448363792151213
2024-11-05 03:04:25,778 - INFO - [diffusion][Epoch 10955] diffusion learning rate: 0.001
2024-11-05 03:04:25,780 - INFO - [diffusion][Epoch 10955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:25,781 - INFO - [diffusion][Epoch 10956] Epoch 10957/12000
2024-11-05 03:04:28,966 - INFO - [diffusion][Epoch 10956] diffusion training Loss: 0.06905450485646725
2024-11-05 03:04:28,968 - INFO - [diffusion][Epoch 10956] diffusion learning rate: 0.001
2024-11-05 03:04:28,970 - INFO - [diffusion][Epoch 10956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:28,971 - INFO - [diffusion][Epoch 10957] Epoch 10958/12000
2024-11-05 03:04:32,608 - INFO - [diffusion][Epoch 10957] diffusion training Loss: 0.06455406174063683
2024-11-05 03:04:32,610 - INFO - [diffusion][Epoch 10957] diffusion learning rate: 0.001
2024-11-05 03:04:32,612 - INFO - [diffusion][Epoch 10957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:32,613 - INFO - [diffusion][Epoch 10958] Epoch 10959/12000
2024-11-05 03:04:35,872 - INFO - [diffusion][Epoch 10958] diffusion training Loss: 0.06399838998913765
2024-11-05 03:04:35,874 - INFO - [diffusion][Epoch 10958] diffusion learning rate: 0.001
2024-11-05 03:04:35,875 - INFO - [diffusion][Epoch 10958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:35,877 - INFO - [diffusion][Epoch 10959] Epoch 10960/12000
2024-11-05 03:04:39,058 - INFO - [diffusion][Epoch 10959] diffusion training Loss: 0.0625728266313672
2024-11-05 03:04:39,060 - INFO - [diffusion][Epoch 10959] diffusion learning rate: 0.001
2024-11-05 03:04:39,062 - INFO - [diffusion][Epoch 10959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:39,064 - INFO - [diffusion][Epoch 10960] Epoch 10961/12000
2024-11-05 03:04:42,187 - INFO - [diffusion][Epoch 10960] diffusion training Loss: 0.06279691960662603
2024-11-05 03:04:42,189 - INFO - [diffusion][Epoch 10960] diffusion learning rate: 0.001
2024-11-05 03:04:42,190 - INFO - [diffusion][Epoch 10960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:42,192 - INFO - [diffusion][Epoch 10961] Epoch 10962/12000
2024-11-05 03:04:45,650 - INFO - [diffusion][Epoch 10961] diffusion training Loss: 0.06242356542497873
2024-11-05 03:04:45,652 - INFO - [diffusion][Epoch 10961] diffusion learning rate: 0.001
2024-11-05 03:04:45,654 - INFO - [diffusion][Epoch 10961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:45,655 - INFO - [diffusion][Epoch 10962] Epoch 10963/12000
2024-11-05 03:04:49,536 - INFO - [diffusion][Epoch 10962] diffusion training Loss: 0.05940417945384979
2024-11-05 03:04:49,538 - INFO - [diffusion][Epoch 10962] diffusion learning rate: 0.001
2024-11-05 03:04:49,540 - INFO - [diffusion][Epoch 10962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:49,541 - INFO - [diffusion][Epoch 10963] Epoch 10964/12000
2024-11-05 03:04:53,118 - INFO - [diffusion][Epoch 10963] diffusion training Loss: 0.06772534549236298
2024-11-05 03:04:53,119 - INFO - [diffusion][Epoch 10963] diffusion learning rate: 0.001
2024-11-05 03:04:53,121 - INFO - [diffusion][Epoch 10963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:53,122 - INFO - [diffusion][Epoch 10964] Epoch 10965/12000
2024-11-05 03:04:56,288 - INFO - [diffusion][Epoch 10964] diffusion training Loss: 0.06268629245460033
2024-11-05 03:04:56,290 - INFO - [diffusion][Epoch 10964] diffusion learning rate: 0.001
2024-11-05 03:04:56,292 - INFO - [diffusion][Epoch 10964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:56,293 - INFO - [diffusion][Epoch 10965] Epoch 10966/12000
2024-11-05 03:04:59,379 - INFO - [diffusion][Epoch 10965] diffusion training Loss: 0.06275355629622936
2024-11-05 03:04:59,381 - INFO - [diffusion][Epoch 10965] diffusion learning rate: 0.001
2024-11-05 03:04:59,383 - INFO - [diffusion][Epoch 10965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:59,384 - INFO - [diffusion][Epoch 10966] Epoch 10967/12000
2024-11-05 03:05:02,446 - INFO - [diffusion][Epoch 10966] diffusion training Loss: 0.05891922488808632
2024-11-05 03:05:02,448 - INFO - [diffusion][Epoch 10966] diffusion learning rate: 0.001
2024-11-05 03:05:02,450 - INFO - [diffusion][Epoch 10966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:02,451 - INFO - [diffusion][Epoch 10967] Epoch 10968/12000
2024-11-05 03:05:05,995 - INFO - [diffusion][Epoch 10967] diffusion training Loss: 0.06342435348778963
2024-11-05 03:05:05,996 - INFO - [diffusion][Epoch 10967] diffusion learning rate: 0.001
2024-11-05 03:05:05,998 - INFO - [diffusion][Epoch 10967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:06,000 - INFO - [diffusion][Epoch 10968] Epoch 10969/12000
2024-11-05 03:05:09,637 - INFO - [diffusion][Epoch 10968] diffusion training Loss: 0.06592361815273762
2024-11-05 03:05:09,639 - INFO - [diffusion][Epoch 10968] diffusion learning rate: 0.001
2024-11-05 03:05:09,641 - INFO - [diffusion][Epoch 10968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:09,642 - INFO - [diffusion][Epoch 10969] Epoch 10970/12000
2024-11-05 03:05:12,766 - INFO - [diffusion][Epoch 10969] diffusion training Loss: 0.06253420002758503
2024-11-05 03:05:12,769 - INFO - [diffusion][Epoch 10969] diffusion learning rate: 0.001
2024-11-05 03:05:12,770 - INFO - [diffusion][Epoch 10969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:12,771 - INFO - [diffusion][Epoch 10970] Epoch 10971/12000
2024-11-05 03:05:15,865 - INFO - [diffusion][Epoch 10970] diffusion training Loss: 0.07029891014099121
2024-11-05 03:05:15,867 - INFO - [diffusion][Epoch 10970] diffusion learning rate: 0.001
2024-11-05 03:05:15,869 - INFO - [diffusion][Epoch 10970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:15,870 - INFO - [diffusion][Epoch 10971] Epoch 10972/12000
2024-11-05 03:05:19,013 - INFO - [diffusion][Epoch 10971] diffusion training Loss: 0.06538354605436325
2024-11-05 03:05:19,015 - INFO - [diffusion][Epoch 10971] diffusion learning rate: 0.001
2024-11-05 03:05:19,018 - INFO - [diffusion][Epoch 10971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:19,019 - INFO - [diffusion][Epoch 10972] Epoch 10973/12000
2024-11-05 03:05:22,613 - INFO - [diffusion][Epoch 10972] diffusion training Loss: 0.06548574566841125
2024-11-05 03:05:22,615 - INFO - [diffusion][Epoch 10972] diffusion learning rate: 0.001
2024-11-05 03:05:22,617 - INFO - [diffusion][Epoch 10972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:22,618 - INFO - [diffusion][Epoch 10973] Epoch 10974/12000
2024-11-05 03:05:26,064 - INFO - [diffusion][Epoch 10973] diffusion training Loss: 0.06788258999586105
2024-11-05 03:05:26,067 - INFO - [diffusion][Epoch 10973] diffusion learning rate: 0.001
2024-11-05 03:05:26,068 - INFO - [diffusion][Epoch 10973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:26,070 - INFO - [diffusion][Epoch 10974] Epoch 10975/12000
2024-11-05 03:05:29,232 - INFO - [diffusion][Epoch 10974] diffusion training Loss: 0.06803501024842262
2024-11-05 03:05:29,235 - INFO - [diffusion][Epoch 10974] diffusion learning rate: 0.001
2024-11-05 03:05:29,237 - INFO - [diffusion][Epoch 10974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:29,238 - INFO - [diffusion][Epoch 10975] Epoch 10976/12000
2024-11-05 03:05:32,256 - INFO - [diffusion][Epoch 10975] diffusion training Loss: 0.05964907631278038
2024-11-05 03:05:32,258 - INFO - [diffusion][Epoch 10975] diffusion learning rate: 0.001
2024-11-05 03:05:32,259 - INFO - [diffusion][Epoch 10975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:32,260 - INFO - [diffusion][Epoch 10976] Epoch 10977/12000
2024-11-05 03:05:35,480 - INFO - [diffusion][Epoch 10976] diffusion training Loss: 0.0670115165412426
2024-11-05 03:05:35,481 - INFO - [diffusion][Epoch 10976] diffusion learning rate: 0.001
2024-11-05 03:05:35,483 - INFO - [diffusion][Epoch 10976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:35,485 - INFO - [diffusion][Epoch 10977] Epoch 10978/12000
2024-11-05 03:05:39,180 - INFO - [diffusion][Epoch 10977] diffusion training Loss: 0.058402858674526215
2024-11-05 03:05:39,181 - INFO - [diffusion][Epoch 10977] diffusion learning rate: 0.001
2024-11-05 03:05:39,183 - INFO - [diffusion][Epoch 10977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:39,185 - INFO - [diffusion][Epoch 10978] Epoch 10979/12000
2024-11-05 03:05:42,551 - INFO - [diffusion][Epoch 10978] diffusion training Loss: 0.0645005451515317
2024-11-05 03:05:42,553 - INFO - [diffusion][Epoch 10978] diffusion learning rate: 0.001
2024-11-05 03:05:42,554 - INFO - [diffusion][Epoch 10978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:42,555 - INFO - [diffusion][Epoch 10979] Epoch 10980/12000
2024-11-05 03:05:45,671 - INFO - [diffusion][Epoch 10979] diffusion training Loss: 0.06413232535123825
2024-11-05 03:05:45,674 - INFO - [diffusion][Epoch 10979] diffusion learning rate: 0.001
2024-11-05 03:05:45,676 - INFO - [diffusion][Epoch 10979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:45,677 - INFO - [diffusion][Epoch 10980] Epoch 10981/12000
2024-11-05 03:05:48,652 - INFO - [diffusion][Epoch 10980] diffusion training Loss: 0.0650758147239685
2024-11-05 03:05:48,653 - INFO - [diffusion][Epoch 10980] diffusion learning rate: 0.001
2024-11-05 03:05:48,655 - INFO - [diffusion][Epoch 10980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:48,656 - INFO - [diffusion][Epoch 10981] Epoch 10982/12000
2024-11-05 03:05:52,202 - INFO - [diffusion][Epoch 10981] diffusion training Loss: 0.056343465112149715
2024-11-05 03:05:52,204 - INFO - [diffusion][Epoch 10981] diffusion learning rate: 0.001
2024-11-05 03:05:52,206 - INFO - [diffusion][Epoch 10981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:52,207 - INFO - [diffusion][Epoch 10982] Epoch 10983/12000
2024-11-05 03:05:55,636 - INFO - [diffusion][Epoch 10982] diffusion training Loss: 0.06463988497853279
2024-11-05 03:05:55,638 - INFO - [diffusion][Epoch 10982] diffusion learning rate: 0.001
2024-11-05 03:05:55,640 - INFO - [diffusion][Epoch 10982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:55,641 - INFO - [diffusion][Epoch 10983] Epoch 10984/12000
2024-11-05 03:05:59,408 - INFO - [diffusion][Epoch 10983] diffusion training Loss: 0.06465696170926094
2024-11-05 03:05:59,410 - INFO - [diffusion][Epoch 10983] diffusion learning rate: 0.001
2024-11-05 03:05:59,411 - INFO - [diffusion][Epoch 10983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:59,413 - INFO - [diffusion][Epoch 10984] Epoch 10985/12000
2024-11-05 03:06:02,481 - INFO - [diffusion][Epoch 10984] diffusion training Loss: 0.06223226711153984
2024-11-05 03:06:02,483 - INFO - [diffusion][Epoch 10984] diffusion learning rate: 0.001
2024-11-05 03:06:02,485 - INFO - [diffusion][Epoch 10984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:02,486 - INFO - [diffusion][Epoch 10985] Epoch 10986/12000
2024-11-05 03:06:05,598 - INFO - [diffusion][Epoch 10985] diffusion training Loss: 0.061604407615959644
2024-11-05 03:06:05,600 - INFO - [diffusion][Epoch 10985] diffusion learning rate: 0.001
2024-11-05 03:06:05,602 - INFO - [diffusion][Epoch 10985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:05,603 - INFO - [diffusion][Epoch 10986] Epoch 10987/12000
2024-11-05 03:06:09,159 - INFO - [diffusion][Epoch 10986] diffusion training Loss: 0.06601571198552847
2024-11-05 03:06:09,161 - INFO - [diffusion][Epoch 10986] diffusion learning rate: 0.001
2024-11-05 03:06:09,162 - INFO - [diffusion][Epoch 10986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:09,164 - INFO - [diffusion][Epoch 10987] Epoch 10988/12000
2024-11-05 03:06:12,695 - INFO - [diffusion][Epoch 10987] diffusion training Loss: 0.06490253563970327
2024-11-05 03:06:12,697 - INFO - [diffusion][Epoch 10987] diffusion learning rate: 0.001
2024-11-05 03:06:12,699 - INFO - [diffusion][Epoch 10987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:12,700 - INFO - [diffusion][Epoch 10988] Epoch 10989/12000
2024-11-05 03:06:15,690 - INFO - [diffusion][Epoch 10988] diffusion training Loss: 0.07134569808840752
2024-11-05 03:06:15,692 - INFO - [diffusion][Epoch 10988] diffusion learning rate: 0.001
2024-11-05 03:06:15,694 - INFO - [diffusion][Epoch 10988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:15,695 - INFO - [diffusion][Epoch 10989] Epoch 10990/12000
2024-11-05 03:06:18,710 - INFO - [diffusion][Epoch 10989] diffusion training Loss: 0.06180464942008257
2024-11-05 03:06:18,713 - INFO - [diffusion][Epoch 10989] diffusion learning rate: 0.001
2024-11-05 03:06:18,714 - INFO - [diffusion][Epoch 10989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:18,716 - INFO - [diffusion][Epoch 10990] Epoch 10991/12000
2024-11-05 03:06:22,189 - INFO - [diffusion][Epoch 10990] diffusion training Loss: 0.06006773468106985
2024-11-05 03:06:22,192 - INFO - [diffusion][Epoch 10990] diffusion learning rate: 0.001
2024-11-05 03:06:22,193 - INFO - [diffusion][Epoch 10990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:22,195 - INFO - [diffusion][Epoch 10991] Epoch 10992/12000
2024-11-05 03:06:25,735 - INFO - [diffusion][Epoch 10991] diffusion training Loss: 0.06219383515417576
2024-11-05 03:06:25,737 - INFO - [diffusion][Epoch 10991] diffusion learning rate: 0.001
2024-11-05 03:06:25,738 - INFO - [diffusion][Epoch 10991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:25,740 - INFO - [diffusion][Epoch 10992] Epoch 10993/12000
2024-11-05 03:06:28,848 - INFO - [diffusion][Epoch 10992] diffusion training Loss: 0.06343240104615688
2024-11-05 03:06:28,850 - INFO - [diffusion][Epoch 10992] diffusion learning rate: 0.001
2024-11-05 03:06:28,852 - INFO - [diffusion][Epoch 10992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:28,853 - INFO - [diffusion][Epoch 10993] Epoch 10994/12000
2024-11-05 03:06:31,885 - INFO - [diffusion][Epoch 10993] diffusion training Loss: 0.06902038678526878
2024-11-05 03:06:31,887 - INFO - [diffusion][Epoch 10993] diffusion learning rate: 0.001
2024-11-05 03:06:31,889 - INFO - [diffusion][Epoch 10993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:31,890 - INFO - [diffusion][Epoch 10994] Epoch 10995/12000
2024-11-05 03:06:35,185 - INFO - [diffusion][Epoch 10994] diffusion training Loss: 0.061850426718592644
2024-11-05 03:06:35,187 - INFO - [diffusion][Epoch 10994] diffusion learning rate: 0.001
2024-11-05 03:06:35,189 - INFO - [diffusion][Epoch 10994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:35,190 - INFO - [diffusion][Epoch 10995] Epoch 10996/12000
2024-11-05 03:06:38,806 - INFO - [diffusion][Epoch 10995] diffusion training Loss: 0.0667293593287468
2024-11-05 03:06:38,808 - INFO - [diffusion][Epoch 10995] diffusion learning rate: 0.001
2024-11-05 03:06:38,810 - INFO - [diffusion][Epoch 10995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:38,811 - INFO - [diffusion][Epoch 10996] Epoch 10997/12000
2024-11-05 03:06:42,108 - INFO - [diffusion][Epoch 10996] diffusion training Loss: 0.05714034382253885
2024-11-05 03:06:42,110 - INFO - [diffusion][Epoch 10996] diffusion learning rate: 0.001
2024-11-05 03:06:42,112 - INFO - [diffusion][Epoch 10996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:42,113 - INFO - [diffusion][Epoch 10997] Epoch 10998/12000
2024-11-05 03:06:45,292 - INFO - [diffusion][Epoch 10997] diffusion training Loss: 0.0687527097761631
2024-11-05 03:06:45,295 - INFO - [diffusion][Epoch 10997] diffusion learning rate: 0.001
2024-11-05 03:06:45,297 - INFO - [diffusion][Epoch 10997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:45,298 - INFO - [diffusion][Epoch 10998] Epoch 10999/12000
2024-11-05 03:06:48,411 - INFO - [diffusion][Epoch 10998] diffusion training Loss: 0.06751152966171503
2024-11-05 03:06:48,413 - INFO - [diffusion][Epoch 10998] diffusion learning rate: 0.001
2024-11-05 03:06:48,415 - INFO - [diffusion][Epoch 10998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:48,416 - INFO - [diffusion][Epoch 10999] Epoch 11000/12000
2024-11-05 03:06:51,920 - INFO - [diffusion][Epoch 10999] diffusion training Loss: 0.0649598240852356
2024-11-05 03:06:51,922 - INFO - [diffusion][Epoch 10999] diffusion learning rate: 0.001
2024-11-05 03:06:52,006 - INFO - [diffusion][Epoch 10999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:52,008 - INFO - [diffusion][Epoch 11000] Epoch 11001/12000
2024-11-05 03:06:55,590 - INFO - [diffusion][Epoch 11000] diffusion training Loss: 0.06569263059645891
2024-11-05 03:06:55,592 - INFO - [diffusion][Epoch 11000] diffusion learning rate: 0.001
2024-11-05 03:06:55,593 - INFO - [diffusion][Epoch 11000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:55,595 - INFO - [diffusion][Epoch 11001] Epoch 11002/12000
2024-11-05 03:06:58,705 - INFO - [diffusion][Epoch 11001] diffusion training Loss: 0.05664412584155798
2024-11-05 03:06:58,707 - INFO - [diffusion][Epoch 11001] diffusion learning rate: 0.001
2024-11-05 03:06:58,708 - INFO - [diffusion][Epoch 11001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:58,710 - INFO - [diffusion][Epoch 11002] Epoch 11003/12000
2024-11-05 03:07:02,032 - INFO - [diffusion][Epoch 11002] diffusion training Loss: 0.06320632994174957
2024-11-05 03:07:02,035 - INFO - [diffusion][Epoch 11002] diffusion learning rate: 0.001
2024-11-05 03:07:02,036 - INFO - [diffusion][Epoch 11002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:02,038 - INFO - [diffusion][Epoch 11003] Epoch 11004/12000
2024-11-05 03:07:05,196 - INFO - [diffusion][Epoch 11003] diffusion training Loss: 0.06544041819870472
2024-11-05 03:07:05,199 - INFO - [diffusion][Epoch 11003] diffusion learning rate: 0.001
2024-11-05 03:07:05,201 - INFO - [diffusion][Epoch 11003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:05,202 - INFO - [diffusion][Epoch 11004] Epoch 11005/12000
2024-11-05 03:07:08,696 - INFO - [diffusion][Epoch 11004] diffusion training Loss: 0.06383531633764505
2024-11-05 03:07:08,698 - INFO - [diffusion][Epoch 11004] diffusion learning rate: 0.001
2024-11-05 03:07:08,700 - INFO - [diffusion][Epoch 11004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:08,701 - INFO - [diffusion][Epoch 11005] Epoch 11006/12000
2024-11-05 03:07:12,229 - INFO - [diffusion][Epoch 11005] diffusion training Loss: 0.05750200990587473
2024-11-05 03:07:12,504 - INFO - [diffusion][Epoch 11005] diffusion learning rate: 0.001
2024-11-05 03:07:12,506 - INFO - [diffusion][Epoch 11005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:12,507 - INFO - [diffusion][Epoch 11006] Epoch 11007/12000
2024-11-05 03:07:15,765 - INFO - [diffusion][Epoch 11006] diffusion training Loss: 0.06358671840280294
2024-11-05 03:07:15,767 - INFO - [diffusion][Epoch 11006] diffusion learning rate: 0.001
2024-11-05 03:07:15,769 - INFO - [diffusion][Epoch 11006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:15,770 - INFO - [diffusion][Epoch 11007] Epoch 11008/12000
2024-11-05 03:07:18,436 - INFO - [diffusion][Epoch 11007] diffusion training Loss: 0.06192287243902683
2024-11-05 03:07:18,438 - INFO - [diffusion][Epoch 11007] diffusion learning rate: 0.001
2024-11-05 03:07:18,440 - INFO - [diffusion][Epoch 11007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:18,441 - INFO - [diffusion][Epoch 11008] Epoch 11009/12000
2024-11-05 03:07:21,982 - INFO - [diffusion][Epoch 11008] diffusion training Loss: 0.058811331167817116
2024-11-05 03:07:21,983 - INFO - [diffusion][Epoch 11008] diffusion learning rate: 0.001
2024-11-05 03:07:21,985 - INFO - [diffusion][Epoch 11008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:21,986 - INFO - [diffusion][Epoch 11009] Epoch 11010/12000
2024-11-05 03:07:25,531 - INFO - [diffusion][Epoch 11009] diffusion training Loss: 0.06107959523797035
2024-11-05 03:07:25,533 - INFO - [diffusion][Epoch 11009] diffusion learning rate: 0.001
2024-11-05 03:07:25,535 - INFO - [diffusion][Epoch 11009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:25,536 - INFO - [diffusion][Epoch 11010] Epoch 11011/12000
2024-11-05 03:07:28,653 - INFO - [diffusion][Epoch 11010] diffusion training Loss: 0.06577239837497473
2024-11-05 03:07:28,655 - INFO - [diffusion][Epoch 11010] diffusion learning rate: 0.001
2024-11-05 03:07:28,656 - INFO - [diffusion][Epoch 11010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:28,657 - INFO - [diffusion][Epoch 11011] Epoch 11012/12000
2024-11-05 03:07:31,556 - INFO - [diffusion][Epoch 11011] diffusion training Loss: 0.06480918359011412
2024-11-05 03:07:31,558 - INFO - [diffusion][Epoch 11011] diffusion learning rate: 0.001
2024-11-05 03:07:31,561 - INFO - [diffusion][Epoch 11011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:31,563 - INFO - [diffusion][Epoch 11012] Epoch 11013/12000
2024-11-05 03:07:34,891 - INFO - [diffusion][Epoch 11012] diffusion training Loss: 0.0642706910148263
2024-11-05 03:07:34,894 - INFO - [diffusion][Epoch 11012] diffusion learning rate: 0.001
2024-11-05 03:07:34,895 - INFO - [diffusion][Epoch 11012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:34,897 - INFO - [diffusion][Epoch 11013] Epoch 11014/12000
2024-11-05 03:07:38,569 - INFO - [diffusion][Epoch 11013] diffusion training Loss: 0.060820343904197216
2024-11-05 03:07:38,571 - INFO - [diffusion][Epoch 11013] diffusion learning rate: 0.001
2024-11-05 03:07:38,573 - INFO - [diffusion][Epoch 11013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:38,575 - INFO - [diffusion][Epoch 11014] Epoch 11015/12000
2024-11-05 03:07:42,004 - INFO - [diffusion][Epoch 11014] diffusion training Loss: 0.06835869792848825
2024-11-05 03:07:42,006 - INFO - [diffusion][Epoch 11014] diffusion learning rate: 0.001
2024-11-05 03:07:42,007 - INFO - [diffusion][Epoch 11014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:42,009 - INFO - [diffusion][Epoch 11015] Epoch 11016/12000
2024-11-05 03:07:45,124 - INFO - [diffusion][Epoch 11015] diffusion training Loss: 0.06076471880078316
2024-11-05 03:07:45,126 - INFO - [diffusion][Epoch 11015] diffusion learning rate: 0.001
2024-11-05 03:07:45,127 - INFO - [diffusion][Epoch 11015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:45,129 - INFO - [diffusion][Epoch 11016] Epoch 11017/12000
2024-11-05 03:07:48,217 - INFO - [diffusion][Epoch 11016] diffusion training Loss: 0.06820422038435936
2024-11-05 03:07:48,219 - INFO - [diffusion][Epoch 11016] diffusion learning rate: 0.001
2024-11-05 03:07:48,221 - INFO - [diffusion][Epoch 11016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:48,222 - INFO - [diffusion][Epoch 11017] Epoch 11018/12000
2024-11-05 03:07:51,740 - INFO - [diffusion][Epoch 11017] diffusion training Loss: 0.0605329554527998
2024-11-05 03:07:51,742 - INFO - [diffusion][Epoch 11017] diffusion learning rate: 0.001
2024-11-05 03:07:51,744 - INFO - [diffusion][Epoch 11017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:51,745 - INFO - [diffusion][Epoch 11018] Epoch 11019/12000
2024-11-05 03:07:55,378 - INFO - [diffusion][Epoch 11018] diffusion training Loss: 0.06664406973868608
2024-11-05 03:07:55,380 - INFO - [diffusion][Epoch 11018] diffusion learning rate: 0.001
2024-11-05 03:07:55,381 - INFO - [diffusion][Epoch 11018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:55,383 - INFO - [diffusion][Epoch 11019] Epoch 11020/12000
2024-11-05 03:07:58,507 - INFO - [diffusion][Epoch 11019] diffusion training Loss: 0.06251723878085613
2024-11-05 03:07:58,509 - INFO - [diffusion][Epoch 11019] diffusion learning rate: 0.001
2024-11-05 03:07:58,511 - INFO - [diffusion][Epoch 11019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:58,512 - INFO - [diffusion][Epoch 11020] Epoch 11021/12000
2024-11-05 03:08:01,614 - INFO - [diffusion][Epoch 11020] diffusion training Loss: 0.06218299828469753
2024-11-05 03:08:01,616 - INFO - [diffusion][Epoch 11020] diffusion learning rate: 0.001
2024-11-05 03:08:01,618 - INFO - [diffusion][Epoch 11020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:01,619 - INFO - [diffusion][Epoch 11021] Epoch 11022/12000
2024-11-05 03:08:04,710 - INFO - [diffusion][Epoch 11021] diffusion training Loss: 0.06180047523230314
2024-11-05 03:08:04,712 - INFO - [diffusion][Epoch 11021] diffusion learning rate: 0.001
2024-11-05 03:08:04,714 - INFO - [diffusion][Epoch 11021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:04,715 - INFO - [diffusion][Epoch 11022] Epoch 11023/12000
2024-11-05 03:08:08,310 - INFO - [diffusion][Epoch 11022] diffusion training Loss: 0.06268221139907837
2024-11-05 03:08:08,312 - INFO - [diffusion][Epoch 11022] diffusion learning rate: 0.001
2024-11-05 03:08:08,314 - INFO - [diffusion][Epoch 11022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:08,315 - INFO - [diffusion][Epoch 11023] Epoch 11024/12000
2024-11-05 03:08:12,163 - INFO - [diffusion][Epoch 11023] diffusion training Loss: 0.058288722299039364
2024-11-05 03:08:12,166 - INFO - [diffusion][Epoch 11023] diffusion learning rate: 0.001
2024-11-05 03:08:12,167 - INFO - [diffusion][Epoch 11023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:12,169 - INFO - [diffusion][Epoch 11024] Epoch 11025/12000
2024-11-05 03:08:15,267 - INFO - [diffusion][Epoch 11024] diffusion training Loss: 0.062043773010373116
2024-11-05 03:08:15,269 - INFO - [diffusion][Epoch 11024] diffusion learning rate: 0.001
2024-11-05 03:08:15,271 - INFO - [diffusion][Epoch 11024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:15,273 - INFO - [diffusion][Epoch 11025] Epoch 11026/12000
2024-11-05 03:08:18,427 - INFO - [diffusion][Epoch 11025] diffusion training Loss: 0.06645910255610943
2024-11-05 03:08:18,429 - INFO - [diffusion][Epoch 11025] diffusion learning rate: 0.001
2024-11-05 03:08:18,472 - INFO - [diffusion][Epoch 11025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:18,474 - INFO - [diffusion][Epoch 11026] Epoch 11027/12000
2024-11-05 03:08:21,620 - INFO - [diffusion][Epoch 11026] diffusion training Loss: 0.06960473023355007
2024-11-05 03:08:21,622 - INFO - [diffusion][Epoch 11026] diffusion learning rate: 0.001
2024-11-05 03:08:21,623 - INFO - [diffusion][Epoch 11026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:21,625 - INFO - [diffusion][Epoch 11027] Epoch 11028/12000
2024-11-05 03:08:25,183 - INFO - [diffusion][Epoch 11027] diffusion training Loss: 0.06574380025267601
2024-11-05 03:08:25,185 - INFO - [diffusion][Epoch 11027] diffusion learning rate: 0.001
2024-11-05 03:08:25,187 - INFO - [diffusion][Epoch 11027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:25,188 - INFO - [diffusion][Epoch 11028] Epoch 11029/12000
2024-11-05 03:08:28,723 - INFO - [diffusion][Epoch 11028] diffusion training Loss: 0.06555965170264244
2024-11-05 03:08:28,726 - INFO - [diffusion][Epoch 11028] diffusion learning rate: 0.001
2024-11-05 03:08:28,786 - INFO - [diffusion][Epoch 11028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:28,787 - INFO - [diffusion][Epoch 11029] Epoch 11030/12000
2024-11-05 03:08:31,916 - INFO - [diffusion][Epoch 11029] diffusion training Loss: 0.0693044476211071
2024-11-05 03:08:31,918 - INFO - [diffusion][Epoch 11029] diffusion learning rate: 0.001
2024-11-05 03:08:31,920 - INFO - [diffusion][Epoch 11029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:31,921 - INFO - [diffusion][Epoch 11030] Epoch 11031/12000
2024-11-05 03:08:35,032 - INFO - [diffusion][Epoch 11030] diffusion training Loss: 0.06272675842046738
2024-11-05 03:08:35,035 - INFO - [diffusion][Epoch 11030] diffusion learning rate: 0.001
2024-11-05 03:08:35,037 - INFO - [diffusion][Epoch 11030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:35,038 - INFO - [diffusion][Epoch 11031] Epoch 11032/12000
2024-11-05 03:08:38,167 - INFO - [diffusion][Epoch 11031] diffusion training Loss: 0.06614401005208492
2024-11-05 03:08:38,169 - INFO - [diffusion][Epoch 11031] diffusion learning rate: 0.001
2024-11-05 03:08:38,171 - INFO - [diffusion][Epoch 11031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:38,172 - INFO - [diffusion][Epoch 11032] Epoch 11033/12000
2024-11-05 03:08:41,732 - INFO - [diffusion][Epoch 11032] diffusion training Loss: 0.06087803468108177
2024-11-05 03:08:41,733 - INFO - [diffusion][Epoch 11032] diffusion learning rate: 0.001
2024-11-05 03:08:41,735 - INFO - [diffusion][Epoch 11032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:41,736 - INFO - [diffusion][Epoch 11033] Epoch 11034/12000
2024-11-05 03:08:44,657 - INFO - [diffusion][Epoch 11033] diffusion training Loss: 0.0606185644865036
2024-11-05 03:08:44,659 - INFO - [diffusion][Epoch 11033] diffusion learning rate: 0.001
2024-11-05 03:08:44,661 - INFO - [diffusion][Epoch 11033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:44,662 - INFO - [diffusion][Epoch 11034] Epoch 11035/12000
2024-11-05 03:08:47,801 - INFO - [diffusion][Epoch 11034] diffusion training Loss: 0.066813163459301
2024-11-05 03:08:47,803 - INFO - [diffusion][Epoch 11034] diffusion learning rate: 0.001
2024-11-05 03:08:47,804 - INFO - [diffusion][Epoch 11034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:47,806 - INFO - [diffusion][Epoch 11035] Epoch 11036/12000
2024-11-05 03:08:51,249 - INFO - [diffusion][Epoch 11035] diffusion training Loss: 0.06496180780231953
2024-11-05 03:08:51,251 - INFO - [diffusion][Epoch 11035] diffusion learning rate: 0.001
2024-11-05 03:08:51,253 - INFO - [diffusion][Epoch 11035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:51,255 - INFO - [diffusion][Epoch 11036] Epoch 11037/12000
2024-11-05 03:08:54,903 - INFO - [diffusion][Epoch 11036] diffusion training Loss: 0.062170849181711674
2024-11-05 03:08:54,905 - INFO - [diffusion][Epoch 11036] diffusion learning rate: 0.001
2024-11-05 03:08:54,906 - INFO - [diffusion][Epoch 11036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:54,907 - INFO - [diffusion][Epoch 11037] Epoch 11038/12000
2024-11-05 03:08:58,190 - INFO - [diffusion][Epoch 11037] diffusion training Loss: 0.06566340290009975
2024-11-05 03:08:58,192 - INFO - [diffusion][Epoch 11037] diffusion learning rate: 0.001
2024-11-05 03:08:58,194 - INFO - [diffusion][Epoch 11037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:58,195 - INFO - [diffusion][Epoch 11038] Epoch 11039/12000
2024-11-05 03:09:01,248 - INFO - [diffusion][Epoch 11038] diffusion training Loss: 0.06018755957484245
2024-11-05 03:09:01,250 - INFO - [diffusion][Epoch 11038] diffusion learning rate: 0.001
2024-11-05 03:09:01,277 - INFO - [diffusion][Epoch 11038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:01,279 - INFO - [diffusion][Epoch 11039] Epoch 11040/12000
2024-11-05 03:09:04,387 - INFO - [diffusion][Epoch 11039] diffusion training Loss: 0.0658438615500927
2024-11-05 03:09:04,389 - INFO - [diffusion][Epoch 11039] diffusion learning rate: 0.001
2024-11-05 03:09:04,391 - INFO - [diffusion][Epoch 11039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:04,392 - INFO - [diffusion][Epoch 11040] Epoch 11041/12000
2024-11-05 03:09:07,941 - INFO - [diffusion][Epoch 11040] diffusion training Loss: 0.06093956436961889
2024-11-05 03:09:07,943 - INFO - [diffusion][Epoch 11040] diffusion learning rate: 0.001
2024-11-05 03:09:07,945 - INFO - [diffusion][Epoch 11040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:07,946 - INFO - [diffusion][Epoch 11041] Epoch 11042/12000
2024-11-05 03:09:11,561 - INFO - [diffusion][Epoch 11041] diffusion training Loss: 0.06585823185741901
2024-11-05 03:09:11,563 - INFO - [diffusion][Epoch 11041] diffusion learning rate: 0.001
2024-11-05 03:09:11,565 - INFO - [diffusion][Epoch 11041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:11,566 - INFO - [diffusion][Epoch 11042] Epoch 11043/12000
2024-11-05 03:09:14,791 - INFO - [diffusion][Epoch 11042] diffusion training Loss: 0.0648766728118062
2024-11-05 03:09:14,793 - INFO - [diffusion][Epoch 11042] diffusion learning rate: 0.001
2024-11-05 03:09:14,795 - INFO - [diffusion][Epoch 11042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:14,796 - INFO - [diffusion][Epoch 11043] Epoch 11044/12000
2024-11-05 03:09:18,624 - INFO - [diffusion][Epoch 11043] diffusion training Loss: 0.06631764024496078
2024-11-05 03:09:18,626 - INFO - [diffusion][Epoch 11043] diffusion learning rate: 0.001
2024-11-05 03:09:18,628 - INFO - [diffusion][Epoch 11043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:18,629 - INFO - [diffusion][Epoch 11044] Epoch 11045/12000
2024-11-05 03:09:21,801 - INFO - [diffusion][Epoch 11044] diffusion training Loss: 0.06596802920103073
2024-11-05 03:09:21,802 - INFO - [diffusion][Epoch 11044] diffusion learning rate: 0.001
2024-11-05 03:09:21,804 - INFO - [diffusion][Epoch 11044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:21,806 - INFO - [diffusion][Epoch 11045] Epoch 11046/12000
2024-11-05 03:09:24,965 - INFO - [diffusion][Epoch 11045] diffusion training Loss: 0.06654800474643707
2024-11-05 03:09:24,967 - INFO - [diffusion][Epoch 11045] diffusion learning rate: 0.001
2024-11-05 03:09:24,969 - INFO - [diffusion][Epoch 11045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:24,970 - INFO - [diffusion][Epoch 11046] Epoch 11047/12000
2024-11-05 03:09:28,394 - INFO - [diffusion][Epoch 11046] diffusion training Loss: 0.06797070987522602
2024-11-05 03:09:28,396 - INFO - [diffusion][Epoch 11046] diffusion learning rate: 0.001
2024-11-05 03:09:28,398 - INFO - [diffusion][Epoch 11046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:28,399 - INFO - [diffusion][Epoch 11047] Epoch 11048/12000
2024-11-05 03:09:32,171 - INFO - [diffusion][Epoch 11047] diffusion training Loss: 0.06438995711505413
2024-11-05 03:09:32,173 - INFO - [diffusion][Epoch 11047] diffusion learning rate: 0.001
2024-11-05 03:09:32,175 - INFO - [diffusion][Epoch 11047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:32,176 - INFO - [diffusion][Epoch 11048] Epoch 11049/12000
2024-11-05 03:09:35,454 - INFO - [diffusion][Epoch 11048] diffusion training Loss: 0.06062400434166193
2024-11-05 03:09:35,457 - INFO - [diffusion][Epoch 11048] diffusion learning rate: 0.001
2024-11-05 03:09:35,459 - INFO - [diffusion][Epoch 11048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:35,460 - INFO - [diffusion][Epoch 11049] Epoch 11050/12000
2024-11-05 03:09:38,546 - INFO - [diffusion][Epoch 11049] diffusion training Loss: 0.06304348167032003
2024-11-05 03:09:38,548 - INFO - [diffusion][Epoch 11049] diffusion learning rate: 0.001
2024-11-05 03:09:38,549 - INFO - [diffusion][Epoch 11049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:38,550 - INFO - [diffusion][Epoch 11050] Epoch 11051/12000
2024-11-05 03:09:41,768 - INFO - [diffusion][Epoch 11050] diffusion training Loss: 0.06854750961065292
2024-11-05 03:09:41,770 - INFO - [diffusion][Epoch 11050] diffusion learning rate: 0.001
2024-11-05 03:09:41,772 - INFO - [diffusion][Epoch 11050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:41,773 - INFO - [diffusion][Epoch 11051] Epoch 11052/12000
2024-11-05 03:09:45,227 - INFO - [diffusion][Epoch 11051] diffusion training Loss: 0.06639686413109303
2024-11-05 03:09:45,229 - INFO - [diffusion][Epoch 11051] diffusion learning rate: 0.001
2024-11-05 03:09:45,232 - INFO - [diffusion][Epoch 11051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:45,233 - INFO - [diffusion][Epoch 11052] Epoch 11053/12000
2024-11-05 03:09:48,989 - INFO - [diffusion][Epoch 11052] diffusion training Loss: 0.0657613892108202
2024-11-05 03:09:48,991 - INFO - [diffusion][Epoch 11052] diffusion learning rate: 0.001
2024-11-05 03:09:48,993 - INFO - [diffusion][Epoch 11052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:48,994 - INFO - [diffusion][Epoch 11053] Epoch 11054/12000
2024-11-05 03:09:52,385 - INFO - [diffusion][Epoch 11053] diffusion training Loss: 0.06588251050561666
2024-11-05 03:09:52,387 - INFO - [diffusion][Epoch 11053] diffusion learning rate: 0.001
2024-11-05 03:09:52,414 - INFO - [diffusion][Epoch 11053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:52,415 - INFO - [diffusion][Epoch 11054] Epoch 11055/12000
2024-11-05 03:09:55,492 - INFO - [diffusion][Epoch 11054] diffusion training Loss: 0.057119883596897125
2024-11-05 03:09:55,494 - INFO - [diffusion][Epoch 11054] diffusion learning rate: 0.001
2024-11-05 03:09:55,496 - INFO - [diffusion][Epoch 11054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:55,497 - INFO - [diffusion][Epoch 11055] Epoch 11056/12000
2024-11-05 03:09:58,531 - INFO - [diffusion][Epoch 11055] diffusion training Loss: 0.06789413560181856
2024-11-05 03:09:58,533 - INFO - [diffusion][Epoch 11055] diffusion learning rate: 0.001
2024-11-05 03:09:58,535 - INFO - [diffusion][Epoch 11055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:58,536 - INFO - [diffusion][Epoch 11056] Epoch 11057/12000
2024-11-05 03:10:02,063 - INFO - [diffusion][Epoch 11056] diffusion training Loss: 0.061482553370296955
2024-11-05 03:10:02,065 - INFO - [diffusion][Epoch 11056] diffusion learning rate: 0.001
2024-11-05 03:10:02,067 - INFO - [diffusion][Epoch 11056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:02,068 - INFO - [diffusion][Epoch 11057] Epoch 11058/12000
2024-11-05 03:10:05,575 - INFO - [diffusion][Epoch 11057] diffusion training Loss: 0.06603546999394894
2024-11-05 03:10:05,577 - INFO - [diffusion][Epoch 11057] diffusion learning rate: 0.001
2024-11-05 03:10:05,580 - INFO - [diffusion][Epoch 11057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:05,581 - INFO - [diffusion][Epoch 11058] Epoch 11059/12000
2024-11-05 03:10:08,281 - INFO - [diffusion][Epoch 11058] diffusion training Loss: 0.06707462668418884
2024-11-05 03:10:08,283 - INFO - [diffusion][Epoch 11058] diffusion learning rate: 0.001
2024-11-05 03:10:08,285 - INFO - [diffusion][Epoch 11058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:08,286 - INFO - [diffusion][Epoch 11059] Epoch 11060/12000
2024-11-05 03:10:11,334 - INFO - [diffusion][Epoch 11059] diffusion training Loss: 0.06096573639661074
2024-11-05 03:10:11,336 - INFO - [diffusion][Epoch 11059] diffusion learning rate: 0.001
2024-11-05 03:10:11,337 - INFO - [diffusion][Epoch 11059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:11,339 - INFO - [diffusion][Epoch 11060] Epoch 11061/12000
2024-11-05 03:10:15,061 - INFO - [diffusion][Epoch 11060] diffusion training Loss: 0.06763328146189451
2024-11-05 03:10:15,063 - INFO - [diffusion][Epoch 11060] diffusion learning rate: 0.001
2024-11-05 03:10:15,065 - INFO - [diffusion][Epoch 11060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:15,066 - INFO - [diffusion][Epoch 11061] Epoch 11062/12000
2024-11-05 03:10:18,349 - INFO - [diffusion][Epoch 11061] diffusion training Loss: 0.06333968043327332
2024-11-05 03:10:18,351 - INFO - [diffusion][Epoch 11061] diffusion learning rate: 0.001
2024-11-05 03:10:18,352 - INFO - [diffusion][Epoch 11061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:18,354 - INFO - [diffusion][Epoch 11062] Epoch 11063/12000
2024-11-05 03:10:21,546 - INFO - [diffusion][Epoch 11062] diffusion training Loss: 0.06835469603538513
2024-11-05 03:10:21,548 - INFO - [diffusion][Epoch 11062] diffusion learning rate: 0.001
2024-11-05 03:10:21,549 - INFO - [diffusion][Epoch 11062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:21,551 - INFO - [diffusion][Epoch 11063] Epoch 11064/12000
2024-11-05 03:10:24,621 - INFO - [diffusion][Epoch 11063] diffusion training Loss: 0.06129359360784292
2024-11-05 03:10:24,623 - INFO - [diffusion][Epoch 11063] diffusion learning rate: 0.001
2024-11-05 03:10:24,626 - INFO - [diffusion][Epoch 11063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:24,627 - INFO - [diffusion][Epoch 11064] Epoch 11065/12000
2024-11-05 03:10:28,148 - INFO - [diffusion][Epoch 11064] diffusion training Loss: 0.06518491357564926
2024-11-05 03:10:28,150 - INFO - [diffusion][Epoch 11064] diffusion learning rate: 0.001
2024-11-05 03:10:28,152 - INFO - [diffusion][Epoch 11064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:28,153 - INFO - [diffusion][Epoch 11065] Epoch 11066/12000
2024-11-05 03:10:31,799 - INFO - [diffusion][Epoch 11065] diffusion training Loss: 0.0659025963395834
2024-11-05 03:10:31,801 - INFO - [diffusion][Epoch 11065] diffusion learning rate: 0.001
2024-11-05 03:10:31,803 - INFO - [diffusion][Epoch 11065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:31,804 - INFO - [diffusion][Epoch 11066] Epoch 11067/12000
2024-11-05 03:10:34,945 - INFO - [diffusion][Epoch 11066] diffusion training Loss: 0.06139899045228958
2024-11-05 03:10:34,947 - INFO - [diffusion][Epoch 11066] diffusion learning rate: 0.001
2024-11-05 03:10:34,948 - INFO - [diffusion][Epoch 11066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:34,950 - INFO - [diffusion][Epoch 11067] Epoch 11068/12000
2024-11-05 03:10:38,051 - INFO - [diffusion][Epoch 11067] diffusion training Loss: 0.06300671584904194
2024-11-05 03:10:38,054 - INFO - [diffusion][Epoch 11067] diffusion learning rate: 0.001
2024-11-05 03:10:38,055 - INFO - [diffusion][Epoch 11067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:38,057 - INFO - [diffusion][Epoch 11068] Epoch 11069/12000
2024-11-05 03:10:41,196 - INFO - [diffusion][Epoch 11068] diffusion training Loss: 0.06585920415818691
2024-11-05 03:10:41,198 - INFO - [diffusion][Epoch 11068] diffusion learning rate: 0.001
2024-11-05 03:10:41,199 - INFO - [diffusion][Epoch 11068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:41,201 - INFO - [diffusion][Epoch 11069] Epoch 11070/12000
2024-11-05 03:10:44,748 - INFO - [diffusion][Epoch 11069] diffusion training Loss: 0.06074649374932051
2024-11-05 03:10:44,750 - INFO - [diffusion][Epoch 11069] diffusion learning rate: 0.001
2024-11-05 03:10:44,752 - INFO - [diffusion][Epoch 11069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:44,754 - INFO - [diffusion][Epoch 11070] Epoch 11071/12000
2024-11-05 03:10:48,174 - INFO - [diffusion][Epoch 11070] diffusion training Loss: 0.06409010849893093
2024-11-05 03:10:48,176 - INFO - [diffusion][Epoch 11070] diffusion learning rate: 0.001
2024-11-05 03:10:48,178 - INFO - [diffusion][Epoch 11070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:48,180 - INFO - [diffusion][Epoch 11071] Epoch 11072/12000
2024-11-05 03:10:51,303 - INFO - [diffusion][Epoch 11071] diffusion training Loss: 0.06587713491171598
2024-11-05 03:10:51,305 - INFO - [diffusion][Epoch 11071] diffusion learning rate: 0.001
2024-11-05 03:10:51,306 - INFO - [diffusion][Epoch 11071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:51,308 - INFO - [diffusion][Epoch 11072] Epoch 11073/12000
2024-11-05 03:10:54,519 - INFO - [diffusion][Epoch 11072] diffusion training Loss: 0.060525013133883476
2024-11-05 03:10:54,521 - INFO - [diffusion][Epoch 11072] diffusion learning rate: 0.001
2024-11-05 03:10:54,523 - INFO - [diffusion][Epoch 11072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:54,524 - INFO - [diffusion][Epoch 11073] Epoch 11074/12000
2024-11-05 03:10:57,868 - INFO - [diffusion][Epoch 11073] diffusion training Loss: 0.06223808415234089
2024-11-05 03:10:57,870 - INFO - [diffusion][Epoch 11073] diffusion learning rate: 0.001
2024-11-05 03:10:57,871 - INFO - [diffusion][Epoch 11073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:57,873 - INFO - [diffusion][Epoch 11074] Epoch 11075/12000
2024-11-05 03:11:01,523 - INFO - [diffusion][Epoch 11074] diffusion training Loss: 0.05914962198585272
2024-11-05 03:11:01,525 - INFO - [diffusion][Epoch 11074] diffusion learning rate: 0.001
2024-11-05 03:11:01,526 - INFO - [diffusion][Epoch 11074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:01,527 - INFO - [diffusion][Epoch 11075] Epoch 11076/12000
2024-11-05 03:11:04,546 - INFO - [diffusion][Epoch 11075] diffusion training Loss: 0.06595196295529604
2024-11-05 03:11:04,548 - INFO - [diffusion][Epoch 11075] diffusion learning rate: 0.001
2024-11-05 03:11:04,551 - INFO - [diffusion][Epoch 11075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:04,552 - INFO - [diffusion][Epoch 11076] Epoch 11077/12000
2024-11-05 03:11:07,649 - INFO - [diffusion][Epoch 11076] diffusion training Loss: 0.06303797196596861
2024-11-05 03:11:07,651 - INFO - [diffusion][Epoch 11076] diffusion learning rate: 0.001
2024-11-05 03:11:07,653 - INFO - [diffusion][Epoch 11076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:07,655 - INFO - [diffusion][Epoch 11077] Epoch 11078/12000
2024-11-05 03:11:10,783 - INFO - [diffusion][Epoch 11077] diffusion training Loss: 0.06320876535028219
2024-11-05 03:11:10,785 - INFO - [diffusion][Epoch 11077] diffusion learning rate: 0.001
2024-11-05 03:11:10,786 - INFO - [diffusion][Epoch 11077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:10,787 - INFO - [diffusion][Epoch 11078] Epoch 11079/12000
2024-11-05 03:11:14,442 - INFO - [diffusion][Epoch 11078] diffusion training Loss: 0.06324686203151941
2024-11-05 03:11:14,444 - INFO - [diffusion][Epoch 11078] diffusion learning rate: 0.001
2024-11-05 03:11:14,472 - INFO - [diffusion][Epoch 11078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:14,473 - INFO - [diffusion][Epoch 11079] Epoch 11080/12000
2024-11-05 03:11:17,808 - INFO - [diffusion][Epoch 11079] diffusion training Loss: 0.0652976268902421
2024-11-05 03:11:17,811 - INFO - [diffusion][Epoch 11079] diffusion learning rate: 0.001
2024-11-05 03:11:17,813 - INFO - [diffusion][Epoch 11079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:17,814 - INFO - [diffusion][Epoch 11080] Epoch 11081/12000
2024-11-05 03:11:20,890 - INFO - [diffusion][Epoch 11080] diffusion training Loss: 0.07116073649376631
2024-11-05 03:11:20,892 - INFO - [diffusion][Epoch 11080] diffusion learning rate: 0.001
2024-11-05 03:11:20,894 - INFO - [diffusion][Epoch 11080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:20,895 - INFO - [diffusion][Epoch 11081] Epoch 11082/12000
2024-11-05 03:11:23,868 - INFO - [diffusion][Epoch 11081] diffusion training Loss: 0.06417494360357523
2024-11-05 03:11:23,870 - INFO - [diffusion][Epoch 11081] diffusion learning rate: 0.001
2024-11-05 03:11:23,872 - INFO - [diffusion][Epoch 11081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:23,874 - INFO - [diffusion][Epoch 11082] Epoch 11083/12000
2024-11-05 03:11:27,422 - INFO - [diffusion][Epoch 11082] diffusion training Loss: 0.0680266534909606
2024-11-05 03:11:27,424 - INFO - [diffusion][Epoch 11082] diffusion learning rate: 0.001
2024-11-05 03:11:27,426 - INFO - [diffusion][Epoch 11082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:27,427 - INFO - [diffusion][Epoch 11083] Epoch 11084/12000
2024-11-05 03:11:30,978 - INFO - [diffusion][Epoch 11083] diffusion training Loss: 0.059382954612374306
2024-11-05 03:11:30,981 - INFO - [diffusion][Epoch 11083] diffusion learning rate: 0.001
2024-11-05 03:11:31,001 - INFO - [diffusion][Epoch 11083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:31,002 - INFO - [diffusion][Epoch 11084] Epoch 11085/12000
2024-11-05 03:11:35,088 - INFO - [diffusion][Epoch 11084] diffusion training Loss: 0.059933836571872234
2024-11-05 03:11:35,090 - INFO - [diffusion][Epoch 11084] diffusion learning rate: 0.001
2024-11-05 03:11:35,092 - INFO - [diffusion][Epoch 11084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:35,094 - INFO - [diffusion][Epoch 11085] Epoch 11086/12000
2024-11-05 03:11:37,790 - INFO - [diffusion][Epoch 11085] diffusion training Loss: 0.06668376736342907
2024-11-05 03:11:37,792 - INFO - [diffusion][Epoch 11085] diffusion learning rate: 0.001
2024-11-05 03:11:37,794 - INFO - [diffusion][Epoch 11085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:37,796 - INFO - [diffusion][Epoch 11086] Epoch 11087/12000
2024-11-05 03:11:40,803 - INFO - [diffusion][Epoch 11086] diffusion training Loss: 0.06471442896872759
2024-11-05 03:11:40,805 - INFO - [diffusion][Epoch 11086] diffusion learning rate: 0.001
2024-11-05 03:11:40,807 - INFO - [diffusion][Epoch 11086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:40,809 - INFO - [diffusion][Epoch 11087] Epoch 11088/12000
2024-11-05 03:11:44,391 - INFO - [diffusion][Epoch 11087] diffusion training Loss: 0.06429309770464897
2024-11-05 03:11:44,393 - INFO - [diffusion][Epoch 11087] diffusion learning rate: 0.001
2024-11-05 03:11:44,395 - INFO - [diffusion][Epoch 11087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:44,396 - INFO - [diffusion][Epoch 11088] Epoch 11089/12000
2024-11-05 03:11:47,968 - INFO - [diffusion][Epoch 11088] diffusion training Loss: 0.06389261037111282
2024-11-05 03:11:47,970 - INFO - [diffusion][Epoch 11088] diffusion learning rate: 0.001
2024-11-05 03:11:47,997 - INFO - [diffusion][Epoch 11088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:47,998 - INFO - [diffusion][Epoch 11089] Epoch 11090/12000
2024-11-05 03:11:51,132 - INFO - [diffusion][Epoch 11089] diffusion training Loss: 0.06823866348713636
2024-11-05 03:11:51,134 - INFO - [diffusion][Epoch 11089] diffusion learning rate: 0.001
2024-11-05 03:11:51,136 - INFO - [diffusion][Epoch 11089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:51,137 - INFO - [diffusion][Epoch 11090] Epoch 11091/12000
2024-11-05 03:11:54,214 - INFO - [diffusion][Epoch 11090] diffusion training Loss: 0.07016292586922646
2024-11-05 03:11:54,216 - INFO - [diffusion][Epoch 11090] diffusion learning rate: 0.001
2024-11-05 03:11:54,218 - INFO - [diffusion][Epoch 11090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:54,219 - INFO - [diffusion][Epoch 11091] Epoch 11092/12000
2024-11-05 03:11:57,343 - INFO - [diffusion][Epoch 11091] diffusion training Loss: 0.06326651759445667
2024-11-05 03:11:57,345 - INFO - [diffusion][Epoch 11091] diffusion learning rate: 0.001
2024-11-05 03:11:57,347 - INFO - [diffusion][Epoch 11091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:57,348 - INFO - [diffusion][Epoch 11092] Epoch 11093/12000
2024-11-05 03:12:01,024 - INFO - [diffusion][Epoch 11092] diffusion training Loss: 0.06620972510427237
2024-11-05 03:12:01,026 - INFO - [diffusion][Epoch 11092] diffusion learning rate: 0.001
2024-11-05 03:12:01,027 - INFO - [diffusion][Epoch 11092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:01,029 - INFO - [diffusion][Epoch 11093] Epoch 11094/12000
2024-11-05 03:12:04,475 - INFO - [diffusion][Epoch 11093] diffusion training Loss: 0.06194906309247017
2024-11-05 03:12:04,477 - INFO - [diffusion][Epoch 11093] diffusion learning rate: 0.001
2024-11-05 03:12:04,479 - INFO - [diffusion][Epoch 11093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:04,480 - INFO - [diffusion][Epoch 11094] Epoch 11095/12000
2024-11-05 03:12:07,482 - INFO - [diffusion][Epoch 11094] diffusion training Loss: 0.060644143261015415
2024-11-05 03:12:07,485 - INFO - [diffusion][Epoch 11094] diffusion learning rate: 0.001
2024-11-05 03:12:07,488 - INFO - [diffusion][Epoch 11094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:07,489 - INFO - [diffusion][Epoch 11095] Epoch 11096/12000
2024-11-05 03:12:10,639 - INFO - [diffusion][Epoch 11095] diffusion training Loss: 0.060543292202055454
2024-11-05 03:12:10,640 - INFO - [diffusion][Epoch 11095] diffusion learning rate: 0.001
2024-11-05 03:12:10,642 - INFO - [diffusion][Epoch 11095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:10,643 - INFO - [diffusion][Epoch 11096] Epoch 11097/12000
2024-11-05 03:12:14,125 - INFO - [diffusion][Epoch 11096] diffusion training Loss: 0.06242504622787237
2024-11-05 03:12:14,127 - INFO - [diffusion][Epoch 11096] diffusion learning rate: 0.001
2024-11-05 03:12:14,129 - INFO - [diffusion][Epoch 11096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:14,130 - INFO - [diffusion][Epoch 11097] Epoch 11098/12000
2024-11-05 03:12:17,652 - INFO - [diffusion][Epoch 11097] diffusion training Loss: 0.06595335248857737
2024-11-05 03:12:17,653 - INFO - [diffusion][Epoch 11097] diffusion learning rate: 0.001
2024-11-05 03:12:17,655 - INFO - [diffusion][Epoch 11097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:17,657 - INFO - [diffusion][Epoch 11098] Epoch 11099/12000
2024-11-05 03:12:20,782 - INFO - [diffusion][Epoch 11098] diffusion training Loss: 0.06860272772610188
2024-11-05 03:12:20,784 - INFO - [diffusion][Epoch 11098] diffusion learning rate: 0.001
2024-11-05 03:12:20,786 - INFO - [diffusion][Epoch 11098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:20,787 - INFO - [diffusion][Epoch 11099] Epoch 11100/12000
2024-11-05 03:12:23,780 - INFO - [diffusion][Epoch 11099] diffusion training Loss: 0.06504289340227842
2024-11-05 03:12:23,782 - INFO - [diffusion][Epoch 11099] diffusion learning rate: 0.001
2024-11-05 03:12:23,785 - INFO - [diffusion][Epoch 11099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:23,786 - INFO - [diffusion][Epoch 11100] Epoch 11101/12000
2024-11-05 03:12:26,916 - INFO - [diffusion][Epoch 11100] diffusion training Loss: 0.06192678026854992
2024-11-05 03:12:26,918 - INFO - [diffusion][Epoch 11100] diffusion learning rate: 0.001
2024-11-05 03:12:26,920 - INFO - [diffusion][Epoch 11100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:26,921 - INFO - [diffusion][Epoch 11101] Epoch 11102/12000
2024-11-05 03:12:30,413 - INFO - [diffusion][Epoch 11101] diffusion training Loss: 0.06862307898700237
2024-11-05 03:12:30,415 - INFO - [diffusion][Epoch 11101] diffusion learning rate: 0.001
2024-11-05 03:12:30,417 - INFO - [diffusion][Epoch 11101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:30,418 - INFO - [diffusion][Epoch 11102] Epoch 11103/12000
2024-11-05 03:12:33,701 - INFO - [diffusion][Epoch 11102] diffusion training Loss: 0.07398162223398685
2024-11-05 03:12:33,703 - INFO - [diffusion][Epoch 11102] diffusion learning rate: 0.001
2024-11-05 03:12:33,704 - INFO - [diffusion][Epoch 11102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:33,706 - INFO - [diffusion][Epoch 11103] Epoch 11104/12000
2024-11-05 03:12:36,717 - INFO - [diffusion][Epoch 11103] diffusion training Loss: 0.06474931351840496
2024-11-05 03:12:36,719 - INFO - [diffusion][Epoch 11103] diffusion learning rate: 0.001
2024-11-05 03:12:36,720 - INFO - [diffusion][Epoch 11103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:36,722 - INFO - [diffusion][Epoch 11104] Epoch 11105/12000
2024-11-05 03:12:40,339 - INFO - [diffusion][Epoch 11104] diffusion training Loss: 0.06467198207974434
2024-11-05 03:12:40,341 - INFO - [diffusion][Epoch 11104] diffusion learning rate: 0.001
2024-11-05 03:12:40,343 - INFO - [diffusion][Epoch 11104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:40,344 - INFO - [diffusion][Epoch 11105] Epoch 11106/12000
2024-11-05 03:12:43,425 - INFO - [diffusion][Epoch 11105] diffusion training Loss: 0.06623063422739506
2024-11-05 03:12:43,427 - INFO - [diffusion][Epoch 11105] diffusion learning rate: 0.001
2024-11-05 03:12:43,429 - INFO - [diffusion][Epoch 11105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:43,431 - INFO - [diffusion][Epoch 11106] Epoch 11107/12000
2024-11-05 03:12:47,035 - INFO - [diffusion][Epoch 11106] diffusion training Loss: 0.06737308762967587
2024-11-05 03:12:47,037 - INFO - [diffusion][Epoch 11106] diffusion learning rate: 0.001
2024-11-05 03:12:47,039 - INFO - [diffusion][Epoch 11106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:47,040 - INFO - [diffusion][Epoch 11107] Epoch 11108/12000
2024-11-05 03:12:50,364 - INFO - [diffusion][Epoch 11107] diffusion training Loss: 0.0676984041929245
2024-11-05 03:12:50,366 - INFO - [diffusion][Epoch 11107] diffusion learning rate: 0.001
2024-11-05 03:12:50,368 - INFO - [diffusion][Epoch 11107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:50,369 - INFO - [diffusion][Epoch 11108] Epoch 11109/12000
2024-11-05 03:12:53,356 - INFO - [diffusion][Epoch 11108] diffusion training Loss: 0.0642920434474945
2024-11-05 03:12:53,358 - INFO - [diffusion][Epoch 11108] diffusion learning rate: 0.001
2024-11-05 03:12:53,360 - INFO - [diffusion][Epoch 11108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:53,361 - INFO - [diffusion][Epoch 11109] Epoch 11110/12000
2024-11-05 03:12:56,696 - INFO - [diffusion][Epoch 11109] diffusion training Loss: 0.06527329795062542
2024-11-05 03:12:56,698 - INFO - [diffusion][Epoch 11109] diffusion learning rate: 0.001
2024-11-05 03:12:56,699 - INFO - [diffusion][Epoch 11109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:56,701 - INFO - [diffusion][Epoch 11110] Epoch 11111/12000
2024-11-05 03:13:00,053 - INFO - [diffusion][Epoch 11110] diffusion training Loss: 0.06587395630776882
2024-11-05 03:13:00,055 - INFO - [diffusion][Epoch 11110] diffusion learning rate: 0.001
2024-11-05 03:13:00,056 - INFO - [diffusion][Epoch 11110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:00,057 - INFO - [diffusion][Epoch 11111] Epoch 11112/12000
2024-11-05 03:13:03,733 - INFO - [diffusion][Epoch 11111] diffusion training Loss: 0.07423615269362926
2024-11-05 03:13:03,735 - INFO - [diffusion][Epoch 11111] diffusion learning rate: 0.001
2024-11-05 03:13:03,737 - INFO - [diffusion][Epoch 11111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:03,739 - INFO - [diffusion][Epoch 11112] Epoch 11113/12000
2024-11-05 03:13:06,647 - INFO - [diffusion][Epoch 11112] diffusion training Loss: 0.06728256959468126
2024-11-05 03:13:06,649 - INFO - [diffusion][Epoch 11112] diffusion learning rate: 0.001
2024-11-05 03:13:06,667 - INFO - [diffusion][Epoch 11112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:06,669 - INFO - [diffusion][Epoch 11113] Epoch 11114/12000
2024-11-05 03:13:09,881 - INFO - [diffusion][Epoch 11113] diffusion training Loss: 0.06560213677585125
2024-11-05 03:13:09,883 - INFO - [diffusion][Epoch 11113] diffusion learning rate: 0.001
2024-11-05 03:13:09,885 - INFO - [diffusion][Epoch 11113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:09,887 - INFO - [diffusion][Epoch 11114] Epoch 11115/12000
2024-11-05 03:13:13,201 - INFO - [diffusion][Epoch 11114] diffusion training Loss: 0.06550650019198656
2024-11-05 03:13:13,203 - INFO - [diffusion][Epoch 11114] diffusion learning rate: 0.001
2024-11-05 03:13:13,205 - INFO - [diffusion][Epoch 11114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:13,207 - INFO - [diffusion][Epoch 11115] Epoch 11116/12000
2024-11-05 03:13:16,821 - INFO - [diffusion][Epoch 11115] diffusion training Loss: 0.06229877099394798
2024-11-05 03:13:16,823 - INFO - [diffusion][Epoch 11115] diffusion learning rate: 0.001
2024-11-05 03:13:16,826 - INFO - [diffusion][Epoch 11115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:16,827 - INFO - [diffusion][Epoch 11116] Epoch 11117/12000
2024-11-05 03:13:20,365 - INFO - [diffusion][Epoch 11116] diffusion training Loss: 0.059748162515461445
2024-11-05 03:13:20,366 - INFO - [diffusion][Epoch 11116] diffusion learning rate: 0.001
2024-11-05 03:13:20,368 - INFO - [diffusion][Epoch 11116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:20,369 - INFO - [diffusion][Epoch 11117] Epoch 11118/12000
2024-11-05 03:13:23,474 - INFO - [diffusion][Epoch 11117] diffusion training Loss: 0.06617998518049717
2024-11-05 03:13:23,476 - INFO - [diffusion][Epoch 11117] diffusion learning rate: 0.001
2024-11-05 03:13:23,478 - INFO - [diffusion][Epoch 11117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:23,479 - INFO - [diffusion][Epoch 11118] Epoch 11119/12000
2024-11-05 03:13:26,589 - INFO - [diffusion][Epoch 11118] diffusion training Loss: 0.06126247625797987
2024-11-05 03:13:26,591 - INFO - [diffusion][Epoch 11118] diffusion learning rate: 0.001
2024-11-05 03:13:26,592 - INFO - [diffusion][Epoch 11118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:26,594 - INFO - [diffusion][Epoch 11119] Epoch 11120/12000
2024-11-05 03:13:29,751 - INFO - [diffusion][Epoch 11119] diffusion training Loss: 0.0681859701871872
2024-11-05 03:13:29,753 - INFO - [diffusion][Epoch 11119] diffusion learning rate: 0.001
2024-11-05 03:13:29,754 - INFO - [diffusion][Epoch 11119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:29,756 - INFO - [diffusion][Epoch 11120] Epoch 11121/12000
2024-11-05 03:13:33,370 - INFO - [diffusion][Epoch 11120] diffusion training Loss: 0.06381976790726185
2024-11-05 03:13:33,373 - INFO - [diffusion][Epoch 11120] diffusion learning rate: 0.001
2024-11-05 03:13:33,374 - INFO - [diffusion][Epoch 11120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:33,376 - INFO - [diffusion][Epoch 11121] Epoch 11122/12000
2024-11-05 03:13:36,824 - INFO - [diffusion][Epoch 11121] diffusion training Loss: 0.06780765671283007
2024-11-05 03:13:36,826 - INFO - [diffusion][Epoch 11121] diffusion learning rate: 0.001
2024-11-05 03:13:36,828 - INFO - [diffusion][Epoch 11121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:36,829 - INFO - [diffusion][Epoch 11122] Epoch 11123/12000
2024-11-05 03:13:39,985 - INFO - [diffusion][Epoch 11122] diffusion training Loss: 0.0631619980558753
2024-11-05 03:13:39,986 - INFO - [diffusion][Epoch 11122] diffusion learning rate: 0.001
2024-11-05 03:13:39,988 - INFO - [diffusion][Epoch 11122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:39,989 - INFO - [diffusion][Epoch 11123] Epoch 11124/12000
2024-11-05 03:13:43,237 - INFO - [diffusion][Epoch 11123] diffusion training Loss: 0.06433222256600857
2024-11-05 03:13:43,239 - INFO - [diffusion][Epoch 11123] diffusion learning rate: 0.001
2024-11-05 03:13:43,241 - INFO - [diffusion][Epoch 11123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:43,242 - INFO - [diffusion][Epoch 11124] Epoch 11125/12000
2024-11-05 03:13:46,455 - INFO - [diffusion][Epoch 11124] diffusion training Loss: 0.06672339141368866
2024-11-05 03:13:46,457 - INFO - [diffusion][Epoch 11124] diffusion learning rate: 0.001
2024-11-05 03:13:46,459 - INFO - [diffusion][Epoch 11124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:46,460 - INFO - [diffusion][Epoch 11125] Epoch 11126/12000
2024-11-05 03:13:50,148 - INFO - [diffusion][Epoch 11125] diffusion training Loss: 0.06743943318724632
2024-11-05 03:13:50,150 - INFO - [diffusion][Epoch 11125] diffusion learning rate: 0.001
2024-11-05 03:13:50,153 - INFO - [diffusion][Epoch 11125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:50,154 - INFO - [diffusion][Epoch 11126] Epoch 11127/12000
2024-11-05 03:13:53,720 - INFO - [diffusion][Epoch 11126] diffusion training Loss: 0.06292118597775698
2024-11-05 03:13:53,722 - INFO - [diffusion][Epoch 11126] diffusion learning rate: 0.001
2024-11-05 03:13:53,724 - INFO - [diffusion][Epoch 11126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:53,725 - INFO - [diffusion][Epoch 11127] Epoch 11128/12000
2024-11-05 03:13:56,792 - INFO - [diffusion][Epoch 11127] diffusion training Loss: 0.06003418657928705
2024-11-05 03:13:56,794 - INFO - [diffusion][Epoch 11127] diffusion learning rate: 0.001
2024-11-05 03:13:56,796 - INFO - [diffusion][Epoch 11127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:56,797 - INFO - [diffusion][Epoch 11128] Epoch 11129/12000
2024-11-05 03:13:59,941 - INFO - [diffusion][Epoch 11128] diffusion training Loss: 0.06202983949333429
2024-11-05 03:13:59,943 - INFO - [diffusion][Epoch 11128] diffusion learning rate: 0.001
2024-11-05 03:13:59,945 - INFO - [diffusion][Epoch 11128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:59,946 - INFO - [diffusion][Epoch 11129] Epoch 11130/12000
2024-11-05 03:14:03,173 - INFO - [diffusion][Epoch 11129] diffusion training Loss: 0.061569501645863056
2024-11-05 03:14:03,175 - INFO - [diffusion][Epoch 11129] diffusion learning rate: 0.001
2024-11-05 03:14:03,177 - INFO - [diffusion][Epoch 11129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:03,178 - INFO - [diffusion][Epoch 11130] Epoch 11131/12000
2024-11-05 03:14:06,810 - INFO - [diffusion][Epoch 11130] diffusion training Loss: 0.06398181430995464
2024-11-05 03:14:06,812 - INFO - [diffusion][Epoch 11130] diffusion learning rate: 0.001
2024-11-05 03:14:06,814 - INFO - [diffusion][Epoch 11130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:06,815 - INFO - [diffusion][Epoch 11131] Epoch 11132/12000
2024-11-05 03:14:10,087 - INFO - [diffusion][Epoch 11131] diffusion training Loss: 0.06466540042310953
2024-11-05 03:14:10,089 - INFO - [diffusion][Epoch 11131] diffusion learning rate: 0.001
2024-11-05 03:14:10,091 - INFO - [diffusion][Epoch 11131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:10,093 - INFO - [diffusion][Epoch 11132] Epoch 11133/12000
2024-11-05 03:14:13,182 - INFO - [diffusion][Epoch 11132] diffusion training Loss: 0.06047827657312155
2024-11-05 03:14:13,185 - INFO - [diffusion][Epoch 11132] diffusion learning rate: 0.001
2024-11-05 03:14:13,187 - INFO - [diffusion][Epoch 11132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:13,188 - INFO - [diffusion][Epoch 11133] Epoch 11134/12000
2024-11-05 03:14:16,280 - INFO - [diffusion][Epoch 11133] diffusion training Loss: 0.05965988524258137
2024-11-05 03:14:16,282 - INFO - [diffusion][Epoch 11133] diffusion learning rate: 0.001
2024-11-05 03:14:16,284 - INFO - [diffusion][Epoch 11133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:16,285 - INFO - [diffusion][Epoch 11134] Epoch 11135/12000
2024-11-05 03:14:19,844 - INFO - [diffusion][Epoch 11134] diffusion training Loss: 0.06471290532499552
2024-11-05 03:14:19,846 - INFO - [diffusion][Epoch 11134] diffusion learning rate: 0.001
2024-11-05 03:14:19,848 - INFO - [diffusion][Epoch 11134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:19,849 - INFO - [diffusion][Epoch 11135] Epoch 11136/12000
2024-11-05 03:14:23,445 - INFO - [diffusion][Epoch 11135] diffusion training Loss: 0.06120449211448431
2024-11-05 03:14:23,447 - INFO - [diffusion][Epoch 11135] diffusion learning rate: 0.001
2024-11-05 03:14:23,449 - INFO - [diffusion][Epoch 11135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:23,450 - INFO - [diffusion][Epoch 11136] Epoch 11137/12000
2024-11-05 03:14:26,581 - INFO - [diffusion][Epoch 11136] diffusion training Loss: 0.06085572764277458
2024-11-05 03:14:26,583 - INFO - [diffusion][Epoch 11136] diffusion learning rate: 0.001
2024-11-05 03:14:26,585 - INFO - [diffusion][Epoch 11136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:26,587 - INFO - [diffusion][Epoch 11137] Epoch 11138/12000
2024-11-05 03:14:29,716 - INFO - [diffusion][Epoch 11137] diffusion training Loss: 0.06660891324281693
2024-11-05 03:14:29,718 - INFO - [diffusion][Epoch 11137] diffusion learning rate: 0.001
2024-11-05 03:14:29,720 - INFO - [diffusion][Epoch 11137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:29,721 - INFO - [diffusion][Epoch 11138] Epoch 11139/12000
2024-11-05 03:14:32,904 - INFO - [diffusion][Epoch 11138] diffusion training Loss: 0.06472019478678703
2024-11-05 03:14:32,906 - INFO - [diffusion][Epoch 11138] diffusion learning rate: 0.001
2024-11-05 03:14:32,908 - INFO - [diffusion][Epoch 11138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:32,909 - INFO - [diffusion][Epoch 11139] Epoch 11140/12000
2024-11-05 03:14:36,363 - INFO - [diffusion][Epoch 11139] diffusion training Loss: 0.06618906185030937
2024-11-05 03:14:36,365 - INFO - [diffusion][Epoch 11139] diffusion learning rate: 0.001
2024-11-05 03:14:36,367 - INFO - [diffusion][Epoch 11139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:36,368 - INFO - [diffusion][Epoch 11140] Epoch 11141/12000
2024-11-05 03:14:39,459 - INFO - [diffusion][Epoch 11140] diffusion training Loss: 0.06099671870470047
2024-11-05 03:14:39,461 - INFO - [diffusion][Epoch 11140] diffusion learning rate: 0.001
2024-11-05 03:14:39,462 - INFO - [diffusion][Epoch 11140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:39,464 - INFO - [diffusion][Epoch 11141] Epoch 11142/12000
2024-11-05 03:14:42,547 - INFO - [diffusion][Epoch 11141] diffusion training Loss: 0.06594535708427429
2024-11-05 03:14:42,548 - INFO - [diffusion][Epoch 11141] diffusion learning rate: 0.001
2024-11-05 03:14:42,550 - INFO - [diffusion][Epoch 11141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:42,552 - INFO - [diffusion][Epoch 11142] Epoch 11143/12000
2024-11-05 03:14:45,898 - INFO - [diffusion][Epoch 11142] diffusion training Loss: 0.06093385349959135
2024-11-05 03:14:45,900 - INFO - [diffusion][Epoch 11142] diffusion learning rate: 0.001
2024-11-05 03:14:45,902 - INFO - [diffusion][Epoch 11142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:45,903 - INFO - [diffusion][Epoch 11143] Epoch 11144/12000
2024-11-05 03:14:49,537 - INFO - [diffusion][Epoch 11143] diffusion training Loss: 0.06773002445697784
2024-11-05 03:14:49,540 - INFO - [diffusion][Epoch 11143] diffusion learning rate: 0.001
2024-11-05 03:14:49,542 - INFO - [diffusion][Epoch 11143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:49,543 - INFO - [diffusion][Epoch 11144] Epoch 11145/12000
2024-11-05 03:14:53,341 - INFO - [diffusion][Epoch 11144] diffusion training Loss: 0.06422399170696735
2024-11-05 03:14:53,343 - INFO - [diffusion][Epoch 11144] diffusion learning rate: 0.001
2024-11-05 03:14:53,345 - INFO - [diffusion][Epoch 11144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:53,346 - INFO - [diffusion][Epoch 11145] Epoch 11146/12000
2024-11-05 03:14:57,537 - INFO - [diffusion][Epoch 11145] diffusion training Loss: 0.06503384467214346
2024-11-05 03:14:57,540 - INFO - [diffusion][Epoch 11145] diffusion learning rate: 0.001
2024-11-05 03:14:57,542 - INFO - [diffusion][Epoch 11145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:57,543 - INFO - [diffusion][Epoch 11146] Epoch 11147/12000
2024-11-05 03:15:00,649 - INFO - [diffusion][Epoch 11146] diffusion training Loss: 0.06358367018401623
2024-11-05 03:15:00,651 - INFO - [diffusion][Epoch 11146] diffusion learning rate: 0.001
2024-11-05 03:15:00,653 - INFO - [diffusion][Epoch 11146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:00,654 - INFO - [diffusion][Epoch 11147] Epoch 11148/12000
2024-11-05 03:15:03,743 - INFO - [diffusion][Epoch 11147] diffusion training Loss: 0.061861300840973854
2024-11-05 03:15:03,745 - INFO - [diffusion][Epoch 11147] diffusion learning rate: 0.001
2024-11-05 03:15:03,747 - INFO - [diffusion][Epoch 11147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:03,748 - INFO - [diffusion][Epoch 11148] Epoch 11149/12000
2024-11-05 03:15:07,112 - INFO - [diffusion][Epoch 11148] diffusion training Loss: 0.0679065715521574
2024-11-05 03:15:07,114 - INFO - [diffusion][Epoch 11148] diffusion learning rate: 0.001
2024-11-05 03:15:07,116 - INFO - [diffusion][Epoch 11148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:07,117 - INFO - [diffusion][Epoch 11149] Epoch 11150/12000
2024-11-05 03:15:10,680 - INFO - [diffusion][Epoch 11149] diffusion training Loss: 0.06189437583088875
2024-11-05 03:15:10,682 - INFO - [diffusion][Epoch 11149] diffusion learning rate: 0.001
2024-11-05 03:15:10,683 - INFO - [diffusion][Epoch 11149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:10,685 - INFO - [diffusion][Epoch 11150] Epoch 11151/12000
2024-11-05 03:15:13,624 - INFO - [diffusion][Epoch 11150] diffusion training Loss: 0.06304147746413946
2024-11-05 03:15:13,626 - INFO - [diffusion][Epoch 11150] diffusion learning rate: 0.001
2024-11-05 03:15:13,628 - INFO - [diffusion][Epoch 11150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:13,629 - INFO - [diffusion][Epoch 11151] Epoch 11152/12000
2024-11-05 03:15:16,812 - INFO - [diffusion][Epoch 11151] diffusion training Loss: 0.06004147604107857
2024-11-05 03:15:16,814 - INFO - [diffusion][Epoch 11151] diffusion learning rate: 0.001
2024-11-05 03:15:16,816 - INFO - [diffusion][Epoch 11151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:16,817 - INFO - [diffusion][Epoch 11152] Epoch 11153/12000
2024-11-05 03:15:19,971 - INFO - [diffusion][Epoch 11152] diffusion training Loss: 0.06146702263504267
2024-11-05 03:15:19,974 - INFO - [diffusion][Epoch 11152] diffusion learning rate: 0.001
2024-11-05 03:15:19,976 - INFO - [diffusion][Epoch 11152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:19,977 - INFO - [diffusion][Epoch 11153] Epoch 11154/12000
2024-11-05 03:15:23,515 - INFO - [diffusion][Epoch 11153] diffusion training Loss: 0.0628133499994874
2024-11-05 03:15:23,517 - INFO - [diffusion][Epoch 11153] diffusion learning rate: 0.001
2024-11-05 03:15:23,519 - INFO - [diffusion][Epoch 11153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:23,520 - INFO - [diffusion][Epoch 11154] Epoch 11155/12000
2024-11-05 03:15:27,088 - INFO - [diffusion][Epoch 11154] diffusion training Loss: 0.057898182421922684
2024-11-05 03:15:27,090 - INFO - [diffusion][Epoch 11154] diffusion learning rate: 0.001
2024-11-05 03:15:27,092 - INFO - [diffusion][Epoch 11154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:27,093 - INFO - [diffusion][Epoch 11155] Epoch 11156/12000
2024-11-05 03:15:30,232 - INFO - [diffusion][Epoch 11155] diffusion training Loss: 0.06398258358240128
2024-11-05 03:15:30,234 - INFO - [diffusion][Epoch 11155] diffusion learning rate: 0.001
2024-11-05 03:15:30,260 - INFO - [diffusion][Epoch 11155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:30,261 - INFO - [diffusion][Epoch 11156] Epoch 11157/12000
2024-11-05 03:15:33,379 - INFO - [diffusion][Epoch 11156] diffusion training Loss: 0.06482746079564095
2024-11-05 03:15:33,382 - INFO - [diffusion][Epoch 11156] diffusion learning rate: 0.001
2024-11-05 03:15:33,384 - INFO - [diffusion][Epoch 11156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:33,386 - INFO - [diffusion][Epoch 11157] Epoch 11158/12000
2024-11-05 03:15:36,534 - INFO - [diffusion][Epoch 11157] diffusion training Loss: 0.06457569170743227
2024-11-05 03:15:36,536 - INFO - [diffusion][Epoch 11157] diffusion learning rate: 0.001
2024-11-05 03:15:36,538 - INFO - [diffusion][Epoch 11157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:36,539 - INFO - [diffusion][Epoch 11158] Epoch 11159/12000
2024-11-05 03:15:40,087 - INFO - [diffusion][Epoch 11158] diffusion training Loss: 0.05833496991544962
2024-11-05 03:15:40,089 - INFO - [diffusion][Epoch 11158] diffusion learning rate: 0.001
2024-11-05 03:15:40,091 - INFO - [diffusion][Epoch 11158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:40,093 - INFO - [diffusion][Epoch 11159] Epoch 11160/12000
2024-11-05 03:15:43,633 - INFO - [diffusion][Epoch 11159] diffusion training Loss: 0.06019100546836853
2024-11-05 03:15:43,635 - INFO - [diffusion][Epoch 11159] diffusion learning rate: 0.001
2024-11-05 03:15:43,637 - INFO - [diffusion][Epoch 11159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:43,638 - INFO - [diffusion][Epoch 11160] Epoch 11161/12000
2024-11-05 03:15:46,741 - INFO - [diffusion][Epoch 11160] diffusion training Loss: 0.06386868841946125
2024-11-05 03:15:46,743 - INFO - [diffusion][Epoch 11160] diffusion learning rate: 0.001
2024-11-05 03:15:46,770 - INFO - [diffusion][Epoch 11160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:46,771 - INFO - [diffusion][Epoch 11161] Epoch 11162/12000
2024-11-05 03:15:49,879 - INFO - [diffusion][Epoch 11161] diffusion training Loss: 0.0657747182995081
2024-11-05 03:15:49,881 - INFO - [diffusion][Epoch 11161] diffusion learning rate: 0.001
2024-11-05 03:15:49,883 - INFO - [diffusion][Epoch 11161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:49,884 - INFO - [diffusion][Epoch 11162] Epoch 11163/12000
2024-11-05 03:15:53,032 - INFO - [diffusion][Epoch 11162] diffusion training Loss: 0.0678903441876173
2024-11-05 03:15:53,034 - INFO - [diffusion][Epoch 11162] diffusion learning rate: 0.001
2024-11-05 03:15:53,035 - INFO - [diffusion][Epoch 11162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:53,037 - INFO - [diffusion][Epoch 11163] Epoch 11164/12000
2024-11-05 03:15:56,653 - INFO - [diffusion][Epoch 11163] diffusion training Loss: 0.0641707619652152
2024-11-05 03:15:56,655 - INFO - [diffusion][Epoch 11163] diffusion learning rate: 0.001
2024-11-05 03:15:56,656 - INFO - [diffusion][Epoch 11163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:56,658 - INFO - [diffusion][Epoch 11164] Epoch 11165/12000
2024-11-05 03:16:00,105 - INFO - [diffusion][Epoch 11164] diffusion training Loss: 0.06774615217000246
2024-11-05 03:16:00,106 - INFO - [diffusion][Epoch 11164] diffusion learning rate: 0.001
2024-11-05 03:16:00,108 - INFO - [diffusion][Epoch 11164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:00,109 - INFO - [diffusion][Epoch 11165] Epoch 11166/12000
2024-11-05 03:16:03,203 - INFO - [diffusion][Epoch 11165] diffusion training Loss: 0.0696691432967782
2024-11-05 03:16:03,205 - INFO - [diffusion][Epoch 11165] diffusion learning rate: 0.001
2024-11-05 03:16:03,207 - INFO - [diffusion][Epoch 11165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:03,208 - INFO - [diffusion][Epoch 11166] Epoch 11167/12000
2024-11-05 03:16:06,367 - INFO - [diffusion][Epoch 11166] diffusion training Loss: 0.06827022507786751
2024-11-05 03:16:06,369 - INFO - [diffusion][Epoch 11166] diffusion learning rate: 0.001
2024-11-05 03:16:06,371 - INFO - [diffusion][Epoch 11166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:06,373 - INFO - [diffusion][Epoch 11167] Epoch 11168/12000
2024-11-05 03:16:09,787 - INFO - [diffusion][Epoch 11167] diffusion training Loss: 0.0643161628395319
2024-11-05 03:16:09,789 - INFO - [diffusion][Epoch 11167] diffusion learning rate: 0.001
2024-11-05 03:16:09,791 - INFO - [diffusion][Epoch 11167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:09,792 - INFO - [diffusion][Epoch 11168] Epoch 11169/12000
2024-11-05 03:16:13,581 - INFO - [diffusion][Epoch 11168] diffusion training Loss: 0.0635538762435317
2024-11-05 03:16:13,583 - INFO - [diffusion][Epoch 11168] diffusion learning rate: 0.001
2024-11-05 03:16:13,622 - INFO - [diffusion][Epoch 11168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:13,624 - INFO - [diffusion][Epoch 11169] Epoch 11170/12000
2024-11-05 03:16:16,742 - INFO - [diffusion][Epoch 11169] diffusion training Loss: 0.06639724131673574
2024-11-05 03:16:16,744 - INFO - [diffusion][Epoch 11169] diffusion learning rate: 0.001
2024-11-05 03:16:16,745 - INFO - [diffusion][Epoch 11169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:16,747 - INFO - [diffusion][Epoch 11170] Epoch 11171/12000
2024-11-05 03:16:19,782 - INFO - [diffusion][Epoch 11170] diffusion training Loss: 0.06542688142508268
2024-11-05 03:16:19,784 - INFO - [diffusion][Epoch 11170] diffusion learning rate: 0.001
2024-11-05 03:16:19,786 - INFO - [diffusion][Epoch 11170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:19,788 - INFO - [diffusion][Epoch 11171] Epoch 11172/12000
2024-11-05 03:16:22,927 - INFO - [diffusion][Epoch 11171] diffusion training Loss: 0.060800615698099136
2024-11-05 03:16:22,929 - INFO - [diffusion][Epoch 11171] diffusion learning rate: 0.001
2024-11-05 03:16:22,931 - INFO - [diffusion][Epoch 11171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:22,932 - INFO - [diffusion][Epoch 11172] Epoch 11173/12000
2024-11-05 03:16:26,448 - INFO - [diffusion][Epoch 11172] diffusion training Loss: 0.06651491858065128
2024-11-05 03:16:26,449 - INFO - [diffusion][Epoch 11172] diffusion learning rate: 0.001
2024-11-05 03:16:26,451 - INFO - [diffusion][Epoch 11172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:26,452 - INFO - [diffusion][Epoch 11173] Epoch 11174/12000
2024-11-05 03:16:29,694 - INFO - [diffusion][Epoch 11173] diffusion training Loss: 0.06514303106814623
2024-11-05 03:16:29,696 - INFO - [diffusion][Epoch 11173] diffusion learning rate: 0.001
2024-11-05 03:16:29,698 - INFO - [diffusion][Epoch 11173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:29,699 - INFO - [diffusion][Epoch 11174] Epoch 11175/12000
2024-11-05 03:16:32,676 - INFO - [diffusion][Epoch 11174] diffusion training Loss: 0.05618475191295147
2024-11-05 03:16:32,678 - INFO - [diffusion][Epoch 11174] diffusion learning rate: 0.001
2024-11-05 03:16:32,680 - INFO - [diffusion][Epoch 11174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:32,681 - INFO - [diffusion][Epoch 11175] Epoch 11176/12000
2024-11-05 03:16:35,798 - INFO - [diffusion][Epoch 11175] diffusion training Loss: 0.06926265731453896
2024-11-05 03:16:35,800 - INFO - [diffusion][Epoch 11175] diffusion learning rate: 0.001
2024-11-05 03:16:35,840 - INFO - [diffusion][Epoch 11175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:35,842 - INFO - [diffusion][Epoch 11176] Epoch 11177/12000
2024-11-05 03:16:39,396 - INFO - [diffusion][Epoch 11176] diffusion training Loss: 0.06872249767184258
2024-11-05 03:16:39,399 - INFO - [diffusion][Epoch 11176] diffusion learning rate: 0.001
2024-11-05 03:16:39,400 - INFO - [diffusion][Epoch 11176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:39,402 - INFO - [diffusion][Epoch 11177] Epoch 11178/12000
2024-11-05 03:16:42,961 - INFO - [diffusion][Epoch 11177] diffusion training Loss: 0.06416961923241615
2024-11-05 03:16:42,963 - INFO - [diffusion][Epoch 11177] diffusion learning rate: 0.001
2024-11-05 03:16:42,964 - INFO - [diffusion][Epoch 11177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:42,965 - INFO - [diffusion][Epoch 11178] Epoch 11179/12000
2024-11-05 03:16:46,058 - INFO - [diffusion][Epoch 11178] diffusion training Loss: 0.06345544289797544
2024-11-05 03:16:46,059 - INFO - [diffusion][Epoch 11178] diffusion learning rate: 0.001
2024-11-05 03:16:46,092 - INFO - [diffusion][Epoch 11178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:46,095 - INFO - [diffusion][Epoch 11179] Epoch 11180/12000
2024-11-05 03:16:49,112 - INFO - [diffusion][Epoch 11179] diffusion training Loss: 0.06396116875112057
2024-11-05 03:16:49,114 - INFO - [diffusion][Epoch 11179] diffusion learning rate: 0.001
2024-11-05 03:16:49,115 - INFO - [diffusion][Epoch 11179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:49,116 - INFO - [diffusion][Epoch 11180] Epoch 11181/12000
2024-11-05 03:16:52,324 - INFO - [diffusion][Epoch 11180] diffusion training Loss: 0.06012716516852379
2024-11-05 03:16:52,326 - INFO - [diffusion][Epoch 11180] diffusion learning rate: 0.001
2024-11-05 03:16:52,328 - INFO - [diffusion][Epoch 11180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:52,329 - INFO - [diffusion][Epoch 11181] Epoch 11182/12000
2024-11-05 03:16:55,839 - INFO - [diffusion][Epoch 11181] diffusion training Loss: 0.059071547351777554
2024-11-05 03:16:55,841 - INFO - [diffusion][Epoch 11181] diffusion learning rate: 0.001
2024-11-05 03:16:55,843 - INFO - [diffusion][Epoch 11181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:55,845 - INFO - [diffusion][Epoch 11182] Epoch 11183/12000
2024-11-05 03:16:59,038 - INFO - [diffusion][Epoch 11182] diffusion training Loss: 0.05860714893788099
2024-11-05 03:16:59,040 - INFO - [diffusion][Epoch 11182] diffusion learning rate: 0.001
2024-11-05 03:16:59,042 - INFO - [diffusion][Epoch 11182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:59,043 - INFO - [diffusion][Epoch 11183] Epoch 11184/12000
2024-11-05 03:17:01,975 - INFO - [diffusion][Epoch 11183] diffusion training Loss: 0.057740459218621254
2024-11-05 03:17:01,977 - INFO - [diffusion][Epoch 11183] diffusion learning rate: 0.001
2024-11-05 03:17:01,979 - INFO - [diffusion][Epoch 11183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:01,980 - INFO - [diffusion][Epoch 11184] Epoch 11185/12000
2024-11-05 03:17:05,392 - INFO - [diffusion][Epoch 11184] diffusion training Loss: 0.0536615327000618
2024-11-05 03:17:05,394 - INFO - [diffusion][Epoch 11184] diffusion learning rate: 0.001
2024-11-05 03:17:05,396 - INFO - [diffusion][Epoch 11184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:05,397 - INFO - [diffusion][Epoch 11185] Epoch 11186/12000
2024-11-05 03:17:08,898 - INFO - [diffusion][Epoch 11185] diffusion training Loss: 0.06321943551301956
2024-11-05 03:17:08,900 - INFO - [diffusion][Epoch 11185] diffusion learning rate: 0.001
2024-11-05 03:17:08,901 - INFO - [diffusion][Epoch 11185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:08,903 - INFO - [diffusion][Epoch 11186] Epoch 11187/12000
2024-11-05 03:17:12,444 - INFO - [diffusion][Epoch 11186] diffusion training Loss: 0.0664138151332736
2024-11-05 03:17:12,446 - INFO - [diffusion][Epoch 11186] diffusion learning rate: 0.001
2024-11-05 03:17:12,448 - INFO - [diffusion][Epoch 11186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:12,450 - INFO - [diffusion][Epoch 11187] Epoch 11188/12000
2024-11-05 03:17:15,416 - INFO - [diffusion][Epoch 11187] diffusion training Loss: 0.06458819005638361
2024-11-05 03:17:15,418 - INFO - [diffusion][Epoch 11187] diffusion learning rate: 0.001
2024-11-05 03:17:15,420 - INFO - [diffusion][Epoch 11187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:15,422 - INFO - [diffusion][Epoch 11188] Epoch 11189/12000
2024-11-05 03:17:18,455 - INFO - [diffusion][Epoch 11188] diffusion training Loss: 0.07000592164695263
2024-11-05 03:17:18,457 - INFO - [diffusion][Epoch 11188] diffusion learning rate: 0.001
2024-11-05 03:17:18,490 - INFO - [diffusion][Epoch 11188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:18,491 - INFO - [diffusion][Epoch 11189] Epoch 11190/12000
2024-11-05 03:17:21,811 - INFO - [diffusion][Epoch 11189] diffusion training Loss: 0.06754702143371105
2024-11-05 03:17:21,813 - INFO - [diffusion][Epoch 11189] diffusion learning rate: 0.001
2024-11-05 03:17:21,815 - INFO - [diffusion][Epoch 11189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:21,816 - INFO - [diffusion][Epoch 11190] Epoch 11191/12000
2024-11-05 03:17:24,816 - INFO - [diffusion][Epoch 11190] diffusion training Loss: 0.06635977327823639
2024-11-05 03:17:24,818 - INFO - [diffusion][Epoch 11190] diffusion learning rate: 0.001
2024-11-05 03:17:24,820 - INFO - [diffusion][Epoch 11190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:24,821 - INFO - [diffusion][Epoch 11191] Epoch 11192/12000
2024-11-05 03:17:27,751 - INFO - [diffusion][Epoch 11191] diffusion training Loss: 0.06650128401815891
2024-11-05 03:17:27,753 - INFO - [diffusion][Epoch 11191] diffusion learning rate: 0.001
2024-11-05 03:17:27,754 - INFO - [diffusion][Epoch 11191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:27,755 - INFO - [diffusion][Epoch 11192] Epoch 11193/12000
2024-11-05 03:17:31,052 - INFO - [diffusion][Epoch 11192] diffusion training Loss: 0.06042673718184233
2024-11-05 03:17:31,055 - INFO - [diffusion][Epoch 11192] diffusion learning rate: 0.001
2024-11-05 03:17:31,057 - INFO - [diffusion][Epoch 11192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:31,059 - INFO - [diffusion][Epoch 11193] Epoch 11194/12000
2024-11-05 03:17:34,524 - INFO - [diffusion][Epoch 11193] diffusion training Loss: 0.06639779917895794
2024-11-05 03:17:34,526 - INFO - [diffusion][Epoch 11193] diffusion learning rate: 0.001
2024-11-05 03:17:34,528 - INFO - [diffusion][Epoch 11193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:34,529 - INFO - [diffusion][Epoch 11194] Epoch 11195/12000
2024-11-05 03:17:37,500 - INFO - [diffusion][Epoch 11194] diffusion training Loss: 0.07000447250902653
2024-11-05 03:17:37,503 - INFO - [diffusion][Epoch 11194] diffusion learning rate: 0.001
2024-11-05 03:17:37,505 - INFO - [diffusion][Epoch 11194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:37,507 - INFO - [diffusion][Epoch 11195] Epoch 11196/12000
2024-11-05 03:17:40,594 - INFO - [diffusion][Epoch 11195] diffusion training Loss: 0.06778006255626678
2024-11-05 03:17:40,597 - INFO - [diffusion][Epoch 11195] diffusion learning rate: 0.001
2024-11-05 03:17:40,599 - INFO - [diffusion][Epoch 11195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:40,600 - INFO - [diffusion][Epoch 11196] Epoch 11197/12000
2024-11-05 03:17:43,765 - INFO - [diffusion][Epoch 11196] diffusion training Loss: 0.07218135055154562
2024-11-05 03:17:43,768 - INFO - [diffusion][Epoch 11196] diffusion learning rate: 0.001
2024-11-05 03:17:43,770 - INFO - [diffusion][Epoch 11196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:43,771 - INFO - [diffusion][Epoch 11197] Epoch 11198/12000
2024-11-05 03:17:47,294 - INFO - [diffusion][Epoch 11197] diffusion training Loss: 0.06845521926879883
2024-11-05 03:17:47,296 - INFO - [diffusion][Epoch 11197] diffusion learning rate: 0.001
2024-11-05 03:17:47,298 - INFO - [diffusion][Epoch 11197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:47,300 - INFO - [diffusion][Epoch 11198] Epoch 11199/12000
2024-11-05 03:17:50,356 - INFO - [diffusion][Epoch 11198] diffusion training Loss: 0.06109767407178879
2024-11-05 03:17:50,359 - INFO - [diffusion][Epoch 11198] diffusion learning rate: 0.001
2024-11-05 03:17:50,362 - INFO - [diffusion][Epoch 11198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:50,364 - INFO - [diffusion][Epoch 11199] Epoch 11200/12000
2024-11-05 03:17:53,366 - INFO - [diffusion][Epoch 11199] diffusion training Loss: 0.059666456654667854
2024-11-05 03:17:53,369 - INFO - [diffusion][Epoch 11199] diffusion learning rate: 0.001
2024-11-05 03:17:53,371 - INFO - [diffusion][Epoch 11199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:53,372 - INFO - [diffusion][Epoch 11200] Epoch 11201/12000
2024-11-05 03:17:56,615 - INFO - [diffusion][Epoch 11200] diffusion training Loss: 0.06079084239900112
2024-11-05 03:17:56,617 - INFO - [diffusion][Epoch 11200] diffusion learning rate: 0.001
2024-11-05 03:17:56,620 - INFO - [diffusion][Epoch 11200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:56,621 - INFO - [diffusion][Epoch 11201] Epoch 11202/12000
2024-11-05 03:18:00,011 - INFO - [diffusion][Epoch 11201] diffusion training Loss: 0.061161527410149574
2024-11-05 03:18:00,013 - INFO - [diffusion][Epoch 11201] diffusion learning rate: 0.001
2024-11-05 03:18:00,015 - INFO - [diffusion][Epoch 11201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:00,016 - INFO - [diffusion][Epoch 11202] Epoch 11203/12000
2024-11-05 03:18:03,029 - INFO - [diffusion][Epoch 11202] diffusion training Loss: 0.06169905513525009
2024-11-05 03:18:03,031 - INFO - [diffusion][Epoch 11202] diffusion learning rate: 0.001
2024-11-05 03:18:03,032 - INFO - [diffusion][Epoch 11202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:03,034 - INFO - [diffusion][Epoch 11203] Epoch 11204/12000
2024-11-05 03:18:06,132 - INFO - [diffusion][Epoch 11203] diffusion training Loss: 0.06480286829173565
2024-11-05 03:18:06,134 - INFO - [diffusion][Epoch 11203] diffusion learning rate: 0.001
2024-11-05 03:18:06,136 - INFO - [diffusion][Epoch 11203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:06,137 - INFO - [diffusion][Epoch 11204] Epoch 11205/12000
2024-11-05 03:18:09,296 - INFO - [diffusion][Epoch 11204] diffusion training Loss: 0.06232293415814638
2024-11-05 03:18:09,298 - INFO - [diffusion][Epoch 11204] diffusion learning rate: 0.001
2024-11-05 03:18:09,300 - INFO - [diffusion][Epoch 11204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:09,301 - INFO - [diffusion][Epoch 11205] Epoch 11206/12000
2024-11-05 03:18:13,023 - INFO - [diffusion][Epoch 11205] diffusion training Loss: 0.0589714078232646
2024-11-05 03:18:13,025 - INFO - [diffusion][Epoch 11205] diffusion learning rate: 0.001
2024-11-05 03:18:13,027 - INFO - [diffusion][Epoch 11205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:13,029 - INFO - [diffusion][Epoch 11206] Epoch 11207/12000
2024-11-05 03:18:16,629 - INFO - [diffusion][Epoch 11206] diffusion training Loss: 0.06358134001493454
2024-11-05 03:18:16,631 - INFO - [diffusion][Epoch 11206] diffusion learning rate: 0.001
2024-11-05 03:18:16,633 - INFO - [diffusion][Epoch 11206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:16,634 - INFO - [diffusion][Epoch 11207] Epoch 11208/12000
2024-11-05 03:18:20,152 - INFO - [diffusion][Epoch 11207] diffusion training Loss: 0.06598380208015442
2024-11-05 03:18:20,154 - INFO - [diffusion][Epoch 11207] diffusion learning rate: 0.001
2024-11-05 03:18:20,156 - INFO - [diffusion][Epoch 11207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:20,157 - INFO - [diffusion][Epoch 11208] Epoch 11209/12000
2024-11-05 03:18:23,319 - INFO - [diffusion][Epoch 11208] diffusion training Loss: 0.05927004665136337
2024-11-05 03:18:23,321 - INFO - [diffusion][Epoch 11208] diffusion learning rate: 0.001
2024-11-05 03:18:23,322 - INFO - [diffusion][Epoch 11208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:23,323 - INFO - [diffusion][Epoch 11209] Epoch 11210/12000
2024-11-05 03:18:26,447 - INFO - [diffusion][Epoch 11209] diffusion training Loss: 0.0640306118875742
2024-11-05 03:18:26,449 - INFO - [diffusion][Epoch 11209] diffusion learning rate: 0.001
2024-11-05 03:18:26,451 - INFO - [diffusion][Epoch 11209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:26,452 - INFO - [diffusion][Epoch 11210] Epoch 11211/12000
2024-11-05 03:18:29,757 - INFO - [diffusion][Epoch 11210] diffusion training Loss: 0.06499976944178343
2024-11-05 03:18:29,759 - INFO - [diffusion][Epoch 11210] diffusion learning rate: 0.001
2024-11-05 03:18:29,761 - INFO - [diffusion][Epoch 11210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:29,762 - INFO - [diffusion][Epoch 11211] Epoch 11212/12000
2024-11-05 03:18:33,554 - INFO - [diffusion][Epoch 11211] diffusion training Loss: 0.061804517172276974
2024-11-05 03:18:33,556 - INFO - [diffusion][Epoch 11211] diffusion learning rate: 0.001
2024-11-05 03:18:33,558 - INFO - [diffusion][Epoch 11211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:33,559 - INFO - [diffusion][Epoch 11212] Epoch 11213/12000
2024-11-05 03:18:37,055 - INFO - [diffusion][Epoch 11212] diffusion training Loss: 0.0650683781132102
2024-11-05 03:18:37,058 - INFO - [diffusion][Epoch 11212] diffusion learning rate: 0.001
2024-11-05 03:18:37,060 - INFO - [diffusion][Epoch 11212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:37,061 - INFO - [diffusion][Epoch 11213] Epoch 11214/12000
2024-11-05 03:18:40,210 - INFO - [diffusion][Epoch 11213] diffusion training Loss: 0.061386216431856155
2024-11-05 03:18:40,212 - INFO - [diffusion][Epoch 11213] diffusion learning rate: 0.001
2024-11-05 03:18:40,214 - INFO - [diffusion][Epoch 11213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:40,216 - INFO - [diffusion][Epoch 11214] Epoch 11215/12000
2024-11-05 03:18:43,327 - INFO - [diffusion][Epoch 11214] diffusion training Loss: 0.06241441983729601
2024-11-05 03:18:43,329 - INFO - [diffusion][Epoch 11214] diffusion learning rate: 0.001
2024-11-05 03:18:43,332 - INFO - [diffusion][Epoch 11214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:43,333 - INFO - [diffusion][Epoch 11215] Epoch 11216/12000
2024-11-05 03:18:46,509 - INFO - [diffusion][Epoch 11215] diffusion training Loss: 0.06771586369723082
2024-11-05 03:18:46,511 - INFO - [diffusion][Epoch 11215] diffusion learning rate: 0.001
2024-11-05 03:18:46,513 - INFO - [diffusion][Epoch 11215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:46,514 - INFO - [diffusion][Epoch 11216] Epoch 11217/12000
2024-11-05 03:18:50,245 - INFO - [diffusion][Epoch 11216] diffusion training Loss: 0.06211850792169571
2024-11-05 03:18:50,247 - INFO - [diffusion][Epoch 11216] diffusion learning rate: 0.001
2024-11-05 03:18:50,250 - INFO - [diffusion][Epoch 11216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:50,251 - INFO - [diffusion][Epoch 11217] Epoch 11218/12000
2024-11-05 03:18:53,370 - INFO - [diffusion][Epoch 11217] diffusion training Loss: 0.05899497028440237
2024-11-05 03:18:53,373 - INFO - [diffusion][Epoch 11217] diffusion learning rate: 0.001
2024-11-05 03:18:53,375 - INFO - [diffusion][Epoch 11217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:53,376 - INFO - [diffusion][Epoch 11218] Epoch 11219/12000
2024-11-05 03:18:56,416 - INFO - [diffusion][Epoch 11218] diffusion training Loss: 0.06584607716649771
2024-11-05 03:18:56,418 - INFO - [diffusion][Epoch 11218] diffusion learning rate: 0.001
2024-11-05 03:18:56,419 - INFO - [diffusion][Epoch 11218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:56,421 - INFO - [diffusion][Epoch 11219] Epoch 11220/12000
2024-11-05 03:18:59,552 - INFO - [diffusion][Epoch 11219] diffusion training Loss: 0.07093258947134018
2024-11-05 03:18:59,554 - INFO - [diffusion][Epoch 11219] diffusion learning rate: 0.001
2024-11-05 03:18:59,556 - INFO - [diffusion][Epoch 11219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:59,557 - INFO - [diffusion][Epoch 11220] Epoch 11221/12000
2024-11-05 03:19:03,103 - INFO - [diffusion][Epoch 11220] diffusion training Loss: 0.060628827661275864
2024-11-05 03:19:03,105 - INFO - [diffusion][Epoch 11220] diffusion learning rate: 0.001
2024-11-05 03:19:03,107 - INFO - [diffusion][Epoch 11220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:03,108 - INFO - [diffusion][Epoch 11221] Epoch 11222/12000
2024-11-05 03:19:06,613 - INFO - [diffusion][Epoch 11221] diffusion training Loss: 0.0611915048211813
2024-11-05 03:19:06,615 - INFO - [diffusion][Epoch 11221] diffusion learning rate: 0.001
2024-11-05 03:19:06,617 - INFO - [diffusion][Epoch 11221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:06,618 - INFO - [diffusion][Epoch 11222] Epoch 11223/12000
2024-11-05 03:19:09,681 - INFO - [diffusion][Epoch 11222] diffusion training Loss: 0.06347441580146551
2024-11-05 03:19:09,683 - INFO - [diffusion][Epoch 11222] diffusion learning rate: 0.001
2024-11-05 03:19:09,685 - INFO - [diffusion][Epoch 11222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:09,686 - INFO - [diffusion][Epoch 11223] Epoch 11224/12000
2024-11-05 03:19:12,707 - INFO - [diffusion][Epoch 11223] diffusion training Loss: 0.06477576587349176
2024-11-05 03:19:12,709 - INFO - [diffusion][Epoch 11223] diffusion learning rate: 0.001
2024-11-05 03:19:12,711 - INFO - [diffusion][Epoch 11223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:12,712 - INFO - [diffusion][Epoch 11224] Epoch 11225/12000
2024-11-05 03:19:16,005 - INFO - [diffusion][Epoch 11224] diffusion training Loss: 0.0635767886415124
2024-11-05 03:19:16,007 - INFO - [diffusion][Epoch 11224] diffusion learning rate: 0.001
2024-11-05 03:19:16,008 - INFO - [diffusion][Epoch 11224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:16,009 - INFO - [diffusion][Epoch 11225] Epoch 11226/12000
2024-11-05 03:19:19,780 - INFO - [diffusion][Epoch 11225] diffusion training Loss: 0.05886924732476473
2024-11-05 03:19:19,808 - INFO - [diffusion][Epoch 11225] diffusion learning rate: 0.001
2024-11-05 03:19:19,810 - INFO - [diffusion][Epoch 11225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:19,811 - INFO - [diffusion][Epoch 11226] Epoch 11227/12000
2024-11-05 03:19:23,340 - INFO - [diffusion][Epoch 11226] diffusion training Loss: 0.06857620365917683
2024-11-05 03:19:23,342 - INFO - [diffusion][Epoch 11226] diffusion learning rate: 0.001
2024-11-05 03:19:23,344 - INFO - [diffusion][Epoch 11226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:23,345 - INFO - [diffusion][Epoch 11227] Epoch 11228/12000
2024-11-05 03:19:26,530 - INFO - [diffusion][Epoch 11227] diffusion training Loss: 0.06243372242897749
2024-11-05 03:19:26,532 - INFO - [diffusion][Epoch 11227] diffusion learning rate: 0.001
2024-11-05 03:19:26,534 - INFO - [diffusion][Epoch 11227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:26,535 - INFO - [diffusion][Epoch 11228] Epoch 11229/12000
2024-11-05 03:19:29,774 - INFO - [diffusion][Epoch 11228] diffusion training Loss: 0.06299694254994392
2024-11-05 03:19:29,776 - INFO - [diffusion][Epoch 11228] diffusion learning rate: 0.001
2024-11-05 03:19:29,778 - INFO - [diffusion][Epoch 11228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:29,779 - INFO - [diffusion][Epoch 11229] Epoch 11230/12000
2024-11-05 03:19:32,956 - INFO - [diffusion][Epoch 11229] diffusion training Loss: 0.06487883906811476
2024-11-05 03:19:32,958 - INFO - [diffusion][Epoch 11229] diffusion learning rate: 0.001
2024-11-05 03:19:32,960 - INFO - [diffusion][Epoch 11229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:32,961 - INFO - [diffusion][Epoch 11230] Epoch 11231/12000
2024-11-05 03:19:36,299 - INFO - [diffusion][Epoch 11230] diffusion training Loss: 0.060992428101599216
2024-11-05 03:19:36,301 - INFO - [diffusion][Epoch 11230] diffusion learning rate: 0.001
2024-11-05 03:19:36,303 - INFO - [diffusion][Epoch 11230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:36,304 - INFO - [diffusion][Epoch 11231] Epoch 11232/12000
2024-11-05 03:19:39,711 - INFO - [diffusion][Epoch 11231] diffusion training Loss: 0.062115442007780075
2024-11-05 03:19:39,714 - INFO - [diffusion][Epoch 11231] diffusion learning rate: 0.001
2024-11-05 03:19:39,717 - INFO - [diffusion][Epoch 11231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:39,718 - INFO - [diffusion][Epoch 11232] Epoch 11233/12000
2024-11-05 03:19:42,851 - INFO - [diffusion][Epoch 11232] diffusion training Loss: 0.0656533557921648
2024-11-05 03:19:42,853 - INFO - [diffusion][Epoch 11232] diffusion learning rate: 0.001
2024-11-05 03:19:42,854 - INFO - [diffusion][Epoch 11232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:42,855 - INFO - [diffusion][Epoch 11233] Epoch 11234/12000
2024-11-05 03:19:45,974 - INFO - [diffusion][Epoch 11233] diffusion training Loss: 0.058974457904696465
2024-11-05 03:19:45,975 - INFO - [diffusion][Epoch 11233] diffusion learning rate: 0.001
2024-11-05 03:19:45,977 - INFO - [diffusion][Epoch 11233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:45,978 - INFO - [diffusion][Epoch 11234] Epoch 11235/12000
2024-11-05 03:19:49,006 - INFO - [diffusion][Epoch 11234] diffusion training Loss: 0.0618723388761282
2024-11-05 03:19:49,008 - INFO - [diffusion][Epoch 11234] diffusion learning rate: 0.001
2024-11-05 03:19:49,010 - INFO - [diffusion][Epoch 11234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:49,011 - INFO - [diffusion][Epoch 11235] Epoch 11236/12000
2024-11-05 03:19:52,553 - INFO - [diffusion][Epoch 11235] diffusion training Loss: 0.06657920405268669
2024-11-05 03:19:52,555 - INFO - [diffusion][Epoch 11235] diffusion learning rate: 0.001
2024-11-05 03:19:52,557 - INFO - [diffusion][Epoch 11235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:52,558 - INFO - [diffusion][Epoch 11236] Epoch 11237/12000
2024-11-05 03:19:56,096 - INFO - [diffusion][Epoch 11236] diffusion training Loss: 0.060309503227472305
2024-11-05 03:19:56,098 - INFO - [diffusion][Epoch 11236] diffusion learning rate: 0.001
2024-11-05 03:19:56,100 - INFO - [diffusion][Epoch 11236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:56,101 - INFO - [diffusion][Epoch 11237] Epoch 11238/12000
2024-11-05 03:19:59,293 - INFO - [diffusion][Epoch 11237] diffusion training Loss: 0.06552023813128471
2024-11-05 03:19:59,295 - INFO - [diffusion][Epoch 11237] diffusion learning rate: 0.001
2024-11-05 03:19:59,296 - INFO - [diffusion][Epoch 11237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:59,297 - INFO - [diffusion][Epoch 11238] Epoch 11239/12000
2024-11-05 03:20:02,405 - INFO - [diffusion][Epoch 11238] diffusion training Loss: 0.06622097175568342
2024-11-05 03:20:02,407 - INFO - [diffusion][Epoch 11238] diffusion learning rate: 0.001
2024-11-05 03:20:02,409 - INFO - [diffusion][Epoch 11238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:02,410 - INFO - [diffusion][Epoch 11239] Epoch 11240/12000
2024-11-05 03:20:05,697 - INFO - [diffusion][Epoch 11239] diffusion training Loss: 0.06807068642228842
2024-11-05 03:20:05,699 - INFO - [diffusion][Epoch 11239] diffusion learning rate: 0.001
2024-11-05 03:20:05,701 - INFO - [diffusion][Epoch 11239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:05,702 - INFO - [diffusion][Epoch 11240] Epoch 11241/12000
2024-11-05 03:20:09,330 - INFO - [diffusion][Epoch 11240] diffusion training Loss: 0.05910907220095396
2024-11-05 03:20:09,332 - INFO - [diffusion][Epoch 11240] diffusion learning rate: 0.001
2024-11-05 03:20:09,334 - INFO - [diffusion][Epoch 11240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:09,335 - INFO - [diffusion][Epoch 11241] Epoch 11242/12000
2024-11-05 03:20:12,694 - INFO - [diffusion][Epoch 11241] diffusion training Loss: 0.06775560602545738
2024-11-05 03:20:12,696 - INFO - [diffusion][Epoch 11241] diffusion learning rate: 0.001
2024-11-05 03:20:12,698 - INFO - [diffusion][Epoch 11241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:12,699 - INFO - [diffusion][Epoch 11242] Epoch 11243/12000
2024-11-05 03:20:15,722 - INFO - [diffusion][Epoch 11242] diffusion training Loss: 0.0655436422675848
2024-11-05 03:20:15,724 - INFO - [diffusion][Epoch 11242] diffusion learning rate: 0.001
2024-11-05 03:20:15,726 - INFO - [diffusion][Epoch 11242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:15,727 - INFO - [diffusion][Epoch 11243] Epoch 11244/12000
2024-11-05 03:20:18,754 - INFO - [diffusion][Epoch 11243] diffusion training Loss: 0.06807532534003258
2024-11-05 03:20:18,756 - INFO - [diffusion][Epoch 11243] diffusion learning rate: 0.001
2024-11-05 03:20:18,758 - INFO - [diffusion][Epoch 11243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:18,760 - INFO - [diffusion][Epoch 11244] Epoch 11245/12000
2024-11-05 03:20:22,329 - INFO - [diffusion][Epoch 11244] diffusion training Loss: 0.059364160522818565
2024-11-05 03:20:22,331 - INFO - [diffusion][Epoch 11244] diffusion learning rate: 0.001
2024-11-05 03:20:22,333 - INFO - [diffusion][Epoch 11244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:22,335 - INFO - [diffusion][Epoch 11245] Epoch 11246/12000
2024-11-05 03:20:26,133 - INFO - [diffusion][Epoch 11245] diffusion training Loss: 0.06636353302747011
2024-11-05 03:20:26,135 - INFO - [diffusion][Epoch 11245] diffusion learning rate: 0.001
2024-11-05 03:20:26,136 - INFO - [diffusion][Epoch 11245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:26,137 - INFO - [diffusion][Epoch 11246] Epoch 11247/12000
2024-11-05 03:20:29,384 - INFO - [diffusion][Epoch 11246] diffusion training Loss: 0.06954137235879898
2024-11-05 03:20:29,386 - INFO - [diffusion][Epoch 11246] diffusion learning rate: 0.001
2024-11-05 03:20:29,387 - INFO - [diffusion][Epoch 11246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:29,389 - INFO - [diffusion][Epoch 11247] Epoch 11248/12000
2024-11-05 03:20:32,457 - INFO - [diffusion][Epoch 11247] diffusion training Loss: 0.061528049409389496
2024-11-05 03:20:32,458 - INFO - [diffusion][Epoch 11247] diffusion learning rate: 0.001
2024-11-05 03:20:32,460 - INFO - [diffusion][Epoch 11247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:32,461 - INFO - [diffusion][Epoch 11248] Epoch 11249/12000
2024-11-05 03:20:35,595 - INFO - [diffusion][Epoch 11248] diffusion training Loss: 0.05969077628105879
2024-11-05 03:20:35,597 - INFO - [diffusion][Epoch 11248] diffusion learning rate: 0.001
2024-11-05 03:20:35,599 - INFO - [diffusion][Epoch 11248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:35,600 - INFO - [diffusion][Epoch 11249] Epoch 11250/12000
2024-11-05 03:20:39,137 - INFO - [diffusion][Epoch 11249] diffusion training Loss: 0.06283033359795809
2024-11-05 03:20:39,139 - INFO - [diffusion][Epoch 11249] diffusion learning rate: 0.001
2024-11-05 03:20:39,141 - INFO - [diffusion][Epoch 11249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:39,142 - INFO - [diffusion][Epoch 11250] Epoch 11251/12000
2024-11-05 03:20:42,698 - INFO - [diffusion][Epoch 11250] diffusion training Loss: 0.05999627057462931
2024-11-05 03:20:42,700 - INFO - [diffusion][Epoch 11250] diffusion learning rate: 0.001
2024-11-05 03:20:42,701 - INFO - [diffusion][Epoch 11250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:42,703 - INFO - [diffusion][Epoch 11251] Epoch 11252/12000
2024-11-05 03:20:45,811 - INFO - [diffusion][Epoch 11251] diffusion training Loss: 0.059446875005960464
2024-11-05 03:20:45,812 - INFO - [diffusion][Epoch 11251] diffusion learning rate: 0.001
2024-11-05 03:20:45,814 - INFO - [diffusion][Epoch 11251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:45,815 - INFO - [diffusion][Epoch 11252] Epoch 11253/12000
2024-11-05 03:20:48,904 - INFO - [diffusion][Epoch 11252] diffusion training Loss: 0.06379993259906769
2024-11-05 03:20:48,906 - INFO - [diffusion][Epoch 11252] diffusion learning rate: 0.001
2024-11-05 03:20:48,908 - INFO - [diffusion][Epoch 11252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:48,910 - INFO - [diffusion][Epoch 11253] Epoch 11254/12000
2024-11-05 03:20:52,048 - INFO - [diffusion][Epoch 11253] diffusion training Loss: 0.06213132478296757
2024-11-05 03:20:52,050 - INFO - [diffusion][Epoch 11253] diffusion learning rate: 0.001
2024-11-05 03:20:52,052 - INFO - [diffusion][Epoch 11253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:52,053 - INFO - [diffusion][Epoch 11254] Epoch 11255/12000
2024-11-05 03:20:55,636 - INFO - [diffusion][Epoch 11254] diffusion training Loss: 0.06358334701508284
2024-11-05 03:20:55,638 - INFO - [diffusion][Epoch 11254] diffusion learning rate: 0.001
2024-11-05 03:20:55,642 - INFO - [diffusion][Epoch 11254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:55,643 - INFO - [diffusion][Epoch 11255] Epoch 11256/12000
2024-11-05 03:20:58,902 - INFO - [diffusion][Epoch 11255] diffusion training Loss: 0.061404893174767494
2024-11-05 03:20:58,905 - INFO - [diffusion][Epoch 11255] diffusion learning rate: 0.001
2024-11-05 03:20:58,907 - INFO - [diffusion][Epoch 11255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:58,908 - INFO - [diffusion][Epoch 11256] Epoch 11257/12000
2024-11-05 03:21:01,982 - INFO - [diffusion][Epoch 11256] diffusion training Loss: 0.06429094728082418
2024-11-05 03:21:01,984 - INFO - [diffusion][Epoch 11256] diffusion learning rate: 0.001
2024-11-05 03:21:01,986 - INFO - [diffusion][Epoch 11256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:01,987 - INFO - [diffusion][Epoch 11257] Epoch 11258/12000
2024-11-05 03:21:05,081 - INFO - [diffusion][Epoch 11257] diffusion training Loss: 0.0656629353761673
2024-11-05 03:21:05,083 - INFO - [diffusion][Epoch 11257] diffusion learning rate: 0.001
2024-11-05 03:21:05,085 - INFO - [diffusion][Epoch 11257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:05,086 - INFO - [diffusion][Epoch 11258] Epoch 11259/12000
2024-11-05 03:21:08,573 - INFO - [diffusion][Epoch 11258] diffusion training Loss: 0.06419479195028543
2024-11-05 03:21:08,575 - INFO - [diffusion][Epoch 11258] diffusion learning rate: 0.001
2024-11-05 03:21:08,577 - INFO - [diffusion][Epoch 11258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:08,579 - INFO - [diffusion][Epoch 11259] Epoch 11260/12000
2024-11-05 03:21:12,118 - INFO - [diffusion][Epoch 11259] diffusion training Loss: 0.06203996483236551
2024-11-05 03:21:12,120 - INFO - [diffusion][Epoch 11259] diffusion learning rate: 0.001
2024-11-05 03:21:12,121 - INFO - [diffusion][Epoch 11259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:12,123 - INFO - [diffusion][Epoch 11260] Epoch 11261/12000
2024-11-05 03:21:15,215 - INFO - [diffusion][Epoch 11260] diffusion training Loss: 0.06214097887277603
2024-11-05 03:21:15,217 - INFO - [diffusion][Epoch 11260] diffusion learning rate: 0.001
2024-11-05 03:21:15,219 - INFO - [diffusion][Epoch 11260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:15,220 - INFO - [diffusion][Epoch 11261] Epoch 11262/12000
2024-11-05 03:21:18,335 - INFO - [diffusion][Epoch 11261] diffusion training Loss: 0.07189490087330341
2024-11-05 03:21:18,337 - INFO - [diffusion][Epoch 11261] diffusion learning rate: 0.001
2024-11-05 03:21:18,339 - INFO - [diffusion][Epoch 11261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:18,340 - INFO - [diffusion][Epoch 11262] Epoch 11263/12000
2024-11-05 03:21:21,597 - INFO - [diffusion][Epoch 11262] diffusion training Loss: 0.06382424477487803
2024-11-05 03:21:21,599 - INFO - [diffusion][Epoch 11262] diffusion learning rate: 0.001
2024-11-05 03:21:21,601 - INFO - [diffusion][Epoch 11262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:21,602 - INFO - [diffusion][Epoch 11263] Epoch 11264/12000
2024-11-05 03:21:25,277 - INFO - [diffusion][Epoch 11263] diffusion training Loss: 0.06599242985248566
2024-11-05 03:21:25,279 - INFO - [diffusion][Epoch 11263] diffusion learning rate: 0.001
2024-11-05 03:21:25,281 - INFO - [diffusion][Epoch 11263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:25,282 - INFO - [diffusion][Epoch 11264] Epoch 11265/12000
2024-11-05 03:21:29,572 - INFO - [diffusion][Epoch 11264] diffusion training Loss: 0.07075376156717539
2024-11-05 03:21:29,574 - INFO - [diffusion][Epoch 11264] diffusion learning rate: 0.001
2024-11-05 03:21:29,576 - INFO - [diffusion][Epoch 11264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:29,577 - INFO - [diffusion][Epoch 11265] Epoch 11266/12000
2024-11-05 03:21:33,097 - INFO - [diffusion][Epoch 11265] diffusion training Loss: 0.06452105659991503
2024-11-05 03:21:33,099 - INFO - [diffusion][Epoch 11265] diffusion learning rate: 0.001
2024-11-05 03:21:33,101 - INFO - [diffusion][Epoch 11265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:33,102 - INFO - [diffusion][Epoch 11266] Epoch 11267/12000
2024-11-05 03:21:36,244 - INFO - [diffusion][Epoch 11266] diffusion training Loss: 0.06517176236957312
2024-11-05 03:21:36,246 - INFO - [diffusion][Epoch 11266] diffusion learning rate: 0.001
2024-11-05 03:21:36,248 - INFO - [diffusion][Epoch 11266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:36,249 - INFO - [diffusion][Epoch 11267] Epoch 11268/12000
2024-11-05 03:21:39,419 - INFO - [diffusion][Epoch 11267] diffusion training Loss: 0.06291336379945278
2024-11-05 03:21:39,422 - INFO - [diffusion][Epoch 11267] diffusion learning rate: 0.001
2024-11-05 03:21:39,423 - INFO - [diffusion][Epoch 11267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:39,425 - INFO - [diffusion][Epoch 11268] Epoch 11269/12000
2024-11-05 03:21:42,735 - INFO - [diffusion][Epoch 11268] diffusion training Loss: 0.06707955338060856
2024-11-05 03:21:42,737 - INFO - [diffusion][Epoch 11268] diffusion learning rate: 0.001
2024-11-05 03:21:42,739 - INFO - [diffusion][Epoch 11268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:42,741 - INFO - [diffusion][Epoch 11269] Epoch 11270/12000
2024-11-05 03:21:46,452 - INFO - [diffusion][Epoch 11269] diffusion training Loss: 0.06135172862559557
2024-11-05 03:21:46,454 - INFO - [diffusion][Epoch 11269] diffusion learning rate: 0.001
2024-11-05 03:21:46,456 - INFO - [diffusion][Epoch 11269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:46,457 - INFO - [diffusion][Epoch 11270] Epoch 11271/12000
2024-11-05 03:21:49,739 - INFO - [diffusion][Epoch 11270] diffusion training Loss: 0.06339102052152157
2024-11-05 03:21:49,741 - INFO - [diffusion][Epoch 11270] diffusion learning rate: 0.001
2024-11-05 03:21:49,743 - INFO - [diffusion][Epoch 11270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:49,744 - INFO - [diffusion][Epoch 11271] Epoch 11272/12000
2024-11-05 03:21:52,933 - INFO - [diffusion][Epoch 11271] diffusion training Loss: 0.06704074330627918
2024-11-05 03:21:52,934 - INFO - [diffusion][Epoch 11271] diffusion learning rate: 0.001
2024-11-05 03:21:52,936 - INFO - [diffusion][Epoch 11271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:52,937 - INFO - [diffusion][Epoch 11272] Epoch 11273/12000
2024-11-05 03:21:56,028 - INFO - [diffusion][Epoch 11272] diffusion training Loss: 0.06705505773425102
2024-11-05 03:21:56,030 - INFO - [diffusion][Epoch 11272] diffusion learning rate: 0.001
2024-11-05 03:21:56,032 - INFO - [diffusion][Epoch 11272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:56,033 - INFO - [diffusion][Epoch 11273] Epoch 11274/12000
2024-11-05 03:21:59,556 - INFO - [diffusion][Epoch 11273] diffusion training Loss: 0.06290546245872974
2024-11-05 03:21:59,559 - INFO - [diffusion][Epoch 11273] diffusion learning rate: 0.001
2024-11-05 03:21:59,561 - INFO - [diffusion][Epoch 11273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:59,563 - INFO - [diffusion][Epoch 11274] Epoch 11275/12000
2024-11-05 03:22:03,266 - INFO - [diffusion][Epoch 11274] diffusion training Loss: 0.06512029096484184
2024-11-05 03:22:03,267 - INFO - [diffusion][Epoch 11274] diffusion learning rate: 0.001
2024-11-05 03:22:03,269 - INFO - [diffusion][Epoch 11274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:03,270 - INFO - [diffusion][Epoch 11275] Epoch 11276/12000
2024-11-05 03:22:06,415 - INFO - [diffusion][Epoch 11275] diffusion training Loss: 0.06607284490019083
2024-11-05 03:22:06,417 - INFO - [diffusion][Epoch 11275] diffusion learning rate: 0.001
2024-11-05 03:22:06,419 - INFO - [diffusion][Epoch 11275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:06,420 - INFO - [diffusion][Epoch 11276] Epoch 11277/12000
2024-11-05 03:22:09,510 - INFO - [diffusion][Epoch 11276] diffusion training Loss: 0.06772536411881447
2024-11-05 03:22:09,512 - INFO - [diffusion][Epoch 11276] diffusion learning rate: 0.001
2024-11-05 03:22:09,514 - INFO - [diffusion][Epoch 11276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:09,515 - INFO - [diffusion][Epoch 11277] Epoch 11278/12000
2024-11-05 03:22:12,623 - INFO - [diffusion][Epoch 11277] diffusion training Loss: 0.06435353495180607
2024-11-05 03:22:12,625 - INFO - [diffusion][Epoch 11277] diffusion learning rate: 0.001
2024-11-05 03:22:12,627 - INFO - [diffusion][Epoch 11277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:12,629 - INFO - [diffusion][Epoch 11278] Epoch 11279/12000
2024-11-05 03:22:16,144 - INFO - [diffusion][Epoch 11278] diffusion training Loss: 0.0660054013133049
2024-11-05 03:22:16,146 - INFO - [diffusion][Epoch 11278] diffusion learning rate: 0.001
2024-11-05 03:22:16,176 - INFO - [diffusion][Epoch 11278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:16,178 - INFO - [diffusion][Epoch 11279] Epoch 11280/12000
2024-11-05 03:22:19,672 - INFO - [diffusion][Epoch 11279] diffusion training Loss: 0.06136106047779322
2024-11-05 03:22:19,674 - INFO - [diffusion][Epoch 11279] diffusion learning rate: 0.001
2024-11-05 03:22:19,676 - INFO - [diffusion][Epoch 11279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:19,678 - INFO - [diffusion][Epoch 11280] Epoch 11281/12000
2024-11-05 03:22:22,905 - INFO - [diffusion][Epoch 11280] diffusion training Loss: 0.0644907122477889
2024-11-05 03:22:22,907 - INFO - [diffusion][Epoch 11280] diffusion learning rate: 0.001
2024-11-05 03:22:22,909 - INFO - [diffusion][Epoch 11280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:22,910 - INFO - [diffusion][Epoch 11281] Epoch 11282/12000
2024-11-05 03:22:26,076 - INFO - [diffusion][Epoch 11281] diffusion training Loss: 0.06738708354532719
2024-11-05 03:22:26,078 - INFO - [diffusion][Epoch 11281] diffusion learning rate: 0.001
2024-11-05 03:22:26,080 - INFO - [diffusion][Epoch 11281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:26,081 - INFO - [diffusion][Epoch 11282] Epoch 11283/12000
2024-11-05 03:22:29,269 - INFO - [diffusion][Epoch 11282] diffusion training Loss: 0.07120010815560818
2024-11-05 03:22:29,270 - INFO - [diffusion][Epoch 11282] diffusion learning rate: 0.001
2024-11-05 03:22:29,272 - INFO - [diffusion][Epoch 11282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:29,274 - INFO - [diffusion][Epoch 11283] Epoch 11284/12000
2024-11-05 03:22:32,834 - INFO - [diffusion][Epoch 11283] diffusion training Loss: 0.06772076711058617
2024-11-05 03:22:32,836 - INFO - [diffusion][Epoch 11283] diffusion learning rate: 0.001
2024-11-05 03:22:32,838 - INFO - [diffusion][Epoch 11283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:32,839 - INFO - [diffusion][Epoch 11284] Epoch 11285/12000
2024-11-05 03:22:36,581 - INFO - [diffusion][Epoch 11284] diffusion training Loss: 0.06061606667935848
2024-11-05 03:22:36,583 - INFO - [diffusion][Epoch 11284] diffusion learning rate: 0.001
2024-11-05 03:22:36,584 - INFO - [diffusion][Epoch 11284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:36,586 - INFO - [diffusion][Epoch 11285] Epoch 11286/12000
2024-11-05 03:22:39,821 - INFO - [diffusion][Epoch 11285] diffusion training Loss: 0.06487369164824486
2024-11-05 03:22:39,824 - INFO - [diffusion][Epoch 11285] diffusion learning rate: 0.001
2024-11-05 03:22:39,826 - INFO - [diffusion][Epoch 11285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:39,827 - INFO - [diffusion][Epoch 11286] Epoch 11287/12000
2024-11-05 03:22:42,906 - INFO - [diffusion][Epoch 11286] diffusion training Loss: 0.06290339678525925
2024-11-05 03:22:42,909 - INFO - [diffusion][Epoch 11286] diffusion learning rate: 0.001
2024-11-05 03:22:42,911 - INFO - [diffusion][Epoch 11286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:42,913 - INFO - [diffusion][Epoch 11287] Epoch 11288/12000
2024-11-05 03:22:45,977 - INFO - [diffusion][Epoch 11287] diffusion training Loss: 0.06330834608525038
2024-11-05 03:22:45,979 - INFO - [diffusion][Epoch 11287] diffusion learning rate: 0.001
2024-11-05 03:22:45,981 - INFO - [diffusion][Epoch 11287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:45,982 - INFO - [diffusion][Epoch 11288] Epoch 11289/12000
2024-11-05 03:22:49,542 - INFO - [diffusion][Epoch 11288] diffusion training Loss: 0.06087525188922882
2024-11-05 03:22:49,544 - INFO - [diffusion][Epoch 11288] diffusion learning rate: 0.001
2024-11-05 03:22:49,546 - INFO - [diffusion][Epoch 11288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:49,547 - INFO - [diffusion][Epoch 11289] Epoch 11290/12000
2024-11-05 03:22:53,184 - INFO - [diffusion][Epoch 11289] diffusion training Loss: 0.06312659196555614
2024-11-05 03:22:53,186 - INFO - [diffusion][Epoch 11289] diffusion learning rate: 0.001
2024-11-05 03:22:53,188 - INFO - [diffusion][Epoch 11289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:53,189 - INFO - [diffusion][Epoch 11290] Epoch 11291/12000
2024-11-05 03:22:56,415 - INFO - [diffusion][Epoch 11290] diffusion training Loss: 0.06259133294224739
2024-11-05 03:22:56,417 - INFO - [diffusion][Epoch 11290] diffusion learning rate: 0.001
2024-11-05 03:22:56,418 - INFO - [diffusion][Epoch 11290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:56,420 - INFO - [diffusion][Epoch 11291] Epoch 11292/12000
2024-11-05 03:22:59,676 - INFO - [diffusion][Epoch 11291] diffusion training Loss: 0.06639882549643517
2024-11-05 03:22:59,679 - INFO - [diffusion][Epoch 11291] diffusion learning rate: 0.001
2024-11-05 03:22:59,681 - INFO - [diffusion][Epoch 11291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:59,682 - INFO - [diffusion][Epoch 11292] Epoch 11293/12000
2024-11-05 03:23:02,884 - INFO - [diffusion][Epoch 11292] diffusion training Loss: 0.06341308448463678
2024-11-05 03:23:02,886 - INFO - [diffusion][Epoch 11292] diffusion learning rate: 0.001
2024-11-05 03:23:02,888 - INFO - [diffusion][Epoch 11292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:02,889 - INFO - [diffusion][Epoch 11293] Epoch 11294/12000
2024-11-05 03:23:06,218 - INFO - [diffusion][Epoch 11293] diffusion training Loss: 0.06393675412982702
2024-11-05 03:23:06,220 - INFO - [diffusion][Epoch 11293] diffusion learning rate: 0.001
2024-11-05 03:23:06,222 - INFO - [diffusion][Epoch 11293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:06,224 - INFO - [diffusion][Epoch 11294] Epoch 11295/12000
2024-11-05 03:23:09,947 - INFO - [diffusion][Epoch 11294] diffusion training Loss: 0.061316004022955894
2024-11-05 03:23:09,949 - INFO - [diffusion][Epoch 11294] diffusion learning rate: 0.001
2024-11-05 03:23:09,952 - INFO - [diffusion][Epoch 11294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:09,953 - INFO - [diffusion][Epoch 11295] Epoch 11296/12000
2024-11-05 03:23:13,461 - INFO - [diffusion][Epoch 11295] diffusion training Loss: 0.06276268512010574
2024-11-05 03:23:13,464 - INFO - [diffusion][Epoch 11295] diffusion learning rate: 0.001
2024-11-05 03:23:13,465 - INFO - [diffusion][Epoch 11295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:13,467 - INFO - [diffusion][Epoch 11296] Epoch 11297/12000
2024-11-05 03:23:16,589 - INFO - [diffusion][Epoch 11296] diffusion training Loss: 0.06078910268843174
2024-11-05 03:23:16,592 - INFO - [diffusion][Epoch 11296] diffusion learning rate: 0.001
2024-11-05 03:23:16,594 - INFO - [diffusion][Epoch 11296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:16,595 - INFO - [diffusion][Epoch 11297] Epoch 11298/12000
2024-11-05 03:23:19,783 - INFO - [diffusion][Epoch 11297] diffusion training Loss: 0.0709418635815382
2024-11-05 03:23:19,785 - INFO - [diffusion][Epoch 11297] diffusion learning rate: 0.001
2024-11-05 03:23:19,787 - INFO - [diffusion][Epoch 11297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:19,788 - INFO - [diffusion][Epoch 11298] Epoch 11299/12000
2024-11-05 03:23:23,056 - INFO - [diffusion][Epoch 11298] diffusion training Loss: 0.05998479016125202
2024-11-05 03:23:23,058 - INFO - [diffusion][Epoch 11298] diffusion learning rate: 0.001
2024-11-05 03:23:23,060 - INFO - [diffusion][Epoch 11298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:23,061 - INFO - [diffusion][Epoch 11299] Epoch 11300/12000
2024-11-05 03:23:26,734 - INFO - [diffusion][Epoch 11299] diffusion training Loss: 0.05833038222044706
2024-11-05 03:23:26,736 - INFO - [diffusion][Epoch 11299] diffusion learning rate: 0.001
2024-11-05 03:23:26,738 - INFO - [diffusion][Epoch 11299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:26,739 - INFO - [diffusion][Epoch 11300] Epoch 11301/12000
2024-11-05 03:23:30,169 - INFO - [diffusion][Epoch 11300] diffusion training Loss: 0.06616907194256783
2024-11-05 03:23:30,170 - INFO - [diffusion][Epoch 11300] diffusion learning rate: 0.001
2024-11-05 03:23:30,220 - INFO - [diffusion][Epoch 11300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:30,221 - INFO - [diffusion][Epoch 11301] Epoch 11302/12000
2024-11-05 03:23:33,311 - INFO - [diffusion][Epoch 11301] diffusion training Loss: 0.06835658103227615
2024-11-05 03:23:33,313 - INFO - [diffusion][Epoch 11301] diffusion learning rate: 0.001
2024-11-05 03:23:33,315 - INFO - [diffusion][Epoch 11301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:33,316 - INFO - [diffusion][Epoch 11302] Epoch 11303/12000
2024-11-05 03:23:36,361 - INFO - [diffusion][Epoch 11302] diffusion training Loss: 0.06449839938431978
2024-11-05 03:23:36,363 - INFO - [diffusion][Epoch 11302] diffusion learning rate: 0.001
2024-11-05 03:23:36,366 - INFO - [diffusion][Epoch 11302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:36,367 - INFO - [diffusion][Epoch 11303] Epoch 11304/12000
2024-11-05 03:23:39,690 - INFO - [diffusion][Epoch 11303] diffusion training Loss: 0.06294042337685823
2024-11-05 03:23:39,692 - INFO - [diffusion][Epoch 11303] diffusion learning rate: 0.001
2024-11-05 03:23:39,694 - INFO - [diffusion][Epoch 11303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:39,695 - INFO - [diffusion][Epoch 11304] Epoch 11305/12000
2024-11-05 03:23:43,410 - INFO - [diffusion][Epoch 11304] diffusion training Loss: 0.06662872154265642
2024-11-05 03:23:43,412 - INFO - [diffusion][Epoch 11304] diffusion learning rate: 0.001
2024-11-05 03:23:43,414 - INFO - [diffusion][Epoch 11304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:43,415 - INFO - [diffusion][Epoch 11305] Epoch 11306/12000
2024-11-05 03:23:46,922 - INFO - [diffusion][Epoch 11305] diffusion training Loss: 0.05684342607855797
2024-11-05 03:23:46,924 - INFO - [diffusion][Epoch 11305] diffusion learning rate: 0.001
2024-11-05 03:23:46,925 - INFO - [diffusion][Epoch 11305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:46,927 - INFO - [diffusion][Epoch 11306] Epoch 11307/12000
2024-11-05 03:23:50,036 - INFO - [diffusion][Epoch 11306] diffusion training Loss: 0.061730753630399704
2024-11-05 03:23:50,038 - INFO - [diffusion][Epoch 11306] diffusion learning rate: 0.001
2024-11-05 03:23:50,040 - INFO - [diffusion][Epoch 11306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:50,041 - INFO - [diffusion][Epoch 11307] Epoch 11308/12000
2024-11-05 03:23:53,224 - INFO - [diffusion][Epoch 11307] diffusion training Loss: 0.062044099904596806
2024-11-05 03:23:53,226 - INFO - [diffusion][Epoch 11307] diffusion learning rate: 0.001
2024-11-05 03:23:53,228 - INFO - [diffusion][Epoch 11307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:53,229 - INFO - [diffusion][Epoch 11308] Epoch 11309/12000
2024-11-05 03:23:56,473 - INFO - [diffusion][Epoch 11308] diffusion training Loss: 0.058352491818368435
2024-11-05 03:23:56,475 - INFO - [diffusion][Epoch 11308] diffusion learning rate: 0.001
2024-11-05 03:23:56,477 - INFO - [diffusion][Epoch 11308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:56,478 - INFO - [diffusion][Epoch 11309] Epoch 11310/12000
2024-11-05 03:24:00,097 - INFO - [diffusion][Epoch 11309] diffusion training Loss: 0.06321761384606361
2024-11-05 03:24:00,099 - INFO - [diffusion][Epoch 11309] diffusion learning rate: 0.001
2024-11-05 03:24:00,101 - INFO - [diffusion][Epoch 11309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:00,102 - INFO - [diffusion][Epoch 11310] Epoch 11311/12000
2024-11-05 03:24:03,498 - INFO - [diffusion][Epoch 11310] diffusion training Loss: 0.062011380679905415
2024-11-05 03:24:03,499 - INFO - [diffusion][Epoch 11310] diffusion learning rate: 0.001
2024-11-05 03:24:03,501 - INFO - [diffusion][Epoch 11310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:03,503 - INFO - [diffusion][Epoch 11311] Epoch 11312/12000
2024-11-05 03:24:06,325 - INFO - [diffusion][Epoch 11311] diffusion training Loss: 0.05909860320389271
2024-11-05 03:24:06,327 - INFO - [diffusion][Epoch 11311] diffusion learning rate: 0.001
2024-11-05 03:24:06,328 - INFO - [diffusion][Epoch 11311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:06,329 - INFO - [diffusion][Epoch 11312] Epoch 11313/12000
2024-11-05 03:24:09,386 - INFO - [diffusion][Epoch 11312] diffusion training Loss: 0.0646938057616353
2024-11-05 03:24:09,388 - INFO - [diffusion][Epoch 11312] diffusion learning rate: 0.001
2024-11-05 03:24:09,390 - INFO - [diffusion][Epoch 11312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:09,391 - INFO - [diffusion][Epoch 11313] Epoch 11314/12000
2024-11-05 03:24:12,861 - INFO - [diffusion][Epoch 11313] diffusion training Loss: 0.06354180444031954
2024-11-05 03:24:12,863 - INFO - [diffusion][Epoch 11313] diffusion learning rate: 0.001
2024-11-05 03:24:12,865 - INFO - [diffusion][Epoch 11313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:12,867 - INFO - [diffusion][Epoch 11314] Epoch 11315/12000
2024-11-05 03:24:16,126 - INFO - [diffusion][Epoch 11314] diffusion training Loss: 0.058077617548406124
2024-11-05 03:24:16,128 - INFO - [diffusion][Epoch 11314] diffusion learning rate: 0.001
2024-11-05 03:24:16,130 - INFO - [diffusion][Epoch 11314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:16,131 - INFO - [diffusion][Epoch 11315] Epoch 11316/12000
2024-11-05 03:24:19,300 - INFO - [diffusion][Epoch 11315] diffusion training Loss: 0.060275028459727764
2024-11-05 03:24:19,302 - INFO - [diffusion][Epoch 11315] diffusion learning rate: 0.001
2024-11-05 03:24:19,304 - INFO - [diffusion][Epoch 11315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:19,305 - INFO - [diffusion][Epoch 11316] Epoch 11317/12000
2024-11-05 03:24:22,446 - INFO - [diffusion][Epoch 11316] diffusion training Loss: 0.06646094098687172
2024-11-05 03:24:22,448 - INFO - [diffusion][Epoch 11316] diffusion learning rate: 0.001
2024-11-05 03:24:22,450 - INFO - [diffusion][Epoch 11316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:22,451 - INFO - [diffusion][Epoch 11317] Epoch 11318/12000
2024-11-05 03:24:25,795 - INFO - [diffusion][Epoch 11317] diffusion training Loss: 0.0625140480697155
2024-11-05 03:24:25,797 - INFO - [diffusion][Epoch 11317] diffusion learning rate: 0.001
2024-11-05 03:24:25,799 - INFO - [diffusion][Epoch 11317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:25,800 - INFO - [diffusion][Epoch 11318] Epoch 11319/12000
2024-11-05 03:24:29,293 - INFO - [diffusion][Epoch 11318] diffusion training Loss: 0.062583620660007
2024-11-05 03:24:29,294 - INFO - [diffusion][Epoch 11318] diffusion learning rate: 0.001
2024-11-05 03:24:29,296 - INFO - [diffusion][Epoch 11318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:29,297 - INFO - [diffusion][Epoch 11319] Epoch 11320/12000
2024-11-05 03:24:32,342 - INFO - [diffusion][Epoch 11319] diffusion training Loss: 0.06232762150466442
2024-11-05 03:24:32,344 - INFO - [diffusion][Epoch 11319] diffusion learning rate: 0.001
2024-11-05 03:24:32,345 - INFO - [diffusion][Epoch 11319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:32,347 - INFO - [diffusion][Epoch 11320] Epoch 11321/12000
2024-11-05 03:24:35,449 - INFO - [diffusion][Epoch 11320] diffusion training Loss: 0.0645886855199933
2024-11-05 03:24:35,451 - INFO - [diffusion][Epoch 11320] diffusion learning rate: 0.001
2024-11-05 03:24:35,452 - INFO - [diffusion][Epoch 11320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:35,454 - INFO - [diffusion][Epoch 11321] Epoch 11322/12000
2024-11-05 03:24:38,792 - INFO - [diffusion][Epoch 11321] diffusion training Loss: 0.06691147666424513
2024-11-05 03:24:38,794 - INFO - [diffusion][Epoch 11321] diffusion learning rate: 0.001
2024-11-05 03:24:38,796 - INFO - [diffusion][Epoch 11321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:38,798 - INFO - [diffusion][Epoch 11322] Epoch 11323/12000
2024-11-05 03:24:42,235 - INFO - [diffusion][Epoch 11322] diffusion training Loss: 0.06534209661185741
2024-11-05 03:24:42,238 - INFO - [diffusion][Epoch 11322] diffusion learning rate: 0.001
2024-11-05 03:24:42,240 - INFO - [diffusion][Epoch 11322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:42,242 - INFO - [diffusion][Epoch 11323] Epoch 11324/12000
2024-11-05 03:24:45,164 - INFO - [diffusion][Epoch 11323] diffusion training Loss: 0.06297127529978752
2024-11-05 03:24:45,166 - INFO - [diffusion][Epoch 11323] diffusion learning rate: 0.001
2024-11-05 03:24:45,167 - INFO - [diffusion][Epoch 11323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:45,168 - INFO - [diffusion][Epoch 11324] Epoch 11325/12000
2024-11-05 03:24:48,230 - INFO - [diffusion][Epoch 11324] diffusion training Loss: 0.06853757053613663
2024-11-05 03:24:48,232 - INFO - [diffusion][Epoch 11324] diffusion learning rate: 0.001
2024-11-05 03:24:48,234 - INFO - [diffusion][Epoch 11324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:48,235 - INFO - [diffusion][Epoch 11325] Epoch 11326/12000
2024-11-05 03:24:51,804 - INFO - [diffusion][Epoch 11325] diffusion training Loss: 0.0660945326089859
2024-11-05 03:24:51,805 - INFO - [diffusion][Epoch 11325] diffusion learning rate: 0.001
2024-11-05 03:24:51,807 - INFO - [diffusion][Epoch 11325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:51,809 - INFO - [diffusion][Epoch 11326] Epoch 11327/12000
2024-11-05 03:24:55,248 - INFO - [diffusion][Epoch 11326] diffusion training Loss: 0.06521742790937424
2024-11-05 03:24:55,250 - INFO - [diffusion][Epoch 11326] diffusion learning rate: 0.001
2024-11-05 03:24:55,252 - INFO - [diffusion][Epoch 11326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:55,253 - INFO - [diffusion][Epoch 11327] Epoch 11328/12000
2024-11-05 03:24:58,340 - INFO - [diffusion][Epoch 11327] diffusion training Loss: 0.0628479365259409
2024-11-05 03:24:58,342 - INFO - [diffusion][Epoch 11327] diffusion learning rate: 0.001
2024-11-05 03:24:58,344 - INFO - [diffusion][Epoch 11327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:58,345 - INFO - [diffusion][Epoch 11328] Epoch 11329/12000
2024-11-05 03:25:01,234 - INFO - [diffusion][Epoch 11328] diffusion training Loss: 0.06546357832849026
2024-11-05 03:25:01,237 - INFO - [diffusion][Epoch 11328] diffusion learning rate: 0.001
2024-11-05 03:25:01,238 - INFO - [diffusion][Epoch 11328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:01,240 - INFO - [diffusion][Epoch 11329] Epoch 11330/12000
2024-11-05 03:25:04,724 - INFO - [diffusion][Epoch 11329] diffusion training Loss: 0.0590308727696538
2024-11-05 03:25:04,726 - INFO - [diffusion][Epoch 11329] diffusion learning rate: 0.001
2024-11-05 03:25:04,728 - INFO - [diffusion][Epoch 11329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:04,729 - INFO - [diffusion][Epoch 11330] Epoch 11331/12000
2024-11-05 03:25:08,236 - INFO - [diffusion][Epoch 11330] diffusion training Loss: 0.06019802950322628
2024-11-05 03:25:08,238 - INFO - [diffusion][Epoch 11330] diffusion learning rate: 0.001
2024-11-05 03:25:08,240 - INFO - [diffusion][Epoch 11330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:08,241 - INFO - [diffusion][Epoch 11331] Epoch 11332/12000
2024-11-05 03:25:11,299 - INFO - [diffusion][Epoch 11331] diffusion training Loss: 0.06653785146772861
2024-11-05 03:25:11,301 - INFO - [diffusion][Epoch 11331] diffusion learning rate: 0.001
2024-11-05 03:25:11,302 - INFO - [diffusion][Epoch 11331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:11,304 - INFO - [diffusion][Epoch 11332] Epoch 11333/12000
2024-11-05 03:25:14,292 - INFO - [diffusion][Epoch 11332] diffusion training Loss: 0.06036025378853083
2024-11-05 03:25:14,294 - INFO - [diffusion][Epoch 11332] diffusion learning rate: 0.001
2024-11-05 03:25:14,296 - INFO - [diffusion][Epoch 11332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:14,297 - INFO - [diffusion][Epoch 11333] Epoch 11334/12000
2024-11-05 03:25:17,515 - INFO - [diffusion][Epoch 11333] diffusion training Loss: 0.0653428053483367
2024-11-05 03:25:17,517 - INFO - [diffusion][Epoch 11333] diffusion learning rate: 0.001
2024-11-05 03:25:17,518 - INFO - [diffusion][Epoch 11333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:17,520 - INFO - [diffusion][Epoch 11334] Epoch 11335/12000
2024-11-05 03:25:21,173 - INFO - [diffusion][Epoch 11334] diffusion training Loss: 0.06410677451640368
2024-11-05 03:25:21,175 - INFO - [diffusion][Epoch 11334] diffusion learning rate: 0.001
2024-11-05 03:25:21,177 - INFO - [diffusion][Epoch 11334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:21,178 - INFO - [diffusion][Epoch 11335] Epoch 11336/12000
2024-11-05 03:25:24,464 - INFO - [diffusion][Epoch 11335] diffusion training Loss: 0.06384390313178301
2024-11-05 03:25:24,466 - INFO - [diffusion][Epoch 11335] diffusion learning rate: 0.001
2024-11-05 03:25:24,468 - INFO - [diffusion][Epoch 11335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:24,469 - INFO - [diffusion][Epoch 11336] Epoch 11337/12000
2024-11-05 03:25:27,582 - INFO - [diffusion][Epoch 11336] diffusion training Loss: 0.0625612661242485
2024-11-05 03:25:27,584 - INFO - [diffusion][Epoch 11336] diffusion learning rate: 0.001
2024-11-05 03:25:27,586 - INFO - [diffusion][Epoch 11336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:27,588 - INFO - [diffusion][Epoch 11337] Epoch 11338/12000
2024-11-05 03:25:30,656 - INFO - [diffusion][Epoch 11337] diffusion training Loss: 0.06504481006413698
2024-11-05 03:25:30,658 - INFO - [diffusion][Epoch 11337] diffusion learning rate: 0.001
2024-11-05 03:25:30,659 - INFO - [diffusion][Epoch 11337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:30,660 - INFO - [diffusion][Epoch 11338] Epoch 11339/12000
2024-11-05 03:25:34,178 - INFO - [diffusion][Epoch 11338] diffusion training Loss: 0.05771045573055744
2024-11-05 03:25:34,181 - INFO - [diffusion][Epoch 11338] diffusion learning rate: 0.001
2024-11-05 03:25:34,183 - INFO - [diffusion][Epoch 11338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:34,185 - INFO - [diffusion][Epoch 11339] Epoch 11340/12000
2024-11-05 03:25:37,488 - INFO - [diffusion][Epoch 11339] diffusion training Loss: 0.0704336054623127
2024-11-05 03:25:37,491 - INFO - [diffusion][Epoch 11339] diffusion learning rate: 0.001
2024-11-05 03:25:37,493 - INFO - [diffusion][Epoch 11339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:37,494 - INFO - [diffusion][Epoch 11340] Epoch 11341/12000
2024-11-05 03:25:40,554 - INFO - [diffusion][Epoch 11340] diffusion training Loss: 0.05851913336664438
2024-11-05 03:25:40,557 - INFO - [diffusion][Epoch 11340] diffusion learning rate: 0.001
2024-11-05 03:25:40,583 - INFO - [diffusion][Epoch 11340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:40,585 - INFO - [diffusion][Epoch 11341] Epoch 11342/12000
2024-11-05 03:25:43,837 - INFO - [diffusion][Epoch 11341] diffusion training Loss: 0.06425637938082218
2024-11-05 03:25:43,839 - INFO - [diffusion][Epoch 11341] diffusion learning rate: 0.001
2024-11-05 03:25:43,841 - INFO - [diffusion][Epoch 11341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:43,842 - INFO - [diffusion][Epoch 11342] Epoch 11343/12000
2024-11-05 03:25:47,135 - INFO - [diffusion][Epoch 11342] diffusion training Loss: 0.06311111710965633
2024-11-05 03:25:47,137 - INFO - [diffusion][Epoch 11342] diffusion learning rate: 0.001
2024-11-05 03:25:47,139 - INFO - [diffusion][Epoch 11342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:47,140 - INFO - [diffusion][Epoch 11343] Epoch 11344/12000
2024-11-05 03:25:50,801 - INFO - [diffusion][Epoch 11343] diffusion training Loss: 0.06735176127403975
2024-11-05 03:25:50,803 - INFO - [diffusion][Epoch 11343] diffusion learning rate: 0.001
2024-11-05 03:25:50,831 - INFO - [diffusion][Epoch 11343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:50,832 - INFO - [diffusion][Epoch 11344] Epoch 11345/12000
2024-11-05 03:25:54,099 - INFO - [diffusion][Epoch 11344] diffusion training Loss: 0.059465620666742325
2024-11-05 03:25:54,101 - INFO - [diffusion][Epoch 11344] diffusion learning rate: 0.001
2024-11-05 03:25:54,102 - INFO - [diffusion][Epoch 11344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:54,103 - INFO - [diffusion][Epoch 11345] Epoch 11346/12000
2024-11-05 03:25:57,315 - INFO - [diffusion][Epoch 11345] diffusion training Loss: 0.06485910154879093
2024-11-05 03:25:57,317 - INFO - [diffusion][Epoch 11345] diffusion learning rate: 0.001
2024-11-05 03:25:57,319 - INFO - [diffusion][Epoch 11345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:57,320 - INFO - [diffusion][Epoch 11346] Epoch 11347/12000
2024-11-05 03:26:00,563 - INFO - [diffusion][Epoch 11346] diffusion training Loss: 0.06888392567634583
2024-11-05 03:26:00,565 - INFO - [diffusion][Epoch 11346] diffusion learning rate: 0.001
2024-11-05 03:26:00,567 - INFO - [diffusion][Epoch 11346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:00,569 - INFO - [diffusion][Epoch 11347] Epoch 11348/12000
2024-11-05 03:26:04,368 - INFO - [diffusion][Epoch 11347] diffusion training Loss: 0.0629819193854928
2024-11-05 03:26:04,371 - INFO - [diffusion][Epoch 11347] diffusion learning rate: 0.001
2024-11-05 03:26:04,373 - INFO - [diffusion][Epoch 11347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:04,374 - INFO - [diffusion][Epoch 11348] Epoch 11349/12000
2024-11-05 03:26:07,688 - INFO - [diffusion][Epoch 11348] diffusion training Loss: 0.06395285949110985
2024-11-05 03:26:07,690 - INFO - [diffusion][Epoch 11348] diffusion learning rate: 0.001
2024-11-05 03:26:07,692 - INFO - [diffusion][Epoch 11348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:07,693 - INFO - [diffusion][Epoch 11349] Epoch 11350/12000
2024-11-05 03:26:11,329 - INFO - [diffusion][Epoch 11349] diffusion training Loss: 0.06115856301039457
2024-11-05 03:26:11,331 - INFO - [diffusion][Epoch 11349] diffusion learning rate: 0.001
2024-11-05 03:26:11,333 - INFO - [diffusion][Epoch 11349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:11,334 - INFO - [diffusion][Epoch 11350] Epoch 11351/12000
2024-11-05 03:26:14,819 - INFO - [diffusion][Epoch 11350] diffusion training Loss: 0.06461150478571653
2024-11-05 03:26:14,821 - INFO - [diffusion][Epoch 11350] diffusion learning rate: 0.001
2024-11-05 03:26:14,822 - INFO - [diffusion][Epoch 11350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:14,825 - INFO - [diffusion][Epoch 11351] Epoch 11352/12000
2024-11-05 03:26:17,921 - INFO - [diffusion][Epoch 11351] diffusion training Loss: 0.06259313505142927
2024-11-05 03:26:17,923 - INFO - [diffusion][Epoch 11351] diffusion learning rate: 0.001
2024-11-05 03:26:17,925 - INFO - [diffusion][Epoch 11351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:17,926 - INFO - [diffusion][Epoch 11352] Epoch 11353/12000
2024-11-05 03:26:21,005 - INFO - [diffusion][Epoch 11352] diffusion training Loss: 0.06452316418290138
2024-11-05 03:26:21,007 - INFO - [diffusion][Epoch 11352] diffusion learning rate: 0.001
2024-11-05 03:26:21,009 - INFO - [diffusion][Epoch 11352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:21,010 - INFO - [diffusion][Epoch 11353] Epoch 11354/12000
2024-11-05 03:26:24,289 - INFO - [diffusion][Epoch 11353] diffusion training Loss: 0.0638113021850586
2024-11-05 03:26:24,291 - INFO - [diffusion][Epoch 11353] diffusion learning rate: 0.001
2024-11-05 03:26:24,293 - INFO - [diffusion][Epoch 11353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:24,294 - INFO - [diffusion][Epoch 11354] Epoch 11355/12000
2024-11-05 03:26:27,917 - INFO - [diffusion][Epoch 11354] diffusion training Loss: 0.06523079238831997
2024-11-05 03:26:27,918 - INFO - [diffusion][Epoch 11354] diffusion learning rate: 0.001
2024-11-05 03:26:27,920 - INFO - [diffusion][Epoch 11354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:27,922 - INFO - [diffusion][Epoch 11355] Epoch 11356/12000
2024-11-05 03:26:31,165 - INFO - [diffusion][Epoch 11355] diffusion training Loss: 0.0643239263445139
2024-11-05 03:26:31,168 - INFO - [diffusion][Epoch 11355] diffusion learning rate: 0.001
2024-11-05 03:26:31,169 - INFO - [diffusion][Epoch 11355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:31,171 - INFO - [diffusion][Epoch 11356] Epoch 11357/12000
2024-11-05 03:26:34,277 - INFO - [diffusion][Epoch 11356] diffusion training Loss: 0.06414213497191668
2024-11-05 03:26:34,279 - INFO - [diffusion][Epoch 11356] diffusion learning rate: 0.001
2024-11-05 03:26:34,281 - INFO - [diffusion][Epoch 11356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:34,283 - INFO - [diffusion][Epoch 11357] Epoch 11358/12000
2024-11-05 03:26:37,436 - INFO - [diffusion][Epoch 11357] diffusion training Loss: 0.06329476181417704
2024-11-05 03:26:37,438 - INFO - [diffusion][Epoch 11357] diffusion learning rate: 0.001
2024-11-05 03:26:37,440 - INFO - [diffusion][Epoch 11357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:37,441 - INFO - [diffusion][Epoch 11358] Epoch 11359/12000
2024-11-05 03:26:40,930 - INFO - [diffusion][Epoch 11358] diffusion training Loss: 0.06110859103500843
2024-11-05 03:26:40,932 - INFO - [diffusion][Epoch 11358] diffusion learning rate: 0.001
2024-11-05 03:26:40,936 - INFO - [diffusion][Epoch 11358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:40,937 - INFO - [diffusion][Epoch 11359] Epoch 11360/12000
2024-11-05 03:26:44,463 - INFO - [diffusion][Epoch 11359] diffusion training Loss: 0.06484983302652836
2024-11-05 03:26:44,466 - INFO - [diffusion][Epoch 11359] diffusion learning rate: 0.001
2024-11-05 03:26:44,467 - INFO - [diffusion][Epoch 11359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:44,469 - INFO - [diffusion][Epoch 11360] Epoch 11361/12000
2024-11-05 03:26:47,570 - INFO - [diffusion][Epoch 11360] diffusion training Loss: 0.06472264789044857
2024-11-05 03:26:47,572 - INFO - [diffusion][Epoch 11360] diffusion learning rate: 0.001
2024-11-05 03:26:47,574 - INFO - [diffusion][Epoch 11360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:47,575 - INFO - [diffusion][Epoch 11361] Epoch 11362/12000
2024-11-05 03:26:50,667 - INFO - [diffusion][Epoch 11361] diffusion training Loss: 0.06016030069440603
2024-11-05 03:26:50,669 - INFO - [diffusion][Epoch 11361] diffusion learning rate: 0.001
2024-11-05 03:26:50,671 - INFO - [diffusion][Epoch 11361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:50,672 - INFO - [diffusion][Epoch 11362] Epoch 11363/12000
2024-11-05 03:26:53,791 - INFO - [diffusion][Epoch 11362] diffusion training Loss: 0.061988480389118195
2024-11-05 03:26:53,793 - INFO - [diffusion][Epoch 11362] diffusion learning rate: 0.001
2024-11-05 03:26:53,795 - INFO - [diffusion][Epoch 11362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:53,797 - INFO - [diffusion][Epoch 11363] Epoch 11364/12000
2024-11-05 03:26:57,333 - INFO - [diffusion][Epoch 11363] diffusion training Loss: 0.06526786368340254
2024-11-05 03:26:57,335 - INFO - [diffusion][Epoch 11363] diffusion learning rate: 0.001
2024-11-05 03:26:57,337 - INFO - [diffusion][Epoch 11363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:57,338 - INFO - [diffusion][Epoch 11364] Epoch 11365/12000
2024-11-05 03:27:00,736 - INFO - [diffusion][Epoch 11364] diffusion training Loss: 0.05684366263449192
2024-11-05 03:27:00,738 - INFO - [diffusion][Epoch 11364] diffusion learning rate: 0.001
2024-11-05 03:27:00,739 - INFO - [diffusion][Epoch 11364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:00,741 - INFO - [diffusion][Epoch 11365] Epoch 11366/12000
2024-11-05 03:27:03,764 - INFO - [diffusion][Epoch 11365] diffusion training Loss: 0.059645053930580616
2024-11-05 03:27:03,766 - INFO - [diffusion][Epoch 11365] diffusion learning rate: 0.001
2024-11-05 03:27:03,767 - INFO - [diffusion][Epoch 11365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:03,769 - INFO - [diffusion][Epoch 11366] Epoch 11367/12000
2024-11-05 03:27:06,794 - INFO - [diffusion][Epoch 11366] diffusion training Loss: 0.06203026045113802
2024-11-05 03:27:06,798 - INFO - [diffusion][Epoch 11366] diffusion learning rate: 0.001
2024-11-05 03:27:06,800 - INFO - [diffusion][Epoch 11366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:06,801 - INFO - [diffusion][Epoch 11367] Epoch 11368/12000
2024-11-05 03:27:10,599 - INFO - [diffusion][Epoch 11367] diffusion training Loss: 0.06391490995883942
2024-11-05 03:27:10,600 - INFO - [diffusion][Epoch 11367] diffusion learning rate: 0.001
2024-11-05 03:27:10,629 - INFO - [diffusion][Epoch 11367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:10,630 - INFO - [diffusion][Epoch 11368] Epoch 11369/12000
2024-11-05 03:27:13,898 - INFO - [diffusion][Epoch 11368] diffusion training Loss: 0.05947734694927931
2024-11-05 03:27:13,900 - INFO - [diffusion][Epoch 11368] diffusion learning rate: 0.001
2024-11-05 03:27:13,902 - INFO - [diffusion][Epoch 11368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:13,903 - INFO - [diffusion][Epoch 11369] Epoch 11370/12000
2024-11-05 03:27:17,523 - INFO - [diffusion][Epoch 11369] diffusion training Loss: 0.0631561204791069
2024-11-05 03:27:17,525 - INFO - [diffusion][Epoch 11369] diffusion learning rate: 0.001
2024-11-05 03:27:17,526 - INFO - [diffusion][Epoch 11369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:17,528 - INFO - [diffusion][Epoch 11370] Epoch 11371/12000
2024-11-05 03:27:21,053 - INFO - [diffusion][Epoch 11370] diffusion training Loss: 0.06501055508852005
2024-11-05 03:27:21,063 - INFO - [diffusion][Epoch 11370] diffusion learning rate: 0.001
2024-11-05 03:27:21,065 - INFO - [diffusion][Epoch 11370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:21,066 - INFO - [diffusion][Epoch 11371] Epoch 11372/12000
2024-11-05 03:27:24,233 - INFO - [diffusion][Epoch 11371] diffusion training Loss: 0.0650584027171135
2024-11-05 03:27:24,235 - INFO - [diffusion][Epoch 11371] diffusion learning rate: 0.001
2024-11-05 03:27:24,237 - INFO - [diffusion][Epoch 11371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:24,238 - INFO - [diffusion][Epoch 11372] Epoch 11373/12000
2024-11-05 03:27:27,331 - INFO - [diffusion][Epoch 11372] diffusion training Loss: 0.06588741205632687
2024-11-05 03:27:27,333 - INFO - [diffusion][Epoch 11372] diffusion learning rate: 0.001
2024-11-05 03:27:27,334 - INFO - [diffusion][Epoch 11372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:27,336 - INFO - [diffusion][Epoch 11373] Epoch 11374/12000
2024-11-05 03:27:30,506 - INFO - [diffusion][Epoch 11373] diffusion training Loss: 0.06353502906858921
2024-11-05 03:27:30,508 - INFO - [diffusion][Epoch 11373] diffusion learning rate: 0.001
2024-11-05 03:27:30,510 - INFO - [diffusion][Epoch 11373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:30,512 - INFO - [diffusion][Epoch 11374] Epoch 11375/12000
2024-11-05 03:27:34,045 - INFO - [diffusion][Epoch 11374] diffusion training Loss: 0.06836718320846558
2024-11-05 03:27:34,046 - INFO - [diffusion][Epoch 11374] diffusion learning rate: 0.001
2024-11-05 03:27:34,048 - INFO - [diffusion][Epoch 11374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:34,049 - INFO - [diffusion][Epoch 11375] Epoch 11376/12000
2024-11-05 03:27:37,568 - INFO - [diffusion][Epoch 11375] diffusion training Loss: 0.06649495754390955
2024-11-05 03:27:37,570 - INFO - [diffusion][Epoch 11375] diffusion learning rate: 0.001
2024-11-05 03:27:37,572 - INFO - [diffusion][Epoch 11375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:37,573 - INFO - [diffusion][Epoch 11376] Epoch 11377/12000
2024-11-05 03:27:40,661 - INFO - [diffusion][Epoch 11376] diffusion training Loss: 0.06927350349724293
2024-11-05 03:27:40,662 - INFO - [diffusion][Epoch 11376] diffusion learning rate: 0.001
2024-11-05 03:27:40,664 - INFO - [diffusion][Epoch 11376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:40,665 - INFO - [diffusion][Epoch 11377] Epoch 11378/12000
2024-11-05 03:27:43,795 - INFO - [diffusion][Epoch 11377] diffusion training Loss: 0.06460245046764612
2024-11-05 03:27:43,797 - INFO - [diffusion][Epoch 11377] diffusion learning rate: 0.001
2024-11-05 03:27:43,799 - INFO - [diffusion][Epoch 11377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:43,800 - INFO - [diffusion][Epoch 11378] Epoch 11379/12000
2024-11-05 03:27:47,167 - INFO - [diffusion][Epoch 11378] diffusion training Loss: 0.06187523156404495
2024-11-05 03:27:47,169 - INFO - [diffusion][Epoch 11378] diffusion learning rate: 0.001
2024-11-05 03:27:47,171 - INFO - [diffusion][Epoch 11378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:47,172 - INFO - [diffusion][Epoch 11379] Epoch 11380/12000
2024-11-05 03:27:50,908 - INFO - [diffusion][Epoch 11379] diffusion training Loss: 0.06291355565190315
2024-11-05 03:27:50,910 - INFO - [diffusion][Epoch 11379] diffusion learning rate: 0.001
2024-11-05 03:27:50,913 - INFO - [diffusion][Epoch 11379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:50,914 - INFO - [diffusion][Epoch 11380] Epoch 11381/12000
2024-11-05 03:27:54,286 - INFO - [diffusion][Epoch 11380] diffusion training Loss: 0.0646825060248375
2024-11-05 03:27:54,288 - INFO - [diffusion][Epoch 11380] diffusion learning rate: 0.001
2024-11-05 03:27:54,290 - INFO - [diffusion][Epoch 11380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:54,291 - INFO - [diffusion][Epoch 11381] Epoch 11382/12000
2024-11-05 03:27:57,442 - INFO - [diffusion][Epoch 11381] diffusion training Loss: 0.0656287707388401
2024-11-05 03:27:57,444 - INFO - [diffusion][Epoch 11381] diffusion learning rate: 0.001
2024-11-05 03:27:57,446 - INFO - [diffusion][Epoch 11381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:57,447 - INFO - [diffusion][Epoch 11382] Epoch 11383/12000
2024-11-05 03:28:00,547 - INFO - [diffusion][Epoch 11382] diffusion training Loss: 0.06580470502376556
2024-11-05 03:28:00,549 - INFO - [diffusion][Epoch 11382] diffusion learning rate: 0.001
2024-11-05 03:28:00,551 - INFO - [diffusion][Epoch 11382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:00,552 - INFO - [diffusion][Epoch 11383] Epoch 11384/12000
2024-11-05 03:28:04,051 - INFO - [diffusion][Epoch 11383] diffusion training Loss: 0.06541088595986366
2024-11-05 03:28:04,053 - INFO - [diffusion][Epoch 11383] diffusion learning rate: 0.001
2024-11-05 03:28:04,054 - INFO - [diffusion][Epoch 11383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:04,056 - INFO - [diffusion][Epoch 11384] Epoch 11385/12000
2024-11-05 03:28:07,825 - INFO - [diffusion][Epoch 11384] diffusion training Loss: 0.060275289230048656
2024-11-05 03:28:07,827 - INFO - [diffusion][Epoch 11384] diffusion learning rate: 0.001
2024-11-05 03:28:07,829 - INFO - [diffusion][Epoch 11384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:07,830 - INFO - [diffusion][Epoch 11385] Epoch 11386/12000
2024-11-05 03:28:11,289 - INFO - [diffusion][Epoch 11385] diffusion training Loss: 0.060822658240795135
2024-11-05 03:28:11,291 - INFO - [diffusion][Epoch 11385] diffusion learning rate: 0.001
2024-11-05 03:28:11,293 - INFO - [diffusion][Epoch 11385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:11,295 - INFO - [diffusion][Epoch 11386] Epoch 11387/12000
2024-11-05 03:28:14,310 - INFO - [diffusion][Epoch 11386] diffusion training Loss: 0.05936318635940552
2024-11-05 03:28:14,312 - INFO - [diffusion][Epoch 11386] diffusion learning rate: 0.001
2024-11-05 03:28:14,314 - INFO - [diffusion][Epoch 11386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:14,315 - INFO - [diffusion][Epoch 11387] Epoch 11388/12000
2024-11-05 03:28:17,758 - INFO - [diffusion][Epoch 11387] diffusion training Loss: 0.06318953819572926
2024-11-05 03:28:17,760 - INFO - [diffusion][Epoch 11387] diffusion learning rate: 0.001
2024-11-05 03:28:17,762 - INFO - [diffusion][Epoch 11387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:17,763 - INFO - [diffusion][Epoch 11388] Epoch 11389/12000
2024-11-05 03:28:20,866 - INFO - [diffusion][Epoch 11388] diffusion training Loss: 0.0609667282551527
2024-11-05 03:28:20,868 - INFO - [diffusion][Epoch 11388] diffusion learning rate: 0.001
2024-11-05 03:28:20,870 - INFO - [diffusion][Epoch 11388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:20,871 - INFO - [diffusion][Epoch 11389] Epoch 11390/12000
2024-11-05 03:28:24,426 - INFO - [diffusion][Epoch 11389] diffusion training Loss: 0.06249039247632027
2024-11-05 03:28:24,428 - INFO - [diffusion][Epoch 11389] diffusion learning rate: 0.001
2024-11-05 03:28:24,430 - INFO - [diffusion][Epoch 11389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:24,431 - INFO - [diffusion][Epoch 11390] Epoch 11391/12000
2024-11-05 03:28:28,070 - INFO - [diffusion][Epoch 11390] diffusion training Loss: 0.0611698292195797
2024-11-05 03:28:28,072 - INFO - [diffusion][Epoch 11390] diffusion learning rate: 0.001
2024-11-05 03:28:28,074 - INFO - [diffusion][Epoch 11390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:28,076 - INFO - [diffusion][Epoch 11391] Epoch 11392/12000
2024-11-05 03:28:31,335 - INFO - [diffusion][Epoch 11391] diffusion training Loss: 0.06512763723731041
2024-11-05 03:28:31,337 - INFO - [diffusion][Epoch 11391] diffusion learning rate: 0.001
2024-11-05 03:28:31,339 - INFO - [diffusion][Epoch 11391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:31,340 - INFO - [diffusion][Epoch 11392] Epoch 11393/12000
2024-11-05 03:28:34,502 - INFO - [diffusion][Epoch 11392] diffusion training Loss: 0.06590391881763935
2024-11-05 03:28:34,504 - INFO - [diffusion][Epoch 11392] diffusion learning rate: 0.001
2024-11-05 03:28:34,506 - INFO - [diffusion][Epoch 11392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:34,507 - INFO - [diffusion][Epoch 11393] Epoch 11394/12000
2024-11-05 03:28:37,429 - INFO - [diffusion][Epoch 11393] diffusion training Loss: 0.06841852329671383
2024-11-05 03:28:37,431 - INFO - [diffusion][Epoch 11393] diffusion learning rate: 0.001
2024-11-05 03:28:37,432 - INFO - [diffusion][Epoch 11393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:37,434 - INFO - [diffusion][Epoch 11394] Epoch 11395/12000
2024-11-05 03:28:40,977 - INFO - [diffusion][Epoch 11394] diffusion training Loss: 0.062053123489022255
2024-11-05 03:28:40,979 - INFO - [diffusion][Epoch 11394] diffusion learning rate: 0.001
2024-11-05 03:28:40,981 - INFO - [diffusion][Epoch 11394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:40,982 - INFO - [diffusion][Epoch 11395] Epoch 11396/12000
2024-11-05 03:28:44,518 - INFO - [diffusion][Epoch 11395] diffusion training Loss: 0.06679079588502645
2024-11-05 03:28:44,520 - INFO - [diffusion][Epoch 11395] diffusion learning rate: 0.001
2024-11-05 03:28:44,522 - INFO - [diffusion][Epoch 11395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:44,523 - INFO - [diffusion][Epoch 11396] Epoch 11397/12000
2024-11-05 03:28:47,624 - INFO - [diffusion][Epoch 11396] diffusion training Loss: 0.061652788892388344
2024-11-05 03:28:47,626 - INFO - [diffusion][Epoch 11396] diffusion learning rate: 0.001
2024-11-05 03:28:47,627 - INFO - [diffusion][Epoch 11396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:47,629 - INFO - [diffusion][Epoch 11397] Epoch 11398/12000
2024-11-05 03:28:50,761 - INFO - [diffusion][Epoch 11397] diffusion training Loss: 0.06479538138955832
2024-11-05 03:28:50,763 - INFO - [diffusion][Epoch 11397] diffusion learning rate: 0.001
2024-11-05 03:28:50,764 - INFO - [diffusion][Epoch 11397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:50,766 - INFO - [diffusion][Epoch 11398] Epoch 11399/12000
2024-11-05 03:28:53,916 - INFO - [diffusion][Epoch 11398] diffusion training Loss: 0.06260918453335762
2024-11-05 03:28:53,918 - INFO - [diffusion][Epoch 11398] diffusion learning rate: 0.001
2024-11-05 03:28:53,920 - INFO - [diffusion][Epoch 11398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:53,921 - INFO - [diffusion][Epoch 11399] Epoch 11400/12000
2024-11-05 03:28:57,481 - INFO - [diffusion][Epoch 11399] diffusion training Loss: 0.06305160280317068
2024-11-05 03:28:57,483 - INFO - [diffusion][Epoch 11399] diffusion learning rate: 0.001
2024-11-05 03:28:57,485 - INFO - [diffusion][Epoch 11399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:57,486 - INFO - [diffusion][Epoch 11400] Epoch 11401/12000
2024-11-05 03:29:00,990 - INFO - [diffusion][Epoch 11400] diffusion training Loss: 0.05453362222760916
2024-11-05 03:29:00,992 - INFO - [diffusion][Epoch 11400] diffusion learning rate: 0.001
2024-11-05 03:29:00,994 - INFO - [diffusion][Epoch 11400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:00,995 - INFO - [diffusion][Epoch 11401] Epoch 11402/12000
2024-11-05 03:29:04,064 - INFO - [diffusion][Epoch 11401] diffusion training Loss: 0.06322077661752701
2024-11-05 03:29:04,066 - INFO - [diffusion][Epoch 11401] diffusion learning rate: 0.001
2024-11-05 03:29:04,068 - INFO - [diffusion][Epoch 11401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:04,069 - INFO - [diffusion][Epoch 11402] Epoch 11403/12000
2024-11-05 03:29:07,190 - INFO - [diffusion][Epoch 11402] diffusion training Loss: 0.058935136534273624
2024-11-05 03:29:07,192 - INFO - [diffusion][Epoch 11402] diffusion learning rate: 0.001
2024-11-05 03:29:07,194 - INFO - [diffusion][Epoch 11402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:07,195 - INFO - [diffusion][Epoch 11403] Epoch 11404/12000
2024-11-05 03:29:10,360 - INFO - [diffusion][Epoch 11403] diffusion training Loss: 0.0643862783908844
2024-11-05 03:29:10,362 - INFO - [diffusion][Epoch 11403] diffusion learning rate: 0.001
2024-11-05 03:29:10,364 - INFO - [diffusion][Epoch 11403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:10,365 - INFO - [diffusion][Epoch 11404] Epoch 11405/12000
2024-11-05 03:29:13,922 - INFO - [diffusion][Epoch 11404] diffusion training Loss: 0.0589345833286643
2024-11-05 03:29:13,924 - INFO - [diffusion][Epoch 11404] diffusion learning rate: 0.001
2024-11-05 03:29:13,925 - INFO - [diffusion][Epoch 11404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:13,926 - INFO - [diffusion][Epoch 11405] Epoch 11406/12000
2024-11-05 03:29:17,448 - INFO - [diffusion][Epoch 11405] diffusion training Loss: 0.06524881534278393
2024-11-05 03:29:17,450 - INFO - [diffusion][Epoch 11405] diffusion learning rate: 0.001
2024-11-05 03:29:17,452 - INFO - [diffusion][Epoch 11405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:17,453 - INFO - [diffusion][Epoch 11406] Epoch 11407/12000
2024-11-05 03:29:21,471 - INFO - [diffusion][Epoch 11406] diffusion training Loss: 0.06894698832184076
2024-11-05 03:29:21,473 - INFO - [diffusion][Epoch 11406] diffusion learning rate: 0.001
2024-11-05 03:29:21,474 - INFO - [diffusion][Epoch 11406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:21,476 - INFO - [diffusion][Epoch 11407] Epoch 11408/12000
2024-11-05 03:29:24,587 - INFO - [diffusion][Epoch 11407] diffusion training Loss: 0.06497609429061413
2024-11-05 03:29:24,589 - INFO - [diffusion][Epoch 11407] diffusion learning rate: 0.001
2024-11-05 03:29:24,591 - INFO - [diffusion][Epoch 11407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:24,592 - INFO - [diffusion][Epoch 11408] Epoch 11409/12000
2024-11-05 03:29:27,754 - INFO - [diffusion][Epoch 11408] diffusion training Loss: 0.0655244616791606
2024-11-05 03:29:27,756 - INFO - [diffusion][Epoch 11408] diffusion learning rate: 0.001
2024-11-05 03:29:27,758 - INFO - [diffusion][Epoch 11408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:27,759 - INFO - [diffusion][Epoch 11409] Epoch 11410/12000
2024-11-05 03:29:31,033 - INFO - [diffusion][Epoch 11409] diffusion training Loss: 0.06789584923535585
2024-11-05 03:29:31,035 - INFO - [diffusion][Epoch 11409] diffusion learning rate: 0.001
2024-11-05 03:29:31,036 - INFO - [diffusion][Epoch 11409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:31,038 - INFO - [diffusion][Epoch 11410] Epoch 11411/12000
2024-11-05 03:29:34,658 - INFO - [diffusion][Epoch 11410] diffusion training Loss: 0.07154928706586361
2024-11-05 03:29:34,660 - INFO - [diffusion][Epoch 11410] diffusion learning rate: 0.001
2024-11-05 03:29:34,661 - INFO - [diffusion][Epoch 11410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:34,663 - INFO - [diffusion][Epoch 11411] Epoch 11412/12000
2024-11-05 03:29:37,994 - INFO - [diffusion][Epoch 11411] diffusion training Loss: 0.06477289460599422
2024-11-05 03:29:37,996 - INFO - [diffusion][Epoch 11411] diffusion learning rate: 0.001
2024-11-05 03:29:37,998 - INFO - [diffusion][Epoch 11411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:37,999 - INFO - [diffusion][Epoch 11412] Epoch 11413/12000
2024-11-05 03:29:41,083 - INFO - [diffusion][Epoch 11412] diffusion training Loss: 0.06585360132157803
2024-11-05 03:29:41,085 - INFO - [diffusion][Epoch 11412] diffusion learning rate: 0.001
2024-11-05 03:29:41,087 - INFO - [diffusion][Epoch 11412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:41,088 - INFO - [diffusion][Epoch 11413] Epoch 11414/12000
2024-11-05 03:29:44,134 - INFO - [diffusion][Epoch 11413] diffusion training Loss: 0.06544026918709278
2024-11-05 03:29:44,136 - INFO - [diffusion][Epoch 11413] diffusion learning rate: 0.001
2024-11-05 03:29:44,138 - INFO - [diffusion][Epoch 11413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:44,139 - INFO - [diffusion][Epoch 11414] Epoch 11415/12000
2024-11-05 03:29:47,677 - INFO - [diffusion][Epoch 11414] diffusion training Loss: 0.06314317137002945
2024-11-05 03:29:47,678 - INFO - [diffusion][Epoch 11414] diffusion learning rate: 0.001
2024-11-05 03:29:47,680 - INFO - [diffusion][Epoch 11414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:47,681 - INFO - [diffusion][Epoch 11415] Epoch 11416/12000
2024-11-05 03:29:51,424 - INFO - [diffusion][Epoch 11415] diffusion training Loss: 0.06088053993880749
2024-11-05 03:29:51,426 - INFO - [diffusion][Epoch 11415] diffusion learning rate: 0.001
2024-11-05 03:29:51,428 - INFO - [diffusion][Epoch 11415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:51,429 - INFO - [diffusion][Epoch 11416] Epoch 11417/12000
2024-11-05 03:29:54,755 - INFO - [diffusion][Epoch 11416] diffusion training Loss: 0.06363784801214933
2024-11-05 03:29:54,757 - INFO - [diffusion][Epoch 11416] diffusion learning rate: 0.001
2024-11-05 03:29:54,758 - INFO - [diffusion][Epoch 11416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:54,760 - INFO - [diffusion][Epoch 11417] Epoch 11418/12000
2024-11-05 03:29:57,891 - INFO - [diffusion][Epoch 11417] diffusion training Loss: 0.06354301888495684
2024-11-05 03:29:57,893 - INFO - [diffusion][Epoch 11417] diffusion learning rate: 0.001
2024-11-05 03:29:57,895 - INFO - [diffusion][Epoch 11417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:57,896 - INFO - [diffusion][Epoch 11418] Epoch 11419/12000
2024-11-05 03:30:01,112 - INFO - [diffusion][Epoch 11418] diffusion training Loss: 0.06722231023013592
2024-11-05 03:30:01,115 - INFO - [diffusion][Epoch 11418] diffusion learning rate: 0.001
2024-11-05 03:30:01,116 - INFO - [diffusion][Epoch 11418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:01,118 - INFO - [diffusion][Epoch 11419] Epoch 11420/12000
2024-11-05 03:30:04,436 - INFO - [diffusion][Epoch 11419] diffusion training Loss: 0.07246753573417664
2024-11-05 03:30:04,438 - INFO - [diffusion][Epoch 11419] diffusion learning rate: 0.001
2024-11-05 03:30:04,439 - INFO - [diffusion][Epoch 11419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:04,441 - INFO - [diffusion][Epoch 11420] Epoch 11421/12000
2024-11-05 03:30:07,955 - INFO - [diffusion][Epoch 11420] diffusion training Loss: 0.06478732638061047
2024-11-05 03:30:07,957 - INFO - [diffusion][Epoch 11420] diffusion learning rate: 0.001
2024-11-05 03:30:07,959 - INFO - [diffusion][Epoch 11420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:07,960 - INFO - [diffusion][Epoch 11421] Epoch 11422/12000
2024-11-05 03:30:11,119 - INFO - [diffusion][Epoch 11421] diffusion training Loss: 0.06574701145291328
2024-11-05 03:30:11,121 - INFO - [diffusion][Epoch 11421] diffusion learning rate: 0.001
2024-11-05 03:30:11,123 - INFO - [diffusion][Epoch 11421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:11,124 - INFO - [diffusion][Epoch 11422] Epoch 11423/12000
2024-11-05 03:30:14,124 - INFO - [diffusion][Epoch 11422] diffusion training Loss: 0.06415470689535141
2024-11-05 03:30:14,126 - INFO - [diffusion][Epoch 11422] diffusion learning rate: 0.001
2024-11-05 03:30:14,128 - INFO - [diffusion][Epoch 11422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:14,129 - INFO - [diffusion][Epoch 11423] Epoch 11424/12000
2024-11-05 03:30:17,204 - INFO - [diffusion][Epoch 11423] diffusion training Loss: 0.06640556827187538
2024-11-05 03:30:17,206 - INFO - [diffusion][Epoch 11423] diffusion learning rate: 0.001
2024-11-05 03:30:17,207 - INFO - [diffusion][Epoch 11423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:17,209 - INFO - [diffusion][Epoch 11424] Epoch 11425/12000
2024-11-05 03:30:20,773 - INFO - [diffusion][Epoch 11424] diffusion training Loss: 0.06419102847576141
2024-11-05 03:30:20,775 - INFO - [diffusion][Epoch 11424] diffusion learning rate: 0.001
2024-11-05 03:30:20,777 - INFO - [diffusion][Epoch 11424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:20,778 - INFO - [diffusion][Epoch 11425] Epoch 11426/12000
2024-11-05 03:30:24,005 - INFO - [diffusion][Epoch 11425] diffusion training Loss: 0.06082060746848583
2024-11-05 03:30:24,008 - INFO - [diffusion][Epoch 11425] diffusion learning rate: 0.001
2024-11-05 03:30:24,010 - INFO - [diffusion][Epoch 11425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:24,012 - INFO - [diffusion][Epoch 11426] Epoch 11427/12000
2024-11-05 03:30:26,987 - INFO - [diffusion][Epoch 11426] diffusion training Loss: 0.06346487626433372
2024-11-05 03:30:26,989 - INFO - [diffusion][Epoch 11426] diffusion learning rate: 0.001
2024-11-05 03:30:26,991 - INFO - [diffusion][Epoch 11426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:26,992 - INFO - [diffusion][Epoch 11427] Epoch 11428/12000
2024-11-05 03:30:30,081 - INFO - [diffusion][Epoch 11427] diffusion training Loss: 0.06441537849605083
2024-11-05 03:30:30,083 - INFO - [diffusion][Epoch 11427] diffusion learning rate: 0.001
2024-11-05 03:30:30,085 - INFO - [diffusion][Epoch 11427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:30,086 - INFO - [diffusion][Epoch 11428] Epoch 11429/12000
2024-11-05 03:30:33,522 - INFO - [diffusion][Epoch 11428] diffusion training Loss: 0.06529534421861172
2024-11-05 03:30:33,524 - INFO - [diffusion][Epoch 11428] diffusion learning rate: 0.001
2024-11-05 03:30:33,526 - INFO - [diffusion][Epoch 11428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:33,527 - INFO - [diffusion][Epoch 11429] Epoch 11430/12000
2024-11-05 03:30:37,041 - INFO - [diffusion][Epoch 11429] diffusion training Loss: 0.06608143448829651
2024-11-05 03:30:37,042 - INFO - [diffusion][Epoch 11429] diffusion learning rate: 0.001
2024-11-05 03:30:37,044 - INFO - [diffusion][Epoch 11429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:37,045 - INFO - [diffusion][Epoch 11430] Epoch 11431/12000
2024-11-05 03:30:40,142 - INFO - [diffusion][Epoch 11430] diffusion training Loss: 0.06711396761238575
2024-11-05 03:30:40,144 - INFO - [diffusion][Epoch 11430] diffusion learning rate: 0.001
2024-11-05 03:30:40,145 - INFO - [diffusion][Epoch 11430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:40,147 - INFO - [diffusion][Epoch 11431] Epoch 11432/12000
2024-11-05 03:30:43,297 - INFO - [diffusion][Epoch 11431] diffusion training Loss: 0.06957169435918331
2024-11-05 03:30:43,299 - INFO - [diffusion][Epoch 11431] diffusion learning rate: 0.001
2024-11-05 03:30:43,301 - INFO - [diffusion][Epoch 11431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:43,302 - INFO - [diffusion][Epoch 11432] Epoch 11433/12000
2024-11-05 03:30:46,427 - INFO - [diffusion][Epoch 11432] diffusion training Loss: 0.0687912255525589
2024-11-05 03:30:46,428 - INFO - [diffusion][Epoch 11432] diffusion learning rate: 0.001
2024-11-05 03:30:46,430 - INFO - [diffusion][Epoch 11432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:46,432 - INFO - [diffusion][Epoch 11433] Epoch 11434/12000
2024-11-05 03:30:49,970 - INFO - [diffusion][Epoch 11433] diffusion training Loss: 0.06680091936141253
2024-11-05 03:30:49,972 - INFO - [diffusion][Epoch 11433] diffusion learning rate: 0.001
2024-11-05 03:30:49,974 - INFO - [diffusion][Epoch 11433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:49,975 - INFO - [diffusion][Epoch 11434] Epoch 11435/12000
2024-11-05 03:30:53,515 - INFO - [diffusion][Epoch 11434] diffusion training Loss: 0.06718621961772442
2024-11-05 03:30:53,517 - INFO - [diffusion][Epoch 11434] diffusion learning rate: 0.001
2024-11-05 03:30:53,519 - INFO - [diffusion][Epoch 11434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:53,520 - INFO - [diffusion][Epoch 11435] Epoch 11436/12000
2024-11-05 03:30:56,566 - INFO - [diffusion][Epoch 11435] diffusion training Loss: 0.06310700066387653
2024-11-05 03:30:56,568 - INFO - [diffusion][Epoch 11435] diffusion learning rate: 0.001
2024-11-05 03:30:56,569 - INFO - [diffusion][Epoch 11435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:56,571 - INFO - [diffusion][Epoch 11436] Epoch 11437/12000
2024-11-05 03:30:59,623 - INFO - [diffusion][Epoch 11436] diffusion training Loss: 0.05987868271768093
2024-11-05 03:30:59,625 - INFO - [diffusion][Epoch 11436] diffusion learning rate: 0.001
2024-11-05 03:30:59,626 - INFO - [diffusion][Epoch 11436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:59,628 - INFO - [diffusion][Epoch 11437] Epoch 11438/12000
2024-11-05 03:31:02,773 - INFO - [diffusion][Epoch 11437] diffusion training Loss: 0.0703584011644125
2024-11-05 03:31:02,775 - INFO - [diffusion][Epoch 11437] diffusion learning rate: 0.001
2024-11-05 03:31:02,776 - INFO - [diffusion][Epoch 11437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:02,777 - INFO - [diffusion][Epoch 11438] Epoch 11439/12000
2024-11-05 03:31:06,230 - INFO - [diffusion][Epoch 11438] diffusion training Loss: 0.06343563552945852
2024-11-05 03:31:06,232 - INFO - [diffusion][Epoch 11438] diffusion learning rate: 0.001
2024-11-05 03:31:06,234 - INFO - [diffusion][Epoch 11438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:06,235 - INFO - [diffusion][Epoch 11439] Epoch 11440/12000
2024-11-05 03:31:09,361 - INFO - [diffusion][Epoch 11439] diffusion training Loss: 0.061200144700706005
2024-11-05 03:31:09,363 - INFO - [diffusion][Epoch 11439] diffusion learning rate: 0.001
2024-11-05 03:31:09,365 - INFO - [diffusion][Epoch 11439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:09,366 - INFO - [diffusion][Epoch 11440] Epoch 11441/12000
2024-11-05 03:31:12,288 - INFO - [diffusion][Epoch 11440] diffusion training Loss: 0.06259279139339924
2024-11-05 03:31:12,291 - INFO - [diffusion][Epoch 11440] diffusion learning rate: 0.001
2024-11-05 03:31:12,293 - INFO - [diffusion][Epoch 11440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:12,294 - INFO - [diffusion][Epoch 11441] Epoch 11442/12000
2024-11-05 03:31:15,607 - INFO - [diffusion][Epoch 11441] diffusion training Loss: 0.06000422406941652
2024-11-05 03:31:15,609 - INFO - [diffusion][Epoch 11441] diffusion learning rate: 0.001
2024-11-05 03:31:15,610 - INFO - [diffusion][Epoch 11441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:15,612 - INFO - [diffusion][Epoch 11442] Epoch 11443/12000
2024-11-05 03:31:19,101 - INFO - [diffusion][Epoch 11442] diffusion training Loss: 0.06537422817200422
2024-11-05 03:31:19,103 - INFO - [diffusion][Epoch 11442] diffusion learning rate: 0.001
2024-11-05 03:31:19,104 - INFO - [diffusion][Epoch 11442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:19,106 - INFO - [diffusion][Epoch 11443] Epoch 11444/12000
2024-11-05 03:31:22,473 - INFO - [diffusion][Epoch 11443] diffusion training Loss: 0.06579483300447464
2024-11-05 03:31:22,474 - INFO - [diffusion][Epoch 11443] diffusion learning rate: 0.001
2024-11-05 03:31:22,476 - INFO - [diffusion][Epoch 11443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:22,477 - INFO - [diffusion][Epoch 11444] Epoch 11445/12000
2024-11-05 03:31:25,587 - INFO - [diffusion][Epoch 11444] diffusion training Loss: 0.0695943757891655
2024-11-05 03:31:25,589 - INFO - [diffusion][Epoch 11444] diffusion learning rate: 0.001
2024-11-05 03:31:25,591 - INFO - [diffusion][Epoch 11444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:25,592 - INFO - [diffusion][Epoch 11445] Epoch 11446/12000
2024-11-05 03:31:28,951 - INFO - [diffusion][Epoch 11445] diffusion training Loss: 0.059892451390624046
2024-11-05 03:31:28,953 - INFO - [diffusion][Epoch 11445] diffusion learning rate: 0.001
2024-11-05 03:31:28,954 - INFO - [diffusion][Epoch 11445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:28,956 - INFO - [diffusion][Epoch 11446] Epoch 11447/12000
2024-11-05 03:31:32,349 - INFO - [diffusion][Epoch 11446] diffusion training Loss: 0.06671942211687565
2024-11-05 03:31:32,351 - INFO - [diffusion][Epoch 11446] diffusion learning rate: 0.001
2024-11-05 03:31:32,353 - INFO - [diffusion][Epoch 11446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:32,354 - INFO - [diffusion][Epoch 11447] Epoch 11448/12000
2024-11-05 03:31:36,052 - INFO - [diffusion][Epoch 11447] diffusion training Loss: 0.06804076954722404
2024-11-05 03:31:36,054 - INFO - [diffusion][Epoch 11447] diffusion learning rate: 0.001
2024-11-05 03:31:36,056 - INFO - [diffusion][Epoch 11447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:36,057 - INFO - [diffusion][Epoch 11448] Epoch 11449/12000
2024-11-05 03:31:39,277 - INFO - [diffusion][Epoch 11448] diffusion training Loss: 0.06995034497231245
2024-11-05 03:31:39,279 - INFO - [diffusion][Epoch 11448] diffusion learning rate: 0.001
2024-11-05 03:31:39,281 - INFO - [diffusion][Epoch 11448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:39,282 - INFO - [diffusion][Epoch 11449] Epoch 11450/12000
2024-11-05 03:31:42,445 - INFO - [diffusion][Epoch 11449] diffusion training Loss: 0.06834244355559349
2024-11-05 03:31:42,447 - INFO - [diffusion][Epoch 11449] diffusion learning rate: 0.001
2024-11-05 03:31:42,449 - INFO - [diffusion][Epoch 11449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:42,450 - INFO - [diffusion][Epoch 11450] Epoch 11451/12000
2024-11-05 03:31:45,515 - INFO - [diffusion][Epoch 11450] diffusion training Loss: 0.06693114712834358
2024-11-05 03:31:45,517 - INFO - [diffusion][Epoch 11450] diffusion learning rate: 0.001
2024-11-05 03:31:45,519 - INFO - [diffusion][Epoch 11450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:45,520 - INFO - [diffusion][Epoch 11451] Epoch 11452/12000
2024-11-05 03:31:49,063 - INFO - [diffusion][Epoch 11451] diffusion training Loss: 0.06468311045318842
2024-11-05 03:31:49,065 - INFO - [diffusion][Epoch 11451] diffusion learning rate: 0.001
2024-11-05 03:31:49,067 - INFO - [diffusion][Epoch 11451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:49,068 - INFO - [diffusion][Epoch 11452] Epoch 11453/12000
2024-11-05 03:31:52,617 - INFO - [diffusion][Epoch 11452] diffusion training Loss: 0.07174229994416237
2024-11-05 03:31:52,619 - INFO - [diffusion][Epoch 11452] diffusion learning rate: 0.001
2024-11-05 03:31:52,621 - INFO - [diffusion][Epoch 11452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:52,622 - INFO - [diffusion][Epoch 11453] Epoch 11454/12000
2024-11-05 03:31:55,708 - INFO - [diffusion][Epoch 11453] diffusion training Loss: 0.06770028732717037
2024-11-05 03:31:55,710 - INFO - [diffusion][Epoch 11453] diffusion learning rate: 0.001
2024-11-05 03:31:55,711 - INFO - [diffusion][Epoch 11453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:55,712 - INFO - [diffusion][Epoch 11454] Epoch 11455/12000
2024-11-05 03:31:58,824 - INFO - [diffusion][Epoch 11454] diffusion training Loss: 0.06081788893789053
2024-11-05 03:31:58,826 - INFO - [diffusion][Epoch 11454] diffusion learning rate: 0.001
2024-11-05 03:31:58,828 - INFO - [diffusion][Epoch 11454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:58,829 - INFO - [diffusion][Epoch 11455] Epoch 11456/12000
2024-11-05 03:32:02,181 - INFO - [diffusion][Epoch 11455] diffusion training Loss: 0.06503239646553993
2024-11-05 03:32:02,182 - INFO - [diffusion][Epoch 11455] diffusion learning rate: 0.001
2024-11-05 03:32:02,184 - INFO - [diffusion][Epoch 11455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:02,185 - INFO - [diffusion][Epoch 11456] Epoch 11457/12000
2024-11-05 03:32:05,782 - INFO - [diffusion][Epoch 11456] diffusion training Loss: 0.060961044393479824
2024-11-05 03:32:05,784 - INFO - [diffusion][Epoch 11456] diffusion learning rate: 0.001
2024-11-05 03:32:05,785 - INFO - [diffusion][Epoch 11456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:05,787 - INFO - [diffusion][Epoch 11457] Epoch 11458/12000
2024-11-05 03:32:08,966 - INFO - [diffusion][Epoch 11457] diffusion training Loss: 0.062461127527058125
2024-11-05 03:32:08,968 - INFO - [diffusion][Epoch 11457] diffusion learning rate: 0.001
2024-11-05 03:32:08,970 - INFO - [diffusion][Epoch 11457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:08,971 - INFO - [diffusion][Epoch 11458] Epoch 11459/12000
2024-11-05 03:32:12,085 - INFO - [diffusion][Epoch 11458] diffusion training Loss: 0.06477864645421505
2024-11-05 03:32:12,088 - INFO - [diffusion][Epoch 11458] diffusion learning rate: 0.001
2024-11-05 03:32:12,090 - INFO - [diffusion][Epoch 11458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:12,091 - INFO - [diffusion][Epoch 11459] Epoch 11460/12000
2024-11-05 03:32:15,242 - INFO - [diffusion][Epoch 11459] diffusion training Loss: 0.06070429552346468
2024-11-05 03:32:15,245 - INFO - [diffusion][Epoch 11459] diffusion learning rate: 0.001
2024-11-05 03:32:15,247 - INFO - [diffusion][Epoch 11459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:15,248 - INFO - [diffusion][Epoch 11460] Epoch 11461/12000
2024-11-05 03:32:18,751 - INFO - [diffusion][Epoch 11460] diffusion training Loss: 0.0727225411683321
2024-11-05 03:32:18,753 - INFO - [diffusion][Epoch 11460] diffusion learning rate: 0.001
2024-11-05 03:32:18,755 - INFO - [diffusion][Epoch 11460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:18,756 - INFO - [diffusion][Epoch 11461] Epoch 11462/12000
2024-11-05 03:32:22,264 - INFO - [diffusion][Epoch 11461] diffusion training Loss: 0.06752085871994495
2024-11-05 03:32:22,266 - INFO - [diffusion][Epoch 11461] diffusion learning rate: 0.001
2024-11-05 03:32:22,268 - INFO - [diffusion][Epoch 11461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:22,269 - INFO - [diffusion][Epoch 11462] Epoch 11463/12000
2024-11-05 03:32:25,361 - INFO - [diffusion][Epoch 11462] diffusion training Loss: 0.06092689838260412
2024-11-05 03:32:25,363 - INFO - [diffusion][Epoch 11462] diffusion learning rate: 0.001
2024-11-05 03:32:25,365 - INFO - [diffusion][Epoch 11462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:25,366 - INFO - [diffusion][Epoch 11463] Epoch 11464/12000
2024-11-05 03:32:28,496 - INFO - [diffusion][Epoch 11463] diffusion training Loss: 0.060697718523442745
2024-11-05 03:32:28,498 - INFO - [diffusion][Epoch 11463] diffusion learning rate: 0.001
2024-11-05 03:32:28,500 - INFO - [diffusion][Epoch 11463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:28,501 - INFO - [diffusion][Epoch 11464] Epoch 11465/12000
2024-11-05 03:32:31,621 - INFO - [diffusion][Epoch 11464] diffusion training Loss: 0.06293912697583437
2024-11-05 03:32:31,624 - INFO - [diffusion][Epoch 11464] diffusion learning rate: 0.001
2024-11-05 03:32:31,626 - INFO - [diffusion][Epoch 11464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:31,627 - INFO - [diffusion][Epoch 11465] Epoch 11466/12000
2024-11-05 03:32:35,250 - INFO - [diffusion][Epoch 11465] diffusion training Loss: 0.0613159965723753
2024-11-05 03:32:35,252 - INFO - [diffusion][Epoch 11465] diffusion learning rate: 0.001
2024-11-05 03:32:35,254 - INFO - [diffusion][Epoch 11465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:35,256 - INFO - [diffusion][Epoch 11466] Epoch 11467/12000
2024-11-05 03:32:39,053 - INFO - [diffusion][Epoch 11466] diffusion training Loss: 0.06481979694217443
2024-11-05 03:32:39,055 - INFO - [diffusion][Epoch 11466] diffusion learning rate: 0.001
2024-11-05 03:32:39,057 - INFO - [diffusion][Epoch 11466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:39,059 - INFO - [diffusion][Epoch 11467] Epoch 11468/12000
2024-11-05 03:32:42,608 - INFO - [diffusion][Epoch 11467] diffusion training Loss: 0.061687566339969635
2024-11-05 03:32:42,610 - INFO - [diffusion][Epoch 11467] diffusion learning rate: 0.001
2024-11-05 03:32:42,611 - INFO - [diffusion][Epoch 11467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:42,613 - INFO - [diffusion][Epoch 11468] Epoch 11469/12000
2024-11-05 03:32:45,712 - INFO - [diffusion][Epoch 11468] diffusion training Loss: 0.06364533863961697
2024-11-05 03:32:45,714 - INFO - [diffusion][Epoch 11468] diffusion learning rate: 0.001
2024-11-05 03:32:45,716 - INFO - [diffusion][Epoch 11468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:45,717 - INFO - [diffusion][Epoch 11469] Epoch 11470/12000
2024-11-05 03:32:48,823 - INFO - [diffusion][Epoch 11469] diffusion training Loss: 0.06833408307284117
2024-11-05 03:32:48,825 - INFO - [diffusion][Epoch 11469] diffusion learning rate: 0.001
2024-11-05 03:32:48,827 - INFO - [diffusion][Epoch 11469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:48,828 - INFO - [diffusion][Epoch 11470] Epoch 11471/12000
2024-11-05 03:32:52,005 - INFO - [diffusion][Epoch 11470] diffusion training Loss: 0.06444512121379375
2024-11-05 03:32:52,007 - INFO - [diffusion][Epoch 11470] diffusion learning rate: 0.001
2024-11-05 03:32:52,009 - INFO - [diffusion][Epoch 11470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:52,010 - INFO - [diffusion][Epoch 11471] Epoch 11472/12000
2024-11-05 03:32:55,504 - INFO - [diffusion][Epoch 11471] diffusion training Loss: 0.05981001537293196
2024-11-05 03:32:55,507 - INFO - [diffusion][Epoch 11471] diffusion learning rate: 0.001
2024-11-05 03:32:55,510 - INFO - [diffusion][Epoch 11471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:55,511 - INFO - [diffusion][Epoch 11472] Epoch 11473/12000
2024-11-05 03:32:58,817 - INFO - [diffusion][Epoch 11472] diffusion training Loss: 0.05933360941708088
2024-11-05 03:32:58,819 - INFO - [diffusion][Epoch 11472] diffusion learning rate: 0.001
2024-11-05 03:32:58,821 - INFO - [diffusion][Epoch 11472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:58,822 - INFO - [diffusion][Epoch 11473] Epoch 11474/12000
2024-11-05 03:33:01,894 - INFO - [diffusion][Epoch 11473] diffusion training Loss: 0.0645343828946352
2024-11-05 03:33:01,896 - INFO - [diffusion][Epoch 11473] diffusion learning rate: 0.001
2024-11-05 03:33:01,898 - INFO - [diffusion][Epoch 11473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:01,899 - INFO - [diffusion][Epoch 11474] Epoch 11475/12000
2024-11-05 03:33:05,053 - INFO - [diffusion][Epoch 11474] diffusion training Loss: 0.06272874027490616
2024-11-05 03:33:05,055 - INFO - [diffusion][Epoch 11474] diffusion learning rate: 0.001
2024-11-05 03:33:05,057 - INFO - [diffusion][Epoch 11474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:05,058 - INFO - [diffusion][Epoch 11475] Epoch 11476/12000
2024-11-05 03:33:08,509 - INFO - [diffusion][Epoch 11475] diffusion training Loss: 0.07060816884040833
2024-11-05 03:33:08,511 - INFO - [diffusion][Epoch 11475] diffusion learning rate: 0.001
2024-11-05 03:33:08,513 - INFO - [diffusion][Epoch 11475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:08,514 - INFO - [diffusion][Epoch 11476] Epoch 11477/12000
2024-11-05 03:33:12,286 - INFO - [diffusion][Epoch 11476] diffusion training Loss: 0.05932179931551218
2024-11-05 03:33:12,504 - INFO - [diffusion][Epoch 11476] diffusion learning rate: 0.001
2024-11-05 03:33:12,506 - INFO - [diffusion][Epoch 11476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:12,508 - INFO - [diffusion][Epoch 11477] Epoch 11478/12000
2024-11-05 03:33:16,031 - INFO - [diffusion][Epoch 11477] diffusion training Loss: 0.0630211653187871
2024-11-05 03:33:16,033 - INFO - [diffusion][Epoch 11477] diffusion learning rate: 0.001
2024-11-05 03:33:16,035 - INFO - [diffusion][Epoch 11477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:16,037 - INFO - [diffusion][Epoch 11478] Epoch 11479/12000
2024-11-05 03:33:19,166 - INFO - [diffusion][Epoch 11478] diffusion training Loss: 0.06127713155001402
2024-11-05 03:33:19,168 - INFO - [diffusion][Epoch 11478] diffusion learning rate: 0.001
2024-11-05 03:33:19,170 - INFO - [diffusion][Epoch 11478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:19,171 - INFO - [diffusion][Epoch 11479] Epoch 11480/12000
2024-11-05 03:33:21,859 - INFO - [diffusion][Epoch 11479] diffusion training Loss: 0.061022987589240074
2024-11-05 03:33:21,861 - INFO - [diffusion][Epoch 11479] diffusion learning rate: 0.001
2024-11-05 03:33:21,862 - INFO - [diffusion][Epoch 11479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:21,864 - INFO - [diffusion][Epoch 11480] Epoch 11481/12000
2024-11-05 03:33:25,406 - INFO - [diffusion][Epoch 11480] diffusion training Loss: 0.06700045615434647
2024-11-05 03:33:25,408 - INFO - [diffusion][Epoch 11480] diffusion learning rate: 0.001
2024-11-05 03:33:25,410 - INFO - [diffusion][Epoch 11480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:25,411 - INFO - [diffusion][Epoch 11481] Epoch 11482/12000
2024-11-05 03:33:28,925 - INFO - [diffusion][Epoch 11481] diffusion training Loss: 0.06245487183332443
2024-11-05 03:33:28,927 - INFO - [diffusion][Epoch 11481] diffusion learning rate: 0.001
2024-11-05 03:33:28,929 - INFO - [diffusion][Epoch 11481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:28,931 - INFO - [diffusion][Epoch 11482] Epoch 11483/12000
2024-11-05 03:33:31,933 - INFO - [diffusion][Epoch 11482] diffusion training Loss: 0.05948486737906933
2024-11-05 03:33:31,934 - INFO - [diffusion][Epoch 11482] diffusion learning rate: 0.001
2024-11-05 03:33:31,936 - INFO - [diffusion][Epoch 11482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:31,937 - INFO - [diffusion][Epoch 11483] Epoch 11484/12000
2024-11-05 03:33:35,240 - INFO - [diffusion][Epoch 11483] diffusion training Loss: 0.056243572384119034
2024-11-05 03:33:35,242 - INFO - [diffusion][Epoch 11483] diffusion learning rate: 0.001
2024-11-05 03:33:35,244 - INFO - [diffusion][Epoch 11483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:35,245 - INFO - [diffusion][Epoch 11484] Epoch 11485/12000
2024-11-05 03:33:38,379 - INFO - [diffusion][Epoch 11484] diffusion training Loss: 0.06401525437831879
2024-11-05 03:33:38,381 - INFO - [diffusion][Epoch 11484] diffusion learning rate: 0.001
2024-11-05 03:33:38,383 - INFO - [diffusion][Epoch 11484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:38,384 - INFO - [diffusion][Epoch 11485] Epoch 11486/12000
2024-11-05 03:33:42,343 - INFO - [diffusion][Epoch 11485] diffusion training Loss: 0.0617695152759552
2024-11-05 03:33:42,345 - INFO - [diffusion][Epoch 11485] diffusion learning rate: 0.001
2024-11-05 03:33:42,347 - INFO - [diffusion][Epoch 11485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:42,348 - INFO - [diffusion][Epoch 11486] Epoch 11487/12000
2024-11-05 03:33:46,037 - INFO - [diffusion][Epoch 11486] diffusion training Loss: 0.06169975735247135
2024-11-05 03:33:46,039 - INFO - [diffusion][Epoch 11486] diffusion learning rate: 0.001
2024-11-05 03:33:46,041 - INFO - [diffusion][Epoch 11486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:46,042 - INFO - [diffusion][Epoch 11487] Epoch 11488/12000
2024-11-05 03:33:49,425 - INFO - [diffusion][Epoch 11487] diffusion training Loss: 0.058554074726998806
2024-11-05 03:33:49,427 - INFO - [diffusion][Epoch 11487] diffusion learning rate: 0.001
2024-11-05 03:33:49,429 - INFO - [diffusion][Epoch 11487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:49,430 - INFO - [diffusion][Epoch 11488] Epoch 11489/12000
2024-11-05 03:33:52,599 - INFO - [diffusion][Epoch 11488] diffusion training Loss: 0.06686670612543821
2024-11-05 03:33:52,601 - INFO - [diffusion][Epoch 11488] diffusion learning rate: 0.001
2024-11-05 03:33:52,603 - INFO - [diffusion][Epoch 11488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:52,604 - INFO - [diffusion][Epoch 11489] Epoch 11490/12000
2024-11-05 03:33:55,760 - INFO - [diffusion][Epoch 11489] diffusion training Loss: 0.06422589160501957
2024-11-05 03:33:55,762 - INFO - [diffusion][Epoch 11489] diffusion learning rate: 0.001
2024-11-05 03:33:55,764 - INFO - [diffusion][Epoch 11489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:55,765 - INFO - [diffusion][Epoch 11490] Epoch 11491/12000
2024-11-05 03:33:59,072 - INFO - [diffusion][Epoch 11490] diffusion training Loss: 0.0658130208030343
2024-11-05 03:33:59,074 - INFO - [diffusion][Epoch 11490] diffusion learning rate: 0.001
2024-11-05 03:33:59,076 - INFO - [diffusion][Epoch 11490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:59,077 - INFO - [diffusion][Epoch 11491] Epoch 11492/12000
2024-11-05 03:34:02,774 - INFO - [diffusion][Epoch 11491] diffusion training Loss: 0.059730665758252144
2024-11-05 03:34:02,776 - INFO - [diffusion][Epoch 11491] diffusion learning rate: 0.001
2024-11-05 03:34:02,778 - INFO - [diffusion][Epoch 11491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:02,779 - INFO - [diffusion][Epoch 11492] Epoch 11493/12000
2024-11-05 03:34:06,307 - INFO - [diffusion][Epoch 11492] diffusion training Loss: 0.0588531531393528
2024-11-05 03:34:06,309 - INFO - [diffusion][Epoch 11492] diffusion learning rate: 0.001
2024-11-05 03:34:06,350 - INFO - [diffusion][Epoch 11492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:06,352 - INFO - [diffusion][Epoch 11493] Epoch 11494/12000
2024-11-05 03:34:09,475 - INFO - [diffusion][Epoch 11493] diffusion training Loss: 0.06099709589034319
2024-11-05 03:34:09,477 - INFO - [diffusion][Epoch 11493] diffusion learning rate: 0.001
2024-11-05 03:34:09,479 - INFO - [diffusion][Epoch 11493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:09,481 - INFO - [diffusion][Epoch 11494] Epoch 11495/12000
2024-11-05 03:34:12,636 - INFO - [diffusion][Epoch 11494] diffusion training Loss: 0.06188826449215412
2024-11-05 03:34:12,638 - INFO - [diffusion][Epoch 11494] diffusion learning rate: 0.001
2024-11-05 03:34:12,640 - INFO - [diffusion][Epoch 11494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:12,641 - INFO - [diffusion][Epoch 11495] Epoch 11496/12000
2024-11-05 03:34:15,846 - INFO - [diffusion][Epoch 11495] diffusion training Loss: 0.06265969760715961
2024-11-05 03:34:15,847 - INFO - [diffusion][Epoch 11495] diffusion learning rate: 0.001
2024-11-05 03:34:15,849 - INFO - [diffusion][Epoch 11495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:15,850 - INFO - [diffusion][Epoch 11496] Epoch 11497/12000
2024-11-05 03:34:19,619 - INFO - [diffusion][Epoch 11496] diffusion training Loss: 0.0687941750511527
2024-11-05 03:34:19,622 - INFO - [diffusion][Epoch 11496] diffusion learning rate: 0.001
2024-11-05 03:34:19,624 - INFO - [diffusion][Epoch 11496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:19,625 - INFO - [diffusion][Epoch 11497] Epoch 11498/12000
2024-11-05 03:34:23,174 - INFO - [diffusion][Epoch 11497] diffusion training Loss: 0.06276673823595047
2024-11-05 03:34:23,176 - INFO - [diffusion][Epoch 11497] diffusion learning rate: 0.001
2024-11-05 03:34:23,177 - INFO - [diffusion][Epoch 11497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:23,178 - INFO - [diffusion][Epoch 11498] Epoch 11499/12000
2024-11-05 03:34:26,129 - INFO - [diffusion][Epoch 11498] diffusion training Loss: 0.0609630960971117
2024-11-05 03:34:26,131 - INFO - [diffusion][Epoch 11498] diffusion learning rate: 0.001
2024-11-05 03:34:26,132 - INFO - [diffusion][Epoch 11498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:26,134 - INFO - [diffusion][Epoch 11499] Epoch 11500/12000
2024-11-05 03:34:29,220 - INFO - [diffusion][Epoch 11499] diffusion training Loss: 0.06281553208827972
2024-11-05 03:34:29,222 - INFO - [diffusion][Epoch 11499] diffusion learning rate: 0.001
2024-11-05 03:34:29,224 - INFO - [diffusion][Epoch 11499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:29,225 - INFO - [diffusion][Epoch 11500] Epoch 11501/12000
2024-11-05 03:34:32,559 - INFO - [diffusion][Epoch 11500] diffusion training Loss: 0.06437335535883904
2024-11-05 03:34:32,561 - INFO - [diffusion][Epoch 11500] diffusion learning rate: 0.001
2024-11-05 03:34:32,563 - INFO - [diffusion][Epoch 11500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:32,564 - INFO - [diffusion][Epoch 11501] Epoch 11502/12000
2024-11-05 03:34:36,002 - INFO - [diffusion][Epoch 11501] diffusion training Loss: 0.06479951925575733
2024-11-05 03:34:36,004 - INFO - [diffusion][Epoch 11501] diffusion learning rate: 0.001
2024-11-05 03:34:36,005 - INFO - [diffusion][Epoch 11501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:36,007 - INFO - [diffusion][Epoch 11502] Epoch 11503/12000
2024-11-05 03:34:39,080 - INFO - [diffusion][Epoch 11502] diffusion training Loss: 0.06543366611003876
2024-11-05 03:34:39,082 - INFO - [diffusion][Epoch 11502] diffusion learning rate: 0.001
2024-11-05 03:34:39,083 - INFO - [diffusion][Epoch 11502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:39,084 - INFO - [diffusion][Epoch 11503] Epoch 11504/12000
2024-11-05 03:34:42,102 - INFO - [diffusion][Epoch 11503] diffusion training Loss: 0.06722571514546871
2024-11-05 03:34:42,104 - INFO - [diffusion][Epoch 11503] diffusion learning rate: 0.001
2024-11-05 03:34:42,154 - INFO - [diffusion][Epoch 11503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:42,156 - INFO - [diffusion][Epoch 11504] Epoch 11505/12000
2024-11-05 03:34:45,281 - INFO - [diffusion][Epoch 11504] diffusion training Loss: 0.06277755741029978
2024-11-05 03:34:45,284 - INFO - [diffusion][Epoch 11504] diffusion learning rate: 0.001
2024-11-05 03:34:45,286 - INFO - [diffusion][Epoch 11504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:45,287 - INFO - [diffusion][Epoch 11505] Epoch 11506/12000
2024-11-05 03:34:49,079 - INFO - [diffusion][Epoch 11505] diffusion training Loss: 0.06341256946325302
2024-11-05 03:34:49,081 - INFO - [diffusion][Epoch 11505] diffusion learning rate: 0.001
2024-11-05 03:34:49,083 - INFO - [diffusion][Epoch 11505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:49,084 - INFO - [diffusion][Epoch 11506] Epoch 11507/12000
2024-11-05 03:34:53,295 - INFO - [diffusion][Epoch 11506] diffusion training Loss: 0.06510516442358494
2024-11-05 03:34:53,297 - INFO - [diffusion][Epoch 11506] diffusion learning rate: 0.001
2024-11-05 03:34:53,298 - INFO - [diffusion][Epoch 11506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:53,300 - INFO - [diffusion][Epoch 11507] Epoch 11508/12000
2024-11-05 03:34:56,802 - INFO - [diffusion][Epoch 11507] diffusion training Loss: 0.062404084019362926
2024-11-05 03:34:56,803 - INFO - [diffusion][Epoch 11507] diffusion learning rate: 0.001
2024-11-05 03:34:56,805 - INFO - [diffusion][Epoch 11507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:56,806 - INFO - [diffusion][Epoch 11508] Epoch 11509/12000
2024-11-05 03:34:59,886 - INFO - [diffusion][Epoch 11508] diffusion training Loss: 0.06206414941698313
2024-11-05 03:34:59,888 - INFO - [diffusion][Epoch 11508] diffusion learning rate: 0.001
2024-11-05 03:34:59,889 - INFO - [diffusion][Epoch 11508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:59,890 - INFO - [diffusion][Epoch 11509] Epoch 11510/12000
2024-11-05 03:35:02,959 - INFO - [diffusion][Epoch 11509] diffusion training Loss: 0.058324627578258514
2024-11-05 03:35:02,961 - INFO - [diffusion][Epoch 11509] diffusion learning rate: 0.001
2024-11-05 03:35:02,963 - INFO - [diffusion][Epoch 11509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:02,964 - INFO - [diffusion][Epoch 11510] Epoch 11511/12000
2024-11-05 03:35:06,147 - INFO - [diffusion][Epoch 11510] diffusion training Loss: 0.06320834439247847
2024-11-05 03:35:06,149 - INFO - [diffusion][Epoch 11510] diffusion learning rate: 0.001
2024-11-05 03:35:06,151 - INFO - [diffusion][Epoch 11510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:06,152 - INFO - [diffusion][Epoch 11511] Epoch 11512/12000
2024-11-05 03:35:09,646 - INFO - [diffusion][Epoch 11511] diffusion training Loss: 0.0627239616587758
2024-11-05 03:35:09,648 - INFO - [diffusion][Epoch 11511] diffusion learning rate: 0.001
2024-11-05 03:35:09,650 - INFO - [diffusion][Epoch 11511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:09,651 - INFO - [diffusion][Epoch 11512] Epoch 11513/12000
2024-11-05 03:35:12,807 - INFO - [diffusion][Epoch 11512] diffusion training Loss: 0.05892112851142883
2024-11-05 03:35:12,808 - INFO - [diffusion][Epoch 11512] diffusion learning rate: 0.001
2024-11-05 03:35:12,810 - INFO - [diffusion][Epoch 11512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:12,811 - INFO - [diffusion][Epoch 11513] Epoch 11514/12000
2024-11-05 03:35:15,901 - INFO - [diffusion][Epoch 11513] diffusion training Loss: 0.06482033431529999
2024-11-05 03:35:15,903 - INFO - [diffusion][Epoch 11513] diffusion learning rate: 0.001
2024-11-05 03:35:15,905 - INFO - [diffusion][Epoch 11513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:15,906 - INFO - [diffusion][Epoch 11514] Epoch 11515/12000
2024-11-05 03:35:19,069 - INFO - [diffusion][Epoch 11514] diffusion training Loss: 0.06467551179230213
2024-11-05 03:35:19,071 - INFO - [diffusion][Epoch 11514] diffusion learning rate: 0.001
2024-11-05 03:35:19,073 - INFO - [diffusion][Epoch 11514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:19,074 - INFO - [diffusion][Epoch 11515] Epoch 11516/12000
2024-11-05 03:35:22,690 - INFO - [diffusion][Epoch 11515] diffusion training Loss: 0.05966658238321543
2024-11-05 03:35:22,693 - INFO - [diffusion][Epoch 11515] diffusion learning rate: 0.001
2024-11-05 03:35:22,695 - INFO - [diffusion][Epoch 11515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:22,696 - INFO - [diffusion][Epoch 11516] Epoch 11517/12000
2024-11-05 03:35:26,216 - INFO - [diffusion][Epoch 11516] diffusion training Loss: 0.06229344289749861
2024-11-05 03:35:26,218 - INFO - [diffusion][Epoch 11516] diffusion learning rate: 0.001
2024-11-05 03:35:26,220 - INFO - [diffusion][Epoch 11516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:26,221 - INFO - [diffusion][Epoch 11517] Epoch 11518/12000
2024-11-05 03:35:29,308 - INFO - [diffusion][Epoch 11517] diffusion training Loss: 0.05668335687369108
2024-11-05 03:35:29,310 - INFO - [diffusion][Epoch 11517] diffusion learning rate: 0.001
2024-11-05 03:35:29,312 - INFO - [diffusion][Epoch 11517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:29,313 - INFO - [diffusion][Epoch 11518] Epoch 11519/12000
2024-11-05 03:35:32,395 - INFO - [diffusion][Epoch 11518] diffusion training Loss: 0.060380817390978336
2024-11-05 03:35:32,397 - INFO - [diffusion][Epoch 11518] diffusion learning rate: 0.001
2024-11-05 03:35:32,398 - INFO - [diffusion][Epoch 11518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:32,400 - INFO - [diffusion][Epoch 11519] Epoch 11520/12000
2024-11-05 03:35:35,594 - INFO - [diffusion][Epoch 11519] diffusion training Loss: 0.06032445468008518
2024-11-05 03:35:35,596 - INFO - [diffusion][Epoch 11519] diffusion learning rate: 0.001
2024-11-05 03:35:35,598 - INFO - [diffusion][Epoch 11519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:35,599 - INFO - [diffusion][Epoch 11520] Epoch 11521/12000
2024-11-05 03:35:39,237 - INFO - [diffusion][Epoch 11520] diffusion training Loss: 0.062050286680459976
2024-11-05 03:35:39,239 - INFO - [diffusion][Epoch 11520] diffusion learning rate: 0.001
2024-11-05 03:35:39,241 - INFO - [diffusion][Epoch 11520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:39,242 - INFO - [diffusion][Epoch 11521] Epoch 11522/12000
2024-11-05 03:35:42,714 - INFO - [diffusion][Epoch 11521] diffusion training Loss: 0.06287132203578949
2024-11-05 03:35:42,716 - INFO - [diffusion][Epoch 11521] diffusion learning rate: 0.001
2024-11-05 03:35:42,737 - INFO - [diffusion][Epoch 11521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:42,738 - INFO - [diffusion][Epoch 11522] Epoch 11523/12000
2024-11-05 03:35:45,586 - INFO - [diffusion][Epoch 11522] diffusion training Loss: 0.06848982442170382
2024-11-05 03:35:45,588 - INFO - [diffusion][Epoch 11522] diffusion learning rate: 0.001
2024-11-05 03:35:45,590 - INFO - [diffusion][Epoch 11522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:45,591 - INFO - [diffusion][Epoch 11523] Epoch 11524/12000
2024-11-05 03:35:48,790 - INFO - [diffusion][Epoch 11523] diffusion training Loss: 0.060891043394804
2024-11-05 03:35:48,791 - INFO - [diffusion][Epoch 11523] diffusion learning rate: 0.001
2024-11-05 03:35:48,793 - INFO - [diffusion][Epoch 11523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:48,794 - INFO - [diffusion][Epoch 11524] Epoch 11525/12000
2024-11-05 03:35:52,293 - INFO - [diffusion][Epoch 11524] diffusion training Loss: 0.061876957304775715
2024-11-05 03:35:52,295 - INFO - [diffusion][Epoch 11524] diffusion learning rate: 0.001
2024-11-05 03:35:52,297 - INFO - [diffusion][Epoch 11524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:52,298 - INFO - [diffusion][Epoch 11525] Epoch 11526/12000
2024-11-05 03:35:56,200 - INFO - [diffusion][Epoch 11525] diffusion training Loss: 0.06352268066257238
2024-11-05 03:35:56,202 - INFO - [diffusion][Epoch 11525] diffusion learning rate: 0.001
2024-11-05 03:35:56,203 - INFO - [diffusion][Epoch 11525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:56,204 - INFO - [diffusion][Epoch 11526] Epoch 11527/12000
2024-11-05 03:35:59,705 - INFO - [diffusion][Epoch 11526] diffusion training Loss: 0.06443468295037746
2024-11-05 03:35:59,707 - INFO - [diffusion][Epoch 11526] diffusion learning rate: 0.001
2024-11-05 03:35:59,708 - INFO - [diffusion][Epoch 11526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:59,710 - INFO - [diffusion][Epoch 11527] Epoch 11528/12000
2024-11-05 03:36:02,824 - INFO - [diffusion][Epoch 11527] diffusion training Loss: 0.0608028918504715
2024-11-05 03:36:02,826 - INFO - [diffusion][Epoch 11527] diffusion learning rate: 0.001
2024-11-05 03:36:02,827 - INFO - [diffusion][Epoch 11527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:02,829 - INFO - [diffusion][Epoch 11528] Epoch 11529/12000
2024-11-05 03:36:05,963 - INFO - [diffusion][Epoch 11528] diffusion training Loss: 0.06252076476812363
2024-11-05 03:36:05,965 - INFO - [diffusion][Epoch 11528] diffusion learning rate: 0.001
2024-11-05 03:36:05,966 - INFO - [diffusion][Epoch 11528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:05,968 - INFO - [diffusion][Epoch 11529] Epoch 11530/12000
2024-11-05 03:36:09,186 - INFO - [diffusion][Epoch 11529] diffusion training Loss: 0.06234993785619736
2024-11-05 03:36:09,187 - INFO - [diffusion][Epoch 11529] diffusion learning rate: 0.001
2024-11-05 03:36:09,189 - INFO - [diffusion][Epoch 11529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:09,191 - INFO - [diffusion][Epoch 11530] Epoch 11531/12000
2024-11-05 03:36:12,808 - INFO - [diffusion][Epoch 11530] diffusion training Loss: 0.0629440676420927
2024-11-05 03:36:12,810 - INFO - [diffusion][Epoch 11530] diffusion learning rate: 0.001
2024-11-05 03:36:12,811 - INFO - [diffusion][Epoch 11530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:12,812 - INFO - [diffusion][Epoch 11531] Epoch 11532/12000
2024-11-05 03:36:16,104 - INFO - [diffusion][Epoch 11531] diffusion training Loss: 0.05839805956929922
2024-11-05 03:36:16,105 - INFO - [diffusion][Epoch 11531] diffusion learning rate: 0.001
2024-11-05 03:36:16,107 - INFO - [diffusion][Epoch 11531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:16,108 - INFO - [diffusion][Epoch 11532] Epoch 11533/12000
2024-11-05 03:36:19,208 - INFO - [diffusion][Epoch 11532] diffusion training Loss: 0.06020956579595804
2024-11-05 03:36:19,209 - INFO - [diffusion][Epoch 11532] diffusion learning rate: 0.001
2024-11-05 03:36:19,211 - INFO - [diffusion][Epoch 11532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:19,212 - INFO - [diffusion][Epoch 11533] Epoch 11534/12000
2024-11-05 03:36:22,344 - INFO - [diffusion][Epoch 11533] diffusion training Loss: 0.06655369140207767
2024-11-05 03:36:22,346 - INFO - [diffusion][Epoch 11533] diffusion learning rate: 0.001
2024-11-05 03:36:22,348 - INFO - [diffusion][Epoch 11533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:22,349 - INFO - [diffusion][Epoch 11534] Epoch 11535/12000
2024-11-05 03:36:25,935 - INFO - [diffusion][Epoch 11534] diffusion training Loss: 0.06481576152145863
2024-11-05 03:36:25,937 - INFO - [diffusion][Epoch 11534] diffusion learning rate: 0.001
2024-11-05 03:36:25,939 - INFO - [diffusion][Epoch 11534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:25,940 - INFO - [diffusion][Epoch 11535] Epoch 11536/12000
2024-11-05 03:36:29,420 - INFO - [diffusion][Epoch 11535] diffusion training Loss: 0.07006870117038488
2024-11-05 03:36:29,422 - INFO - [diffusion][Epoch 11535] diffusion learning rate: 0.001
2024-11-05 03:36:29,423 - INFO - [diffusion][Epoch 11535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:29,424 - INFO - [diffusion][Epoch 11536] Epoch 11537/12000
2024-11-05 03:36:32,465 - INFO - [diffusion][Epoch 11536] diffusion training Loss: 0.06154577899724245
2024-11-05 03:36:32,466 - INFO - [diffusion][Epoch 11536] diffusion learning rate: 0.001
2024-11-05 03:36:32,468 - INFO - [diffusion][Epoch 11536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:32,469 - INFO - [diffusion][Epoch 11537] Epoch 11538/12000
2024-11-05 03:36:35,627 - INFO - [diffusion][Epoch 11537] diffusion training Loss: 0.06228847522288561
2024-11-05 03:36:35,630 - INFO - [diffusion][Epoch 11537] diffusion learning rate: 0.001
2024-11-05 03:36:35,631 - INFO - [diffusion][Epoch 11537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:35,633 - INFO - [diffusion][Epoch 11538] Epoch 11539/12000
2024-11-05 03:36:38,923 - INFO - [diffusion][Epoch 11538] diffusion training Loss: 0.06276189349591732
2024-11-05 03:36:38,924 - INFO - [diffusion][Epoch 11538] diffusion learning rate: 0.001
2024-11-05 03:36:38,926 - INFO - [diffusion][Epoch 11538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:38,927 - INFO - [diffusion][Epoch 11539] Epoch 11540/12000
2024-11-05 03:36:42,568 - INFO - [diffusion][Epoch 11539] diffusion training Loss: 0.06541838590055704
2024-11-05 03:36:42,570 - INFO - [diffusion][Epoch 11539] diffusion learning rate: 0.001
2024-11-05 03:36:42,572 - INFO - [diffusion][Epoch 11539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:42,573 - INFO - [diffusion][Epoch 11540] Epoch 11541/12000
2024-11-05 03:36:45,802 - INFO - [diffusion][Epoch 11540] diffusion training Loss: 0.060139717534184456
2024-11-05 03:36:45,804 - INFO - [diffusion][Epoch 11540] diffusion learning rate: 0.001
2024-11-05 03:36:45,805 - INFO - [diffusion][Epoch 11540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:45,807 - INFO - [diffusion][Epoch 11541] Epoch 11542/12000
2024-11-05 03:36:49,000 - INFO - [diffusion][Epoch 11541] diffusion training Loss: 0.06182088144123554
2024-11-05 03:36:49,002 - INFO - [diffusion][Epoch 11541] diffusion learning rate: 0.001
2024-11-05 03:36:49,004 - INFO - [diffusion][Epoch 11541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:49,005 - INFO - [diffusion][Epoch 11542] Epoch 11543/12000
2024-11-05 03:36:52,167 - INFO - [diffusion][Epoch 11542] diffusion training Loss: 0.06251694448292255
2024-11-05 03:36:52,181 - INFO - [diffusion][Epoch 11542] diffusion learning rate: 0.001
2024-11-05 03:36:52,183 - INFO - [diffusion][Epoch 11542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:52,185 - INFO - [diffusion][Epoch 11543] Epoch 11544/12000
2024-11-05 03:36:55,520 - INFO - [diffusion][Epoch 11543] diffusion training Loss: 0.07260955590754747
2024-11-05 03:36:55,522 - INFO - [diffusion][Epoch 11543] diffusion learning rate: 0.001
2024-11-05 03:36:55,523 - INFO - [diffusion][Epoch 11543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:55,525 - INFO - [diffusion][Epoch 11544] Epoch 11545/12000
2024-11-05 03:36:59,631 - INFO - [diffusion][Epoch 11544] diffusion training Loss: 0.06430483888834715
2024-11-05 03:36:59,633 - INFO - [diffusion][Epoch 11544] diffusion learning rate: 0.001
2024-11-05 03:36:59,635 - INFO - [diffusion][Epoch 11544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:59,636 - INFO - [diffusion][Epoch 11545] Epoch 11546/12000
2024-11-05 03:37:03,329 - INFO - [diffusion][Epoch 11545] diffusion training Loss: 0.06589541397988796
2024-11-05 03:37:03,331 - INFO - [diffusion][Epoch 11545] diffusion learning rate: 0.001
2024-11-05 03:37:03,333 - INFO - [diffusion][Epoch 11545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:03,335 - INFO - [diffusion][Epoch 11546] Epoch 11547/12000
2024-11-05 03:37:06,589 - INFO - [diffusion][Epoch 11546] diffusion training Loss: 0.06486232206225395
2024-11-05 03:37:06,591 - INFO - [diffusion][Epoch 11546] diffusion learning rate: 0.001
2024-11-05 03:37:06,592 - INFO - [diffusion][Epoch 11546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:06,593 - INFO - [diffusion][Epoch 11547] Epoch 11548/12000
2024-11-05 03:37:09,813 - INFO - [diffusion][Epoch 11547] diffusion training Loss: 0.0639953725039959
2024-11-05 03:37:09,815 - INFO - [diffusion][Epoch 11547] diffusion learning rate: 0.001
2024-11-05 03:37:09,817 - INFO - [diffusion][Epoch 11547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:09,818 - INFO - [diffusion][Epoch 11548] Epoch 11549/12000
2024-11-05 03:37:12,965 - INFO - [diffusion][Epoch 11548] diffusion training Loss: 0.0625977125018835
2024-11-05 03:37:12,966 - INFO - [diffusion][Epoch 11548] diffusion learning rate: 0.001
2024-11-05 03:37:12,968 - INFO - [diffusion][Epoch 11548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:12,969 - INFO - [diffusion][Epoch 11549] Epoch 11550/12000
2024-11-05 03:37:16,372 - INFO - [diffusion][Epoch 11549] diffusion training Loss: 0.06369705125689507
2024-11-05 03:37:16,374 - INFO - [diffusion][Epoch 11549] diffusion learning rate: 0.001
2024-11-05 03:37:16,375 - INFO - [diffusion][Epoch 11549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:16,377 - INFO - [diffusion][Epoch 11550] Epoch 11551/12000
2024-11-05 03:37:19,937 - INFO - [diffusion][Epoch 11550] diffusion training Loss: 0.06659531407058239
2024-11-05 03:37:19,938 - INFO - [diffusion][Epoch 11550] diffusion learning rate: 0.001
2024-11-05 03:37:19,940 - INFO - [diffusion][Epoch 11550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:19,941 - INFO - [diffusion][Epoch 11551] Epoch 11552/12000
2024-11-05 03:37:23,056 - INFO - [diffusion][Epoch 11551] diffusion training Loss: 0.07136539183557034
2024-11-05 03:37:23,059 - INFO - [diffusion][Epoch 11551] diffusion learning rate: 0.001
2024-11-05 03:37:23,061 - INFO - [diffusion][Epoch 11551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:23,062 - INFO - [diffusion][Epoch 11552] Epoch 11553/12000
2024-11-05 03:37:26,181 - INFO - [diffusion][Epoch 11552] diffusion training Loss: 0.06387249659746885
2024-11-05 03:37:26,183 - INFO - [diffusion][Epoch 11552] diffusion learning rate: 0.001
2024-11-05 03:37:26,185 - INFO - [diffusion][Epoch 11552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:26,186 - INFO - [diffusion][Epoch 11553] Epoch 11554/12000
2024-11-05 03:37:29,271 - INFO - [diffusion][Epoch 11553] diffusion training Loss: 0.06517823785543442
2024-11-05 03:37:29,273 - INFO - [diffusion][Epoch 11553] diffusion learning rate: 0.001
2024-11-05 03:37:29,274 - INFO - [diffusion][Epoch 11553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:29,276 - INFO - [diffusion][Epoch 11554] Epoch 11555/12000
2024-11-05 03:37:32,883 - INFO - [diffusion][Epoch 11554] diffusion training Loss: 0.06476584449410439
2024-11-05 03:37:32,885 - INFO - [diffusion][Epoch 11554] diffusion learning rate: 0.001
2024-11-05 03:37:32,887 - INFO - [diffusion][Epoch 11554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:32,888 - INFO - [diffusion][Epoch 11555] Epoch 11556/12000
2024-11-05 03:37:36,118 - INFO - [diffusion][Epoch 11555] diffusion training Loss: 0.06803667545318604
2024-11-05 03:37:36,120 - INFO - [diffusion][Epoch 11555] diffusion learning rate: 0.001
2024-11-05 03:37:36,122 - INFO - [diffusion][Epoch 11555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:36,123 - INFO - [diffusion][Epoch 11556] Epoch 11557/12000
2024-11-05 03:37:39,156 - INFO - [diffusion][Epoch 11556] diffusion training Loss: 0.06108196172863245
2024-11-05 03:37:39,157 - INFO - [diffusion][Epoch 11556] diffusion learning rate: 0.001
2024-11-05 03:37:39,159 - INFO - [diffusion][Epoch 11556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:39,160 - INFO - [diffusion][Epoch 11557] Epoch 11558/12000
2024-11-05 03:37:42,245 - INFO - [diffusion][Epoch 11557] diffusion training Loss: 0.06503002345561981
2024-11-05 03:37:42,247 - INFO - [diffusion][Epoch 11557] diffusion learning rate: 0.001
2024-11-05 03:37:42,249 - INFO - [diffusion][Epoch 11557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:42,250 - INFO - [diffusion][Epoch 11558] Epoch 11559/12000
2024-11-05 03:37:45,915 - INFO - [diffusion][Epoch 11558] diffusion training Loss: 0.06934002600610256
2024-11-05 03:37:45,918 - INFO - [diffusion][Epoch 11558] diffusion learning rate: 0.001
2024-11-05 03:37:45,919 - INFO - [diffusion][Epoch 11558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:45,921 - INFO - [diffusion][Epoch 11559] Epoch 11560/12000
2024-11-05 03:37:49,060 - INFO - [diffusion][Epoch 11559] diffusion training Loss: 0.06404130812734365
2024-11-05 03:37:49,062 - INFO - [diffusion][Epoch 11559] diffusion learning rate: 0.001
2024-11-05 03:37:49,064 - INFO - [diffusion][Epoch 11559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:49,065 - INFO - [diffusion][Epoch 11560] Epoch 11561/12000
2024-11-05 03:37:52,169 - INFO - [diffusion][Epoch 11560] diffusion training Loss: 0.0629352880641818
2024-11-05 03:37:52,171 - INFO - [diffusion][Epoch 11560] diffusion learning rate: 0.001
2024-11-05 03:37:52,172 - INFO - [diffusion][Epoch 11560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:52,174 - INFO - [diffusion][Epoch 11561] Epoch 11562/12000
2024-11-05 03:37:55,289 - INFO - [diffusion][Epoch 11561] diffusion training Loss: 0.061181857250630856
2024-11-05 03:37:55,292 - INFO - [diffusion][Epoch 11561] diffusion learning rate: 0.001
2024-11-05 03:37:55,294 - INFO - [diffusion][Epoch 11561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:55,295 - INFO - [diffusion][Epoch 11562] Epoch 11563/12000
2024-11-05 03:37:58,832 - INFO - [diffusion][Epoch 11562] diffusion training Loss: 0.06684978306293488
2024-11-05 03:37:58,834 - INFO - [diffusion][Epoch 11562] diffusion learning rate: 0.001
2024-11-05 03:37:58,836 - INFO - [diffusion][Epoch 11562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:58,837 - INFO - [diffusion][Epoch 11563] Epoch 11564/12000
2024-11-05 03:38:02,363 - INFO - [diffusion][Epoch 11563] diffusion training Loss: 0.068671315908432
2024-11-05 03:38:02,365 - INFO - [diffusion][Epoch 11563] diffusion learning rate: 0.001
2024-11-05 03:38:02,366 - INFO - [diffusion][Epoch 11563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:02,368 - INFO - [diffusion][Epoch 11564] Epoch 11565/12000
2024-11-05 03:38:06,329 - INFO - [diffusion][Epoch 11564] diffusion training Loss: 0.06539883464574814
2024-11-05 03:38:06,331 - INFO - [diffusion][Epoch 11564] diffusion learning rate: 0.001
2024-11-05 03:38:06,333 - INFO - [diffusion][Epoch 11564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:06,334 - INFO - [diffusion][Epoch 11565] Epoch 11566/12000
2024-11-05 03:38:09,427 - INFO - [diffusion][Epoch 11565] diffusion training Loss: 0.06356126815080643
2024-11-05 03:38:09,429 - INFO - [diffusion][Epoch 11565] diffusion learning rate: 0.001
2024-11-05 03:38:09,430 - INFO - [diffusion][Epoch 11565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:09,432 - INFO - [diffusion][Epoch 11566] Epoch 11567/12000
2024-11-05 03:38:12,577 - INFO - [diffusion][Epoch 11566] diffusion training Loss: 0.07457655854523182
2024-11-05 03:38:12,578 - INFO - [diffusion][Epoch 11566] diffusion learning rate: 0.001
2024-11-05 03:38:12,580 - INFO - [diffusion][Epoch 11566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:12,581 - INFO - [diffusion][Epoch 11567] Epoch 11568/12000
2024-11-05 03:38:15,670 - INFO - [diffusion][Epoch 11567] diffusion training Loss: 0.06701691076159477
2024-11-05 03:38:15,672 - INFO - [diffusion][Epoch 11567] diffusion learning rate: 0.001
2024-11-05 03:38:15,674 - INFO - [diffusion][Epoch 11567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:15,675 - INFO - [diffusion][Epoch 11568] Epoch 11569/12000
2024-11-05 03:38:19,185 - INFO - [diffusion][Epoch 11568] diffusion training Loss: 0.06468111742287874
2024-11-05 03:38:19,187 - INFO - [diffusion][Epoch 11568] diffusion learning rate: 0.001
2024-11-05 03:38:19,189 - INFO - [diffusion][Epoch 11568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:19,190 - INFO - [diffusion][Epoch 11569] Epoch 11570/12000
2024-11-05 03:38:22,725 - INFO - [diffusion][Epoch 11569] diffusion training Loss: 0.06910549476742744
2024-11-05 03:38:22,727 - INFO - [diffusion][Epoch 11569] diffusion learning rate: 0.001
2024-11-05 03:38:22,729 - INFO - [diffusion][Epoch 11569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:22,730 - INFO - [diffusion][Epoch 11570] Epoch 11571/12000
2024-11-05 03:38:25,909 - INFO - [diffusion][Epoch 11570] diffusion training Loss: 0.06517854519188404
2024-11-05 03:38:25,911 - INFO - [diffusion][Epoch 11570] diffusion learning rate: 0.001
2024-11-05 03:38:25,913 - INFO - [diffusion][Epoch 11570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:25,914 - INFO - [diffusion][Epoch 11571] Epoch 11572/12000
2024-11-05 03:38:29,006 - INFO - [diffusion][Epoch 11571] diffusion training Loss: 0.06106392852962017
2024-11-05 03:38:29,008 - INFO - [diffusion][Epoch 11571] diffusion learning rate: 0.001
2024-11-05 03:38:29,010 - INFO - [diffusion][Epoch 11571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:29,011 - INFO - [diffusion][Epoch 11572] Epoch 11573/12000
2024-11-05 03:38:32,102 - INFO - [diffusion][Epoch 11572] diffusion training Loss: 0.06192032527178526
2024-11-05 03:38:32,103 - INFO - [diffusion][Epoch 11572] diffusion learning rate: 0.001
2024-11-05 03:38:32,105 - INFO - [diffusion][Epoch 11572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:32,106 - INFO - [diffusion][Epoch 11573] Epoch 11574/12000
2024-11-05 03:38:35,630 - INFO - [diffusion][Epoch 11573] diffusion training Loss: 0.06738328747451305
2024-11-05 03:38:35,632 - INFO - [diffusion][Epoch 11573] diffusion learning rate: 0.001
2024-11-05 03:38:35,634 - INFO - [diffusion][Epoch 11573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:35,635 - INFO - [diffusion][Epoch 11574] Epoch 11575/12000
2024-11-05 03:38:39,183 - INFO - [diffusion][Epoch 11574] diffusion training Loss: 0.0666449572890997
2024-11-05 03:38:39,185 - INFO - [diffusion][Epoch 11574] diffusion learning rate: 0.001
2024-11-05 03:38:39,187 - INFO - [diffusion][Epoch 11574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:39,188 - INFO - [diffusion][Epoch 11575] Epoch 11576/12000
2024-11-05 03:38:42,326 - INFO - [diffusion][Epoch 11575] diffusion training Loss: 0.06099030002951622
2024-11-05 03:38:42,328 - INFO - [diffusion][Epoch 11575] diffusion learning rate: 0.001
2024-11-05 03:38:42,355 - INFO - [diffusion][Epoch 11575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:42,357 - INFO - [diffusion][Epoch 11576] Epoch 11577/12000
2024-11-05 03:38:45,481 - INFO - [diffusion][Epoch 11576] diffusion training Loss: 0.05874091759324074
2024-11-05 03:38:45,483 - INFO - [diffusion][Epoch 11576] diffusion learning rate: 0.001
2024-11-05 03:38:45,485 - INFO - [diffusion][Epoch 11576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:45,486 - INFO - [diffusion][Epoch 11577] Epoch 11578/12000
2024-11-05 03:38:48,599 - INFO - [diffusion][Epoch 11577] diffusion training Loss: 0.06281069479882717
2024-11-05 03:38:48,601 - INFO - [diffusion][Epoch 11577] diffusion learning rate: 0.001
2024-11-05 03:38:48,603 - INFO - [diffusion][Epoch 11577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:48,604 - INFO - [diffusion][Epoch 11578] Epoch 11579/12000
2024-11-05 03:38:52,222 - INFO - [diffusion][Epoch 11578] diffusion training Loss: 0.06574340537190437
2024-11-05 03:38:52,223 - INFO - [diffusion][Epoch 11578] diffusion learning rate: 0.001
2024-11-05 03:38:52,225 - INFO - [diffusion][Epoch 11578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:52,226 - INFO - [diffusion][Epoch 11579] Epoch 11580/12000
2024-11-05 03:38:55,726 - INFO - [diffusion][Epoch 11579] diffusion training Loss: 0.06543571036309004
2024-11-05 03:38:55,728 - INFO - [diffusion][Epoch 11579] diffusion learning rate: 0.001
2024-11-05 03:38:55,730 - INFO - [diffusion][Epoch 11579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:55,731 - INFO - [diffusion][Epoch 11580] Epoch 11581/12000
2024-11-05 03:38:58,831 - INFO - [diffusion][Epoch 11580] diffusion training Loss: 0.05852258112281561
2024-11-05 03:38:58,833 - INFO - [diffusion][Epoch 11580] diffusion learning rate: 0.001
2024-11-05 03:38:58,852 - INFO - [diffusion][Epoch 11580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:58,854 - INFO - [diffusion][Epoch 11581] Epoch 11582/12000
2024-11-05 03:39:01,836 - INFO - [diffusion][Epoch 11581] diffusion training Loss: 0.06649616919457912
2024-11-05 03:39:01,838 - INFO - [diffusion][Epoch 11581] diffusion learning rate: 0.001
2024-11-05 03:39:01,839 - INFO - [diffusion][Epoch 11581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:01,841 - INFO - [diffusion][Epoch 11582] Epoch 11583/12000
2024-11-05 03:39:05,097 - INFO - [diffusion][Epoch 11582] diffusion training Loss: 0.06613848078995943
2024-11-05 03:39:05,100 - INFO - [diffusion][Epoch 11582] diffusion learning rate: 0.001
2024-11-05 03:39:05,101 - INFO - [diffusion][Epoch 11582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:05,103 - INFO - [diffusion][Epoch 11583] Epoch 11584/12000
2024-11-05 03:39:08,673 - INFO - [diffusion][Epoch 11583] diffusion training Loss: 0.0622327271848917
2024-11-05 03:39:08,675 - INFO - [diffusion][Epoch 11583] diffusion learning rate: 0.001
2024-11-05 03:39:08,676 - INFO - [diffusion][Epoch 11583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:08,678 - INFO - [diffusion][Epoch 11584] Epoch 11585/12000
2024-11-05 03:39:11,885 - INFO - [diffusion][Epoch 11584] diffusion training Loss: 0.06540113687515259
2024-11-05 03:39:11,887 - INFO - [diffusion][Epoch 11584] diffusion learning rate: 0.001
2024-11-05 03:39:11,889 - INFO - [diffusion][Epoch 11584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:11,890 - INFO - [diffusion][Epoch 11585] Epoch 11586/12000
2024-11-05 03:39:15,103 - INFO - [diffusion][Epoch 11585] diffusion training Loss: 0.06232368014752865
2024-11-05 03:39:15,105 - INFO - [diffusion][Epoch 11585] diffusion learning rate: 0.001
2024-11-05 03:39:15,107 - INFO - [diffusion][Epoch 11585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:15,108 - INFO - [diffusion][Epoch 11586] Epoch 11587/12000
2024-11-05 03:39:18,319 - INFO - [diffusion][Epoch 11586] diffusion training Loss: 0.06293945480138063
2024-11-05 03:39:18,321 - INFO - [diffusion][Epoch 11586] diffusion learning rate: 0.001
2024-11-05 03:39:18,324 - INFO - [diffusion][Epoch 11586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:18,325 - INFO - [diffusion][Epoch 11587] Epoch 11588/12000
2024-11-05 03:39:21,763 - INFO - [diffusion][Epoch 11587] diffusion training Loss: 0.06267408840358257
2024-11-05 03:39:21,765 - INFO - [diffusion][Epoch 11587] diffusion learning rate: 0.001
2024-11-05 03:39:21,767 - INFO - [diffusion][Epoch 11587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:21,768 - INFO - [diffusion][Epoch 11588] Epoch 11589/12000
2024-11-05 03:39:25,453 - INFO - [diffusion][Epoch 11588] diffusion training Loss: 0.06698885001242161
2024-11-05 03:39:25,455 - INFO - [diffusion][Epoch 11588] diffusion learning rate: 0.001
2024-11-05 03:39:25,456 - INFO - [diffusion][Epoch 11588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:25,458 - INFO - [diffusion][Epoch 11589] Epoch 11590/12000
2024-11-05 03:39:28,708 - INFO - [diffusion][Epoch 11589] diffusion training Loss: 0.05858873762190342
2024-11-05 03:39:28,709 - INFO - [diffusion][Epoch 11589] diffusion learning rate: 0.001
2024-11-05 03:39:28,711 - INFO - [diffusion][Epoch 11589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:28,712 - INFO - [diffusion][Epoch 11590] Epoch 11591/12000
2024-11-05 03:39:31,778 - INFO - [diffusion][Epoch 11590] diffusion training Loss: 0.06243650708347559
2024-11-05 03:39:31,781 - INFO - [diffusion][Epoch 11590] diffusion learning rate: 0.001
2024-11-05 03:39:31,783 - INFO - [diffusion][Epoch 11590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:31,784 - INFO - [diffusion][Epoch 11591] Epoch 11592/12000
2024-11-05 03:39:34,909 - INFO - [diffusion][Epoch 11591] diffusion training Loss: 0.06805275846272707
2024-11-05 03:39:34,911 - INFO - [diffusion][Epoch 11591] diffusion learning rate: 0.001
2024-11-05 03:39:34,913 - INFO - [diffusion][Epoch 11591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:34,914 - INFO - [diffusion][Epoch 11592] Epoch 11593/12000
2024-11-05 03:39:38,454 - INFO - [diffusion][Epoch 11592] diffusion training Loss: 0.06464850343763828
2024-11-05 03:39:38,456 - INFO - [diffusion][Epoch 11592] diffusion learning rate: 0.001
2024-11-05 03:39:38,458 - INFO - [diffusion][Epoch 11592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:38,459 - INFO - [diffusion][Epoch 11593] Epoch 11594/12000
2024-11-05 03:39:42,106 - INFO - [diffusion][Epoch 11593] diffusion training Loss: 0.05827307514846325
2024-11-05 03:39:42,108 - INFO - [diffusion][Epoch 11593] diffusion learning rate: 0.001
2024-11-05 03:39:42,136 - INFO - [diffusion][Epoch 11593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:42,137 - INFO - [diffusion][Epoch 11594] Epoch 11595/12000
2024-11-05 03:39:45,358 - INFO - [diffusion][Epoch 11594] diffusion training Loss: 0.06761477142572403
2024-11-05 03:39:45,359 - INFO - [diffusion][Epoch 11594] diffusion learning rate: 0.001
2024-11-05 03:39:45,361 - INFO - [diffusion][Epoch 11594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:45,362 - INFO - [diffusion][Epoch 11595] Epoch 11596/12000
2024-11-05 03:39:48,451 - INFO - [diffusion][Epoch 11595] diffusion training Loss: 0.06736595556139946
2024-11-05 03:39:48,453 - INFO - [diffusion][Epoch 11595] diffusion learning rate: 0.001
2024-11-05 03:39:48,455 - INFO - [diffusion][Epoch 11595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:48,456 - INFO - [diffusion][Epoch 11596] Epoch 11597/12000
2024-11-05 03:39:51,506 - INFO - [diffusion][Epoch 11596] diffusion training Loss: 0.06316875014454126
2024-11-05 03:39:51,508 - INFO - [diffusion][Epoch 11596] diffusion learning rate: 0.001
2024-11-05 03:39:51,510 - INFO - [diffusion][Epoch 11596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:51,511 - INFO - [diffusion][Epoch 11597] Epoch 11598/12000
2024-11-05 03:39:54,930 - INFO - [diffusion][Epoch 11597] diffusion training Loss: 0.063005231320858
2024-11-05 03:39:54,932 - INFO - [diffusion][Epoch 11597] diffusion learning rate: 0.001
2024-11-05 03:39:54,934 - INFO - [diffusion][Epoch 11597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:54,935 - INFO - [diffusion][Epoch 11598] Epoch 11599/12000
2024-11-05 03:39:58,066 - INFO - [diffusion][Epoch 11598] diffusion training Loss: 0.06600285694003105
2024-11-05 03:39:58,069 - INFO - [diffusion][Epoch 11598] diffusion learning rate: 0.001
2024-11-05 03:39:58,071 - INFO - [diffusion][Epoch 11598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:58,072 - INFO - [diffusion][Epoch 11599] Epoch 11600/12000
2024-11-05 03:40:01,055 - INFO - [diffusion][Epoch 11599] diffusion training Loss: 0.05904284492135048
2024-11-05 03:40:01,057 - INFO - [diffusion][Epoch 11599] diffusion learning rate: 0.001
2024-11-05 03:40:01,058 - INFO - [diffusion][Epoch 11599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:01,060 - INFO - [diffusion][Epoch 11600] Epoch 11601/12000
2024-11-05 03:40:04,265 - INFO - [diffusion][Epoch 11600] diffusion training Loss: 0.0687711052596569
2024-11-05 03:40:04,267 - INFO - [diffusion][Epoch 11600] diffusion learning rate: 0.001
2024-11-05 03:40:04,269 - INFO - [diffusion][Epoch 11600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:04,270 - INFO - [diffusion][Epoch 11601] Epoch 11602/12000
2024-11-05 03:40:07,935 - INFO - [diffusion][Epoch 11601] diffusion training Loss: 0.06300871819257736
2024-11-05 03:40:07,937 - INFO - [diffusion][Epoch 11601] diffusion learning rate: 0.001
2024-11-05 03:40:07,939 - INFO - [diffusion][Epoch 11601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:07,940 - INFO - [diffusion][Epoch 11602] Epoch 11603/12000
2024-11-05 03:40:11,420 - INFO - [diffusion][Epoch 11602] diffusion training Loss: 0.06590180844068527
2024-11-05 03:40:11,422 - INFO - [diffusion][Epoch 11602] diffusion learning rate: 0.001
2024-11-05 03:40:11,424 - INFO - [diffusion][Epoch 11602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:11,426 - INFO - [diffusion][Epoch 11603] Epoch 11604/12000
2024-11-05 03:40:14,650 - INFO - [diffusion][Epoch 11603] diffusion training Loss: 0.06809028889983892
2024-11-05 03:40:14,652 - INFO - [diffusion][Epoch 11603] diffusion learning rate: 0.001
2024-11-05 03:40:14,653 - INFO - [diffusion][Epoch 11603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:14,655 - INFO - [diffusion][Epoch 11604] Epoch 11605/12000
2024-11-05 03:40:17,724 - INFO - [diffusion][Epoch 11604] diffusion training Loss: 0.06073252111673355
2024-11-05 03:40:17,726 - INFO - [diffusion][Epoch 11604] diffusion learning rate: 0.001
2024-11-05 03:40:17,728 - INFO - [diffusion][Epoch 11604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:17,729 - INFO - [diffusion][Epoch 11605] Epoch 11606/12000
2024-11-05 03:40:20,982 - INFO - [diffusion][Epoch 11605] diffusion training Loss: 0.06089864484965801
2024-11-05 03:40:20,984 - INFO - [diffusion][Epoch 11605] diffusion learning rate: 0.001
2024-11-05 03:40:20,986 - INFO - [diffusion][Epoch 11605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:20,987 - INFO - [diffusion][Epoch 11606] Epoch 11607/12000
2024-11-05 03:40:24,525 - INFO - [diffusion][Epoch 11606] diffusion training Loss: 0.06147284526377916
2024-11-05 03:40:24,527 - INFO - [diffusion][Epoch 11606] diffusion learning rate: 0.001
2024-11-05 03:40:24,528 - INFO - [diffusion][Epoch 11606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:24,530 - INFO - [diffusion][Epoch 11607] Epoch 11608/12000
2024-11-05 03:40:28,086 - INFO - [diffusion][Epoch 11607] diffusion training Loss: 0.06608130503445864
2024-11-05 03:40:28,088 - INFO - [diffusion][Epoch 11607] diffusion learning rate: 0.001
2024-11-05 03:40:28,091 - INFO - [diffusion][Epoch 11607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:28,092 - INFO - [diffusion][Epoch 11608] Epoch 11609/12000
2024-11-05 03:40:31,016 - INFO - [diffusion][Epoch 11608] diffusion training Loss: 0.0667815450578928
2024-11-05 03:40:31,018 - INFO - [diffusion][Epoch 11608] diffusion learning rate: 0.001
2024-11-05 03:40:31,019 - INFO - [diffusion][Epoch 11608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:31,021 - INFO - [diffusion][Epoch 11609] Epoch 11610/12000
2024-11-05 03:40:34,129 - INFO - [diffusion][Epoch 11609] diffusion training Loss: 0.06507652997970581
2024-11-05 03:40:34,132 - INFO - [diffusion][Epoch 11609] diffusion learning rate: 0.001
2024-11-05 03:40:34,134 - INFO - [diffusion][Epoch 11609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:34,135 - INFO - [diffusion][Epoch 11610] Epoch 11611/12000
2024-11-05 03:40:37,667 - INFO - [diffusion][Epoch 11610] diffusion training Loss: 0.06505679152905941
2024-11-05 03:40:37,669 - INFO - [diffusion][Epoch 11610] diffusion learning rate: 0.001
2024-11-05 03:40:37,671 - INFO - [diffusion][Epoch 11610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:37,672 - INFO - [diffusion][Epoch 11611] Epoch 11612/12000
2024-11-05 03:40:41,234 - INFO - [diffusion][Epoch 11611] diffusion training Loss: 0.06246402487158775
2024-11-05 03:40:41,236 - INFO - [diffusion][Epoch 11611] diffusion learning rate: 0.001
2024-11-05 03:40:41,242 - INFO - [diffusion][Epoch 11611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:41,243 - INFO - [diffusion][Epoch 11612] Epoch 11613/12000
2024-11-05 03:40:44,355 - INFO - [diffusion][Epoch 11612] diffusion training Loss: 0.0659462520852685
2024-11-05 03:40:44,357 - INFO - [diffusion][Epoch 11612] diffusion learning rate: 0.001
2024-11-05 03:40:44,359 - INFO - [diffusion][Epoch 11612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:44,360 - INFO - [diffusion][Epoch 11613] Epoch 11614/12000
2024-11-05 03:40:47,498 - INFO - [diffusion][Epoch 11613] diffusion training Loss: 0.061658442951738834
2024-11-05 03:40:47,500 - INFO - [diffusion][Epoch 11613] diffusion learning rate: 0.001
2024-11-05 03:40:47,502 - INFO - [diffusion][Epoch 11613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:47,503 - INFO - [diffusion][Epoch 11614] Epoch 11615/12000
2024-11-05 03:40:50,663 - INFO - [diffusion][Epoch 11614] diffusion training Loss: 0.06437931954860687
2024-11-05 03:40:50,665 - INFO - [diffusion][Epoch 11614] diffusion learning rate: 0.001
2024-11-05 03:40:50,667 - INFO - [diffusion][Epoch 11614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:50,668 - INFO - [diffusion][Epoch 11615] Epoch 11616/12000
2024-11-05 03:40:54,386 - INFO - [diffusion][Epoch 11615] diffusion training Loss: 0.06376694608479738
2024-11-05 03:40:54,388 - INFO - [diffusion][Epoch 11615] diffusion learning rate: 0.001
2024-11-05 03:40:54,390 - INFO - [diffusion][Epoch 11615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:54,391 - INFO - [diffusion][Epoch 11616] Epoch 11617/12000
2024-11-05 03:40:57,708 - INFO - [diffusion][Epoch 11616] diffusion training Loss: 0.06442420650273561
2024-11-05 03:40:57,711 - INFO - [diffusion][Epoch 11616] diffusion learning rate: 0.001
2024-11-05 03:40:57,713 - INFO - [diffusion][Epoch 11616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:57,714 - INFO - [diffusion][Epoch 11617] Epoch 11618/12000
2024-11-05 03:41:00,925 - INFO - [diffusion][Epoch 11617] diffusion training Loss: 0.0688265897333622
2024-11-05 03:41:00,927 - INFO - [diffusion][Epoch 11617] diffusion learning rate: 0.001
2024-11-05 03:41:00,929 - INFO - [diffusion][Epoch 11617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:00,930 - INFO - [diffusion][Epoch 11618] Epoch 11619/12000
2024-11-05 03:41:04,130 - INFO - [diffusion][Epoch 11618] diffusion training Loss: 0.06261757481843233
2024-11-05 03:41:04,131 - INFO - [diffusion][Epoch 11618] diffusion learning rate: 0.001
2024-11-05 03:41:04,133 - INFO - [diffusion][Epoch 11618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:04,135 - INFO - [diffusion][Epoch 11619] Epoch 11620/12000
2024-11-05 03:41:07,574 - INFO - [diffusion][Epoch 11619] diffusion training Loss: 0.0690353475511074
2024-11-05 03:41:07,576 - INFO - [diffusion][Epoch 11619] diffusion learning rate: 0.001
2024-11-05 03:41:07,578 - INFO - [diffusion][Epoch 11619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:07,579 - INFO - [diffusion][Epoch 11620] Epoch 11621/12000
2024-11-05 03:41:11,291 - INFO - [diffusion][Epoch 11620] diffusion training Loss: 0.06339343544095755
2024-11-05 03:41:11,293 - INFO - [diffusion][Epoch 11620] diffusion learning rate: 0.001
2024-11-05 03:41:11,294 - INFO - [diffusion][Epoch 11620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:11,295 - INFO - [diffusion][Epoch 11621] Epoch 11622/12000
2024-11-05 03:41:14,890 - INFO - [diffusion][Epoch 11621] diffusion training Loss: 0.06172460410743952
2024-11-05 03:41:14,892 - INFO - [diffusion][Epoch 11621] diffusion learning rate: 0.001
2024-11-05 03:41:14,893 - INFO - [diffusion][Epoch 11621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:14,895 - INFO - [diffusion][Epoch 11622] Epoch 11623/12000
2024-11-05 03:41:18,029 - INFO - [diffusion][Epoch 11622] diffusion training Loss: 0.06542447209358215
2024-11-05 03:41:18,030 - INFO - [diffusion][Epoch 11622] diffusion learning rate: 0.001
2024-11-05 03:41:18,032 - INFO - [diffusion][Epoch 11622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:18,033 - INFO - [diffusion][Epoch 11623] Epoch 11624/12000
2024-11-05 03:41:21,130 - INFO - [diffusion][Epoch 11623] diffusion training Loss: 0.06386573147028685
2024-11-05 03:41:21,132 - INFO - [diffusion][Epoch 11623] diffusion learning rate: 0.001
2024-11-05 03:41:21,133 - INFO - [diffusion][Epoch 11623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:21,134 - INFO - [diffusion][Epoch 11624] Epoch 11625/12000
2024-11-05 03:41:24,386 - INFO - [diffusion][Epoch 11624] diffusion training Loss: 0.06649239175021648
2024-11-05 03:41:24,388 - INFO - [diffusion][Epoch 11624] diffusion learning rate: 0.001
2024-11-05 03:41:24,390 - INFO - [diffusion][Epoch 11624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:24,391 - INFO - [diffusion][Epoch 11625] Epoch 11626/12000
2024-11-05 03:41:28,086 - INFO - [diffusion][Epoch 11625] diffusion training Loss: 0.06321967858821154
2024-11-05 03:41:28,088 - INFO - [diffusion][Epoch 11625] diffusion learning rate: 0.001
2024-11-05 03:41:28,090 - INFO - [diffusion][Epoch 11625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:28,091 - INFO - [diffusion][Epoch 11626] Epoch 11627/12000
2024-11-05 03:41:31,640 - INFO - [diffusion][Epoch 11626] diffusion training Loss: 0.07149963639676571
2024-11-05 03:41:31,641 - INFO - [diffusion][Epoch 11626] diffusion learning rate: 0.001
2024-11-05 03:41:31,643 - INFO - [diffusion][Epoch 11626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:31,644 - INFO - [diffusion][Epoch 11627] Epoch 11628/12000
2024-11-05 03:41:34,581 - INFO - [diffusion][Epoch 11627] diffusion training Loss: 0.06018975004553795
2024-11-05 03:41:34,584 - INFO - [diffusion][Epoch 11627] diffusion learning rate: 0.001
2024-11-05 03:41:34,586 - INFO - [diffusion][Epoch 11627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:34,587 - INFO - [diffusion][Epoch 11628] Epoch 11629/12000
2024-11-05 03:41:37,738 - INFO - [diffusion][Epoch 11628] diffusion training Loss: 0.06379395350813866
2024-11-05 03:41:37,740 - INFO - [diffusion][Epoch 11628] diffusion learning rate: 0.001
2024-11-05 03:41:37,742 - INFO - [diffusion][Epoch 11628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:37,744 - INFO - [diffusion][Epoch 11629] Epoch 11630/12000
2024-11-05 03:41:41,000 - INFO - [diffusion][Epoch 11629] diffusion training Loss: 0.06923575326800346
2024-11-05 03:41:41,002 - INFO - [diffusion][Epoch 11629] diffusion learning rate: 0.001
2024-11-05 03:41:41,003 - INFO - [diffusion][Epoch 11629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:41,005 - INFO - [diffusion][Epoch 11630] Epoch 11631/12000
2024-11-05 03:41:44,683 - INFO - [diffusion][Epoch 11630] diffusion training Loss: 0.06766992807388306
2024-11-05 03:41:44,685 - INFO - [diffusion][Epoch 11630] diffusion learning rate: 0.001
2024-11-05 03:41:44,713 - INFO - [diffusion][Epoch 11630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:44,714 - INFO - [diffusion][Epoch 11631] Epoch 11632/12000
2024-11-05 03:41:48,198 - INFO - [diffusion][Epoch 11631] diffusion training Loss: 0.06558470334857702
2024-11-05 03:41:48,200 - INFO - [diffusion][Epoch 11631] diffusion learning rate: 0.001
2024-11-05 03:41:48,203 - INFO - [diffusion][Epoch 11631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:48,204 - INFO - [diffusion][Epoch 11632] Epoch 11633/12000
2024-11-05 03:41:51,326 - INFO - [diffusion][Epoch 11632] diffusion training Loss: 0.06307114567607641
2024-11-05 03:41:51,328 - INFO - [diffusion][Epoch 11632] diffusion learning rate: 0.001
2024-11-05 03:41:51,330 - INFO - [diffusion][Epoch 11632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:51,331 - INFO - [diffusion][Epoch 11633] Epoch 11634/12000
2024-11-05 03:41:54,283 - INFO - [diffusion][Epoch 11633] diffusion training Loss: 0.06463321670889854
2024-11-05 03:41:54,285 - INFO - [diffusion][Epoch 11633] diffusion learning rate: 0.001
2024-11-05 03:41:54,287 - INFO - [diffusion][Epoch 11633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:54,289 - INFO - [diffusion][Epoch 11634] Epoch 11635/12000
2024-11-05 03:41:57,684 - INFO - [diffusion][Epoch 11634] diffusion training Loss: 0.06390463560819626
2024-11-05 03:41:57,687 - INFO - [diffusion][Epoch 11634] diffusion learning rate: 0.001
2024-11-05 03:41:57,689 - INFO - [diffusion][Epoch 11634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:57,691 - INFO - [diffusion][Epoch 11635] Epoch 11636/12000
2024-11-05 03:42:01,223 - INFO - [diffusion][Epoch 11635] diffusion training Loss: 0.06807251647114754
2024-11-05 03:42:01,225 - INFO - [diffusion][Epoch 11635] diffusion learning rate: 0.001
2024-11-05 03:42:01,252 - INFO - [diffusion][Epoch 11635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:01,253 - INFO - [diffusion][Epoch 11636] Epoch 11637/12000
2024-11-05 03:42:04,352 - INFO - [diffusion][Epoch 11636] diffusion training Loss: 0.06656838022172451
2024-11-05 03:42:04,354 - INFO - [diffusion][Epoch 11636] diffusion learning rate: 0.001
2024-11-05 03:42:04,356 - INFO - [diffusion][Epoch 11636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:04,358 - INFO - [diffusion][Epoch 11637] Epoch 11638/12000
2024-11-05 03:42:07,499 - INFO - [diffusion][Epoch 11637] diffusion training Loss: 0.05970633216202259
2024-11-05 03:42:07,501 - INFO - [diffusion][Epoch 11637] diffusion learning rate: 0.001
2024-11-05 03:42:07,502 - INFO - [diffusion][Epoch 11637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:07,504 - INFO - [diffusion][Epoch 11638] Epoch 11639/12000
2024-11-05 03:42:10,629 - INFO - [diffusion][Epoch 11638] diffusion training Loss: 0.06140667758882046
2024-11-05 03:42:10,630 - INFO - [diffusion][Epoch 11638] diffusion learning rate: 0.001
2024-11-05 03:42:10,632 - INFO - [diffusion][Epoch 11638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:10,633 - INFO - [diffusion][Epoch 11639] Epoch 11640/12000
2024-11-05 03:42:14,180 - INFO - [diffusion][Epoch 11639] diffusion training Loss: 0.06568555068224669
2024-11-05 03:42:14,182 - INFO - [diffusion][Epoch 11639] diffusion learning rate: 0.001
2024-11-05 03:42:14,184 - INFO - [diffusion][Epoch 11639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:14,186 - INFO - [diffusion][Epoch 11640] Epoch 11641/12000
2024-11-05 03:42:17,681 - INFO - [diffusion][Epoch 11640] diffusion training Loss: 0.06404662132263184
2024-11-05 03:42:17,683 - INFO - [diffusion][Epoch 11640] diffusion learning rate: 0.001
2024-11-05 03:42:17,732 - INFO - [diffusion][Epoch 11640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:17,734 - INFO - [diffusion][Epoch 11641] Epoch 11642/12000
2024-11-05 03:42:20,836 - INFO - [diffusion][Epoch 11641] diffusion training Loss: 0.062025802209973335
2024-11-05 03:42:20,838 - INFO - [diffusion][Epoch 11641] diffusion learning rate: 0.001
2024-11-05 03:42:20,840 - INFO - [diffusion][Epoch 11641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:20,841 - INFO - [diffusion][Epoch 11642] Epoch 11643/12000
2024-11-05 03:42:23,920 - INFO - [diffusion][Epoch 11642] diffusion training Loss: 0.06829871982336044
2024-11-05 03:42:23,922 - INFO - [diffusion][Epoch 11642] diffusion learning rate: 0.001
2024-11-05 03:42:23,924 - INFO - [diffusion][Epoch 11642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:23,925 - INFO - [diffusion][Epoch 11643] Epoch 11644/12000
2024-11-05 03:42:27,049 - INFO - [diffusion][Epoch 11643] diffusion training Loss: 0.060848675668239594
2024-11-05 03:42:27,051 - INFO - [diffusion][Epoch 11643] diffusion learning rate: 0.001
2024-11-05 03:42:27,053 - INFO - [diffusion][Epoch 11643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:27,054 - INFO - [diffusion][Epoch 11644] Epoch 11645/12000
2024-11-05 03:42:30,610 - INFO - [diffusion][Epoch 11644] diffusion training Loss: 0.0639285808429122
2024-11-05 03:42:30,612 - INFO - [diffusion][Epoch 11644] diffusion learning rate: 0.001
2024-11-05 03:42:30,614 - INFO - [diffusion][Epoch 11644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:30,615 - INFO - [diffusion][Epoch 11645] Epoch 11646/12000
2024-11-05 03:42:34,147 - INFO - [diffusion][Epoch 11645] diffusion training Loss: 0.0581331430003047
2024-11-05 03:42:34,149 - INFO - [diffusion][Epoch 11645] diffusion learning rate: 0.001
2024-11-05 03:42:34,170 - INFO - [diffusion][Epoch 11645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:34,171 - INFO - [diffusion][Epoch 11646] Epoch 11647/12000
2024-11-05 03:42:37,286 - INFO - [diffusion][Epoch 11646] diffusion training Loss: 0.06043780967593193
2024-11-05 03:42:37,289 - INFO - [diffusion][Epoch 11646] diffusion learning rate: 0.001
2024-11-05 03:42:37,291 - INFO - [diffusion][Epoch 11646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:37,292 - INFO - [diffusion][Epoch 11647] Epoch 11648/12000
2024-11-05 03:42:40,789 - INFO - [diffusion][Epoch 11647] diffusion training Loss: 0.05863822437822819
2024-11-05 03:42:40,791 - INFO - [diffusion][Epoch 11647] diffusion learning rate: 0.001
2024-11-05 03:42:40,793 - INFO - [diffusion][Epoch 11647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:40,794 - INFO - [diffusion][Epoch 11648] Epoch 11649/12000
2024-11-05 03:42:44,033 - INFO - [diffusion][Epoch 11648] diffusion training Loss: 0.06076263077557087
2024-11-05 03:42:44,035 - INFO - [diffusion][Epoch 11648] diffusion learning rate: 0.001
2024-11-05 03:42:44,037 - INFO - [diffusion][Epoch 11648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:44,038 - INFO - [diffusion][Epoch 11649] Epoch 11650/12000
2024-11-05 03:42:47,294 - INFO - [diffusion][Epoch 11649] diffusion training Loss: 0.0636445814743638
2024-11-05 03:42:47,296 - INFO - [diffusion][Epoch 11649] diffusion learning rate: 0.001
2024-11-05 03:42:47,298 - INFO - [diffusion][Epoch 11649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:47,299 - INFO - [diffusion][Epoch 11650] Epoch 11651/12000
2024-11-05 03:42:51,002 - INFO - [diffusion][Epoch 11650] diffusion training Loss: 0.05401822552084923
2024-11-05 03:42:51,004 - INFO - [diffusion][Epoch 11650] diffusion learning rate: 0.001
2024-11-05 03:42:51,006 - INFO - [diffusion][Epoch 11650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:51,007 - INFO - [diffusion][Epoch 11651] Epoch 11652/12000
2024-11-05 03:42:54,600 - INFO - [diffusion][Epoch 11651] diffusion training Loss: 0.06208203732967377
2024-11-05 03:42:54,602 - INFO - [diffusion][Epoch 11651] diffusion learning rate: 0.001
2024-11-05 03:42:54,604 - INFO - [diffusion][Epoch 11651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:54,606 - INFO - [diffusion][Epoch 11652] Epoch 11653/12000
2024-11-05 03:42:57,718 - INFO - [diffusion][Epoch 11652] diffusion training Loss: 0.06163348350673914
2024-11-05 03:42:57,720 - INFO - [diffusion][Epoch 11652] diffusion learning rate: 0.001
2024-11-05 03:42:57,722 - INFO - [diffusion][Epoch 11652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:57,723 - INFO - [diffusion][Epoch 11653] Epoch 11654/12000
2024-11-05 03:43:00,843 - INFO - [diffusion][Epoch 11653] diffusion training Loss: 0.06501094624400139
2024-11-05 03:43:00,845 - INFO - [diffusion][Epoch 11653] diffusion learning rate: 0.001
2024-11-05 03:43:00,847 - INFO - [diffusion][Epoch 11653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:00,848 - INFO - [diffusion][Epoch 11654] Epoch 11655/12000
2024-11-05 03:43:03,994 - INFO - [diffusion][Epoch 11654] diffusion training Loss: 0.0659822765737772
2024-11-05 03:43:03,995 - INFO - [diffusion][Epoch 11654] diffusion learning rate: 0.001
2024-11-05 03:43:03,997 - INFO - [diffusion][Epoch 11654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:03,998 - INFO - [diffusion][Epoch 11655] Epoch 11656/12000
2024-11-05 03:43:07,574 - INFO - [diffusion][Epoch 11655] diffusion training Loss: 0.06412189453840256
2024-11-05 03:43:07,576 - INFO - [diffusion][Epoch 11655] diffusion learning rate: 0.001
2024-11-05 03:43:07,578 - INFO - [diffusion][Epoch 11655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:07,579 - INFO - [diffusion][Epoch 11656] Epoch 11657/12000
2024-11-05 03:43:11,152 - INFO - [diffusion][Epoch 11656] diffusion training Loss: 0.05922338739037514
2024-11-05 03:43:11,154 - INFO - [diffusion][Epoch 11656] diffusion learning rate: 0.001
2024-11-05 03:43:11,156 - INFO - [diffusion][Epoch 11656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:11,157 - INFO - [diffusion][Epoch 11657] Epoch 11658/12000
2024-11-05 03:43:14,304 - INFO - [diffusion][Epoch 11657] diffusion training Loss: 0.06420901883393526
2024-11-05 03:43:14,306 - INFO - [diffusion][Epoch 11657] diffusion learning rate: 0.001
2024-11-05 03:43:14,307 - INFO - [diffusion][Epoch 11657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:14,308 - INFO - [diffusion][Epoch 11658] Epoch 11659/12000
2024-11-05 03:43:17,087 - INFO - [diffusion][Epoch 11658] diffusion training Loss: 0.0637253001332283
2024-11-05 03:43:17,089 - INFO - [diffusion][Epoch 11658] diffusion learning rate: 0.001
2024-11-05 03:43:17,090 - INFO - [diffusion][Epoch 11658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:17,092 - INFO - [diffusion][Epoch 11659] Epoch 11660/12000
2024-11-05 03:43:20,515 - INFO - [diffusion][Epoch 11659] diffusion training Loss: 0.06830458156764507
2024-11-05 03:43:20,517 - INFO - [diffusion][Epoch 11659] diffusion learning rate: 0.001
2024-11-05 03:43:20,519 - INFO - [diffusion][Epoch 11659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:20,520 - INFO - [diffusion][Epoch 11660] Epoch 11661/12000
2024-11-05 03:43:24,157 - INFO - [diffusion][Epoch 11660] diffusion training Loss: 0.06066778115928173
2024-11-05 03:43:24,159 - INFO - [diffusion][Epoch 11660] diffusion learning rate: 0.001
2024-11-05 03:43:24,161 - INFO - [diffusion][Epoch 11660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:24,162 - INFO - [diffusion][Epoch 11661] Epoch 11662/12000
2024-11-05 03:43:27,363 - INFO - [diffusion][Epoch 11661] diffusion training Loss: 0.062297532334923744
2024-11-05 03:43:27,366 - INFO - [diffusion][Epoch 11661] diffusion learning rate: 0.001
2024-11-05 03:43:27,367 - INFO - [diffusion][Epoch 11661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:27,368 - INFO - [diffusion][Epoch 11662] Epoch 11663/12000
2024-11-05 03:43:30,416 - INFO - [diffusion][Epoch 11662] diffusion training Loss: 0.06262219604104757
2024-11-05 03:43:30,417 - INFO - [diffusion][Epoch 11662] diffusion learning rate: 0.001
2024-11-05 03:43:30,419 - INFO - [diffusion][Epoch 11662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:30,420 - INFO - [diffusion][Epoch 11663] Epoch 11664/12000
2024-11-05 03:43:33,552 - INFO - [diffusion][Epoch 11663] diffusion training Loss: 0.06501809041947126
2024-11-05 03:43:33,554 - INFO - [diffusion][Epoch 11663] diffusion learning rate: 0.001
2024-11-05 03:43:33,556 - INFO - [diffusion][Epoch 11663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:33,557 - INFO - [diffusion][Epoch 11664] Epoch 11665/12000
2024-11-05 03:43:37,143 - INFO - [diffusion][Epoch 11664] diffusion training Loss: 0.0630803694948554
2024-11-05 03:43:37,145 - INFO - [diffusion][Epoch 11664] diffusion learning rate: 0.001
2024-11-05 03:43:37,147 - INFO - [diffusion][Epoch 11664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:37,148 - INFO - [diffusion][Epoch 11665] Epoch 11666/12000
2024-11-05 03:43:40,686 - INFO - [diffusion][Epoch 11665] diffusion training Loss: 0.06554057821631432
2024-11-05 03:43:40,689 - INFO - [diffusion][Epoch 11665] diffusion learning rate: 0.001
2024-11-05 03:43:40,690 - INFO - [diffusion][Epoch 11665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:40,692 - INFO - [diffusion][Epoch 11666] Epoch 11667/12000
2024-11-05 03:43:43,807 - INFO - [diffusion][Epoch 11666] diffusion training Loss: 0.06687090918421745
2024-11-05 03:43:43,808 - INFO - [diffusion][Epoch 11666] diffusion learning rate: 0.001
2024-11-05 03:43:43,810 - INFO - [diffusion][Epoch 11666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:43,811 - INFO - [diffusion][Epoch 11667] Epoch 11668/12000
2024-11-05 03:43:46,918 - INFO - [diffusion][Epoch 11667] diffusion training Loss: 0.0654755774885416
2024-11-05 03:43:46,919 - INFO - [diffusion][Epoch 11667] diffusion learning rate: 0.001
2024-11-05 03:43:46,921 - INFO - [diffusion][Epoch 11667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:46,922 - INFO - [diffusion][Epoch 11668] Epoch 11669/12000
2024-11-05 03:43:50,583 - INFO - [diffusion][Epoch 11668] diffusion training Loss: 0.06109320931136608
2024-11-05 03:43:50,585 - INFO - [diffusion][Epoch 11668] diffusion learning rate: 0.001
2024-11-05 03:43:50,587 - INFO - [diffusion][Epoch 11668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:50,588 - INFO - [diffusion][Epoch 11669] Epoch 11670/12000
2024-11-05 03:43:53,878 - INFO - [diffusion][Epoch 11669] diffusion training Loss: 0.05983268655836582
2024-11-05 03:43:53,880 - INFO - [diffusion][Epoch 11669] diffusion learning rate: 0.001
2024-11-05 03:43:53,882 - INFO - [diffusion][Epoch 11669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:53,883 - INFO - [diffusion][Epoch 11670] Epoch 11671/12000
2024-11-05 03:43:57,568 - INFO - [diffusion][Epoch 11670] diffusion training Loss: 0.0625146608799696
2024-11-05 03:43:57,570 - INFO - [diffusion][Epoch 11670] diffusion learning rate: 0.001
2024-11-05 03:43:57,572 - INFO - [diffusion][Epoch 11670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:57,573 - INFO - [diffusion][Epoch 11671] Epoch 11672/12000
2024-11-05 03:44:00,894 - INFO - [diffusion][Epoch 11671] diffusion training Loss: 0.06433060579001904
2024-11-05 03:44:00,896 - INFO - [diffusion][Epoch 11671] diffusion learning rate: 0.001
2024-11-05 03:44:00,898 - INFO - [diffusion][Epoch 11671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:00,899 - INFO - [diffusion][Epoch 11672] Epoch 11673/12000
2024-11-05 03:44:03,998 - INFO - [diffusion][Epoch 11672] diffusion training Loss: 0.06340317241847515
2024-11-05 03:44:03,999 - INFO - [diffusion][Epoch 11672] diffusion learning rate: 0.001
2024-11-05 03:44:04,001 - INFO - [diffusion][Epoch 11672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:04,003 - INFO - [diffusion][Epoch 11673] Epoch 11674/12000
2024-11-05 03:44:07,094 - INFO - [diffusion][Epoch 11673] diffusion training Loss: 0.06206356734037399
2024-11-05 03:44:07,096 - INFO - [diffusion][Epoch 11673] diffusion learning rate: 0.001
2024-11-05 03:44:07,136 - INFO - [diffusion][Epoch 11673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:07,137 - INFO - [diffusion][Epoch 11674] Epoch 11675/12000
2024-11-05 03:44:10,507 - INFO - [diffusion][Epoch 11674] diffusion training Loss: 0.06473129149526358
2024-11-05 03:44:10,509 - INFO - [diffusion][Epoch 11674] diffusion learning rate: 0.001
2024-11-05 03:44:10,511 - INFO - [diffusion][Epoch 11674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:10,512 - INFO - [diffusion][Epoch 11675] Epoch 11676/12000
2024-11-05 03:44:14,227 - INFO - [diffusion][Epoch 11675] diffusion training Loss: 0.05793909542262554
2024-11-05 03:44:14,229 - INFO - [diffusion][Epoch 11675] diffusion learning rate: 0.001
2024-11-05 03:44:14,231 - INFO - [diffusion][Epoch 11675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:14,232 - INFO - [diffusion][Epoch 11676] Epoch 11677/12000
2024-11-05 03:44:17,737 - INFO - [diffusion][Epoch 11676] diffusion training Loss: 0.06118795368820429
2024-11-05 03:44:17,739 - INFO - [diffusion][Epoch 11676] diffusion learning rate: 0.001
2024-11-05 03:44:17,741 - INFO - [diffusion][Epoch 11676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:17,742 - INFO - [diffusion][Epoch 11677] Epoch 11678/12000
2024-11-05 03:44:20,803 - INFO - [diffusion][Epoch 11677] diffusion training Loss: 0.06562642380595207
2024-11-05 03:44:20,805 - INFO - [diffusion][Epoch 11677] diffusion learning rate: 0.001
2024-11-05 03:44:20,808 - INFO - [diffusion][Epoch 11677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:20,810 - INFO - [diffusion][Epoch 11678] Epoch 11679/12000
2024-11-05 03:44:24,016 - INFO - [diffusion][Epoch 11678] diffusion training Loss: 0.06569412164390087
2024-11-05 03:44:24,018 - INFO - [diffusion][Epoch 11678] diffusion learning rate: 0.001
2024-11-05 03:44:24,019 - INFO - [diffusion][Epoch 11678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:24,021 - INFO - [diffusion][Epoch 11679] Epoch 11680/12000
2024-11-05 03:44:27,203 - INFO - [diffusion][Epoch 11679] diffusion training Loss: 0.06022617965936661
2024-11-05 03:44:27,205 - INFO - [diffusion][Epoch 11679] diffusion learning rate: 0.001
2024-11-05 03:44:27,207 - INFO - [diffusion][Epoch 11679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:27,208 - INFO - [diffusion][Epoch 11680] Epoch 11681/12000
2024-11-05 03:44:30,727 - INFO - [diffusion][Epoch 11680] diffusion training Loss: 0.06164021324366331
2024-11-05 03:44:30,729 - INFO - [diffusion][Epoch 11680] diffusion learning rate: 0.001
2024-11-05 03:44:30,731 - INFO - [diffusion][Epoch 11680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:30,732 - INFO - [diffusion][Epoch 11681] Epoch 11682/12000
2024-11-05 03:44:34,285 - INFO - [diffusion][Epoch 11681] diffusion training Loss: 0.060553218238055706
2024-11-05 03:44:34,287 - INFO - [diffusion][Epoch 11681] diffusion learning rate: 0.001
2024-11-05 03:44:34,289 - INFO - [diffusion][Epoch 11681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:34,290 - INFO - [diffusion][Epoch 11682] Epoch 11683/12000
2024-11-05 03:44:37,397 - INFO - [diffusion][Epoch 11682] diffusion training Loss: 0.0652611656114459
2024-11-05 03:44:37,398 - INFO - [diffusion][Epoch 11682] diffusion learning rate: 0.001
2024-11-05 03:44:37,400 - INFO - [diffusion][Epoch 11682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:37,401 - INFO - [diffusion][Epoch 11683] Epoch 11684/12000
2024-11-05 03:44:40,178 - INFO - [diffusion][Epoch 11683] diffusion training Loss: 0.06475838087499142
2024-11-05 03:44:40,180 - INFO - [diffusion][Epoch 11683] diffusion learning rate: 0.001
2024-11-05 03:44:40,182 - INFO - [diffusion][Epoch 11683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:40,183 - INFO - [diffusion][Epoch 11684] Epoch 11685/12000
2024-11-05 03:44:43,749 - INFO - [diffusion][Epoch 11684] diffusion training Loss: 0.06704294867813587
2024-11-05 03:44:43,752 - INFO - [diffusion][Epoch 11684] diffusion learning rate: 0.001
2024-11-05 03:44:43,753 - INFO - [diffusion][Epoch 11684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:43,755 - INFO - [diffusion][Epoch 11685] Epoch 11686/12000
2024-11-05 03:44:47,282 - INFO - [diffusion][Epoch 11685] diffusion training Loss: 0.06167485099285841
2024-11-05 03:44:47,285 - INFO - [diffusion][Epoch 11685] diffusion learning rate: 0.001
2024-11-05 03:44:47,286 - INFO - [diffusion][Epoch 11685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:47,288 - INFO - [diffusion][Epoch 11686] Epoch 11687/12000
2024-11-05 03:44:50,307 - INFO - [diffusion][Epoch 11686] diffusion training Loss: 0.06053148489445448
2024-11-05 03:44:50,309 - INFO - [diffusion][Epoch 11686] diffusion learning rate: 0.001
2024-11-05 03:44:50,311 - INFO - [diffusion][Epoch 11686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:50,312 - INFO - [diffusion][Epoch 11687] Epoch 11688/12000
2024-11-05 03:44:53,402 - INFO - [diffusion][Epoch 11687] diffusion training Loss: 0.06685911118984222
2024-11-05 03:44:53,405 - INFO - [diffusion][Epoch 11687] diffusion learning rate: 0.001
2024-11-05 03:44:53,406 - INFO - [diffusion][Epoch 11687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:53,408 - INFO - [diffusion][Epoch 11688] Epoch 11689/12000
2024-11-05 03:44:56,748 - INFO - [diffusion][Epoch 11688] diffusion training Loss: 0.057935360819101334
2024-11-05 03:44:56,750 - INFO - [diffusion][Epoch 11688] diffusion learning rate: 0.001
2024-11-05 03:44:56,752 - INFO - [diffusion][Epoch 11688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:56,753 - INFO - [diffusion][Epoch 11689] Epoch 11690/12000
2024-11-05 03:45:00,194 - INFO - [diffusion][Epoch 11689] diffusion training Loss: 0.06460437178611755
2024-11-05 03:45:00,197 - INFO - [diffusion][Epoch 11689] diffusion learning rate: 0.001
2024-11-05 03:45:00,199 - INFO - [diffusion][Epoch 11689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:00,200 - INFO - [diffusion][Epoch 11690] Epoch 11691/12000
2024-11-05 03:45:03,603 - INFO - [diffusion][Epoch 11690] diffusion training Loss: 0.06670510303229094
2024-11-05 03:45:03,606 - INFO - [diffusion][Epoch 11690] diffusion learning rate: 0.001
2024-11-05 03:45:03,608 - INFO - [diffusion][Epoch 11690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:03,609 - INFO - [diffusion][Epoch 11691] Epoch 11692/12000
2024-11-05 03:45:06,682 - INFO - [diffusion][Epoch 11691] diffusion training Loss: 0.06137513928115368
2024-11-05 03:45:06,684 - INFO - [diffusion][Epoch 11691] diffusion learning rate: 0.001
2024-11-05 03:45:06,685 - INFO - [diffusion][Epoch 11691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:06,687 - INFO - [diffusion][Epoch 11692] Epoch 11693/12000
2024-11-05 03:45:09,887 - INFO - [diffusion][Epoch 11692] diffusion training Loss: 0.05939325410872698
2024-11-05 03:45:09,889 - INFO - [diffusion][Epoch 11692] diffusion learning rate: 0.001
2024-11-05 03:45:09,890 - INFO - [diffusion][Epoch 11692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:09,891 - INFO - [diffusion][Epoch 11693] Epoch 11694/12000
2024-11-05 03:45:13,301 - INFO - [diffusion][Epoch 11693] diffusion training Loss: 0.06307294219732285
2024-11-05 03:45:13,303 - INFO - [diffusion][Epoch 11693] diffusion learning rate: 0.001
2024-11-05 03:45:13,352 - INFO - [diffusion][Epoch 11693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:13,354 - INFO - [diffusion][Epoch 11694] Epoch 11695/12000
2024-11-05 03:45:16,969 - INFO - [diffusion][Epoch 11694] diffusion training Loss: 0.0610885638743639
2024-11-05 03:45:16,970 - INFO - [diffusion][Epoch 11694] diffusion learning rate: 0.001
2024-11-05 03:45:16,972 - INFO - [diffusion][Epoch 11694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:16,973 - INFO - [diffusion][Epoch 11695] Epoch 11696/12000
2024-11-05 03:45:20,507 - INFO - [diffusion][Epoch 11695] diffusion training Loss: 0.057629356160759926
2024-11-05 03:45:20,509 - INFO - [diffusion][Epoch 11695] diffusion learning rate: 0.001
2024-11-05 03:45:20,511 - INFO - [diffusion][Epoch 11695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:20,512 - INFO - [diffusion][Epoch 11696] Epoch 11697/12000
2024-11-05 03:45:23,594 - INFO - [diffusion][Epoch 11696] diffusion training Loss: 0.061260681599378586
2024-11-05 03:45:23,596 - INFO - [diffusion][Epoch 11696] diffusion learning rate: 0.001
2024-11-05 03:45:23,597 - INFO - [diffusion][Epoch 11696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:23,599 - INFO - [diffusion][Epoch 11697] Epoch 11698/12000
2024-11-05 03:45:26,635 - INFO - [diffusion][Epoch 11697] diffusion training Loss: 0.06378339789807796
2024-11-05 03:45:26,637 - INFO - [diffusion][Epoch 11697] diffusion learning rate: 0.001
2024-11-05 03:45:26,638 - INFO - [diffusion][Epoch 11697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:26,640 - INFO - [diffusion][Epoch 11698] Epoch 11699/12000
2024-11-05 03:45:30,128 - INFO - [diffusion][Epoch 11698] diffusion training Loss: 0.06347695924341679
2024-11-05 03:45:30,130 - INFO - [diffusion][Epoch 11698] diffusion learning rate: 0.001
2024-11-05 03:45:30,132 - INFO - [diffusion][Epoch 11698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:30,133 - INFO - [diffusion][Epoch 11699] Epoch 11700/12000
2024-11-05 03:45:33,732 - INFO - [diffusion][Epoch 11699] diffusion training Loss: 0.0611912002786994
2024-11-05 03:45:33,734 - INFO - [diffusion][Epoch 11699] diffusion learning rate: 0.001
2024-11-05 03:45:33,736 - INFO - [diffusion][Epoch 11699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:33,738 - INFO - [diffusion][Epoch 11700] Epoch 11701/12000
2024-11-05 03:45:36,932 - INFO - [diffusion][Epoch 11700] diffusion training Loss: 0.05794804356992245
2024-11-05 03:45:36,934 - INFO - [diffusion][Epoch 11700] diffusion learning rate: 0.001
2024-11-05 03:45:36,936 - INFO - [diffusion][Epoch 11700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:36,937 - INFO - [diffusion][Epoch 11701] Epoch 11702/12000
2024-11-05 03:45:40,029 - INFO - [diffusion][Epoch 11701] diffusion training Loss: 0.058740369975566864
2024-11-05 03:45:40,031 - INFO - [diffusion][Epoch 11701] diffusion learning rate: 0.001
2024-11-05 03:45:40,033 - INFO - [diffusion][Epoch 11701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:40,034 - INFO - [diffusion][Epoch 11702] Epoch 11703/12000
2024-11-05 03:45:43,126 - INFO - [diffusion][Epoch 11702] diffusion training Loss: 0.06059019919484854
2024-11-05 03:45:43,128 - INFO - [diffusion][Epoch 11702] diffusion learning rate: 0.001
2024-11-05 03:45:43,130 - INFO - [diffusion][Epoch 11702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:43,132 - INFO - [diffusion][Epoch 11703] Epoch 11704/12000
2024-11-05 03:45:46,678 - INFO - [diffusion][Epoch 11703] diffusion training Loss: 0.0660610506311059
2024-11-05 03:45:46,703 - INFO - [diffusion][Epoch 11703] diffusion learning rate: 0.001
2024-11-05 03:45:46,705 - INFO - [diffusion][Epoch 11703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:46,706 - INFO - [diffusion][Epoch 11704] Epoch 11705/12000
2024-11-05 03:45:50,215 - INFO - [diffusion][Epoch 11704] diffusion training Loss: 0.06069862935692072
2024-11-05 03:45:50,217 - INFO - [diffusion][Epoch 11704] diffusion learning rate: 0.001
2024-11-05 03:45:50,218 - INFO - [diffusion][Epoch 11704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:50,219 - INFO - [diffusion][Epoch 11705] Epoch 11706/12000
2024-11-05 03:45:53,088 - INFO - [diffusion][Epoch 11705] diffusion training Loss: 0.0661443006247282
2024-11-05 03:45:53,091 - INFO - [diffusion][Epoch 11705] diffusion learning rate: 0.001
2024-11-05 03:45:53,092 - INFO - [diffusion][Epoch 11705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:53,094 - INFO - [diffusion][Epoch 11706] Epoch 11707/12000
2024-11-05 03:45:56,335 - INFO - [diffusion][Epoch 11706] diffusion training Loss: 0.06209350749850273
2024-11-05 03:45:56,337 - INFO - [diffusion][Epoch 11706] diffusion learning rate: 0.001
2024-11-05 03:45:56,339 - INFO - [diffusion][Epoch 11706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:56,340 - INFO - [diffusion][Epoch 11707] Epoch 11708/12000
2024-11-05 03:45:59,604 - INFO - [diffusion][Epoch 11707] diffusion training Loss: 0.06521243415772915
2024-11-05 03:45:59,606 - INFO - [diffusion][Epoch 11707] diffusion learning rate: 0.001
2024-11-05 03:45:59,608 - INFO - [diffusion][Epoch 11707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:59,609 - INFO - [diffusion][Epoch 11708] Epoch 11709/12000
2024-11-05 03:46:03,540 - INFO - [diffusion][Epoch 11708] diffusion training Loss: 0.06266641616821289
2024-11-05 03:46:03,542 - INFO - [diffusion][Epoch 11708] diffusion learning rate: 0.001
2024-11-05 03:46:03,543 - INFO - [diffusion][Epoch 11708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:03,545 - INFO - [diffusion][Epoch 11709] Epoch 11710/12000
2024-11-05 03:46:07,105 - INFO - [diffusion][Epoch 11709] diffusion training Loss: 0.056806035339832306
2024-11-05 03:46:07,107 - INFO - [diffusion][Epoch 11709] diffusion learning rate: 0.001
2024-11-05 03:46:07,109 - INFO - [diffusion][Epoch 11709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:07,110 - INFO - [diffusion][Epoch 11710] Epoch 11711/12000
2024-11-05 03:46:10,655 - INFO - [diffusion][Epoch 11710] diffusion training Loss: 0.06050063855946064
2024-11-05 03:46:10,657 - INFO - [diffusion][Epoch 11710] diffusion learning rate: 0.001
2024-11-05 03:46:10,659 - INFO - [diffusion][Epoch 11710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:10,661 - INFO - [diffusion][Epoch 11711] Epoch 11712/12000
2024-11-05 03:46:13,754 - INFO - [diffusion][Epoch 11711] diffusion training Loss: 0.06180210504680872
2024-11-05 03:46:13,756 - INFO - [diffusion][Epoch 11711] diffusion learning rate: 0.001
2024-11-05 03:46:13,757 - INFO - [diffusion][Epoch 11711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:13,759 - INFO - [diffusion][Epoch 11712] Epoch 11713/12000
2024-11-05 03:46:16,880 - INFO - [diffusion][Epoch 11712] diffusion training Loss: 0.06594488583505154
2024-11-05 03:46:16,881 - INFO - [diffusion][Epoch 11712] diffusion learning rate: 0.001
2024-11-05 03:46:16,883 - INFO - [diffusion][Epoch 11712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:16,886 - INFO - [diffusion][Epoch 11713] Epoch 11714/12000
2024-11-05 03:46:20,013 - INFO - [diffusion][Epoch 11713] diffusion training Loss: 0.06180786807090044
2024-11-05 03:46:20,015 - INFO - [diffusion][Epoch 11713] diffusion learning rate: 0.001
2024-11-05 03:46:20,016 - INFO - [diffusion][Epoch 11713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:20,018 - INFO - [diffusion][Epoch 11714] Epoch 11715/12000
2024-11-05 03:46:23,585 - INFO - [diffusion][Epoch 11714] diffusion training Loss: 0.06147519685328007
2024-11-05 03:46:23,586 - INFO - [diffusion][Epoch 11714] diffusion learning rate: 0.001
2024-11-05 03:46:23,588 - INFO - [diffusion][Epoch 11714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:23,589 - INFO - [diffusion][Epoch 11715] Epoch 11716/12000
2024-11-05 03:46:27,119 - INFO - [diffusion][Epoch 11715] diffusion training Loss: 0.06416535936295986
2024-11-05 03:46:27,121 - INFO - [diffusion][Epoch 11715] diffusion learning rate: 0.001
2024-11-05 03:46:27,122 - INFO - [diffusion][Epoch 11715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:27,124 - INFO - [diffusion][Epoch 11716] Epoch 11717/12000
2024-11-05 03:46:30,244 - INFO - [diffusion][Epoch 11716] diffusion training Loss: 0.06253691669553518
2024-11-05 03:46:30,246 - INFO - [diffusion][Epoch 11716] diffusion learning rate: 0.001
2024-11-05 03:46:30,248 - INFO - [diffusion][Epoch 11716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:30,249 - INFO - [diffusion][Epoch 11717] Epoch 11718/12000
2024-11-05 03:46:33,371 - INFO - [diffusion][Epoch 11717] diffusion training Loss: 0.06624632515013218
2024-11-05 03:46:33,373 - INFO - [diffusion][Epoch 11717] diffusion learning rate: 0.001
2024-11-05 03:46:33,375 - INFO - [diffusion][Epoch 11717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:33,376 - INFO - [diffusion][Epoch 11718] Epoch 11719/12000
2024-11-05 03:46:36,500 - INFO - [diffusion][Epoch 11718] diffusion training Loss: 0.0678226426243782
2024-11-05 03:46:36,502 - INFO - [diffusion][Epoch 11718] diffusion learning rate: 0.001
2024-11-05 03:46:36,503 - INFO - [diffusion][Epoch 11718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:36,505 - INFO - [diffusion][Epoch 11719] Epoch 11720/12000
2024-11-05 03:46:40,018 - INFO - [diffusion][Epoch 11719] diffusion training Loss: 0.06360143143683672
2024-11-05 03:46:40,020 - INFO - [diffusion][Epoch 11719] diffusion learning rate: 0.001
2024-11-05 03:46:40,027 - INFO - [diffusion][Epoch 11719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:40,029 - INFO - [diffusion][Epoch 11720] Epoch 11721/12000
2024-11-05 03:46:43,584 - INFO - [diffusion][Epoch 11720] diffusion training Loss: 0.06123952195048332
2024-11-05 03:46:43,586 - INFO - [diffusion][Epoch 11720] diffusion learning rate: 0.001
2024-11-05 03:46:43,587 - INFO - [diffusion][Epoch 11720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:43,589 - INFO - [diffusion][Epoch 11721] Epoch 11722/12000
2024-11-05 03:46:46,723 - INFO - [diffusion][Epoch 11721] diffusion training Loss: 0.07215986400842667
2024-11-05 03:46:46,726 - INFO - [diffusion][Epoch 11721] diffusion learning rate: 0.001
2024-11-05 03:46:46,728 - INFO - [diffusion][Epoch 11721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:46,730 - INFO - [diffusion][Epoch 11722] Epoch 11723/12000
2024-11-05 03:46:49,874 - INFO - [diffusion][Epoch 11722] diffusion training Loss: 0.06359595991671085
2024-11-05 03:46:49,876 - INFO - [diffusion][Epoch 11722] diffusion learning rate: 0.001
2024-11-05 03:46:49,878 - INFO - [diffusion][Epoch 11722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:49,879 - INFO - [diffusion][Epoch 11723] Epoch 11724/12000
2024-11-05 03:46:52,970 - INFO - [diffusion][Epoch 11723] diffusion training Loss: 0.06469326186925173
2024-11-05 03:46:52,972 - INFO - [diffusion][Epoch 11723] diffusion learning rate: 0.001
2024-11-05 03:46:52,974 - INFO - [diffusion][Epoch 11723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:52,975 - INFO - [diffusion][Epoch 11724] Epoch 11725/12000
2024-11-05 03:46:56,629 - INFO - [diffusion][Epoch 11724] diffusion training Loss: 0.06708493456244469
2024-11-05 03:46:56,631 - INFO - [diffusion][Epoch 11724] diffusion learning rate: 0.001
2024-11-05 03:46:56,633 - INFO - [diffusion][Epoch 11724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:56,635 - INFO - [diffusion][Epoch 11725] Epoch 11726/12000
2024-11-05 03:47:00,042 - INFO - [diffusion][Epoch 11725] diffusion training Loss: 0.05980504583567381
2024-11-05 03:47:00,044 - INFO - [diffusion][Epoch 11725] diffusion learning rate: 0.001
2024-11-05 03:47:00,071 - INFO - [diffusion][Epoch 11725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:00,073 - INFO - [diffusion][Epoch 11726] Epoch 11727/12000
2024-11-05 03:47:03,193 - INFO - [diffusion][Epoch 11726] diffusion training Loss: 0.06393731571733952
2024-11-05 03:47:03,196 - INFO - [diffusion][Epoch 11726] diffusion learning rate: 0.001
2024-11-05 03:47:03,198 - INFO - [diffusion][Epoch 11726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:03,199 - INFO - [diffusion][Epoch 11727] Epoch 11728/12000
2024-11-05 03:47:06,525 - INFO - [diffusion][Epoch 11727] diffusion training Loss: 0.06374452915042639
2024-11-05 03:47:06,527 - INFO - [diffusion][Epoch 11727] diffusion learning rate: 0.001
2024-11-05 03:47:06,528 - INFO - [diffusion][Epoch 11727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:06,530 - INFO - [diffusion][Epoch 11728] Epoch 11729/12000
2024-11-05 03:47:09,676 - INFO - [diffusion][Epoch 11728] diffusion training Loss: 0.060500046238303185
2024-11-05 03:47:09,678 - INFO - [diffusion][Epoch 11728] diffusion learning rate: 0.001
2024-11-05 03:47:09,680 - INFO - [diffusion][Epoch 11728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:09,681 - INFO - [diffusion][Epoch 11729] Epoch 11730/12000
2024-11-05 03:47:13,265 - INFO - [diffusion][Epoch 11729] diffusion training Loss: 0.06295399740338326
2024-11-05 03:47:13,267 - INFO - [diffusion][Epoch 11729] diffusion learning rate: 0.001
2024-11-05 03:47:13,270 - INFO - [diffusion][Epoch 11729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:13,271 - INFO - [diffusion][Epoch 11730] Epoch 11731/12000
2024-11-05 03:47:16,818 - INFO - [diffusion][Epoch 11730] diffusion training Loss: 0.06058227550238371
2024-11-05 03:47:16,820 - INFO - [diffusion][Epoch 11730] diffusion learning rate: 0.001
2024-11-05 03:47:16,848 - INFO - [diffusion][Epoch 11730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:16,849 - INFO - [diffusion][Epoch 11731] Epoch 11732/12000
2024-11-05 03:47:19,995 - INFO - [diffusion][Epoch 11731] diffusion training Loss: 0.06281633675098419
2024-11-05 03:47:19,997 - INFO - [diffusion][Epoch 11731] diffusion learning rate: 0.001
2024-11-05 03:47:19,999 - INFO - [diffusion][Epoch 11731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:20,000 - INFO - [diffusion][Epoch 11732] Epoch 11733/12000
2024-11-05 03:47:23,144 - INFO - [diffusion][Epoch 11732] diffusion training Loss: 0.060995278880000114
2024-11-05 03:47:23,146 - INFO - [diffusion][Epoch 11732] diffusion learning rate: 0.001
2024-11-05 03:47:23,148 - INFO - [diffusion][Epoch 11732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:23,149 - INFO - [diffusion][Epoch 11733] Epoch 11734/12000
2024-11-05 03:47:26,247 - INFO - [diffusion][Epoch 11733] diffusion training Loss: 0.060283370316028595
2024-11-05 03:47:26,249 - INFO - [diffusion][Epoch 11733] diffusion learning rate: 0.001
2024-11-05 03:47:26,251 - INFO - [diffusion][Epoch 11733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:26,252 - INFO - [diffusion][Epoch 11734] Epoch 11735/12000
2024-11-05 03:47:29,783 - INFO - [diffusion][Epoch 11734] diffusion training Loss: 0.05932338908314705
2024-11-05 03:47:29,785 - INFO - [diffusion][Epoch 11734] diffusion learning rate: 0.001
2024-11-05 03:47:29,787 - INFO - [diffusion][Epoch 11734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:29,788 - INFO - [diffusion][Epoch 11735] Epoch 11736/12000
2024-11-05 03:47:33,448 - INFO - [diffusion][Epoch 11735] diffusion training Loss: 0.060085076838731766
2024-11-05 03:47:33,450 - INFO - [diffusion][Epoch 11735] diffusion learning rate: 0.001
2024-11-05 03:47:33,452 - INFO - [diffusion][Epoch 11735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:33,453 - INFO - [diffusion][Epoch 11736] Epoch 11737/12000
2024-11-05 03:47:36,564 - INFO - [diffusion][Epoch 11736] diffusion training Loss: 0.058928800746798515
2024-11-05 03:47:36,566 - INFO - [diffusion][Epoch 11736] diffusion learning rate: 0.001
2024-11-05 03:47:36,568 - INFO - [diffusion][Epoch 11736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:36,569 - INFO - [diffusion][Epoch 11737] Epoch 11738/12000
2024-11-05 03:47:39,661 - INFO - [diffusion][Epoch 11737] diffusion training Loss: 0.06163637712597847
2024-11-05 03:47:39,663 - INFO - [diffusion][Epoch 11737] diffusion learning rate: 0.001
2024-11-05 03:47:39,664 - INFO - [diffusion][Epoch 11737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:39,666 - INFO - [diffusion][Epoch 11738] Epoch 11739/12000
2024-11-05 03:47:42,795 - INFO - [diffusion][Epoch 11738] diffusion training Loss: 0.06401482969522476
2024-11-05 03:47:42,797 - INFO - [diffusion][Epoch 11738] diffusion learning rate: 0.001
2024-11-05 03:47:42,799 - INFO - [diffusion][Epoch 11738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:42,800 - INFO - [diffusion][Epoch 11739] Epoch 11740/12000
2024-11-05 03:47:46,386 - INFO - [diffusion][Epoch 11739] diffusion training Loss: 0.06277716625481844
2024-11-05 03:47:46,388 - INFO - [diffusion][Epoch 11739] diffusion learning rate: 0.001
2024-11-05 03:47:46,390 - INFO - [diffusion][Epoch 11739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:46,391 - INFO - [diffusion][Epoch 11740] Epoch 11741/12000
2024-11-05 03:47:49,865 - INFO - [diffusion][Epoch 11740] diffusion training Loss: 0.06071821041405201
2024-11-05 03:47:49,868 - INFO - [diffusion][Epoch 11740] diffusion learning rate: 0.001
2024-11-05 03:47:49,870 - INFO - [diffusion][Epoch 11740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:49,872 - INFO - [diffusion][Epoch 11741] Epoch 11742/12000
2024-11-05 03:47:52,956 - INFO - [diffusion][Epoch 11741] diffusion training Loss: 0.0636766692623496
2024-11-05 03:47:52,958 - INFO - [diffusion][Epoch 11741] diffusion learning rate: 0.001
2024-11-05 03:47:52,960 - INFO - [diffusion][Epoch 11741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:52,961 - INFO - [diffusion][Epoch 11742] Epoch 11743/12000
2024-11-05 03:47:56,099 - INFO - [diffusion][Epoch 11742] diffusion training Loss: 0.06338297389447689
2024-11-05 03:47:56,101 - INFO - [diffusion][Epoch 11742] diffusion learning rate: 0.001
2024-11-05 03:47:56,103 - INFO - [diffusion][Epoch 11742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:56,104 - INFO - [diffusion][Epoch 11743] Epoch 11744/12000
2024-11-05 03:47:59,297 - INFO - [diffusion][Epoch 11743] diffusion training Loss: 0.06415840424597263
2024-11-05 03:47:59,299 - INFO - [diffusion][Epoch 11743] diffusion learning rate: 0.001
2024-11-05 03:47:59,301 - INFO - [diffusion][Epoch 11743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:59,302 - INFO - [diffusion][Epoch 11744] Epoch 11745/12000
2024-11-05 03:48:02,952 - INFO - [diffusion][Epoch 11744] diffusion training Loss: 0.05948598403483629
2024-11-05 03:48:02,954 - INFO - [diffusion][Epoch 11744] diffusion learning rate: 0.001
2024-11-05 03:48:02,956 - INFO - [diffusion][Epoch 11744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:02,957 - INFO - [diffusion][Epoch 11745] Epoch 11746/12000
2024-11-05 03:48:06,528 - INFO - [diffusion][Epoch 11745] diffusion training Loss: 0.06637503765523434
2024-11-05 03:48:06,529 - INFO - [diffusion][Epoch 11745] diffusion learning rate: 0.001
2024-11-05 03:48:06,531 - INFO - [diffusion][Epoch 11745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:06,533 - INFO - [diffusion][Epoch 11746] Epoch 11747/12000
2024-11-05 03:48:09,637 - INFO - [diffusion][Epoch 11746] diffusion training Loss: 0.06661670841276646
2024-11-05 03:48:09,639 - INFO - [diffusion][Epoch 11746] diffusion learning rate: 0.001
2024-11-05 03:48:09,641 - INFO - [diffusion][Epoch 11746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:09,642 - INFO - [diffusion][Epoch 11747] Epoch 11748/12000
2024-11-05 03:48:12,817 - INFO - [diffusion][Epoch 11747] diffusion training Loss: 0.0626206211745739
2024-11-05 03:48:12,819 - INFO - [diffusion][Epoch 11747] diffusion learning rate: 0.001
2024-11-05 03:48:12,820 - INFO - [diffusion][Epoch 11747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:12,821 - INFO - [diffusion][Epoch 11748] Epoch 11749/12000
2024-11-05 03:48:15,997 - INFO - [diffusion][Epoch 11748] diffusion training Loss: 0.06217576749622822
2024-11-05 03:48:15,999 - INFO - [diffusion][Epoch 11748] diffusion learning rate: 0.001
2024-11-05 03:48:16,020 - INFO - [diffusion][Epoch 11748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:16,021 - INFO - [diffusion][Epoch 11749] Epoch 11750/12000
2024-11-05 03:48:19,881 - INFO - [diffusion][Epoch 11749] diffusion training Loss: 0.06422291696071625
2024-11-05 03:48:19,883 - INFO - [diffusion][Epoch 11749] diffusion learning rate: 0.001
2024-11-05 03:48:19,885 - INFO - [diffusion][Epoch 11749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:19,886 - INFO - [diffusion][Epoch 11750] Epoch 11751/12000
2024-11-05 03:48:23,556 - INFO - [diffusion][Epoch 11750] diffusion training Loss: 0.0645260876044631
2024-11-05 03:48:23,558 - INFO - [diffusion][Epoch 11750] diffusion learning rate: 0.001
2024-11-05 03:48:23,560 - INFO - [diffusion][Epoch 11750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:23,561 - INFO - [diffusion][Epoch 11751] Epoch 11752/12000
2024-11-05 03:48:26,954 - INFO - [diffusion][Epoch 11751] diffusion training Loss: 0.06206066533923149
2024-11-05 03:48:26,956 - INFO - [diffusion][Epoch 11751] diffusion learning rate: 0.001
2024-11-05 03:48:27,005 - INFO - [diffusion][Epoch 11751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:27,006 - INFO - [diffusion][Epoch 11752] Epoch 11753/12000
2024-11-05 03:48:30,212 - INFO - [diffusion][Epoch 11752] diffusion training Loss: 0.05931172613054514
2024-11-05 03:48:30,214 - INFO - [diffusion][Epoch 11752] diffusion learning rate: 0.001
2024-11-05 03:48:30,215 - INFO - [diffusion][Epoch 11752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:30,216 - INFO - [diffusion][Epoch 11753] Epoch 11754/12000
2024-11-05 03:48:33,315 - INFO - [diffusion][Epoch 11753] diffusion training Loss: 0.06384504120796919
2024-11-05 03:48:33,317 - INFO - [diffusion][Epoch 11753] diffusion learning rate: 0.001
2024-11-05 03:48:33,319 - INFO - [diffusion][Epoch 11753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:33,321 - INFO - [diffusion][Epoch 11754] Epoch 11755/12000
2024-11-05 03:48:36,626 - INFO - [diffusion][Epoch 11754] diffusion training Loss: 0.06224720925092697
2024-11-05 03:48:36,628 - INFO - [diffusion][Epoch 11754] diffusion learning rate: 0.001
2024-11-05 03:48:36,630 - INFO - [diffusion][Epoch 11754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:36,631 - INFO - [diffusion][Epoch 11755] Epoch 11756/12000
2024-11-05 03:48:40,346 - INFO - [diffusion][Epoch 11755] diffusion training Loss: 0.060957811772823334
2024-11-05 03:48:40,348 - INFO - [diffusion][Epoch 11755] diffusion learning rate: 0.001
2024-11-05 03:48:40,349 - INFO - [diffusion][Epoch 11755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:40,351 - INFO - [diffusion][Epoch 11756] Epoch 11757/12000
2024-11-05 03:48:43,761 - INFO - [diffusion][Epoch 11756] diffusion training Loss: 0.061907874420285225
2024-11-05 03:48:43,763 - INFO - [diffusion][Epoch 11756] diffusion learning rate: 0.001
2024-11-05 03:48:43,765 - INFO - [diffusion][Epoch 11756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:43,766 - INFO - [diffusion][Epoch 11757] Epoch 11758/12000
2024-11-05 03:48:46,945 - INFO - [diffusion][Epoch 11757] diffusion training Loss: 0.06143338046967983
2024-11-05 03:48:46,947 - INFO - [diffusion][Epoch 11757] diffusion learning rate: 0.001
2024-11-05 03:48:46,949 - INFO - [diffusion][Epoch 11757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:46,950 - INFO - [diffusion][Epoch 11758] Epoch 11759/12000
2024-11-05 03:48:50,220 - INFO - [diffusion][Epoch 11758] diffusion training Loss: 0.0601676981896162
2024-11-05 03:48:50,222 - INFO - [diffusion][Epoch 11758] diffusion learning rate: 0.001
2024-11-05 03:48:50,224 - INFO - [diffusion][Epoch 11758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:50,225 - INFO - [diffusion][Epoch 11759] Epoch 11760/12000
2024-11-05 03:48:53,319 - INFO - [diffusion][Epoch 11759] diffusion training Loss: 0.06236819736659527
2024-11-05 03:48:53,321 - INFO - [diffusion][Epoch 11759] diffusion learning rate: 0.001
2024-11-05 03:48:53,323 - INFO - [diffusion][Epoch 11759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:53,324 - INFO - [diffusion][Epoch 11760] Epoch 11761/12000
2024-11-05 03:48:56,899 - INFO - [diffusion][Epoch 11760] diffusion training Loss: 0.06565229501575232
2024-11-05 03:48:56,901 - INFO - [diffusion][Epoch 11760] diffusion learning rate: 0.001
2024-11-05 03:48:56,903 - INFO - [diffusion][Epoch 11760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:56,904 - INFO - [diffusion][Epoch 11761] Epoch 11762/12000
2024-11-05 03:49:00,044 - INFO - [diffusion][Epoch 11761] diffusion training Loss: 0.06320252269506454
2024-11-05 03:49:00,045 - INFO - [diffusion][Epoch 11761] diffusion learning rate: 0.001
2024-11-05 03:49:00,047 - INFO - [diffusion][Epoch 11761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:00,048 - INFO - [diffusion][Epoch 11762] Epoch 11763/12000
2024-11-05 03:49:03,147 - INFO - [diffusion][Epoch 11762] diffusion training Loss: 0.06285702623426914
2024-11-05 03:49:03,149 - INFO - [diffusion][Epoch 11762] diffusion learning rate: 0.001
2024-11-05 03:49:03,151 - INFO - [diffusion][Epoch 11762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:03,152 - INFO - [diffusion][Epoch 11763] Epoch 11764/12000
2024-11-05 03:49:06,361 - INFO - [diffusion][Epoch 11763] diffusion training Loss: 0.06411841884255409
2024-11-05 03:49:06,363 - INFO - [diffusion][Epoch 11763] diffusion learning rate: 0.001
2024-11-05 03:49:06,365 - INFO - [diffusion][Epoch 11763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:06,366 - INFO - [diffusion][Epoch 11764] Epoch 11765/12000
2024-11-05 03:49:09,832 - INFO - [diffusion][Epoch 11764] diffusion training Loss: 0.06098487973213196
2024-11-05 03:49:09,834 - INFO - [diffusion][Epoch 11764] diffusion learning rate: 0.001
2024-11-05 03:49:09,882 - INFO - [diffusion][Epoch 11764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:09,883 - INFO - [diffusion][Epoch 11765] Epoch 11766/12000
2024-11-05 03:49:13,291 - INFO - [diffusion][Epoch 11765] diffusion training Loss: 0.058734603226184845
2024-11-05 03:49:13,293 - INFO - [diffusion][Epoch 11765] diffusion learning rate: 0.001
2024-11-05 03:49:13,295 - INFO - [diffusion][Epoch 11765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:13,297 - INFO - [diffusion][Epoch 11766] Epoch 11767/12000
2024-11-05 03:49:16,395 - INFO - [diffusion][Epoch 11766] diffusion training Loss: 0.06354398187249899
2024-11-05 03:49:16,397 - INFO - [diffusion][Epoch 11766] diffusion learning rate: 0.001
2024-11-05 03:49:16,399 - INFO - [diffusion][Epoch 11766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:16,400 - INFO - [diffusion][Epoch 11767] Epoch 11768/12000
2024-11-05 03:49:19,550 - INFO - [diffusion][Epoch 11767] diffusion training Loss: 0.06120604928582907
2024-11-05 03:49:19,552 - INFO - [diffusion][Epoch 11767] diffusion learning rate: 0.001
2024-11-05 03:49:19,553 - INFO - [diffusion][Epoch 11767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:19,555 - INFO - [diffusion][Epoch 11768] Epoch 11769/12000
2024-11-05 03:49:23,040 - INFO - [diffusion][Epoch 11768] diffusion training Loss: 0.06268478743731976
2024-11-05 03:49:23,042 - INFO - [diffusion][Epoch 11768] diffusion learning rate: 0.001
2024-11-05 03:49:23,044 - INFO - [diffusion][Epoch 11768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:23,045 - INFO - [diffusion][Epoch 11769] Epoch 11770/12000
2024-11-05 03:49:26,677 - INFO - [diffusion][Epoch 11769] diffusion training Loss: 0.06420294288545847
2024-11-05 03:49:26,679 - INFO - [diffusion][Epoch 11769] diffusion learning rate: 0.001
2024-11-05 03:49:26,681 - INFO - [diffusion][Epoch 11769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:26,682 - INFO - [diffusion][Epoch 11770] Epoch 11771/12000
2024-11-05 03:49:30,114 - INFO - [diffusion][Epoch 11770] diffusion training Loss: 0.0670052133500576
2024-11-05 03:49:30,116 - INFO - [diffusion][Epoch 11770] diffusion learning rate: 0.001
2024-11-05 03:49:30,118 - INFO - [diffusion][Epoch 11770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:30,119 - INFO - [diffusion][Epoch 11771] Epoch 11772/12000
2024-11-05 03:49:33,328 - INFO - [diffusion][Epoch 11771] diffusion training Loss: 0.06204795744270086
2024-11-05 03:49:33,330 - INFO - [diffusion][Epoch 11771] diffusion learning rate: 0.001
2024-11-05 03:49:33,357 - INFO - [diffusion][Epoch 11771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:33,359 - INFO - [diffusion][Epoch 11772] Epoch 11773/12000
2024-11-05 03:49:36,503 - INFO - [diffusion][Epoch 11772] diffusion training Loss: 0.06356952805072069
2024-11-05 03:49:36,505 - INFO - [diffusion][Epoch 11772] diffusion learning rate: 0.001
2024-11-05 03:49:36,507 - INFO - [diffusion][Epoch 11772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:36,508 - INFO - [diffusion][Epoch 11773] Epoch 11774/12000
2024-11-05 03:49:39,768 - INFO - [diffusion][Epoch 11773] diffusion training Loss: 0.07095859386026859
2024-11-05 03:49:39,770 - INFO - [diffusion][Epoch 11773] diffusion learning rate: 0.001
2024-11-05 03:49:39,771 - INFO - [diffusion][Epoch 11773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:39,773 - INFO - [diffusion][Epoch 11774] Epoch 11775/12000
2024-11-05 03:49:43,487 - INFO - [diffusion][Epoch 11774] diffusion training Loss: 0.0651197973638773
2024-11-05 03:49:43,489 - INFO - [diffusion][Epoch 11774] diffusion learning rate: 0.001
2024-11-05 03:49:43,491 - INFO - [diffusion][Epoch 11774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:43,492 - INFO - [diffusion][Epoch 11775] Epoch 11776/12000
2024-11-05 03:49:47,038 - INFO - [diffusion][Epoch 11775] diffusion training Loss: 0.06351958028972149
2024-11-05 03:49:47,039 - INFO - [diffusion][Epoch 11775] diffusion learning rate: 0.001
2024-11-05 03:49:47,041 - INFO - [diffusion][Epoch 11775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:47,043 - INFO - [diffusion][Epoch 11776] Epoch 11777/12000
2024-11-05 03:49:50,086 - INFO - [diffusion][Epoch 11776] diffusion training Loss: 0.06348517071455717
2024-11-05 03:49:50,088 - INFO - [diffusion][Epoch 11776] diffusion learning rate: 0.001
2024-11-05 03:49:50,090 - INFO - [diffusion][Epoch 11776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:50,091 - INFO - [diffusion][Epoch 11777] Epoch 11778/12000
2024-11-05 03:49:53,195 - INFO - [diffusion][Epoch 11777] diffusion training Loss: 0.06198515743017197
2024-11-05 03:49:53,198 - INFO - [diffusion][Epoch 11777] diffusion learning rate: 0.001
2024-11-05 03:49:53,200 - INFO - [diffusion][Epoch 11777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:53,201 - INFO - [diffusion][Epoch 11778] Epoch 11779/12000
2024-11-05 03:49:56,436 - INFO - [diffusion][Epoch 11778] diffusion training Loss: 0.05881439056247473
2024-11-05 03:49:56,437 - INFO - [diffusion][Epoch 11778] diffusion learning rate: 0.001
2024-11-05 03:49:56,439 - INFO - [diffusion][Epoch 11778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:56,440 - INFO - [diffusion][Epoch 11779] Epoch 11780/12000
2024-11-05 03:50:00,048 - INFO - [diffusion][Epoch 11779] diffusion training Loss: 0.060067787766456604
2024-11-05 03:50:00,050 - INFO - [diffusion][Epoch 11779] diffusion learning rate: 0.001
2024-11-05 03:50:00,051 - INFO - [diffusion][Epoch 11779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:00,053 - INFO - [diffusion][Epoch 11780] Epoch 11781/12000
2024-11-05 03:50:03,625 - INFO - [diffusion][Epoch 11780] diffusion training Loss: 0.06429935060441494
2024-11-05 03:50:03,627 - INFO - [diffusion][Epoch 11780] diffusion learning rate: 0.001
2024-11-05 03:50:03,629 - INFO - [diffusion][Epoch 11780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:03,630 - INFO - [diffusion][Epoch 11781] Epoch 11782/12000
2024-11-05 03:50:06,734 - INFO - [diffusion][Epoch 11781] diffusion training Loss: 0.06274039950221777
2024-11-05 03:50:06,737 - INFO - [diffusion][Epoch 11781] diffusion learning rate: 0.001
2024-11-05 03:50:06,738 - INFO - [diffusion][Epoch 11781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:06,740 - INFO - [diffusion][Epoch 11782] Epoch 11783/12000
2024-11-05 03:50:09,842 - INFO - [diffusion][Epoch 11782] diffusion training Loss: 0.06385158374905586
2024-11-05 03:50:09,843 - INFO - [diffusion][Epoch 11782] diffusion learning rate: 0.001
2024-11-05 03:50:09,845 - INFO - [diffusion][Epoch 11782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:09,846 - INFO - [diffusion][Epoch 11783] Epoch 11784/12000
2024-11-05 03:50:12,949 - INFO - [diffusion][Epoch 11783] diffusion training Loss: 0.06634713802486658
2024-11-05 03:50:12,951 - INFO - [diffusion][Epoch 11783] diffusion learning rate: 0.001
2024-11-05 03:50:12,952 - INFO - [diffusion][Epoch 11783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:12,953 - INFO - [diffusion][Epoch 11784] Epoch 11785/12000
2024-11-05 03:50:16,512 - INFO - [diffusion][Epoch 11784] diffusion training Loss: 0.06612194515764713
2024-11-05 03:50:16,514 - INFO - [diffusion][Epoch 11784] diffusion learning rate: 0.001
2024-11-05 03:50:16,516 - INFO - [diffusion][Epoch 11784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:16,517 - INFO - [diffusion][Epoch 11785] Epoch 11786/12000
2024-11-05 03:50:20,021 - INFO - [diffusion][Epoch 11785] diffusion training Loss: 0.06445310078561306
2024-11-05 03:50:20,024 - INFO - [diffusion][Epoch 11785] diffusion learning rate: 0.001
2024-11-05 03:50:20,026 - INFO - [diffusion][Epoch 11785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:20,027 - INFO - [diffusion][Epoch 11786] Epoch 11787/12000
2024-11-05 03:50:23,088 - INFO - [diffusion][Epoch 11786] diffusion training Loss: 0.0643375376239419
2024-11-05 03:50:23,090 - INFO - [diffusion][Epoch 11786] diffusion learning rate: 0.001
2024-11-05 03:50:23,092 - INFO - [diffusion][Epoch 11786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:23,093 - INFO - [diffusion][Epoch 11787] Epoch 11788/12000
2024-11-05 03:50:26,208 - INFO - [diffusion][Epoch 11787] diffusion training Loss: 0.056887198239564896
2024-11-05 03:50:26,212 - INFO - [diffusion][Epoch 11787] diffusion learning rate: 0.001
2024-11-05 03:50:26,214 - INFO - [diffusion][Epoch 11787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:26,216 - INFO - [diffusion][Epoch 11788] Epoch 11789/12000
2024-11-05 03:50:29,520 - INFO - [diffusion][Epoch 11788] diffusion training Loss: 0.05713063292205334
2024-11-05 03:50:29,522 - INFO - [diffusion][Epoch 11788] diffusion learning rate: 0.001
2024-11-05 03:50:29,524 - INFO - [diffusion][Epoch 11788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:29,525 - INFO - [diffusion][Epoch 11789] Epoch 11790/12000
2024-11-05 03:50:33,064 - INFO - [diffusion][Epoch 11789] diffusion training Loss: 0.06258914805948734
2024-11-05 03:50:33,066 - INFO - [diffusion][Epoch 11789] diffusion learning rate: 0.001
2024-11-05 03:50:33,068 - INFO - [diffusion][Epoch 11789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:33,069 - INFO - [diffusion][Epoch 11790] Epoch 11791/12000
2024-11-05 03:50:37,249 - INFO - [diffusion][Epoch 11790] diffusion training Loss: 0.061030879616737366
2024-11-05 03:50:37,252 - INFO - [diffusion][Epoch 11790] diffusion learning rate: 0.001
2024-11-05 03:50:37,253 - INFO - [diffusion][Epoch 11790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:37,254 - INFO - [diffusion][Epoch 11791] Epoch 11792/12000
2024-11-05 03:50:40,497 - INFO - [diffusion][Epoch 11791] diffusion training Loss: 0.06388247944414616
2024-11-05 03:50:40,499 - INFO - [diffusion][Epoch 11791] diffusion learning rate: 0.001
2024-11-05 03:50:40,500 - INFO - [diffusion][Epoch 11791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:40,502 - INFO - [diffusion][Epoch 11792] Epoch 11793/12000
2024-11-05 03:50:43,603 - INFO - [diffusion][Epoch 11792] diffusion training Loss: 0.0616550762206316
2024-11-05 03:50:43,605 - INFO - [diffusion][Epoch 11792] diffusion learning rate: 0.001
2024-11-05 03:50:43,607 - INFO - [diffusion][Epoch 11792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:43,608 - INFO - [diffusion][Epoch 11793] Epoch 11794/12000
2024-11-05 03:50:46,701 - INFO - [diffusion][Epoch 11793] diffusion training Loss: 0.07068081013858318
2024-11-05 03:50:46,703 - INFO - [diffusion][Epoch 11793] diffusion learning rate: 0.001
2024-11-05 03:50:46,705 - INFO - [diffusion][Epoch 11793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:46,706 - INFO - [diffusion][Epoch 11794] Epoch 11795/12000
2024-11-05 03:50:50,250 - INFO - [diffusion][Epoch 11794] diffusion training Loss: 0.05836609285324812
2024-11-05 03:50:50,252 - INFO - [diffusion][Epoch 11794] diffusion learning rate: 0.001
2024-11-05 03:50:50,253 - INFO - [diffusion][Epoch 11794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:50,254 - INFO - [diffusion][Epoch 11795] Epoch 11796/12000
2024-11-05 03:50:53,662 - INFO - [diffusion][Epoch 11795] diffusion training Loss: 0.0625074952840805
2024-11-05 03:50:53,665 - INFO - [diffusion][Epoch 11795] diffusion learning rate: 0.001
2024-11-05 03:50:53,667 - INFO - [diffusion][Epoch 11795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:53,669 - INFO - [diffusion][Epoch 11796] Epoch 11797/12000
2024-11-05 03:50:56,670 - INFO - [diffusion][Epoch 11796] diffusion training Loss: 0.06306518893688917
2024-11-05 03:50:56,672 - INFO - [diffusion][Epoch 11796] diffusion learning rate: 0.001
2024-11-05 03:50:56,674 - INFO - [diffusion][Epoch 11796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:56,675 - INFO - [diffusion][Epoch 11797] Epoch 11798/12000
2024-11-05 03:50:59,915 - INFO - [diffusion][Epoch 11797] diffusion training Loss: 0.06530050560832024
2024-11-05 03:50:59,917 - INFO - [diffusion][Epoch 11797] diffusion learning rate: 0.001
2024-11-05 03:50:59,919 - INFO - [diffusion][Epoch 11797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:59,920 - INFO - [diffusion][Epoch 11798] Epoch 11799/12000
2024-11-05 03:51:03,044 - INFO - [diffusion][Epoch 11798] diffusion training Loss: 0.059315307065844536
2024-11-05 03:51:03,047 - INFO - [diffusion][Epoch 11798] diffusion learning rate: 0.001
2024-11-05 03:51:03,049 - INFO - [diffusion][Epoch 11798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:03,050 - INFO - [diffusion][Epoch 11799] Epoch 11800/12000
2024-11-05 03:51:06,609 - INFO - [diffusion][Epoch 11799] diffusion training Loss: 0.06228053290396929
2024-11-05 03:51:06,611 - INFO - [diffusion][Epoch 11799] diffusion learning rate: 0.001
2024-11-05 03:51:06,613 - INFO - [diffusion][Epoch 11799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:06,614 - INFO - [diffusion][Epoch 11800] Epoch 11801/12000
2024-11-05 03:51:10,179 - INFO - [diffusion][Epoch 11800] diffusion training Loss: 0.06210517231374979
2024-11-05 03:51:10,181 - INFO - [diffusion][Epoch 11800] diffusion learning rate: 0.001
2024-11-05 03:51:10,183 - INFO - [diffusion][Epoch 11800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:10,184 - INFO - [diffusion][Epoch 11801] Epoch 11802/12000
2024-11-05 03:51:13,279 - INFO - [diffusion][Epoch 11801] diffusion training Loss: 0.06300394423305988
2024-11-05 03:51:13,281 - INFO - [diffusion][Epoch 11801] diffusion learning rate: 0.001
2024-11-05 03:51:13,283 - INFO - [diffusion][Epoch 11801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:13,284 - INFO - [diffusion][Epoch 11802] Epoch 11803/12000
2024-11-05 03:51:16,419 - INFO - [diffusion][Epoch 11802] diffusion training Loss: 0.06161941681057215
2024-11-05 03:51:16,421 - INFO - [diffusion][Epoch 11802] diffusion learning rate: 0.001
2024-11-05 03:51:16,423 - INFO - [diffusion][Epoch 11802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:16,424 - INFO - [diffusion][Epoch 11803] Epoch 11804/12000
2024-11-05 03:51:19,576 - INFO - [diffusion][Epoch 11803] diffusion training Loss: 0.061706594191491604
2024-11-05 03:51:19,578 - INFO - [diffusion][Epoch 11803] diffusion learning rate: 0.001
2024-11-05 03:51:19,580 - INFO - [diffusion][Epoch 11803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:19,581 - INFO - [diffusion][Epoch 11804] Epoch 11805/12000
2024-11-05 03:51:23,118 - INFO - [diffusion][Epoch 11804] diffusion training Loss: 0.06541173346340656
2024-11-05 03:51:23,120 - INFO - [diffusion][Epoch 11804] diffusion learning rate: 0.001
2024-11-05 03:51:23,121 - INFO - [diffusion][Epoch 11804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:23,123 - INFO - [diffusion][Epoch 11805] Epoch 11806/12000
2024-11-05 03:51:26,653 - INFO - [diffusion][Epoch 11805] diffusion training Loss: 0.060393569990992546
2024-11-05 03:51:26,656 - INFO - [diffusion][Epoch 11805] diffusion learning rate: 0.001
2024-11-05 03:51:26,658 - INFO - [diffusion][Epoch 11805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:26,659 - INFO - [diffusion][Epoch 11806] Epoch 11807/12000
2024-11-05 03:51:29,768 - INFO - [diffusion][Epoch 11806] diffusion training Loss: 0.064553989097476
2024-11-05 03:51:29,770 - INFO - [diffusion][Epoch 11806] diffusion learning rate: 0.001
2024-11-05 03:51:29,792 - INFO - [diffusion][Epoch 11806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:29,793 - INFO - [diffusion][Epoch 11807] Epoch 11808/12000
2024-11-05 03:51:32,898 - INFO - [diffusion][Epoch 11807] diffusion training Loss: 0.07240281347185373
2024-11-05 03:51:32,900 - INFO - [diffusion][Epoch 11807] diffusion learning rate: 0.001
2024-11-05 03:51:32,902 - INFO - [diffusion][Epoch 11807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:32,904 - INFO - [diffusion][Epoch 11808] Epoch 11809/12000
2024-11-05 03:51:36,067 - INFO - [diffusion][Epoch 11808] diffusion training Loss: 0.06526147574186325
2024-11-05 03:51:36,070 - INFO - [diffusion][Epoch 11808] diffusion learning rate: 0.001
2024-11-05 03:51:36,073 - INFO - [diffusion][Epoch 11808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:36,075 - INFO - [diffusion][Epoch 11809] Epoch 11810/12000
2024-11-05 03:51:39,711 - INFO - [diffusion][Epoch 11809] diffusion training Loss: 0.06403647921979427
2024-11-05 03:51:39,713 - INFO - [diffusion][Epoch 11809] diffusion learning rate: 0.001
2024-11-05 03:51:39,715 - INFO - [diffusion][Epoch 11809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:39,716 - INFO - [diffusion][Epoch 11810] Epoch 11811/12000
2024-11-05 03:51:42,906 - INFO - [diffusion][Epoch 11810] diffusion training Loss: 0.06143669690936804
2024-11-05 03:51:42,908 - INFO - [diffusion][Epoch 11810] diffusion learning rate: 0.001
2024-11-05 03:51:42,910 - INFO - [diffusion][Epoch 11810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:42,911 - INFO - [diffusion][Epoch 11811] Epoch 11812/12000
2024-11-05 03:51:46,632 - INFO - [diffusion][Epoch 11811] diffusion training Loss: 0.06131396908313036
2024-11-05 03:51:46,634 - INFO - [diffusion][Epoch 11811] diffusion learning rate: 0.001
2024-11-05 03:51:46,653 - INFO - [diffusion][Epoch 11811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:46,654 - INFO - [diffusion][Epoch 11812] Epoch 11813/12000
2024-11-05 03:51:49,715 - INFO - [diffusion][Epoch 11812] diffusion training Loss: 0.06193509045988321
2024-11-05 03:51:49,717 - INFO - [diffusion][Epoch 11812] diffusion learning rate: 0.001
2024-11-05 03:51:49,719 - INFO - [diffusion][Epoch 11812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:49,721 - INFO - [diffusion][Epoch 11813] Epoch 11814/12000
2024-11-05 03:51:52,883 - INFO - [diffusion][Epoch 11813] diffusion training Loss: 0.05910153593868017
2024-11-05 03:51:52,885 - INFO - [diffusion][Epoch 11813] diffusion learning rate: 0.001
2024-11-05 03:51:52,887 - INFO - [diffusion][Epoch 11813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:52,888 - INFO - [diffusion][Epoch 11814] Epoch 11815/12000
2024-11-05 03:51:56,427 - INFO - [diffusion][Epoch 11814] diffusion training Loss: 0.06358932331204414
2024-11-05 03:51:56,430 - INFO - [diffusion][Epoch 11814] diffusion learning rate: 0.001
2024-11-05 03:51:56,431 - INFO - [diffusion][Epoch 11814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:56,433 - INFO - [diffusion][Epoch 11815] Epoch 11816/12000
2024-11-05 03:51:59,965 - INFO - [diffusion][Epoch 11815] diffusion training Loss: 0.05557074025273323
2024-11-05 03:51:59,967 - INFO - [diffusion][Epoch 11815] diffusion learning rate: 0.001
2024-11-05 03:51:59,969 - INFO - [diffusion][Epoch 11815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:59,971 - INFO - [diffusion][Epoch 11816] Epoch 11817/12000
2024-11-05 03:52:02,659 - INFO - [diffusion][Epoch 11816] diffusion training Loss: 0.06008004862815142
2024-11-05 03:52:02,663 - INFO - [diffusion][Epoch 11816] diffusion learning rate: 0.001
2024-11-05 03:52:02,695 - INFO - [diffusion][Epoch 11816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:02,696 - INFO - [diffusion][Epoch 11817] Epoch 11818/12000
2024-11-05 03:52:05,823 - INFO - [diffusion][Epoch 11817] diffusion training Loss: 0.06265514343976974
2024-11-05 03:52:05,826 - INFO - [diffusion][Epoch 11817] diffusion learning rate: 0.001
2024-11-05 03:52:05,828 - INFO - [diffusion][Epoch 11817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:05,829 - INFO - [diffusion][Epoch 11818] Epoch 11819/12000
2024-11-05 03:52:09,305 - INFO - [diffusion][Epoch 11818] diffusion training Loss: 0.06499725673347712
2024-11-05 03:52:09,308 - INFO - [diffusion][Epoch 11818] diffusion learning rate: 0.001
2024-11-05 03:52:09,310 - INFO - [diffusion][Epoch 11818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:09,311 - INFO - [diffusion][Epoch 11819] Epoch 11820/12000
2024-11-05 03:52:12,817 - INFO - [diffusion][Epoch 11819] diffusion training Loss: 0.05963990278542042
2024-11-05 03:52:12,818 - INFO - [diffusion][Epoch 11819] diffusion learning rate: 0.001
2024-11-05 03:52:12,820 - INFO - [diffusion][Epoch 11819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:12,821 - INFO - [diffusion][Epoch 11820] Epoch 11821/12000
2024-11-05 03:52:15,874 - INFO - [diffusion][Epoch 11820] diffusion training Loss: 0.05850087944418192
2024-11-05 03:52:15,875 - INFO - [diffusion][Epoch 11820] diffusion learning rate: 0.001
2024-11-05 03:52:15,877 - INFO - [diffusion][Epoch 11820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:15,878 - INFO - [diffusion][Epoch 11821] Epoch 11822/12000
2024-11-05 03:52:19,066 - INFO - [diffusion][Epoch 11821] diffusion training Loss: 0.0730274673551321
2024-11-05 03:52:19,068 - INFO - [diffusion][Epoch 11821] diffusion learning rate: 0.001
2024-11-05 03:52:19,070 - INFO - [diffusion][Epoch 11821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:19,072 - INFO - [diffusion][Epoch 11822] Epoch 11823/12000
2024-11-05 03:52:22,450 - INFO - [diffusion][Epoch 11822] diffusion training Loss: 0.06401067785918713
2024-11-05 03:52:22,451 - INFO - [diffusion][Epoch 11822] diffusion learning rate: 0.001
2024-11-05 03:52:22,453 - INFO - [diffusion][Epoch 11822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:22,454 - INFO - [diffusion][Epoch 11823] Epoch 11824/12000
2024-11-05 03:52:26,097 - INFO - [diffusion][Epoch 11823] diffusion training Loss: 0.06266515981405973
2024-11-05 03:52:26,099 - INFO - [diffusion][Epoch 11823] diffusion learning rate: 0.001
2024-11-05 03:52:26,101 - INFO - [diffusion][Epoch 11823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:26,102 - INFO - [diffusion][Epoch 11824] Epoch 11825/12000
2024-11-05 03:52:29,380 - INFO - [diffusion][Epoch 11824] diffusion training Loss: 0.06176028121262789
2024-11-05 03:52:29,383 - INFO - [diffusion][Epoch 11824] diffusion learning rate: 0.001
2024-11-05 03:52:29,385 - INFO - [diffusion][Epoch 11824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:29,386 - INFO - [diffusion][Epoch 11825] Epoch 11826/12000
2024-11-05 03:52:32,504 - INFO - [diffusion][Epoch 11825] diffusion training Loss: 0.059131041169166565
2024-11-05 03:52:32,506 - INFO - [diffusion][Epoch 11825] diffusion learning rate: 0.001
2024-11-05 03:52:32,508 - INFO - [diffusion][Epoch 11825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:32,509 - INFO - [diffusion][Epoch 11826] Epoch 11827/12000
2024-11-05 03:52:35,585 - INFO - [diffusion][Epoch 11826] diffusion training Loss: 0.06197260692715645
2024-11-05 03:52:35,587 - INFO - [diffusion][Epoch 11826] diffusion learning rate: 0.001
2024-11-05 03:52:35,589 - INFO - [diffusion][Epoch 11826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:35,590 - INFO - [diffusion][Epoch 11827] Epoch 11828/12000
2024-11-05 03:52:39,108 - INFO - [diffusion][Epoch 11827] diffusion training Loss: 0.060924576595425606
2024-11-05 03:52:39,110 - INFO - [diffusion][Epoch 11827] diffusion learning rate: 0.001
2024-11-05 03:52:39,112 - INFO - [diffusion][Epoch 11827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:39,113 - INFO - [diffusion][Epoch 11828] Epoch 11829/12000
2024-11-05 03:52:42,707 - INFO - [diffusion][Epoch 11828] diffusion training Loss: 0.0645090863108635
2024-11-05 03:52:42,710 - INFO - [diffusion][Epoch 11828] diffusion learning rate: 0.001
2024-11-05 03:52:42,712 - INFO - [diffusion][Epoch 11828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:42,713 - INFO - [diffusion][Epoch 11829] Epoch 11830/12000
2024-11-05 03:52:45,817 - INFO - [diffusion][Epoch 11829] diffusion training Loss: 0.0587124265730381
2024-11-05 03:52:45,819 - INFO - [diffusion][Epoch 11829] diffusion learning rate: 0.001
2024-11-05 03:52:45,821 - INFO - [diffusion][Epoch 11829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:45,822 - INFO - [diffusion][Epoch 11830] Epoch 11831/12000
2024-11-05 03:52:48,943 - INFO - [diffusion][Epoch 11830] diffusion training Loss: 0.066478680819273
2024-11-05 03:52:48,945 - INFO - [diffusion][Epoch 11830] diffusion learning rate: 0.001
2024-11-05 03:52:48,947 - INFO - [diffusion][Epoch 11830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:48,948 - INFO - [diffusion][Epoch 11831] Epoch 11832/12000
2024-11-05 03:52:52,068 - INFO - [diffusion][Epoch 11831] diffusion training Loss: 0.062113494612276554
2024-11-05 03:52:52,070 - INFO - [diffusion][Epoch 11831] diffusion learning rate: 0.001
2024-11-05 03:52:52,101 - INFO - [diffusion][Epoch 11831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:52,103 - INFO - [diffusion][Epoch 11832] Epoch 11833/12000
2024-11-05 03:52:55,646 - INFO - [diffusion][Epoch 11832] diffusion training Loss: 0.061836544424295425
2024-11-05 03:52:55,648 - INFO - [diffusion][Epoch 11832] diffusion learning rate: 0.001
2024-11-05 03:52:55,650 - INFO - [diffusion][Epoch 11832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:55,651 - INFO - [diffusion][Epoch 11833] Epoch 11834/12000
2024-11-05 03:52:59,144 - INFO - [diffusion][Epoch 11833] diffusion training Loss: 0.06048865057528019
2024-11-05 03:52:59,146 - INFO - [diffusion][Epoch 11833] diffusion learning rate: 0.001
2024-11-05 03:52:59,148 - INFO - [diffusion][Epoch 11833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:59,149 - INFO - [diffusion][Epoch 11834] Epoch 11835/12000
2024-11-05 03:53:02,219 - INFO - [diffusion][Epoch 11834] diffusion training Loss: 0.05934849940240383
2024-11-05 03:53:02,258 - INFO - [diffusion][Epoch 11834] diffusion learning rate: 0.001
2024-11-05 03:53:02,260 - INFO - [diffusion][Epoch 11834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:02,261 - INFO - [diffusion][Epoch 11835] Epoch 11836/12000
2024-11-05 03:53:05,323 - INFO - [diffusion][Epoch 11835] diffusion training Loss: 0.06636766716837883
2024-11-05 03:53:05,325 - INFO - [diffusion][Epoch 11835] diffusion learning rate: 0.001
2024-11-05 03:53:05,326 - INFO - [diffusion][Epoch 11835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:05,328 - INFO - [diffusion][Epoch 11836] Epoch 11837/12000
2024-11-05 03:53:08,736 - INFO - [diffusion][Epoch 11836] diffusion training Loss: 0.06432941742241383
2024-11-05 03:53:08,738 - INFO - [diffusion][Epoch 11836] diffusion learning rate: 0.001
2024-11-05 03:53:08,740 - INFO - [diffusion][Epoch 11836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:08,741 - INFO - [diffusion][Epoch 11837] Epoch 11838/12000
2024-11-05 03:53:12,238 - INFO - [diffusion][Epoch 11837] diffusion training Loss: 0.06498714908957481
2024-11-05 03:53:12,240 - INFO - [diffusion][Epoch 11837] diffusion learning rate: 0.001
2024-11-05 03:53:12,241 - INFO - [diffusion][Epoch 11837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:12,243 - INFO - [diffusion][Epoch 11838] Epoch 11839/12000
2024-11-05 03:53:15,077 - INFO - [diffusion][Epoch 11838] diffusion training Loss: 0.06406868435442448
2024-11-05 03:53:15,079 - INFO - [diffusion][Epoch 11838] diffusion learning rate: 0.001
2024-11-05 03:53:15,080 - INFO - [diffusion][Epoch 11838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:15,082 - INFO - [diffusion][Epoch 11839] Epoch 11840/12000
2024-11-05 03:53:18,068 - INFO - [diffusion][Epoch 11839] diffusion training Loss: 0.06371051352471113
2024-11-05 03:53:18,070 - INFO - [diffusion][Epoch 11839] diffusion learning rate: 0.001
2024-11-05 03:53:18,071 - INFO - [diffusion][Epoch 11839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:18,073 - INFO - [diffusion][Epoch 11840] Epoch 11841/12000
2024-11-05 03:53:21,523 - INFO - [diffusion][Epoch 11840] diffusion training Loss: 0.06661040335893631
2024-11-05 03:53:21,525 - INFO - [diffusion][Epoch 11840] diffusion learning rate: 0.001
2024-11-05 03:53:21,527 - INFO - [diffusion][Epoch 11840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:21,528 - INFO - [diffusion][Epoch 11841] Epoch 11842/12000
2024-11-05 03:53:25,103 - INFO - [diffusion][Epoch 11841] diffusion training Loss: 0.06050778552889824
2024-11-05 03:53:25,106 - INFO - [diffusion][Epoch 11841] diffusion learning rate: 0.001
2024-11-05 03:53:25,125 - INFO - [diffusion][Epoch 11841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:25,126 - INFO - [diffusion][Epoch 11842] Epoch 11843/12000
2024-11-05 03:53:27,850 - INFO - [diffusion][Epoch 11842] diffusion training Loss: 0.057807053439319134
2024-11-05 03:53:27,851 - INFO - [diffusion][Epoch 11842] diffusion learning rate: 0.001
2024-11-05 03:53:27,853 - INFO - [diffusion][Epoch 11842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:27,854 - INFO - [diffusion][Epoch 11843] Epoch 11844/12000
2024-11-05 03:53:30,962 - INFO - [diffusion][Epoch 11843] diffusion training Loss: 0.06630834192037582
2024-11-05 03:53:30,964 - INFO - [diffusion][Epoch 11843] diffusion learning rate: 0.001
2024-11-05 03:53:30,966 - INFO - [diffusion][Epoch 11843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:30,968 - INFO - [diffusion][Epoch 11844] Epoch 11845/12000
2024-11-05 03:53:34,484 - INFO - [diffusion][Epoch 11844] diffusion training Loss: 0.061587296426296234
2024-11-05 03:53:34,486 - INFO - [diffusion][Epoch 11844] diffusion learning rate: 0.001
2024-11-05 03:53:34,488 - INFO - [diffusion][Epoch 11844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:34,489 - INFO - [diffusion][Epoch 11845] Epoch 11846/12000
2024-11-05 03:53:37,604 - INFO - [diffusion][Epoch 11845] diffusion training Loss: 0.06408965960144997
2024-11-05 03:53:37,606 - INFO - [diffusion][Epoch 11845] diffusion learning rate: 0.001
2024-11-05 03:53:37,608 - INFO - [diffusion][Epoch 11845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:37,609 - INFO - [diffusion][Epoch 11846] Epoch 11847/12000
2024-11-05 03:53:40,736 - INFO - [diffusion][Epoch 11846] diffusion training Loss: 0.06343647465109825
2024-11-05 03:53:40,738 - INFO - [diffusion][Epoch 11846] diffusion learning rate: 0.001
2024-11-05 03:53:40,740 - INFO - [diffusion][Epoch 11846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:40,741 - INFO - [diffusion][Epoch 11847] Epoch 11848/12000
2024-11-05 03:53:43,964 - INFO - [diffusion][Epoch 11847] diffusion training Loss: 0.07056056428700686
2024-11-05 03:53:43,966 - INFO - [diffusion][Epoch 11847] diffusion learning rate: 0.001
2024-11-05 03:53:43,968 - INFO - [diffusion][Epoch 11847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:43,969 - INFO - [diffusion][Epoch 11848] Epoch 11849/12000
2024-11-05 03:53:47,517 - INFO - [diffusion][Epoch 11848] diffusion training Loss: 0.057703753001987934
2024-11-05 03:53:47,519 - INFO - [diffusion][Epoch 11848] diffusion learning rate: 0.001
2024-11-05 03:53:47,520 - INFO - [diffusion][Epoch 11848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:47,521 - INFO - [diffusion][Epoch 11849] Epoch 11850/12000
2024-11-05 03:53:50,770 - INFO - [diffusion][Epoch 11849] diffusion training Loss: 0.0663417149335146
2024-11-05 03:53:50,771 - INFO - [diffusion][Epoch 11849] diffusion learning rate: 0.001
2024-11-05 03:53:50,773 - INFO - [diffusion][Epoch 11849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:50,774 - INFO - [diffusion][Epoch 11850] Epoch 11851/12000
2024-11-05 03:53:53,702 - INFO - [diffusion][Epoch 11850] diffusion training Loss: 0.06688585132360458
2024-11-05 03:53:53,705 - INFO - [diffusion][Epoch 11850] diffusion learning rate: 0.001
2024-11-05 03:53:53,706 - INFO - [diffusion][Epoch 11850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:53,708 - INFO - [diffusion][Epoch 11851] Epoch 11852/12000
2024-11-05 03:53:56,881 - INFO - [diffusion][Epoch 11851] diffusion training Loss: 0.07346858270466328
2024-11-05 03:53:56,883 - INFO - [diffusion][Epoch 11851] diffusion learning rate: 0.001
2024-11-05 03:53:56,885 - INFO - [diffusion][Epoch 11851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:56,887 - INFO - [diffusion][Epoch 11852] Epoch 11853/12000
2024-11-05 03:54:00,447 - INFO - [diffusion][Epoch 11852] diffusion training Loss: 0.06012618727982044
2024-11-05 03:54:00,449 - INFO - [diffusion][Epoch 11852] diffusion learning rate: 0.001
2024-11-05 03:54:00,451 - INFO - [diffusion][Epoch 11852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:00,452 - INFO - [diffusion][Epoch 11853] Epoch 11854/12000
2024-11-05 03:54:04,026 - INFO - [diffusion][Epoch 11853] diffusion training Loss: 0.06676588580012321
2024-11-05 03:54:04,028 - INFO - [diffusion][Epoch 11853] diffusion learning rate: 0.001
2024-11-05 03:54:04,030 - INFO - [diffusion][Epoch 11853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:04,031 - INFO - [diffusion][Epoch 11854] Epoch 11855/12000
2024-11-05 03:54:07,129 - INFO - [diffusion][Epoch 11854] diffusion training Loss: 0.060626507736742496
2024-11-05 03:54:07,130 - INFO - [diffusion][Epoch 11854] diffusion learning rate: 0.001
2024-11-05 03:54:07,132 - INFO - [diffusion][Epoch 11854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:07,133 - INFO - [diffusion][Epoch 11855] Epoch 11856/12000
2024-11-05 03:54:10,218 - INFO - [diffusion][Epoch 11855] diffusion training Loss: 0.05932633485645056
2024-11-05 03:54:10,221 - INFO - [diffusion][Epoch 11855] diffusion learning rate: 0.001
2024-11-05 03:54:10,222 - INFO - [diffusion][Epoch 11855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:10,224 - INFO - [diffusion][Epoch 11856] Epoch 11857/12000
2024-11-05 03:54:13,518 - INFO - [diffusion][Epoch 11856] diffusion training Loss: 0.061948628164827824
2024-11-05 03:54:13,519 - INFO - [diffusion][Epoch 11856] diffusion learning rate: 0.001
2024-11-05 03:54:13,521 - INFO - [diffusion][Epoch 11856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:13,522 - INFO - [diffusion][Epoch 11857] Epoch 11858/12000
2024-11-05 03:54:16,947 - INFO - [diffusion][Epoch 11857] diffusion training Loss: 0.06757758930325508
2024-11-05 03:54:16,949 - INFO - [diffusion][Epoch 11857] diffusion learning rate: 0.001
2024-11-05 03:54:16,951 - INFO - [diffusion][Epoch 11857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:16,952 - INFO - [diffusion][Epoch 11858] Epoch 11859/12000
2024-11-05 03:54:20,619 - INFO - [diffusion][Epoch 11858] diffusion training Loss: 0.06687165051698685
2024-11-05 03:54:20,621 - INFO - [diffusion][Epoch 11858] diffusion learning rate: 0.001
2024-11-05 03:54:20,623 - INFO - [diffusion][Epoch 11858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:20,624 - INFO - [diffusion][Epoch 11859] Epoch 11860/12000
2024-11-05 03:54:24,150 - INFO - [diffusion][Epoch 11859] diffusion training Loss: 0.05855887942016125
2024-11-05 03:54:24,152 - INFO - [diffusion][Epoch 11859] diffusion learning rate: 0.001
2024-11-05 03:54:24,154 - INFO - [diffusion][Epoch 11859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:24,155 - INFO - [diffusion][Epoch 11860] Epoch 11861/12000
2024-11-05 03:54:27,271 - INFO - [diffusion][Epoch 11860] diffusion training Loss: 0.05753927584737539
2024-11-05 03:54:27,273 - INFO - [diffusion][Epoch 11860] diffusion learning rate: 0.001
2024-11-05 03:54:27,275 - INFO - [diffusion][Epoch 11860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:27,276 - INFO - [diffusion][Epoch 11861] Epoch 11862/12000
2024-11-05 03:54:30,450 - INFO - [diffusion][Epoch 11861] diffusion training Loss: 0.065805334597826
2024-11-05 03:54:30,452 - INFO - [diffusion][Epoch 11861] diffusion learning rate: 0.001
2024-11-05 03:54:30,453 - INFO - [diffusion][Epoch 11861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:30,455 - INFO - [diffusion][Epoch 11862] Epoch 11863/12000
2024-11-05 03:54:33,628 - INFO - [diffusion][Epoch 11862] diffusion training Loss: 0.0648141810670495
2024-11-05 03:54:33,630 - INFO - [diffusion][Epoch 11862] diffusion learning rate: 0.001
2024-11-05 03:54:33,632 - INFO - [diffusion][Epoch 11862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:33,633 - INFO - [diffusion][Epoch 11863] Epoch 11864/12000
2024-11-05 03:54:37,120 - INFO - [diffusion][Epoch 11863] diffusion training Loss: 0.06677962653338909
2024-11-05 03:54:37,123 - INFO - [diffusion][Epoch 11863] diffusion learning rate: 0.001
2024-11-05 03:54:37,124 - INFO - [diffusion][Epoch 11863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:37,125 - INFO - [diffusion][Epoch 11864] Epoch 11865/12000
2024-11-05 03:54:40,096 - INFO - [diffusion][Epoch 11864] diffusion training Loss: 0.06213661376386881
2024-11-05 03:54:40,098 - INFO - [diffusion][Epoch 11864] diffusion learning rate: 0.001
2024-11-05 03:54:40,100 - INFO - [diffusion][Epoch 11864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:40,101 - INFO - [diffusion][Epoch 11865] Epoch 11866/12000
2024-11-05 03:54:42,999 - INFO - [diffusion][Epoch 11865] diffusion training Loss: 0.06053295824676752
2024-11-05 03:54:43,001 - INFO - [diffusion][Epoch 11865] diffusion learning rate: 0.001
2024-11-05 03:54:43,021 - INFO - [diffusion][Epoch 11865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:43,022 - INFO - [diffusion][Epoch 11866] Epoch 11867/12000
2024-11-05 03:54:46,439 - INFO - [diffusion][Epoch 11866] diffusion training Loss: 0.06607984565198421
2024-11-05 03:54:46,441 - INFO - [diffusion][Epoch 11866] diffusion learning rate: 0.001
2024-11-05 03:54:46,442 - INFO - [diffusion][Epoch 11866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:46,443 - INFO - [diffusion][Epoch 11867] Epoch 11868/12000
2024-11-05 03:54:50,094 - INFO - [diffusion][Epoch 11867] diffusion training Loss: 0.06099587492644787
2024-11-05 03:54:50,096 - INFO - [diffusion][Epoch 11867] diffusion learning rate: 0.001
2024-11-05 03:54:50,098 - INFO - [diffusion][Epoch 11867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:50,099 - INFO - [diffusion][Epoch 11868] Epoch 11869/12000
2024-11-05 03:54:53,289 - INFO - [diffusion][Epoch 11868] diffusion training Loss: 0.06463229469954967
2024-11-05 03:54:53,291 - INFO - [diffusion][Epoch 11868] diffusion learning rate: 0.001
2024-11-05 03:54:53,292 - INFO - [diffusion][Epoch 11868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:53,294 - INFO - [diffusion][Epoch 11869] Epoch 11870/12000
2024-11-05 03:54:56,419 - INFO - [diffusion][Epoch 11869] diffusion training Loss: 0.06319417525082827
2024-11-05 03:54:56,421 - INFO - [diffusion][Epoch 11869] diffusion learning rate: 0.001
2024-11-05 03:54:56,423 - INFO - [diffusion][Epoch 11869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:56,424 - INFO - [diffusion][Epoch 11870] Epoch 11871/12000
2024-11-05 03:54:59,717 - INFO - [diffusion][Epoch 11870] diffusion training Loss: 0.061839230358600616
2024-11-05 03:54:59,719 - INFO - [diffusion][Epoch 11870] diffusion learning rate: 0.001
2024-11-05 03:54:59,745 - INFO - [diffusion][Epoch 11870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:59,747 - INFO - [diffusion][Epoch 11871] Epoch 11872/12000
2024-11-05 03:55:03,377 - INFO - [diffusion][Epoch 11871] diffusion training Loss: 0.06220659427344799
2024-11-05 03:55:03,379 - INFO - [diffusion][Epoch 11871] diffusion learning rate: 0.001
2024-11-05 03:55:03,381 - INFO - [diffusion][Epoch 11871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:03,382 - INFO - [diffusion][Epoch 11872] Epoch 11873/12000
2024-11-05 03:55:06,632 - INFO - [diffusion][Epoch 11872] diffusion training Loss: 0.056615957990288734
2024-11-05 03:55:06,634 - INFO - [diffusion][Epoch 11872] diffusion learning rate: 0.001
2024-11-05 03:55:06,636 - INFO - [diffusion][Epoch 11872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:06,637 - INFO - [diffusion][Epoch 11873] Epoch 11874/12000
2024-11-05 03:55:10,371 - INFO - [diffusion][Epoch 11873] diffusion training Loss: 0.06123804673552513
2024-11-05 03:55:10,373 - INFO - [diffusion][Epoch 11873] diffusion learning rate: 0.001
2024-11-05 03:55:10,374 - INFO - [diffusion][Epoch 11873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:10,376 - INFO - [diffusion][Epoch 11874] Epoch 11875/12000
2024-11-05 03:55:13,485 - INFO - [diffusion][Epoch 11874] diffusion training Loss: 0.062043569050729275
2024-11-05 03:55:13,487 - INFO - [diffusion][Epoch 11874] diffusion learning rate: 0.001
2024-11-05 03:55:13,489 - INFO - [diffusion][Epoch 11874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:13,490 - INFO - [diffusion][Epoch 11875] Epoch 11876/12000
2024-11-05 03:55:16,623 - INFO - [diffusion][Epoch 11875] diffusion training Loss: 0.05855049658566713
2024-11-05 03:55:16,625 - INFO - [diffusion][Epoch 11875] diffusion learning rate: 0.001
2024-11-05 03:55:16,627 - INFO - [diffusion][Epoch 11875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:16,628 - INFO - [diffusion][Epoch 11876] Epoch 11877/12000
2024-11-05 03:55:20,184 - INFO - [diffusion][Epoch 11876] diffusion training Loss: 0.06697900407016277
2024-11-05 03:55:20,186 - INFO - [diffusion][Epoch 11876] diffusion learning rate: 0.001
2024-11-05 03:55:20,188 - INFO - [diffusion][Epoch 11876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:20,189 - INFO - [diffusion][Epoch 11877] Epoch 11878/12000
2024-11-05 03:55:23,775 - INFO - [diffusion][Epoch 11877] diffusion training Loss: 0.06715436466038227
2024-11-05 03:55:23,776 - INFO - [diffusion][Epoch 11877] diffusion learning rate: 0.001
2024-11-05 03:55:23,778 - INFO - [diffusion][Epoch 11877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:23,780 - INFO - [diffusion][Epoch 11878] Epoch 11879/12000
2024-11-05 03:55:26,891 - INFO - [diffusion][Epoch 11878] diffusion training Loss: 0.06599004194140434
2024-11-05 03:55:26,893 - INFO - [diffusion][Epoch 11878] diffusion learning rate: 0.001
2024-11-05 03:55:26,895 - INFO - [diffusion][Epoch 11878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:26,896 - INFO - [diffusion][Epoch 11879] Epoch 11880/12000
2024-11-05 03:55:29,883 - INFO - [diffusion][Epoch 11879] diffusion training Loss: 0.06574319582432508
2024-11-05 03:55:29,885 - INFO - [diffusion][Epoch 11879] diffusion learning rate: 0.001
2024-11-05 03:55:29,886 - INFO - [diffusion][Epoch 11879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:29,888 - INFO - [diffusion][Epoch 11880] Epoch 11881/12000
2024-11-05 03:55:33,183 - INFO - [diffusion][Epoch 11880] diffusion training Loss: 0.06018476840108633
2024-11-05 03:55:33,185 - INFO - [diffusion][Epoch 11880] diffusion learning rate: 0.001
2024-11-05 03:55:33,236 - INFO - [diffusion][Epoch 11880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:33,238 - INFO - [diffusion][Epoch 11881] Epoch 11882/12000
2024-11-05 03:55:36,789 - INFO - [diffusion][Epoch 11881] diffusion training Loss: 0.06328864302486181
2024-11-05 03:55:36,791 - INFO - [diffusion][Epoch 11881] diffusion learning rate: 0.001
2024-11-05 03:55:36,793 - INFO - [diffusion][Epoch 11881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:36,794 - INFO - [diffusion][Epoch 11882] Epoch 11883/12000
2024-11-05 03:55:40,329 - INFO - [diffusion][Epoch 11882] diffusion training Loss: 0.07118837162852287
2024-11-05 03:55:40,331 - INFO - [diffusion][Epoch 11882] diffusion learning rate: 0.001
2024-11-05 03:55:40,333 - INFO - [diffusion][Epoch 11882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:40,335 - INFO - [diffusion][Epoch 11883] Epoch 11884/12000
2024-11-05 03:55:43,448 - INFO - [diffusion][Epoch 11883] diffusion training Loss: 0.06294744461774826
2024-11-05 03:55:43,450 - INFO - [diffusion][Epoch 11883] diffusion learning rate: 0.001
2024-11-05 03:55:43,452 - INFO - [diffusion][Epoch 11883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:43,453 - INFO - [diffusion][Epoch 11884] Epoch 11885/12000
2024-11-05 03:55:46,564 - INFO - [diffusion][Epoch 11884] diffusion training Loss: 0.06350076850503683
2024-11-05 03:55:46,566 - INFO - [diffusion][Epoch 11884] diffusion learning rate: 0.001
2024-11-05 03:55:46,568 - INFO - [diffusion][Epoch 11884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:46,569 - INFO - [diffusion][Epoch 11885] Epoch 11886/12000
2024-11-05 03:55:49,688 - INFO - [diffusion][Epoch 11885] diffusion training Loss: 0.06310863420367241
2024-11-05 03:55:49,691 - INFO - [diffusion][Epoch 11885] diffusion learning rate: 0.001
2024-11-05 03:55:49,693 - INFO - [diffusion][Epoch 11885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:49,694 - INFO - [diffusion][Epoch 11886] Epoch 11887/12000
2024-11-05 03:55:53,283 - INFO - [diffusion][Epoch 11886] diffusion training Loss: 0.06546218600124121
2024-11-05 03:55:53,285 - INFO - [diffusion][Epoch 11886] diffusion learning rate: 0.001
2024-11-05 03:55:53,287 - INFO - [diffusion][Epoch 11886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:53,289 - INFO - [diffusion][Epoch 11887] Epoch 11888/12000
2024-11-05 03:55:56,875 - INFO - [diffusion][Epoch 11887] diffusion training Loss: 0.06355634517967701
2024-11-05 03:55:56,877 - INFO - [diffusion][Epoch 11887] diffusion learning rate: 0.001
2024-11-05 03:55:56,879 - INFO - [diffusion][Epoch 11887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:56,880 - INFO - [diffusion][Epoch 11888] Epoch 11889/12000
2024-11-05 03:56:00,017 - INFO - [diffusion][Epoch 11888] diffusion training Loss: 0.06605083029717207
2024-11-05 03:56:00,021 - INFO - [diffusion][Epoch 11888] diffusion learning rate: 0.001
2024-11-05 03:56:00,023 - INFO - [diffusion][Epoch 11888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:00,024 - INFO - [diffusion][Epoch 11889] Epoch 11890/12000
2024-11-05 03:56:03,136 - INFO - [diffusion][Epoch 11889] diffusion training Loss: 0.06144019030034542
2024-11-05 03:56:03,139 - INFO - [diffusion][Epoch 11889] diffusion learning rate: 0.001
2024-11-05 03:56:03,141 - INFO - [diffusion][Epoch 11889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:03,142 - INFO - [diffusion][Epoch 11890] Epoch 11891/12000
2024-11-05 03:56:06,270 - INFO - [diffusion][Epoch 11890] diffusion training Loss: 0.059661067090928555
2024-11-05 03:56:06,273 - INFO - [diffusion][Epoch 11890] diffusion learning rate: 0.001
2024-11-05 03:56:06,275 - INFO - [diffusion][Epoch 11890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:06,276 - INFO - [diffusion][Epoch 11891] Epoch 11892/12000
2024-11-05 03:56:09,809 - INFO - [diffusion][Epoch 11891] diffusion training Loss: 0.06634712032973766
2024-11-05 03:56:09,811 - INFO - [diffusion][Epoch 11891] diffusion learning rate: 0.001
2024-11-05 03:56:09,813 - INFO - [diffusion][Epoch 11891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:09,814 - INFO - [diffusion][Epoch 11892] Epoch 11893/12000
2024-11-05 03:56:13,359 - INFO - [diffusion][Epoch 11892] diffusion training Loss: 0.06245326064527035
2024-11-05 03:56:13,361 - INFO - [diffusion][Epoch 11892] diffusion learning rate: 0.001
2024-11-05 03:56:13,363 - INFO - [diffusion][Epoch 11892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:13,364 - INFO - [diffusion][Epoch 11893] Epoch 11894/12000
2024-11-05 03:56:16,453 - INFO - [diffusion][Epoch 11893] diffusion training Loss: 0.06264246068894863
2024-11-05 03:56:16,454 - INFO - [diffusion][Epoch 11893] diffusion learning rate: 0.001
2024-11-05 03:56:16,456 - INFO - [diffusion][Epoch 11893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:16,457 - INFO - [diffusion][Epoch 11894] Epoch 11895/12000
2024-11-05 03:56:19,673 - INFO - [diffusion][Epoch 11894] diffusion training Loss: 0.05414772592484951
2024-11-05 03:56:19,675 - INFO - [diffusion][Epoch 11894] diffusion learning rate: 0.001
2024-11-05 03:56:19,677 - INFO - [diffusion][Epoch 11894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:19,678 - INFO - [diffusion][Epoch 11895] Epoch 11896/12000
2024-11-05 03:56:22,812 - INFO - [diffusion][Epoch 11895] diffusion training Loss: 0.06736446730792522
2024-11-05 03:56:22,814 - INFO - [diffusion][Epoch 11895] diffusion learning rate: 0.001
2024-11-05 03:56:22,816 - INFO - [diffusion][Epoch 11895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:22,817 - INFO - [diffusion][Epoch 11896] Epoch 11897/12000
2024-11-05 03:56:26,461 - INFO - [diffusion][Epoch 11896] diffusion training Loss: 0.06213994696736336
2024-11-05 03:56:26,464 - INFO - [diffusion][Epoch 11896] diffusion learning rate: 0.001
2024-11-05 03:56:26,465 - INFO - [diffusion][Epoch 11896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:26,467 - INFO - [diffusion][Epoch 11897] Epoch 11898/12000
2024-11-05 03:56:30,006 - INFO - [diffusion][Epoch 11897] diffusion training Loss: 0.05907130613923073
2024-11-05 03:56:30,008 - INFO - [diffusion][Epoch 11897] diffusion learning rate: 0.001
2024-11-05 03:56:30,009 - INFO - [diffusion][Epoch 11897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:30,011 - INFO - [diffusion][Epoch 11898] Epoch 11899/12000
2024-11-05 03:56:33,170 - INFO - [diffusion][Epoch 11898] diffusion training Loss: 0.06464772392064333
2024-11-05 03:56:33,171 - INFO - [diffusion][Epoch 11898] diffusion learning rate: 0.001
2024-11-05 03:56:33,173 - INFO - [diffusion][Epoch 11898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:33,174 - INFO - [diffusion][Epoch 11899] Epoch 11900/12000
2024-11-05 03:56:36,309 - INFO - [diffusion][Epoch 11899] diffusion training Loss: 0.06095785088837147
2024-11-05 03:56:36,311 - INFO - [diffusion][Epoch 11899] diffusion learning rate: 0.001
2024-11-05 03:56:36,313 - INFO - [diffusion][Epoch 11899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:36,314 - INFO - [diffusion][Epoch 11900] Epoch 11901/12000
2024-11-05 03:56:39,581 - INFO - [diffusion][Epoch 11900] diffusion training Loss: 0.057798584923148155
2024-11-05 03:56:39,583 - INFO - [diffusion][Epoch 11900] diffusion learning rate: 0.001
2024-11-05 03:56:39,585 - INFO - [diffusion][Epoch 11900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:39,588 - INFO - [diffusion][Epoch 11901] Epoch 11902/12000
2024-11-05 03:56:43,139 - INFO - [diffusion][Epoch 11901] diffusion training Loss: 0.05987915676087141
2024-11-05 03:56:43,141 - INFO - [diffusion][Epoch 11901] diffusion learning rate: 0.001
2024-11-05 03:56:43,143 - INFO - [diffusion][Epoch 11901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:43,144 - INFO - [diffusion][Epoch 11902] Epoch 11903/12000
2024-11-05 03:56:46,701 - INFO - [diffusion][Epoch 11902] diffusion training Loss: 0.06323343142867088
2024-11-05 03:56:46,703 - INFO - [diffusion][Epoch 11902] diffusion learning rate: 0.001
2024-11-05 03:56:46,704 - INFO - [diffusion][Epoch 11902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:46,705 - INFO - [diffusion][Epoch 11903] Epoch 11904/12000
2024-11-05 03:56:49,814 - INFO - [diffusion][Epoch 11903] diffusion training Loss: 0.06418958306312561
2024-11-05 03:56:49,816 - INFO - [diffusion][Epoch 11903] diffusion learning rate: 0.001
2024-11-05 03:56:49,818 - INFO - [diffusion][Epoch 11903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:49,819 - INFO - [diffusion][Epoch 11904] Epoch 11905/12000
2024-11-05 03:56:52,865 - INFO - [diffusion][Epoch 11904] diffusion training Loss: 0.05861391592770815
2024-11-05 03:56:52,867 - INFO - [diffusion][Epoch 11904] diffusion learning rate: 0.001
2024-11-05 03:56:52,869 - INFO - [diffusion][Epoch 11904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:52,870 - INFO - [diffusion][Epoch 11905] Epoch 11906/12000
2024-11-05 03:56:56,098 - INFO - [diffusion][Epoch 11905] diffusion training Loss: 0.06437108106911182
2024-11-05 03:56:56,099 - INFO - [diffusion][Epoch 11905] diffusion learning rate: 0.001
2024-11-05 03:56:56,101 - INFO - [diffusion][Epoch 11905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:56,102 - INFO - [diffusion][Epoch 11906] Epoch 11907/12000
2024-11-05 03:56:59,715 - INFO - [diffusion][Epoch 11906] diffusion training Loss: 0.0660089123994112
2024-11-05 03:56:59,718 - INFO - [diffusion][Epoch 11906] diffusion learning rate: 0.001
2024-11-05 03:56:59,719 - INFO - [diffusion][Epoch 11906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:59,721 - INFO - [diffusion][Epoch 11907] Epoch 11908/12000
2024-11-05 03:57:02,880 - INFO - [diffusion][Epoch 11907] diffusion training Loss: 0.06526386272162199
2024-11-05 03:57:02,881 - INFO - [diffusion][Epoch 11907] diffusion learning rate: 0.001
2024-11-05 03:57:02,883 - INFO - [diffusion][Epoch 11907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:02,884 - INFO - [diffusion][Epoch 11908] Epoch 11909/12000
2024-11-05 03:57:05,970 - INFO - [diffusion][Epoch 11908] diffusion training Loss: 0.061185333877801895
2024-11-05 03:57:05,972 - INFO - [diffusion][Epoch 11908] diffusion learning rate: 0.001
2024-11-05 03:57:05,974 - INFO - [diffusion][Epoch 11908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:05,975 - INFO - [diffusion][Epoch 11909] Epoch 11910/12000
2024-11-05 03:57:09,100 - INFO - [diffusion][Epoch 11909] diffusion training Loss: 0.062086330726742744
2024-11-05 03:57:09,103 - INFO - [diffusion][Epoch 11909] diffusion learning rate: 0.001
2024-11-05 03:57:09,105 - INFO - [diffusion][Epoch 11909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:09,106 - INFO - [diffusion][Epoch 11910] Epoch 11911/12000
2024-11-05 03:57:12,683 - INFO - [diffusion][Epoch 11910] diffusion training Loss: 0.060728611424565315
2024-11-05 03:57:12,695 - INFO - [diffusion][Epoch 11910] diffusion learning rate: 0.001
2024-11-05 03:57:12,697 - INFO - [diffusion][Epoch 11910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:12,698 - INFO - [diffusion][Epoch 11911] Epoch 11912/12000
2024-11-05 03:57:16,242 - INFO - [diffusion][Epoch 11911] diffusion training Loss: 0.0642104372382164
2024-11-05 03:57:16,245 - INFO - [diffusion][Epoch 11911] diffusion learning rate: 0.001
2024-11-05 03:57:16,247 - INFO - [diffusion][Epoch 11911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:16,249 - INFO - [diffusion][Epoch 11912] Epoch 11913/12000
2024-11-05 03:57:19,321 - INFO - [diffusion][Epoch 11912] diffusion training Loss: 0.06436784006655216
2024-11-05 03:57:19,323 - INFO - [diffusion][Epoch 11912] diffusion learning rate: 0.001
2024-11-05 03:57:19,325 - INFO - [diffusion][Epoch 11912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:19,327 - INFO - [diffusion][Epoch 11913] Epoch 11914/12000
2024-11-05 03:57:22,447 - INFO - [diffusion][Epoch 11913] diffusion training Loss: 0.06700737122446299
2024-11-05 03:57:22,449 - INFO - [diffusion][Epoch 11913] diffusion learning rate: 0.001
2024-11-05 03:57:22,451 - INFO - [diffusion][Epoch 11913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:22,452 - INFO - [diffusion][Epoch 11914] Epoch 11915/12000
2024-11-05 03:57:25,753 - INFO - [diffusion][Epoch 11914] diffusion training Loss: 0.059788111597299576
2024-11-05 03:57:25,755 - INFO - [diffusion][Epoch 11914] diffusion learning rate: 0.001
2024-11-05 03:57:25,757 - INFO - [diffusion][Epoch 11914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:25,758 - INFO - [diffusion][Epoch 11915] Epoch 11916/12000
2024-11-05 03:57:29,767 - INFO - [diffusion][Epoch 11915] diffusion training Loss: 0.0644264155998826
2024-11-05 03:57:29,769 - INFO - [diffusion][Epoch 11915] diffusion learning rate: 0.001
2024-11-05 03:57:29,771 - INFO - [diffusion][Epoch 11915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:29,772 - INFO - [diffusion][Epoch 11916] Epoch 11917/12000
2024-11-05 03:57:33,312 - INFO - [diffusion][Epoch 11916] diffusion training Loss: 0.06426575966179371
2024-11-05 03:57:33,315 - INFO - [diffusion][Epoch 11916] diffusion learning rate: 0.001
2024-11-05 03:57:33,317 - INFO - [diffusion][Epoch 11916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:33,319 - INFO - [diffusion][Epoch 11917] Epoch 11918/12000
2024-11-05 03:57:36,662 - INFO - [diffusion][Epoch 11917] diffusion training Loss: 0.06784186512231827
2024-11-05 03:57:36,664 - INFO - [diffusion][Epoch 11917] diffusion learning rate: 0.001
2024-11-05 03:57:36,666 - INFO - [diffusion][Epoch 11917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:36,667 - INFO - [diffusion][Epoch 11918] Epoch 11919/12000
2024-11-05 03:57:39,764 - INFO - [diffusion][Epoch 11918] diffusion training Loss: 0.06151160877197981
2024-11-05 03:57:39,766 - INFO - [diffusion][Epoch 11918] diffusion learning rate: 0.001
2024-11-05 03:57:39,768 - INFO - [diffusion][Epoch 11918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:39,769 - INFO - [diffusion][Epoch 11919] Epoch 11920/12000
2024-11-05 03:57:42,852 - INFO - [diffusion][Epoch 11919] diffusion training Loss: 0.06257260497659445
2024-11-05 03:57:42,853 - INFO - [diffusion][Epoch 11919] diffusion learning rate: 0.001
2024-11-05 03:57:42,855 - INFO - [diffusion][Epoch 11919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:42,856 - INFO - [diffusion][Epoch 11920] Epoch 11921/12000
2024-11-05 03:57:46,398 - INFO - [diffusion][Epoch 11920] diffusion training Loss: 0.06508274748921394
2024-11-05 03:57:46,400 - INFO - [diffusion][Epoch 11920] diffusion learning rate: 0.001
2024-11-05 03:57:46,402 - INFO - [diffusion][Epoch 11920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:46,403 - INFO - [diffusion][Epoch 11921] Epoch 11922/12000
2024-11-05 03:57:49,947 - INFO - [diffusion][Epoch 11921] diffusion training Loss: 0.06147966347634792
2024-11-05 03:57:49,949 - INFO - [diffusion][Epoch 11921] diffusion learning rate: 0.001
2024-11-05 03:57:49,951 - INFO - [diffusion][Epoch 11921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:49,952 - INFO - [diffusion][Epoch 11922] Epoch 11923/12000
2024-11-05 03:57:53,085 - INFO - [diffusion][Epoch 11922] diffusion training Loss: 0.06110602617263794
2024-11-05 03:57:53,086 - INFO - [diffusion][Epoch 11922] diffusion learning rate: 0.001
2024-11-05 03:57:53,088 - INFO - [diffusion][Epoch 11922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:53,089 - INFO - [diffusion][Epoch 11923] Epoch 11924/12000
2024-11-05 03:57:56,210 - INFO - [diffusion][Epoch 11923] diffusion training Loss: 0.05829859524965286
2024-11-05 03:57:56,212 - INFO - [diffusion][Epoch 11923] diffusion learning rate: 0.001
2024-11-05 03:57:56,213 - INFO - [diffusion][Epoch 11923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:56,215 - INFO - [diffusion][Epoch 11924] Epoch 11925/12000
2024-11-05 03:57:59,350 - INFO - [diffusion][Epoch 11924] diffusion training Loss: 0.07166627235710621
2024-11-05 03:57:59,352 - INFO - [diffusion][Epoch 11924] diffusion learning rate: 0.001
2024-11-05 03:57:59,354 - INFO - [diffusion][Epoch 11924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:59,355 - INFO - [diffusion][Epoch 11925] Epoch 11926/12000
2024-11-05 03:58:02,873 - INFO - [diffusion][Epoch 11925] diffusion training Loss: 0.060667624697089195
2024-11-05 03:58:02,875 - INFO - [diffusion][Epoch 11925] diffusion learning rate: 0.001
2024-11-05 03:58:02,877 - INFO - [diffusion][Epoch 11925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:02,878 - INFO - [diffusion][Epoch 11926] Epoch 11927/12000
2024-11-05 03:58:06,310 - INFO - [diffusion][Epoch 11926] diffusion training Loss: 0.06049734726548195
2024-11-05 03:58:06,313 - INFO - [diffusion][Epoch 11926] diffusion learning rate: 0.001
2024-11-05 03:58:06,315 - INFO - [diffusion][Epoch 11926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:06,316 - INFO - [diffusion][Epoch 11927] Epoch 11928/12000
2024-11-05 03:58:09,472 - INFO - [diffusion][Epoch 11927] diffusion training Loss: 0.06722407508641481
2024-11-05 03:58:09,475 - INFO - [diffusion][Epoch 11927] diffusion learning rate: 0.001
2024-11-05 03:58:09,476 - INFO - [diffusion][Epoch 11927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:09,478 - INFO - [diffusion][Epoch 11928] Epoch 11929/12000
2024-11-05 03:58:12,570 - INFO - [diffusion][Epoch 11928] diffusion training Loss: 0.059123607352375984
2024-11-05 03:58:12,571 - INFO - [diffusion][Epoch 11928] diffusion learning rate: 0.001
2024-11-05 03:58:12,573 - INFO - [diffusion][Epoch 11928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:12,574 - INFO - [diffusion][Epoch 11929] Epoch 11930/12000
2024-11-05 03:58:15,668 - INFO - [diffusion][Epoch 11929] diffusion training Loss: 0.06324419006705284
2024-11-05 03:58:15,670 - INFO - [diffusion][Epoch 11929] diffusion learning rate: 0.001
2024-11-05 03:58:15,672 - INFO - [diffusion][Epoch 11929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:15,673 - INFO - [diffusion][Epoch 11930] Epoch 11931/12000
2024-11-05 03:58:19,235 - INFO - [diffusion][Epoch 11930] diffusion training Loss: 0.06338172126561403
2024-11-05 03:58:19,237 - INFO - [diffusion][Epoch 11930] diffusion learning rate: 0.001
2024-11-05 03:58:19,239 - INFO - [diffusion][Epoch 11930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:19,241 - INFO - [diffusion][Epoch 11931] Epoch 11932/12000
2024-11-05 03:58:22,774 - INFO - [diffusion][Epoch 11931] diffusion training Loss: 0.06181098520755768
2024-11-05 03:58:22,776 - INFO - [diffusion][Epoch 11931] diffusion learning rate: 0.001
2024-11-05 03:58:22,778 - INFO - [diffusion][Epoch 11931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:22,779 - INFO - [diffusion][Epoch 11932] Epoch 11933/12000
2024-11-05 03:58:25,877 - INFO - [diffusion][Epoch 11932] diffusion training Loss: 0.06085010711103678
2024-11-05 03:58:25,879 - INFO - [diffusion][Epoch 11932] diffusion learning rate: 0.001
2024-11-05 03:58:25,881 - INFO - [diffusion][Epoch 11932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:25,883 - INFO - [diffusion][Epoch 11933] Epoch 11934/12000
2024-11-05 03:58:28,956 - INFO - [diffusion][Epoch 11933] diffusion training Loss: 0.0591631643474102
2024-11-05 03:58:28,958 - INFO - [diffusion][Epoch 11933] diffusion learning rate: 0.001
2024-11-05 03:58:28,960 - INFO - [diffusion][Epoch 11933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:28,961 - INFO - [diffusion][Epoch 11934] Epoch 11935/12000
2024-11-05 03:58:32,140 - INFO - [diffusion][Epoch 11934] diffusion training Loss: 0.06519150733947754
2024-11-05 03:58:32,143 - INFO - [diffusion][Epoch 11934] diffusion learning rate: 0.001
2024-11-05 03:58:32,145 - INFO - [diffusion][Epoch 11934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:32,146 - INFO - [diffusion][Epoch 11935] Epoch 11936/12000
2024-11-05 03:58:35,802 - INFO - [diffusion][Epoch 11935] diffusion training Loss: 0.06692950520664454
2024-11-05 03:58:35,804 - INFO - [diffusion][Epoch 11935] diffusion learning rate: 0.001
2024-11-05 03:58:35,806 - INFO - [diffusion][Epoch 11935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:35,808 - INFO - [diffusion][Epoch 11936] Epoch 11937/12000
2024-11-05 03:58:39,369 - INFO - [diffusion][Epoch 11936] diffusion training Loss: 0.06473162118345499
2024-11-05 03:58:39,371 - INFO - [diffusion][Epoch 11936] diffusion learning rate: 0.001
2024-11-05 03:58:39,372 - INFO - [diffusion][Epoch 11936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:39,374 - INFO - [diffusion][Epoch 11937] Epoch 11938/12000
2024-11-05 03:58:43,059 - INFO - [diffusion][Epoch 11937] diffusion training Loss: 0.05995976645499468
2024-11-05 03:58:43,061 - INFO - [diffusion][Epoch 11937] diffusion learning rate: 0.001
2024-11-05 03:58:43,062 - INFO - [diffusion][Epoch 11937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:43,063 - INFO - [diffusion][Epoch 11938] Epoch 11939/12000
2024-11-05 03:58:46,236 - INFO - [diffusion][Epoch 11938] diffusion training Loss: 0.05685903690755367
2024-11-05 03:58:46,238 - INFO - [diffusion][Epoch 11938] diffusion learning rate: 0.001
2024-11-05 03:58:46,239 - INFO - [diffusion][Epoch 11938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:46,240 - INFO - [diffusion][Epoch 11939] Epoch 11940/12000
2024-11-05 03:58:49,200 - INFO - [diffusion][Epoch 11939] diffusion training Loss: 0.0625600591301918
2024-11-05 03:58:49,202 - INFO - [diffusion][Epoch 11939] diffusion learning rate: 0.001
2024-11-05 03:58:49,203 - INFO - [diffusion][Epoch 11939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:49,205 - INFO - [diffusion][Epoch 11940] Epoch 11941/12000
2024-11-05 03:58:52,342 - INFO - [diffusion][Epoch 11940] diffusion training Loss: 0.06121333409100771
2024-11-05 03:58:52,344 - INFO - [diffusion][Epoch 11940] diffusion learning rate: 0.001
2024-11-05 03:58:52,346 - INFO - [diffusion][Epoch 11940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:52,347 - INFO - [diffusion][Epoch 11941] Epoch 11942/12000
2024-11-05 03:58:55,944 - INFO - [diffusion][Epoch 11941] diffusion training Loss: 0.0610774252563715
2024-11-05 03:58:55,946 - INFO - [diffusion][Epoch 11941] diffusion learning rate: 0.001
2024-11-05 03:58:55,947 - INFO - [diffusion][Epoch 11941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:55,948 - INFO - [diffusion][Epoch 11942] Epoch 11943/12000
2024-11-05 03:58:59,385 - INFO - [diffusion][Epoch 11942] diffusion training Loss: 0.06485571898519993
2024-11-05 03:58:59,387 - INFO - [diffusion][Epoch 11942] diffusion learning rate: 0.001
2024-11-05 03:58:59,389 - INFO - [diffusion][Epoch 11942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:59,390 - INFO - [diffusion][Epoch 11943] Epoch 11944/12000
2024-11-05 03:59:02,495 - INFO - [diffusion][Epoch 11943] diffusion training Loss: 0.061542089097201824
2024-11-05 03:59:02,496 - INFO - [diffusion][Epoch 11943] diffusion learning rate: 0.001
2024-11-05 03:59:02,498 - INFO - [diffusion][Epoch 11943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:02,499 - INFO - [diffusion][Epoch 11944] Epoch 11945/12000
2024-11-05 03:59:05,612 - INFO - [diffusion][Epoch 11944] diffusion training Loss: 0.06772259622812271
2024-11-05 03:59:05,614 - INFO - [diffusion][Epoch 11944] diffusion learning rate: 0.001
2024-11-05 03:59:05,615 - INFO - [diffusion][Epoch 11944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:05,616 - INFO - [diffusion][Epoch 11945] Epoch 11946/12000
2024-11-05 03:59:09,090 - INFO - [diffusion][Epoch 11945] diffusion training Loss: 0.05919949058443308
2024-11-05 03:59:09,092 - INFO - [diffusion][Epoch 11945] diffusion learning rate: 0.001
2024-11-05 03:59:09,094 - INFO - [diffusion][Epoch 11945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:09,095 - INFO - [diffusion][Epoch 11946] Epoch 11947/12000
2024-11-05 03:59:12,669 - INFO - [diffusion][Epoch 11946] diffusion training Loss: 0.06359227746725082
2024-11-05 03:59:12,671 - INFO - [diffusion][Epoch 11946] diffusion learning rate: 0.001
2024-11-05 03:59:12,673 - INFO - [diffusion][Epoch 11946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:12,674 - INFO - [diffusion][Epoch 11947] Epoch 11948/12000
2024-11-05 03:59:15,374 - INFO - [diffusion][Epoch 11947] diffusion training Loss: 0.06835971027612686
2024-11-05 03:59:15,376 - INFO - [diffusion][Epoch 11947] diffusion learning rate: 0.001
2024-11-05 03:59:15,378 - INFO - [diffusion][Epoch 11947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:15,379 - INFO - [diffusion][Epoch 11948] Epoch 11949/12000
2024-11-05 03:59:18,496 - INFO - [diffusion][Epoch 11948] diffusion training Loss: 0.06435241736471653
2024-11-05 03:59:18,498 - INFO - [diffusion][Epoch 11948] diffusion learning rate: 0.001
2024-11-05 03:59:18,500 - INFO - [diffusion][Epoch 11948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:18,501 - INFO - [diffusion][Epoch 11949] Epoch 11950/12000
2024-11-05 03:59:22,042 - INFO - [diffusion][Epoch 11949] diffusion training Loss: 0.06558467261493206
2024-11-05 03:59:22,044 - INFO - [diffusion][Epoch 11949] diffusion learning rate: 0.001
2024-11-05 03:59:22,045 - INFO - [diffusion][Epoch 11949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:22,047 - INFO - [diffusion][Epoch 11950] Epoch 11951/12000
2024-11-05 03:59:25,611 - INFO - [diffusion][Epoch 11950] diffusion training Loss: 0.06482253409922123
2024-11-05 03:59:25,613 - INFO - [diffusion][Epoch 11950] diffusion learning rate: 0.001
2024-11-05 03:59:25,614 - INFO - [diffusion][Epoch 11950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:25,615 - INFO - [diffusion][Epoch 11951] Epoch 11952/12000
2024-11-05 03:59:28,661 - INFO - [diffusion][Epoch 11951] diffusion training Loss: 0.06576864421367645
2024-11-05 03:59:28,663 - INFO - [diffusion][Epoch 11951] diffusion learning rate: 0.001
2024-11-05 03:59:28,692 - INFO - [diffusion][Epoch 11951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:28,693 - INFO - [diffusion][Epoch 11952] Epoch 11953/12000
2024-11-05 03:59:31,799 - INFO - [diffusion][Epoch 11952] diffusion training Loss: 0.057824754156172276
2024-11-05 03:59:31,801 - INFO - [diffusion][Epoch 11952] diffusion learning rate: 0.001
2024-11-05 03:59:31,803 - INFO - [diffusion][Epoch 11952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:31,804 - INFO - [diffusion][Epoch 11953] Epoch 11954/12000
2024-11-05 03:59:35,003 - INFO - [diffusion][Epoch 11953] diffusion training Loss: 0.05813897680491209
2024-11-05 03:59:35,004 - INFO - [diffusion][Epoch 11953] diffusion learning rate: 0.001
2024-11-05 03:59:35,006 - INFO - [diffusion][Epoch 11953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:35,007 - INFO - [diffusion][Epoch 11954] Epoch 11955/12000
2024-11-05 03:59:38,653 - INFO - [diffusion][Epoch 11954] diffusion training Loss: 0.06370543502271175
2024-11-05 03:59:38,655 - INFO - [diffusion][Epoch 11954] diffusion learning rate: 0.001
2024-11-05 03:59:38,657 - INFO - [diffusion][Epoch 11954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:38,658 - INFO - [diffusion][Epoch 11955] Epoch 11956/12000
2024-11-05 03:59:42,823 - INFO - [diffusion][Epoch 11955] diffusion training Loss: 0.05976650584489107
2024-11-05 03:59:42,824 - INFO - [diffusion][Epoch 11955] diffusion learning rate: 0.001
2024-11-05 03:59:42,826 - INFO - [diffusion][Epoch 11955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:42,827 - INFO - [diffusion][Epoch 11956] Epoch 11957/12000
2024-11-05 03:59:46,276 - INFO - [diffusion][Epoch 11956] diffusion training Loss: 0.0633530979976058
2024-11-05 03:59:46,278 - INFO - [diffusion][Epoch 11956] diffusion learning rate: 0.001
2024-11-05 03:59:46,280 - INFO - [diffusion][Epoch 11956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:46,281 - INFO - [diffusion][Epoch 11957] Epoch 11958/12000
2024-11-05 03:59:49,480 - INFO - [diffusion][Epoch 11957] diffusion training Loss: 0.06242590118199587
2024-11-05 03:59:49,482 - INFO - [diffusion][Epoch 11957] diffusion learning rate: 0.001
2024-11-05 03:59:49,484 - INFO - [diffusion][Epoch 11957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:49,485 - INFO - [diffusion][Epoch 11958] Epoch 11959/12000
2024-11-05 03:59:52,592 - INFO - [diffusion][Epoch 11958] diffusion training Loss: 0.06457009725272655
2024-11-05 03:59:52,594 - INFO - [diffusion][Epoch 11958] diffusion learning rate: 0.001
2024-11-05 03:59:52,596 - INFO - [diffusion][Epoch 11958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:52,597 - INFO - [diffusion][Epoch 11959] Epoch 11960/12000
2024-11-05 03:59:55,906 - INFO - [diffusion][Epoch 11959] diffusion training Loss: 0.06172929331660271
2024-11-05 03:59:55,908 - INFO - [diffusion][Epoch 11959] diffusion learning rate: 0.001
2024-11-05 03:59:55,909 - INFO - [diffusion][Epoch 11959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:55,911 - INFO - [diffusion][Epoch 11960] Epoch 11961/12000
2024-11-05 03:59:59,608 - INFO - [diffusion][Epoch 11960] diffusion training Loss: 0.059956686571240425
2024-11-05 03:59:59,610 - INFO - [diffusion][Epoch 11960] diffusion learning rate: 0.001
2024-11-05 03:59:59,612 - INFO - [diffusion][Epoch 11960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:59,613 - INFO - [diffusion][Epoch 11961] Epoch 11962/12000
2024-11-05 04:00:02,946 - INFO - [diffusion][Epoch 11961] diffusion training Loss: 0.06503328122198582
2024-11-05 04:00:02,948 - INFO - [diffusion][Epoch 11961] diffusion learning rate: 0.001
2024-11-05 04:00:02,950 - INFO - [diffusion][Epoch 11961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:02,951 - INFO - [diffusion][Epoch 11962] Epoch 11963/12000
2024-11-05 04:00:06,119 - INFO - [diffusion][Epoch 11962] diffusion training Loss: 0.06602734699845314
2024-11-05 04:00:06,121 - INFO - [diffusion][Epoch 11962] diffusion learning rate: 0.001
2024-11-05 04:00:06,123 - INFO - [diffusion][Epoch 11962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:06,124 - INFO - [diffusion][Epoch 11963] Epoch 11964/12000
2024-11-05 04:00:09,249 - INFO - [diffusion][Epoch 11963] diffusion training Loss: 0.06350967846810818
2024-11-05 04:00:09,251 - INFO - [diffusion][Epoch 11963] diffusion learning rate: 0.001
2024-11-05 04:00:09,253 - INFO - [diffusion][Epoch 11963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:09,254 - INFO - [diffusion][Epoch 11964] Epoch 11965/12000
2024-11-05 04:00:12,753 - INFO - [diffusion][Epoch 11964] diffusion training Loss: 0.06381706707179546
2024-11-05 04:00:12,757 - INFO - [diffusion][Epoch 11964] diffusion learning rate: 0.001
2024-11-05 04:00:12,759 - INFO - [diffusion][Epoch 11964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:12,760 - INFO - [diffusion][Epoch 11965] Epoch 11966/12000
2024-11-05 04:00:16,396 - INFO - [diffusion][Epoch 11965] diffusion training Loss: 0.0651963111013174
2024-11-05 04:00:16,399 - INFO - [diffusion][Epoch 11965] diffusion learning rate: 0.001
2024-11-05 04:00:16,400 - INFO - [diffusion][Epoch 11965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:16,402 - INFO - [diffusion][Epoch 11966] Epoch 11967/12000
2024-11-05 04:00:19,757 - INFO - [diffusion][Epoch 11966] diffusion training Loss: 0.06441460270434618
2024-11-05 04:00:19,759 - INFO - [diffusion][Epoch 11966] diffusion learning rate: 0.001
2024-11-05 04:00:19,761 - INFO - [diffusion][Epoch 11966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:19,762 - INFO - [diffusion][Epoch 11967] Epoch 11968/12000
2024-11-05 04:00:22,938 - INFO - [diffusion][Epoch 11967] diffusion training Loss: 0.05713975615799427
2024-11-05 04:00:22,940 - INFO - [diffusion][Epoch 11967] diffusion learning rate: 0.001
2024-11-05 04:00:22,942 - INFO - [diffusion][Epoch 11967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:22,943 - INFO - [diffusion][Epoch 11968] Epoch 11969/12000
2024-11-05 04:00:26,081 - INFO - [diffusion][Epoch 11968] diffusion training Loss: 0.05974853876978159
2024-11-05 04:00:26,083 - INFO - [diffusion][Epoch 11968] diffusion learning rate: 0.001
2024-11-05 04:00:26,085 - INFO - [diffusion][Epoch 11968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:26,086 - INFO - [diffusion][Epoch 11969] Epoch 11970/12000
2024-11-05 04:00:29,499 - INFO - [diffusion][Epoch 11969] diffusion training Loss: 0.05831340979784727
2024-11-05 04:00:29,501 - INFO - [diffusion][Epoch 11969] diffusion learning rate: 0.001
2024-11-05 04:00:29,503 - INFO - [diffusion][Epoch 11969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:29,504 - INFO - [diffusion][Epoch 11970] Epoch 11971/12000
2024-11-05 04:00:33,241 - INFO - [diffusion][Epoch 11970] diffusion training Loss: 0.06044723745435476
2024-11-05 04:00:33,243 - INFO - [diffusion][Epoch 11970] diffusion learning rate: 0.001
2024-11-05 04:00:33,245 - INFO - [diffusion][Epoch 11970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:33,246 - INFO - [diffusion][Epoch 11971] Epoch 11972/12000
2024-11-05 04:00:36,485 - INFO - [diffusion][Epoch 11971] diffusion training Loss: 0.0663424376398325
2024-11-05 04:00:36,487 - INFO - [diffusion][Epoch 11971] diffusion learning rate: 0.001
2024-11-05 04:00:36,489 - INFO - [diffusion][Epoch 11971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:36,490 - INFO - [diffusion][Epoch 11972] Epoch 11973/12000
2024-11-05 04:00:39,482 - INFO - [diffusion][Epoch 11972] diffusion training Loss: 0.06207433994859457
2024-11-05 04:00:39,484 - INFO - [diffusion][Epoch 11972] diffusion learning rate: 0.001
2024-11-05 04:00:39,485 - INFO - [diffusion][Epoch 11972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:39,486 - INFO - [diffusion][Epoch 11973] Epoch 11974/12000
2024-11-05 04:00:42,753 - INFO - [diffusion][Epoch 11973] diffusion training Loss: 0.06455410458147526
2024-11-05 04:00:42,754 - INFO - [diffusion][Epoch 11973] diffusion learning rate: 0.001
2024-11-05 04:00:42,756 - INFO - [diffusion][Epoch 11973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:42,757 - INFO - [diffusion][Epoch 11974] Epoch 11975/12000
2024-11-05 04:00:46,446 - INFO - [diffusion][Epoch 11974] diffusion training Loss: 0.06650243140757084
2024-11-05 04:00:46,448 - INFO - [diffusion][Epoch 11974] diffusion learning rate: 0.001
2024-11-05 04:00:46,449 - INFO - [diffusion][Epoch 11974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:46,450 - INFO - [diffusion][Epoch 11975] Epoch 11976/12000
2024-11-05 04:00:50,006 - INFO - [diffusion][Epoch 11975] diffusion training Loss: 0.06767277419567108
2024-11-05 04:00:50,008 - INFO - [diffusion][Epoch 11975] diffusion learning rate: 0.001
2024-11-05 04:00:50,010 - INFO - [diffusion][Epoch 11975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:50,011 - INFO - [diffusion][Epoch 11976] Epoch 11977/12000
2024-11-05 04:00:53,061 - INFO - [diffusion][Epoch 11976] diffusion training Loss: 0.06451693270355463
2024-11-05 04:00:53,063 - INFO - [diffusion][Epoch 11976] diffusion learning rate: 0.001
2024-11-05 04:00:53,065 - INFO - [diffusion][Epoch 11976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:53,066 - INFO - [diffusion][Epoch 11977] Epoch 11978/12000
2024-11-05 04:00:55,995 - INFO - [diffusion][Epoch 11977] diffusion training Loss: 0.06151226069778204
2024-11-05 04:00:55,997 - INFO - [diffusion][Epoch 11977] diffusion learning rate: 0.001
2024-11-05 04:00:55,999 - INFO - [diffusion][Epoch 11977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:56,000 - INFO - [diffusion][Epoch 11978] Epoch 11979/12000
2024-11-05 04:00:59,299 - INFO - [diffusion][Epoch 11978] diffusion training Loss: 0.06555063836276531
2024-11-05 04:00:59,301 - INFO - [diffusion][Epoch 11978] diffusion learning rate: 0.001
2024-11-05 04:00:59,303 - INFO - [diffusion][Epoch 11978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:59,304 - INFO - [diffusion][Epoch 11979] Epoch 11980/12000
2024-11-05 04:01:02,906 - INFO - [diffusion][Epoch 11979] diffusion training Loss: 0.06557792890816927
2024-11-05 04:01:02,908 - INFO - [diffusion][Epoch 11979] diffusion learning rate: 0.001
2024-11-05 04:01:02,910 - INFO - [diffusion][Epoch 11979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:02,911 - INFO - [diffusion][Epoch 11980] Epoch 11981/12000
2024-11-05 04:01:06,173 - INFO - [diffusion][Epoch 11980] diffusion training Loss: 0.060502526350319386
2024-11-05 04:01:06,175 - INFO - [diffusion][Epoch 11980] diffusion learning rate: 0.001
2024-11-05 04:01:06,177 - INFO - [diffusion][Epoch 11980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:06,178 - INFO - [diffusion][Epoch 11981] Epoch 11982/12000
2024-11-05 04:01:09,350 - INFO - [diffusion][Epoch 11981] diffusion training Loss: 0.06429149024188519
2024-11-05 04:01:09,352 - INFO - [diffusion][Epoch 11981] diffusion learning rate: 0.001
2024-11-05 04:01:09,354 - INFO - [diffusion][Epoch 11981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:09,355 - INFO - [diffusion][Epoch 11982] Epoch 11983/12000
2024-11-05 04:01:12,376 - INFO - [diffusion][Epoch 11982] diffusion training Loss: 0.059250191785395145
2024-11-05 04:01:12,624 - INFO - [diffusion][Epoch 11982] diffusion learning rate: 0.001
2024-11-05 04:01:12,627 - INFO - [diffusion][Epoch 11982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:12,629 - INFO - [diffusion][Epoch 11983] Epoch 11984/12000
2024-11-05 04:01:15,958 - INFO - [diffusion][Epoch 11983] diffusion training Loss: 0.06323564611375332
2024-11-05 04:01:15,961 - INFO - [diffusion][Epoch 11983] diffusion learning rate: 0.001
2024-11-05 04:01:15,962 - INFO - [diffusion][Epoch 11983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:15,963 - INFO - [diffusion][Epoch 11984] Epoch 11985/12000
2024-11-05 04:01:19,632 - INFO - [diffusion][Epoch 11984] diffusion training Loss: 0.06468790862709284
2024-11-05 04:01:19,633 - INFO - [diffusion][Epoch 11984] diffusion learning rate: 0.001
2024-11-05 04:01:19,635 - INFO - [diffusion][Epoch 11984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:19,636 - INFO - [diffusion][Epoch 11985] Epoch 11986/12000
2024-11-05 04:01:23,009 - INFO - [diffusion][Epoch 11985] diffusion training Loss: 0.05940375383943319
2024-11-05 04:01:23,011 - INFO - [diffusion][Epoch 11985] diffusion learning rate: 0.001
2024-11-05 04:01:23,013 - INFO - [diffusion][Epoch 11985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:23,014 - INFO - [diffusion][Epoch 11986] Epoch 11987/12000
2024-11-05 04:01:26,147 - INFO - [diffusion][Epoch 11986] diffusion training Loss: 0.060080645605921745
2024-11-05 04:01:26,148 - INFO - [diffusion][Epoch 11986] diffusion learning rate: 0.001
2024-11-05 04:01:26,150 - INFO - [diffusion][Epoch 11986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:26,151 - INFO - [diffusion][Epoch 11987] Epoch 11988/12000
2024-11-05 04:01:29,252 - INFO - [diffusion][Epoch 11987] diffusion training Loss: 0.05971748288720846
2024-11-05 04:01:29,254 - INFO - [diffusion][Epoch 11987] diffusion learning rate: 0.001
2024-11-05 04:01:29,256 - INFO - [diffusion][Epoch 11987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:29,257 - INFO - [diffusion][Epoch 11988] Epoch 11989/12000
2024-11-05 04:01:32,683 - INFO - [diffusion][Epoch 11988] diffusion training Loss: 0.06486259773373604
2024-11-05 04:01:32,685 - INFO - [diffusion][Epoch 11988] diffusion learning rate: 0.001
2024-11-05 04:01:32,687 - INFO - [diffusion][Epoch 11988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:32,688 - INFO - [diffusion][Epoch 11989] Epoch 11990/12000
2024-11-05 04:01:36,297 - INFO - [diffusion][Epoch 11989] diffusion training Loss: 0.05944175459444523
2024-11-05 04:01:36,299 - INFO - [diffusion][Epoch 11989] diffusion learning rate: 0.001
2024-11-05 04:01:36,300 - INFO - [diffusion][Epoch 11989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:36,301 - INFO - [diffusion][Epoch 11990] Epoch 11991/12000
2024-11-05 04:01:39,372 - INFO - [diffusion][Epoch 11990] diffusion training Loss: 0.06166518107056618
2024-11-05 04:01:39,374 - INFO - [diffusion][Epoch 11990] diffusion learning rate: 0.001
[INFO|configuration_utils.py:728] 2024-11-05 04:02:05,689 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:05,691 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:02:05,741 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:05,742 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:05,747 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:05,747 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:05,747 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:05,747 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:05,747 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:05,747 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:02:05,748 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:05,749 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:02:05,920 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:02:06,912 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:02:06,912 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/1821 [00:00<?, ? examples/s]The tensors are equal.
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0

Map:  55% 1000/1821 [00:00<00:00, 5550.86 examples/s]
Map: 100% 1821/1821 [00:00<00:00, 6046.90 examples/s]
Map: 100% 1821/1821 [00:00<00:00, 5737.87 examples/s]
[INFO|trainer.py:602] 2024-11-05 04:02:08,365 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:02:08,367 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:02:08,370 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:02:08,370 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:02:08,370 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
[INFO|training_args.py:1902] 2024-11-05 04:02:14,740 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:02:14,741 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:02:14,742 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:02:17,840 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:17,841 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:02:17,894 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:17,895 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:17,897 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:17,897 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:17,897 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:17,897 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:17,897 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:17,897 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:02:17,898 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:17,898 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:02:18,050 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:02:19,009 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:02:19,009 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:02:19,932 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:02:19,934 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:02:19,936 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:02:19,936 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:02:19,936 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]
{'eval_loss': 0.20098212361335754, 'eval_accuracy': 0.9369266055045872, 'eval_runtime': 6.3176, 'eval_samples_per_second': 138.027, 'eval_steps_per_second': 0.158}
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.31
  eval_samples            =        872
  eval_samples_per_second =    138.027
  eval_steps_per_second   =      0.158
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.92it/s]
[INFO|training_args.py:1902] 2024-11-05 04:02:21,022 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:02:21,022 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:02:21,023 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:02:24,076 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:24,077 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:02:24,136 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:24,137 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:24,140 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:24,140 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:24,140 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:24,140 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:24,140 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:24,140 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:02:24,140 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:24,141 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:02:24,290 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:02:25,247 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:02:25,247 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:02:26,161 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:02:26,163 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:02:26,166 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:02:26,166 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:02:26,166 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.02
  eval_samples            =        872
  eval_samples_per_second =    849.445
  eval_steps_per_second   =      0.974
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.58it/s]
[INFO|training_args.py:1902] 2024-11-05 04:02:26,677 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:02:26,677 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:02:26,678 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:02:29,581 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:29,582 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:02:29,630 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:29,631 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:29,633 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:29,633 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:29,633 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:29,633 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:29,633 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:29,633 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:02:29,634 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:29,635 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:02:29,759 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:02:31,135 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:02:31,135 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:02:31,981 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:02:31,983 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:02:31,986 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:02:31,986 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:02:31,986 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6534
  eval_loss               =     0.6462
  eval_runtime            = 0:00:00.44
  eval_samples            =        277
  eval_samples_per_second =    622.152
  eval_steps_per_second   =      2.246
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.73it/s]
[INFO|training_args.py:1902] 2024-11-05 04:02:32,466 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:02:32,467 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:02:32,468 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:02:35,735 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:35,736 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:02:35,788 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:35,789 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:35,791 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:35,792 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:35,792 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:35,792 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:35,792 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:35,792 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:02:35,792 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:35,793 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:02:35,972 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:02:36,960 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:02:36,960 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:02:37,904 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:02:37,907 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:02:37,909 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:02:37,910 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:02:37,910 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6606
  eval_loss               =     0.6462
  eval_runtime            = 0:00:00.41
  eval_samples            =        277
  eval_samples_per_second =    665.089
  eval_steps_per_second   =      2.401
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.01it/s]
[INFO|training_args.py:1902] 2024-11-05 04:02:38,578 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:02:38,578 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:02:38,579 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:02:41,958 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:41,959 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:02:42,014 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:42,015 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:42,017 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:42,017 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:42,017 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:42,017 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:42,017 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:42,017 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:02:42,017 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:42,018 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:02:42,141 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:02:43,082 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:02:43,082 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:02:43,738 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:02:43,740 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:02:43,743 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:02:43,743 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:02:43,743 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4116
  eval_runtime            = 0:00:00.60
  eval_samples            =        408
  eval_samples_per_second =    669.083
  eval_steps_per_second   =       1.64
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

100% 1/1 [00:00<00:00, 37.15it/s]
[INFO|training_args.py:1902] 2024-11-05 04:02:44,429 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:02:44,430 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:02:44,431 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:02:47,320 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:47,321 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:02:47,367 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:47,368 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:47,370 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:47,371 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:47,371 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:47,371 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:47,371 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:47,371 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:02:47,371 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:47,372 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:02:47,501 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:02:48,493 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:02:48,493 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:02:49,271 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:02:49,273 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:02:49,275 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:02:49,276 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:02:49,276 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4117
  eval_runtime            = 0:00:00.62
  eval_samples            =        408
  eval_samples_per_second =    651.432
  eval_steps_per_second   =      1.597
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.34it/s]
[INFO|training_args.py:1902] 2024-11-05 04:02:50,906 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:02:50,906 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:02:50,907 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:02:54,036 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:54,040 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:02:54,087 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:54,088 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:54,090 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:54,091 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:54,091 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:54,091 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:54,091 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:02:54,091 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:02:54,091 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:02:54,092 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:02:54,239 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:02:55,180 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:02:55,180 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:02:55,904 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:02:55,906 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:02:55,909 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:02:55,909 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:02:55,909 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4912
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.56
  eval_samples              =       1043
  eval_samples_per_second   =    666.489
  eval_steps_per_second     =      0.639
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.30it/s]
[INFO|training_args.py:1902] 2024-11-05 04:02:57,190 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:02:57,190 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:02:57,191 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:03:00,277 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:00,278 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:03:00,329 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:00,330 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:00,332 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:00,332 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:00,332 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:00,332 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:00,332 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:00,332 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:03:00,333 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:00,334 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:03:00,460 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:03:01,421 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:03:01,422 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:03:02,368 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:03:02,370 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:03:02,372 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:03:02,372 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:03:02,372 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4912
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.22
  eval_samples              =       1043
  eval_samples_per_second   =    853.994
  eval_steps_per_second     =      0.819
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.06s/it]
100% 3/3 [00:03<00:00,  1.17s/it]
100% 3/3 [00:03<00:00,  1.16s/it]
[INFO|training_args.py:1902] 2024-11-05 04:03:08,356 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:03:08,357 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:03:08,358 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:03:11,988 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:11,989 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:03:12,037 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:12,038 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:12,042 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:12,042 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:12,042 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:12,042 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:12,042 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:12,042 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:03:12,042 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:12,043 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:03:12,203 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:03:13,158 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:03:13,158 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:03:13,957 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:03:13,959 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:03:13,962 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:03:13,962 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:03:13,962 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:05.91
  eval_samples            =       5463
  eval_samples_per_second =     923.67
  eval_steps_per_second   =      0.507
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.06s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
100% 3/3 [00:03<00:00,  1.15s/it]
[INFO|training_args.py:1902] 2024-11-05 04:03:20,206 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:03:20,207 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:03:20,208 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:03:23,244 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:23,245 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:03:23,299 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:23,300 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:23,302 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:23,302 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:23,302 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:23,302 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:23,302 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:23,302 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:03:23,303 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:23,304 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:03:23,472 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:03:24,440 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:03:24,441 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:03:25,440 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:03:25,442 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:03:25,444 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:03:25,444 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:03:25,444 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.16
  eval_samples            =       5463
  eval_samples_per_second =      886.4
  eval_steps_per_second   =      0.487
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 23.99it/s]
[INFO|training_args.py:1902] 2024-11-05 04:03:27,264 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:03:27,264 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:03:27,265 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:03:30,364 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:30,365 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:03:30,430 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:30,431 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:30,433 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:30,433 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:30,433 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:30,433 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:30,433 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:03:30,433 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:03:30,434 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:03:30,435 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:03:30,577 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:03:31,566 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:03:31,567 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:03:33,057 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:03:33,059 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:03:33,062 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:03:33,062 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:03:33,062 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6197
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.75
  eval_samples            =       1500
  eval_samples_per_second =    854.082
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.569
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 29.02it/s]
2024-11-05 04:03:34,816 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:03:34,819 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models rte: 0.6606498194945848
2024-11-05 04:03:34,821 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models mrpc: 0.8683274021352313
2024-11-05 04:03:34,822 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models cola: 0.49626557567073315
2024-11-05 04:03:34,823 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:03:34,824 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models stsb: 0.8719049209859462
2024-11-05 04:03:34,825 - INFO - [diffusion][Epoch 11990] average DIFF reconstruction auto_encoder_models accuracy: 0.7916225087236075
2024-11-05 04:03:34,826 - INFO - [diffusion][Epoch 11990] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6534296028880866), ('rte', 0.6606498194945848), ('mrpc', 0.8683274021352313), ('mrpc', 0.8683274021352313), ('cola', 0.49626557567073315), ('cola', 0.49626557567073315), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.8718963475766003), ('stsb', 0.8719049209859462)]
2024-11-05 04:03:34,828 - INFO - [diffusion][Epoch 11990] ---------------------------------
2024-11-05 04:03:34,911 - INFO - [diffusion][Epoch 11990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:34,913 - INFO - [diffusion][Epoch 11991] Epoch 11992/12000
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =       0.62
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.69
  eval_samples            =       1500
  eval_samples_per_second =    885.189
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =       0.59
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:03:38,179 - INFO - [diffusion][Epoch 11991] diffusion training Loss: 0.06819767691195011
2024-11-05 04:03:38,181 - INFO - [diffusion][Epoch 11991] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:04:00,928 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:04:00,929 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:04:00,931 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:04:03,924 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:03,925 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:04:03,989 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:03,990 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:03,993 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:03,993 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:03,993 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:03,993 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:03,993 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:03,993 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:04:03,993 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:03,994 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:04:04,164 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:04:05,152 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:04:05,152 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:04:06,036 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:04:06,039 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:04:06,042 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:04:06,042 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:04:06,042 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.49it/s]
[INFO|training_args.py:1902] 2024-11-05 04:04:07,259 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:04:07,260 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:04:07,261 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:04:10,295 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:10,296 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:04:10,342 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:10,343 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:10,345 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:10,345 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:10,345 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:10,345 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:10,345 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:10,345 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:04:10,346 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:10,347 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:04:10,476 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:04:11,573 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:04:11,573 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:04:12,708 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:04:12,711 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:04:12,713 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:04:12,713 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:04:12,713 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.14
  eval_samples            =        872
  eval_samples_per_second =    761.496
  eval_steps_per_second   =      0.873
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.91it/s]
[INFO|training_args.py:1902] 2024-11-05 04:04:13,854 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:04:13,854 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:04:13,855 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:04:16,717 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:16,718 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:04:16,776 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:16,777 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:16,780 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:16,780 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:16,780 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:16,780 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:16,780 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:16,780 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:04:16,781 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:16,782 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:04:16,931 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:04:17,908 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:04:17,908 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:04:19,481 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:04:19,483 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:04:19,485 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:04:19,485 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:04:19,485 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.07
  eval_samples            =        872
  eval_samples_per_second =    808.196
  eval_steps_per_second   =      0.927
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.34it/s]
[INFO|training_args.py:1902] 2024-11-05 04:04:19,977 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:04:19,977 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:04:19,978 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:04:22,925 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:22,926 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:04:22,974 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:22,975 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:22,982 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:22,982 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:22,982 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:22,982 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:22,982 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:22,982 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:04:22,983 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:22,984 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:04:23,125 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:04:24,100 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:04:24,100 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:04:25,168 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:04:25,170 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:04:25,173 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:04:25,173 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:04:25,173 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6606
  eval_loss               =     0.6464
  eval_runtime            = 0:00:00.42
  eval_samples            =        277
  eval_samples_per_second =    646.469
  eval_steps_per_second   =      2.334
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.29it/s]
[INFO|training_args.py:1902] 2024-11-05 04:04:25,676 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:04:25,676 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:04:25,677 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:04:28,524 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:28,525 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:04:28,575 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:28,576 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:28,577 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:28,578 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:28,578 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:28,578 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:28,578 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:28,578 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:04:28,578 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:28,579 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:04:28,705 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:04:29,632 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:04:29,633 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:04:30,553 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:04:30,556 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:04:30,559 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:04:30,559 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:04:30,559 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6606
  eval_loss               =     0.6468
  eval_runtime            = 0:00:00.44
  eval_samples            =        277
  eval_samples_per_second =    628.035
  eval_steps_per_second   =      2.267
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.68it/s]
[INFO|training_args.py:1902] 2024-11-05 04:04:31,233 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:04:31,233 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:04:31,234 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:04:34,270 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:34,271 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:04:34,323 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:34,324 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:34,327 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:34,327 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:34,327 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:34,327 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:34,327 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:34,327 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:04:34,328 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:34,329 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:04:34,488 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:04:35,470 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:04:35,470 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:04:36,025 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:04:36,027 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:04:36,029 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:04:36,029 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:04:36,029 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8455
  eval_f1                 =     0.8699
  eval_loss               =     0.4112
  eval_runtime            = 0:00:00.60
  eval_samples            =        408
  eval_samples_per_second =    674.755
  eval_steps_per_second   =      1.654
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.58it/s]
[INFO|training_args.py:1902] 2024-11-05 04:04:36,987 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:04:36,987 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:04:36,988 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:04:39,648 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:39,649 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:04:39,707 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:39,708 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:39,710 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:39,711 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:39,711 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:39,711 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:39,711 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:39,711 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:04:39,711 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:39,712 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:04:39,859 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:04:40,810 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:04:40,810 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:04:41,626 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:04:41,628 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:04:41,631 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:04:41,631 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:04:41,631 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4117
  eval_runtime            = 0:00:00.89
  eval_samples            =        408
  eval_samples_per_second =    454.739
  eval_steps_per_second   =      1.115
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.71it/s]
[INFO|training_args.py:1902] 2024-11-05 04:04:42,853 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:04:42,853 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:04:42,854 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:04:45,814 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:45,816 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:04:45,873 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:45,875 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:45,877 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:45,877 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:45,877 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:45,877 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:45,877 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:45,877 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:04:45,878 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:45,879 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:04:46,022 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:04:46,974 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:04:46,974 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:04:47,715 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:04:47,717 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:04:47,720 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:04:47,720 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:04:47,720 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4911
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.16
  eval_samples              =       1043
  eval_samples_per_second   =    893.725
  eval_steps_per_second     =      0.857
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.22it/s]
[INFO|training_args.py:1902] 2024-11-05 04:04:49,043 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:04:49,043 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:04:49,044 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:04:51,627 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:51,629 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:04:51,728 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:51,729 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:51,731 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:51,731 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:51,731 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:51,731 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:51,731 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:04:51,731 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:04:51,732 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:04:51,733 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:04:51,873 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:04:52,855 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:04:52,856 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:04:53,831 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:04:53,833 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:04:53,836 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:04:53,836 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:04:53,836 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =      0.491
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.26
  eval_samples              =       1043
  eval_samples_per_second   =    826.929
  eval_steps_per_second     =      0.793
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.24s/it]
100% 3/3 [00:03<00:00,  1.26s/it]
100% 3/3 [00:03<00:00,  1.27s/it]
[INFO|training_args.py:1902] 2024-11-05 04:05:00,524 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:05:00,525 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:05:00,526 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:05:04,290 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:04,291 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:05:04,394 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:04,395 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:04,397 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:04,398 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:04,398 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:04,398 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:04,398 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:04,398 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:05:04,398 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:04,399 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:05:04,539 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:05:05,535 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:05:05,536 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:05:06,697 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:05:06,701 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:05:06,705 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:05:06,705 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:05:06,705 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.61
  eval_samples            =       5463
  eval_samples_per_second =    826.395
  eval_steps_per_second   =      0.454
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.00it/s]
100% 3/3 [00:03<00:00,  1.11s/it]
100% 3/3 [00:03<00:00,  1.23s/it]
[INFO|training_args.py:1902] 2024-11-05 04:05:12,887 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:05:12,888 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:05:12,889 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:05:16,919 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:16,921 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:05:16,972 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:16,974 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:16,977 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:16,977 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:16,977 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:16,977 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:16,977 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:16,977 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:05:16,978 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:16,979 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:05:17,168 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:05:18,108 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:05:18,108 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:05:18,986 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:05:18,989 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:05:18,991 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:05:18,991 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:05:18,991 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.10
  eval_samples            =       5463
  eval_samples_per_second =    894.319
  eval_steps_per_second   =      0.491
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.35it/s]
[INFO|training_args.py:1902] 2024-11-05 04:05:20,604 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:05:20,604 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:05:20,605 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:05:23,319 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:23,320 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:05:23,373 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:23,374 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:23,376 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:23,376 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:23,376 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:23,376 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:23,376 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:23,376 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:05:23,377 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:23,378 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:05:23,513 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:05:24,754 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:05:24,755 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:05:25,418 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:05:25,420 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:05:25,423 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:05:25,423 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:05:25,423 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6197
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.56
  eval_samples            =       1500
  eval_samples_per_second =    957.717
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.638
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 26.50it/s]
2024-11-05 04:05:27,253 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:05:27,255 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models rte: 0.6606498194945848
2024-11-05 04:05:27,256 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models mrpc: 0.8698752228163994
2024-11-05 04:05:27,257 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models cola: 0.49626557567073315
2024-11-05 04:05:27,260 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:05:27,262 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models stsb: 0.8719305031925891
2024-11-05 04:05:27,263 - INFO - [diffusion][Epoch 11991] average DIFF reconstruction auto_encoder_models accuracy: 0.7923549466481276
2024-11-05 04:05:27,264 - INFO - [diffusion][Epoch 11991] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6606498194945848), ('rte', 0.6606498194945848), ('mrpc', 0.8698752228163994), ('mrpc', 0.8683274021352313), ('cola', 0.49626557567073315), ('cola', 0.49626557567073315), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.8719305031925891), ('stsb', 0.8718919831765322)]
2024-11-05 04:05:27,265 - INFO - [diffusion][Epoch 11991] ---------------------------------
2024-11-05 04:05:27,357 - INFO - [diffusion][Epoch 11991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:27,359 - INFO - [diffusion][Epoch 11992] Epoch 11993/12000
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =       0.62
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.76
  eval_samples            =       1500
  eval_samples_per_second =     849.69
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.566
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:05:30,838 - INFO - [diffusion][Epoch 11992] diffusion training Loss: 0.05627422593533993
2024-11-05 04:05:30,840 - INFO - [diffusion][Epoch 11992] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:05:53,272 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:05:53,273 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:05:53,274 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:05:55,979 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:55,980 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:05:56,033 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:56,034 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:56,036 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:56,036 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:56,036 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:56,036 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:56,036 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:05:56,037 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:05:56,037 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:05:56,038 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:05:56,194 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:05:57,203 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:05:57,203 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:05:57,986 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:05:57,989 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:05:57,992 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:05:57,992 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:05:57,992 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.99it/s]
[INFO|training_args.py:1902] 2024-11-05 04:05:59,108 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:05:59,108 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:05:59,109 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:06:02,035 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:02,037 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:06:02,091 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:02,092 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:02,094 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:02,094 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:02,094 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:02,094 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:02,094 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:02,094 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:06:02,094 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:02,096 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:06:02,244 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:06:03,224 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:06:03,224 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:06:04,367 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:06:04,370 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:06:04,372 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:06:04,372 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:06:04,373 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.05
  eval_samples            =        872
  eval_samples_per_second =    827.305
  eval_steps_per_second   =      0.949
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.36it/s]
[INFO|training_args.py:1902] 2024-11-05 04:06:05,495 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:06:05,496 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:06:05,496 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:06:09,185 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:09,186 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:06:09,232 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:09,233 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:09,235 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:09,235 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:09,235 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:09,235 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:09,235 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:09,235 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:06:09,235 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:09,236 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:06:09,392 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:06:10,694 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:06:10,694 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:06:11,564 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:06:11,565 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:06:11,567 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:06:11,567 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:06:11,567 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.06
  eval_samples            =        872
  eval_samples_per_second =    821.143
  eval_steps_per_second   =      0.942
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 19.98it/s]
[INFO|training_args.py:1902] 2024-11-05 04:06:12,042 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:06:12,042 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:06:12,043 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:06:15,571 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:15,572 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:06:15,619 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:15,620 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:15,621 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:15,622 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:15,622 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:15,622 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:15,622 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:15,622 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:06:15,622 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:15,623 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:06:15,747 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:06:16,687 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:06:16,687 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:06:17,603 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:06:17,605 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:06:17,608 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:06:17,608 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:06:17,608 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6606
  eval_loss               =     0.6463
  eval_runtime            = 0:00:00.41
  eval_samples            =        277
  eval_samples_per_second =    664.179
  eval_steps_per_second   =      2.398
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 42.76it/s]
[INFO|training_args.py:1902] 2024-11-05 04:06:18,120 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:06:18,120 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:06:18,121 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:06:21,011 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:21,012 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:06:21,064 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:21,065 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:21,069 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:21,069 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:21,069 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:21,069 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:21,069 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:21,069 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:06:21,070 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:21,071 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:06:21,222 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:06:22,168 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:06:22,168 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:06:23,091 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:06:23,093 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:06:23,096 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:06:23,096 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:06:23,096 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6606
  eval_loss               =     0.6465
  eval_runtime            = 0:00:00.44
  eval_samples            =        277
  eval_samples_per_second =    617.318
  eval_steps_per_second   =      2.229
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.00it/s]
[INFO|training_args.py:1902] 2024-11-05 04:06:23,767 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:06:23,767 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:06:23,768 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:06:27,034 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:27,036 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:06:27,101 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:27,102 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:27,104 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:27,104 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:27,104 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:27,104 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:27,104 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:27,104 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:06:27,105 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:27,106 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:06:27,260 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:06:28,214 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:06:28,214 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:06:29,036 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:06:29,038 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:06:29,041 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:06:29,041 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:06:29,041 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4115
  eval_runtime            = 0:00:00.60
  eval_samples            =        408
  eval_samples_per_second =    677.415
  eval_steps_per_second   =       1.66
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.92it/s]
[INFO|training_args.py:1902] 2024-11-05 04:06:29,993 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:06:29,993 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:06:29,994 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:06:34,173 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:34,174 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:06:34,247 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:34,248 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:34,250 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:34,250 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:34,250 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:34,250 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:34,250 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:34,250 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:06:34,251 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:34,252 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:06:34,377 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:06:35,344 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:06:35,344 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:06:36,512 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:06:36,514 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:06:36,517 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:06:36,517 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:06:36,517 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4117
  eval_runtime            = 0:00:00.88
  eval_samples            =        408
  eval_samples_per_second =    460.818
  eval_steps_per_second   =      1.129
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.84it/s]
[INFO|training_args.py:1902] 2024-11-05 04:06:37,810 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:06:37,810 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:06:37,811 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:06:40,977 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:40,978 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:06:41,027 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:41,028 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:41,030 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:41,030 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:41,030 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:41,030 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:41,030 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:41,030 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:06:41,031 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:41,031 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:06:41,153 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:06:42,107 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:06:42,107 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:06:42,928 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:06:42,931 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:06:42,934 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:06:42,934 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:06:42,934 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4911
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.23
  eval_samples              =       1043
  eval_samples_per_second   =    847.416
  eval_steps_per_second     =      0.812
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.86it/s]
[INFO|training_args.py:1902] 2024-11-05 04:06:44,230 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:06:44,230 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:06:44,231 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:06:47,043 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:47,044 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:06:47,109 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:47,110 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:47,112 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:47,112 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:47,112 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:47,112 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:47,112 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:47,112 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:06:47,112 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:47,113 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:06:47,238 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:06:48,254 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:06:48,254 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:06:49,238 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:06:49,240 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:06:49,242 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:06:49,242 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:06:49,242 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4911
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.25
  eval_samples              =       1043
  eval_samples_per_second   =    834.257
  eval_steps_per_second     =        0.8
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.10s/it]
100% 3/3 [00:03<00:00,  1.17s/it]
100% 3/3 [00:03<00:00,  1.17s/it]
[INFO|training_args.py:1902] 2024-11-05 04:06:55,711 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:06:55,712 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:06:55,713 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:06:58,512 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:58,513 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:06:58,568 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:58,569 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:58,571 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:58,571 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:58,571 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:58,571 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:58,571 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:06:58,571 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:06:58,572 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:06:58,573 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:06:58,718 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:06:59,693 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:06:59,693 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:07:00,455 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:07:00,457 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:07:00,459 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:07:00,459 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:07:00,459 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.39
  eval_samples            =       5463
  eval_samples_per_second =    853.973
  eval_steps_per_second   =      0.469
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.01s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
[INFO|training_args.py:1902] 2024-11-05 04:07:06,580 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:07:06,581 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:07:06,582 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:07:09,551 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:09,552 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:07:09,615 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:09,616 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:09,618 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:09,618 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:09,618 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:09,618 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:09,618 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:09,618 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:07:09,618 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:09,619 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:07:09,748 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:07:10,716 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:07:10,716 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:07:11,729 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:07:11,735 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:07:11,737 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:07:11,738 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:07:11,738 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.05
  eval_samples            =       5463
  eval_samples_per_second =    901.939
  eval_steps_per_second   =      0.495
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.57it/s]
[INFO|training_args.py:1902] 2024-11-05 04:07:13,499 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:07:13,500 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:07:13,500 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:07:16,540 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:16,542 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:07:16,597 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:16,598 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:16,600 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:16,600 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:16,600 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:16,600 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:16,600 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:16,600 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:07:16,601 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:16,602 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:07:16,725 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:07:17,680 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:07:17,680 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:07:18,669 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:07:18,671 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:07:18,674 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:07:18,674 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:07:18,674 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6197
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.70
  eval_samples            =       1500
  eval_samples_per_second =    881.125
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.587
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 27.61it/s]
2024-11-05 04:07:20,434 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:07:20,437 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models rte: 0.6606498194945848
2024-11-05 04:07:20,438 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models mrpc: 0.8683274021352313
2024-11-05 04:07:20,439 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models cola: 0.49626557567073315
2024-11-05 04:07:20,440 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:07:20,441 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models stsb: 0.8719448130131456
2024-11-05 04:07:20,443 - INFO - [diffusion][Epoch 11992] average DIFF reconstruction auto_encoder_models accuracy: 0.7922268260691453
2024-11-05 04:07:20,444 - INFO - [diffusion][Epoch 11992] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6606498194945848), ('rte', 0.6606498194945848), ('mrpc', 0.8683274021352313), ('mrpc', 0.8683274021352313), ('cola', 0.49626557567073315), ('cola', 0.49626557567073315), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.8719448130131456), ('stsb', 0.8718880470893552)]
2024-11-05 04:07:20,445 - INFO - [diffusion][Epoch 11992] ---------------------------------
2024-11-05 04:07:20,532 - INFO - [diffusion][Epoch 11992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:20,534 - INFO - [diffusion][Epoch 11993] Epoch 11994/12000
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =       0.62
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.71
  eval_samples            =       1500
  eval_samples_per_second =     876.68
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.584
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:07:24,280 - INFO - [diffusion][Epoch 11993] diffusion training Loss: 0.04984010569751263
2024-11-05 04:07:24,282 - INFO - [diffusion][Epoch 11993] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:07:46,418 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:07:46,419 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:07:46,420 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:07:50,014 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:50,015 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:07:50,079 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:50,080 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:50,084 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:50,084 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:50,084 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:50,084 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:50,084 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:50,084 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:07:50,084 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:50,085 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:07:50,232 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:07:51,186 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:07:51,186 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:07:52,052 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:07:52,055 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:07:52,057 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:07:52,057 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:07:52,057 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.21it/s]
[INFO|training_args.py:1902] 2024-11-05 04:07:53,180 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:07:53,181 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:07:53,181 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:07:56,454 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:56,455 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:07:56,509 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:56,510 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:56,512 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:56,512 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:56,512 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:56,512 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:56,512 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:07:56,512 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:07:56,513 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:07:56,514 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:07:56,637 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:07:57,604 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:07:57,605 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:07:58,550 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:07:58,552 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:07:58,554 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:07:58,555 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:07:58,555 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.06
  eval_samples            =        872
  eval_samples_per_second =    820.436
  eval_steps_per_second   =      0.941
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.15it/s]
[INFO|training_args.py:1902] 2024-11-05 04:07:59,651 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:07:59,651 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:07:59,652 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:08:04,032 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:04,033 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:08:04,127 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:04,128 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:04,130 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:04,130 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:04,130 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:04,130 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:04,130 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:04,130 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:08:04,130 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:04,131 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:08:04,263 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:08:05,318 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:08:05,318 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:08:06,569 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:08:06,571 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:08:06,574 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:08:06,574 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:08:06,574 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.03
  eval_samples            =        872
  eval_samples_per_second =     840.46
  eval_steps_per_second   =      0.964
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.66it/s]
[INFO|training_args.py:1902] 2024-11-05 04:08:07,069 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:08:07,069 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:08:07,070 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:08:10,631 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:10,633 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:08:10,692 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:10,693 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:10,695 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:10,695 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:10,695 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:10,695 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:10,695 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:10,695 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:08:10,696 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:10,697 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:08:10,818 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:08:11,788 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:08:11,788 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:08:13,958 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:08:13,960 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:08:13,966 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:08:13,966 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:08:13,966 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6606
  eval_loss               =     0.6463
  eval_runtime            = 0:00:00.43
  eval_samples            =        277
  eval_samples_per_second =    636.322
  eval_steps_per_second   =      2.297
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.94it/s]
[INFO|training_args.py:1902] 2024-11-05 04:08:14,480 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:08:14,480 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:08:14,481 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:08:17,490 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:17,491 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:08:17,547 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:17,547 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:17,549 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:17,549 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:17,550 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:17,550 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:17,550 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:17,550 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:08:17,550 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:17,551 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:08:17,672 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:08:18,637 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:08:18,637 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:08:19,232 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:08:19,234 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:08:19,237 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:08:19,237 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:08:19,237 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.657
  eval_loss               =     0.6461
  eval_runtime            = 0:00:00.45
  eval_samples            =        277
  eval_samples_per_second =    604.709
  eval_steps_per_second   =      2.183
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  3.63it/s]
[INFO|training_args.py:1902] 2024-11-05 04:08:20,142 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:08:20,143 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:08:20,144 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:08:23,705 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:23,706 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:08:23,754 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:23,755 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:23,757 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:23,757 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:23,757 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:23,757 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:23,757 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:23,757 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:08:23,758 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:23,759 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:08:23,884 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:08:24,896 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:08:24,896 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:08:25,935 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:08:25,937 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:08:25,940 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:08:25,940 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:08:25,940 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4123
  eval_runtime            = 0:00:00.84
  eval_samples            =        408
  eval_samples_per_second =    481.633
  eval_steps_per_second   =       1.18
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.19it/s]
[INFO|training_args.py:1902] 2024-11-05 04:08:26,631 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:08:26,632 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:08:26,632 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:08:29,507 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:29,508 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:08:29,608 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:29,609 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:29,610 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:29,611 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:29,611 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:29,611 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:29,611 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:29,611 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:08:29,611 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:29,612 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:08:29,755 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:08:30,757 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:08:30,762 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:08:31,306 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:08:31,308 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:08:31,311 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:08:31,311 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:08:31,311 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4121
  eval_runtime            = 0:00:00.63
  eval_samples            =        408
  eval_samples_per_second =     647.24
  eval_steps_per_second   =      1.586
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.75it/s]
[INFO|training_args.py:1902] 2024-11-05 04:08:32,613 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:08:32,614 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:08:32,614 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:08:35,880 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:35,882 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:08:35,958 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:35,959 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:35,961 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:35,961 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:35,961 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:35,961 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:35,961 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:35,961 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:08:35,962 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:35,963 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:08:36,086 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:08:37,066 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:08:37,067 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:08:37,799 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:08:37,801 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:08:37,804 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:08:37,806 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:08:37,806 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4911
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.24
  eval_samples              =       1043
  eval_samples_per_second   =    838.969
  eval_steps_per_second     =      0.804
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.72it/s]
[INFO|training_args.py:1902] 2024-11-05 04:08:39,130 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:08:39,130 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:08:39,131 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:08:42,568 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:42,569 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:08:42,625 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:42,626 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:42,628 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:42,628 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:42,628 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:42,628 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:42,628 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:42,628 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:08:42,629 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:42,630 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:08:42,751 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:08:43,912 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:08:43,914 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:08:44,646 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:08:44,648 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:08:44,651 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:08:44,651 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:08:44,651 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/3 [00:00<?, ?it/s]***** eval metrics *****
  eval_loss                 =     0.4911
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.26
  eval_samples              =       1043
  eval_samples_per_second   =    824.149
  eval_steps_per_second     =       0.79
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

 67% 2/3 [00:02<00:01,  1.03s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
[INFO|training_args.py:1902] 2024-11-05 04:08:50,757 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:08:50,757 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:08:50,758 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:08:53,872 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:53,873 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:08:53,936 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:53,937 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:53,941 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:53,941 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:53,941 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:53,941 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:53,941 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:08:53,941 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:08:53,942 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:08:53,942 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:08:54,065 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:08:55,066 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:08:55,066 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:08:56,137 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:08:56,139 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:08:56,142 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:08:56,142 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:08:56,142 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.04
  eval_samples            =       5463
  eval_samples_per_second =    903.905
  eval_steps_per_second   =      0.496
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.20s/it]
100% 3/3 [00:03<00:00,  1.22s/it]
100% 3/3 [00:03<00:00,  1.24s/it]
[INFO|training_args.py:1902] 2024-11-05 04:09:02,623 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:09:02,624 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:09:02,624 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:09:05,861 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:05,862 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:09:05,917 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:05,918 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:05,920 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:05,920 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:05,920 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:05,920 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:05,920 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:05,920 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:09:05,921 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:05,922 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:09:06,061 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:09:07,089 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:09:07,090 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:09:07,640 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:09:07,642 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:09:07,645 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:09:07,645 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:09:07,645 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.41
  eval_samples            =       5463
  eval_samples_per_second =    851.158
  eval_steps_per_second   =      0.467
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
  0% 0/1 [00:00<?, ?it/s]
})]

100% 1/1 [00:00<00:00, 29.05it/s]
[INFO|training_args.py:1902] 2024-11-05 04:09:09,382 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:09:09,382 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:09:09,383 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:09:12,272 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:12,273 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:09:12,327 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:12,328 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:12,330 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:12,330 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:12,330 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:12,330 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:12,330 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:12,330 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:09:12,331 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:12,332 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:09:12,788 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:09:13,758 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:09:13,763 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:09:14,632 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:09:14,634 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:09:14,637 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:09:14,637 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:09:14,637 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6194
  eval_pearson            =      0.872
  eval_runtime            = 0:00:01.67
  eval_samples            =       1500
  eval_samples_per_second =    898.167
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.599
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.89it/s]
2024-11-05 04:09:16,346 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:09:16,348 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models rte: 0.6606498194945848
2024-11-05 04:09:16,349 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models mrpc: 0.8683274021352313
2024-11-05 04:09:16,350 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models cola: 0.49626557567073315
2024-11-05 04:09:16,352 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:09:16,353 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models stsb: 0.8719519336944892
2024-11-05 04:09:16,354 - INFO - [diffusion][Epoch 11993] average DIFF reconstruction auto_encoder_models accuracy: 0.7919289102494295
2024-11-05 04:09:16,355 - INFO - [diffusion][Epoch 11993] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6606498194945848), ('rte', 0.6570397111913358), ('mrpc', 0.8683274021352313), ('mrpc', 0.8683274021352313), ('cola', 0.49626557567073315), ('cola', 0.49626557567073315), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.8719519336944892), ('stsb', 0.871916044874669)]
2024-11-05 04:09:16,356 - INFO - [diffusion][Epoch 11993] ---------------------------------
2024-11-05 04:09:16,449 - INFO - [diffusion][Epoch 11993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:16,450 - INFO - [diffusion][Epoch 11994] Epoch 11995/12000
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6199
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.67
  eval_samples            =       1500
  eval_samples_per_second =    897.638
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.598
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:09:19,753 - INFO - [diffusion][Epoch 11994] diffusion training Loss: 0.04470798373222351
2024-11-05 04:09:19,755 - INFO - [diffusion][Epoch 11994] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:09:42,105 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:09:42,107 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:09:42,109 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:09:45,216 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:45,217 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:09:45,274 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:45,276 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:45,278 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:45,278 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:45,278 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:45,278 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:45,278 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:45,278 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:09:45,279 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:45,280 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:09:45,432 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:09:46,434 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:09:46,434 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:09:47,621 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:09:47,623 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:09:47,626 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:09:47,626 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:09:47,626 >>   Batch size = 2048
The tensors are equal.
  0% 0/1 [00:00<?, ?it/s]
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

100% 1/1 [00:00<00:00, 35.74it/s]
[INFO|training_args.py:1902] 2024-11-05 04:09:49,011 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:09:49,011 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:09:49,012 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:09:51,979 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:51,980 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:09:52,035 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:52,036 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:52,038 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:52,038 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:52,038 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:52,038 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:52,039 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:52,039 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:09:52,039 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:52,040 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:09:52,175 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:09:53,118 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:09:53,123 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:09:53,944 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:09:53,946 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:09:53,949 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:09:53,949 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:09:53,949 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.31
  eval_samples            =        872
  eval_samples_per_second =    665.221
  eval_steps_per_second   =      0.763
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.66it/s]
[INFO|training_args.py:1902] 2024-11-05 04:09:55,073 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:09:55,073 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:09:55,074 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:09:58,081 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:58,082 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:09:58,135 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:58,136 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:58,139 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:58,139 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:58,139 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:58,139 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:58,139 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:09:58,139 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:09:58,140 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:09:58,141 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:09:58,276 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:09:59,236 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:09:59,236 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:10:00,141 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:10:00,143 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:10:00,146 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:10:00,146 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:10:00,146 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.06
  eval_samples            =        872
  eval_samples_per_second =    819.672
  eval_steps_per_second   =       0.94
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.51it/s]
[INFO|training_args.py:1902] 2024-11-05 04:10:00,645 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:10:00,646 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:10:00,647 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:10:04,101 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:04,104 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:10:04,198 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:04,200 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:04,202 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:04,202 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:04,202 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:04,202 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:04,202 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:04,202 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:10:04,203 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:04,204 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:10:04,351 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:10:05,324 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:10:05,325 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:10:05,994 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:10:05,996 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:10:05,999 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:10:05,999 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:10:05,999 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6679
  eval_loss               =     0.6464
  eval_runtime            = 0:00:00.44
  eval_samples            =        277
  eval_samples_per_second =    628.471
  eval_steps_per_second   =      2.269
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.66it/s]
[INFO|training_args.py:1902] 2024-11-05 04:10:06,490 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:10:06,491 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:10:06,492 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:10:10,544 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:10,545 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:10:10,593 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:10,594 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:10,596 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:10,596 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:10,596 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:10,596 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:10,596 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:10,596 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:10:10,597 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:10,598 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:10:10,725 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:10:11,983 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:10:11,983 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:10:13,579 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:10:13,581 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:10:13,584 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:10:13,584 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:10:13,584 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.657
  eval_loss               =     0.6462
  eval_runtime            = 0:00:00.43
  eval_samples            =        277
  eval_samples_per_second =     641.92
  eval_steps_per_second   =      2.317
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.18it/s]
[INFO|training_args.py:1902] 2024-11-05 04:10:14,223 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:10:14,223 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:10:14,224 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:10:17,565 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:17,566 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:10:17,615 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:17,616 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:17,618 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:17,618 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:17,618 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:17,618 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:17,618 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:17,618 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:10:17,618 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:17,619 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:10:17,766 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:10:18,770 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:10:18,770 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:10:19,487 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:10:19,489 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:10:19,492 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:10:19,492 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:10:19,492 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4113
  eval_runtime            = 0:00:00.57
  eval_samples            =        408
  eval_samples_per_second =    707.362
  eval_steps_per_second   =      1.734
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.39it/s]
[INFO|training_args.py:1902] 2024-11-05 04:10:20,117 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:10:20,117 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:10:20,117 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:10:23,264 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:23,266 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:10:23,346 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:23,346 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:23,349 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:23,349 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:23,349 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:23,349 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:23,349 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:23,349 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:10:23,349 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:23,350 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:10:23,482 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:10:24,475 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:10:24,475 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:10:25,428 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:10:25,429 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:10:25,431 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:10:25,431 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:10:25,431 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4116
  eval_runtime            = 0:00:00.58
  eval_samples            =        408
  eval_samples_per_second =    701.755
  eval_steps_per_second   =       1.72
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.35it/s]
[INFO|training_args.py:1902] 2024-11-05 04:10:26,598 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:10:26,599 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:10:26,600 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:10:30,529 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:30,530 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:10:30,577 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:30,578 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:30,580 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:30,580 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:30,580 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:30,580 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:30,580 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:30,580 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:10:30,581 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:30,582 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:10:30,723 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:10:31,670 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:10:31,671 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:10:32,334 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:10:32,336 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:10:32,338 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:10:32,338 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:10:32,338 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4913
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.09
  eval_samples              =       1043
  eval_samples_per_second   =    950.559
  eval_steps_per_second     =      0.911
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.10it/s]
[INFO|training_args.py:1902] 2024-11-05 04:10:33,807 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:10:33,807 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:10:33,808 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:10:36,893 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:36,894 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:10:36,963 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:36,964 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:36,966 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:36,966 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:36,966 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:36,966 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:36,966 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:36,966 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:10:36,966 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:36,967 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:10:37,115 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:10:38,101 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:10:38,101 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:10:38,817 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:10:38,819 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:10:38,821 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:10:38,821 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:10:38,821 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4912
  eval_matthews_correlation =     0.4987
  eval_runtime              = 0:00:01.41
  eval_samples              =       1043
  eval_samples_per_second   =    735.245
  eval_steps_per_second     =      0.705
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.00s/it]
100% 3/3 [00:03<00:00,  1.07s/it]
100% 3/3 [00:03<00:00,  1.08s/it]
[INFO|training_args.py:1902] 2024-11-05 04:10:44,727 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:10:44,728 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:10:44,729 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:10:48,655 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:48,656 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:10:48,720 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:48,721 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:48,724 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:48,724 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:48,724 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:48,724 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:48,724 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:10:48,724 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:10:48,725 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:10:48,727 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:10:48,836 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:10:49,845 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:10:49,845 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:10:50,553 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:10:50,555 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:10:50,558 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:10:50,558 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:10:50,558 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:05.84
  eval_samples            =       5463
  eval_samples_per_second =    935.015
  eval_steps_per_second   =      0.513
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.01it/s]
100% 3/3 [00:03<00:00,  1.20s/it]
100% 3/3 [00:03<00:00,  1.17s/it]
[INFO|training_args.py:1902] 2024-11-05 04:10:56,735 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:10:56,735 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:10:56,735 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:11:00,273 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:00,275 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:11:00,339 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:00,340 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:00,344 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:00,344 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:00,344 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:00,344 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:00,344 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:00,344 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:11:00,344 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:00,345 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:11:00,468 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:11:01,546 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:11:01,546 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:11:02,634 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:11:02,636 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:11:02,638 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:11:02,638 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:11:02,638 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.12
  eval_samples            =       5463
  eval_samples_per_second =    892.073
  eval_steps_per_second   =       0.49
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.24it/s]
[INFO|training_args.py:1902] 2024-11-05 04:11:04,285 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:11:04,285 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:11:04,286 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:11:08,969 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:08,970 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:11:09,026 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:09,027 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:09,029 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:09,029 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:09,029 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:09,029 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:09,029 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:09,029 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:11:09,030 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:09,031 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:11:09,168 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:11:10,179 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:11:10,179 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:11:11,163 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:11:11,166 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:11:11,169 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:11:11,169 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:11:11,169 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6199
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.60
  eval_samples            =       1500
  eval_samples_per_second =    936.817
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.625
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.22it/s]
2024-11-05 04:11:12,985 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:11:12,987 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models rte: 0.6678700361010831
2024-11-05 04:11:12,988 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models mrpc: 0.8683274021352313
2024-11-05 04:11:12,989 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models cola: 0.49867676415016876
2024-11-05 04:11:12,990 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:11:12,992 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models stsb: 0.871929068174217
2024-11-05 04:11:12,993 - INFO - [diffusion][Epoch 11994] average DIFF reconstruction auto_encoder_models accuracy: 0.7927273697721408
2024-11-05 04:11:12,994 - INFO - [diffusion][Epoch 11994] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6678700361010831), ('rte', 0.6570397111913358), ('mrpc', 0.8683274021352313), ('mrpc', 0.8683274021352313), ('cola', 0.49626557567073315), ('cola', 0.49867676415016876), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.871929068174217), ('stsb', 0.8718890195815463)]
2024-11-05 04:11:12,995 - INFO - [diffusion][Epoch 11994] ---------------------------------
2024-11-05 04:11:13,084 - INFO - [diffusion][Epoch 11994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:13,086 - INFO - [diffusion][Epoch 11995] Epoch 11996/12000
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6204
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.75
  eval_samples            =       1500
  eval_samples_per_second =    853.796
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.569
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:11:16,334 - INFO - [diffusion][Epoch 11995] diffusion training Loss: 0.04059625044465065
2024-11-05 04:11:16,336 - INFO - [diffusion][Epoch 11995] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:11:39,515 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:11:39,516 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:11:39,517 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:11:43,095 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:43,097 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:11:43,164 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:43,165 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:43,167 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:43,167 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:43,167 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:43,168 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:43,168 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:43,168 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:11:43,168 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:43,169 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:11:43,315 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:11:44,284 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:11:44,284 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:11:45,307 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:11:45,310 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:11:45,312 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:11:45,313 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:11:45,313 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.48it/s]
[INFO|training_args.py:1902] 2024-11-05 04:11:46,440 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:11:46,441 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:11:46,441 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:11:49,099 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:49,100 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:11:49,154 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:49,155 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:49,158 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:49,158 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:49,158 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:49,158 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:49,158 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:49,158 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:11:49,159 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:49,160 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:11:49,335 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:11:50,766 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:11:50,766 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:11:51,800 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:11:51,802 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:11:51,804 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:11:51,804 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:11:51,804 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.05
  eval_samples            =        872
  eval_samples_per_second =    823.796
  eval_steps_per_second   =      0.945
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.73it/s]
[INFO|training_args.py:1902] 2024-11-05 04:11:52,968 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:11:52,968 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:11:52,969 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:11:55,751 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:55,752 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:11:55,801 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:55,802 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:55,804 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:55,804 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:55,804 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:55,804 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:55,804 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:11:55,804 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:11:55,804 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:11:55,805 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:11:55,944 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:11:56,914 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:11:56,914 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:11:57,627 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:11:57,630 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:11:57,632 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:11:57,632 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:11:57,632 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.10
  eval_samples            =        872
  eval_samples_per_second =    788.842
  eval_steps_per_second   =      0.905
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.28it/s]
[INFO|training_args.py:1902] 2024-11-05 04:11:58,128 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:11:58,128 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:11:58,129 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:12:00,767 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:00,768 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:12:00,828 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:00,830 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:00,833 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:00,833 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:00,833 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:00,833 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:00,834 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:00,834 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:12:00,834 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:00,835 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:12:00,974 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:12:01,942 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:12:01,943 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:12:02,971 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:12:02,973 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:12:02,976 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:12:02,976 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:12:02,976 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6643
  eval_loss               =     0.6463
  eval_runtime            = 0:00:00.44
  eval_samples            =        277
  eval_samples_per_second =    622.116
  eval_steps_per_second   =      2.246
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 45.48it/s]
[INFO|training_args.py:1902] 2024-11-05 04:12:03,480 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:12:03,480 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:12:03,481 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:12:06,486 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:06,488 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:12:06,548 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:06,549 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:06,551 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:06,551 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:06,551 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:06,551 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:06,551 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:06,551 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:12:06,552 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:06,553 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:12:06,686 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:12:07,633 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:12:07,633 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:12:09,154 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:12:09,155 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:12:09,156 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:12:09,157 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:12:09,157 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.657
  eval_loss               =     0.6463
  eval_runtime            = 0:00:00.44
  eval_samples            =        277
  eval_samples_per_second =    628.969
  eval_steps_per_second   =      2.271
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.33it/s]
[INFO|training_args.py:1902] 2024-11-05 04:12:09,754 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:12:09,754 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:12:09,755 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:12:13,202 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:13,203 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:12:13,255 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:13,256 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:13,258 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:13,258 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:13,258 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:13,258 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:13,258 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:13,258 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:12:13,259 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:13,260 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:12:13,392 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:12:14,392 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:12:14,392 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:12:15,264 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:12:15,266 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:12:15,269 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:12:15,269 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:12:15,269 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8455
  eval_f1                 =     0.8699
  eval_loss               =      0.411
  eval_runtime            = 0:00:00.54
  eval_samples            =        408
  eval_samples_per_second =    754.722
  eval_steps_per_second   =       1.85
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.61it/s]
[INFO|training_args.py:1902] 2024-11-05 04:12:15,936 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:12:15,936 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:12:15,937 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:12:18,680 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:18,682 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:12:18,743 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:18,745 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:18,748 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:18,748 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:18,748 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:18,748 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:18,748 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:18,748 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:12:18,749 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:18,751 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:12:18,881 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:12:19,830 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:12:19,830 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:12:20,922 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:12:20,925 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:12:20,928 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:12:20,928 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:12:20,928 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4116
  eval_runtime            = 0:00:00.60
  eval_samples            =        408
  eval_samples_per_second =    670.892
  eval_steps_per_second   =      1.644
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.77it/s]
[INFO|training_args.py:1902] 2024-11-05 04:12:22,206 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:12:22,206 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:12:22,207 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:12:25,622 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:25,624 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:12:25,673 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:25,674 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:25,676 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:25,676 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:25,676 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:25,676 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:25,676 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:25,676 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:12:25,676 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:25,677 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:12:25,816 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:12:26,794 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:12:26,794 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:12:27,670 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:12:27,673 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:12:27,675 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:12:27,675 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:12:27,676 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4913
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.22
  eval_samples              =       1043
  eval_samples_per_second   =    852.293
  eval_steps_per_second     =      0.817
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.67it/s]
[INFO|training_args.py:1902] 2024-11-05 04:12:29,278 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:12:29,279 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:12:29,280 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:12:32,677 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:32,678 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:12:32,732 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:32,733 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:32,735 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:32,735 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:32,735 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:32,735 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:32,735 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:32,735 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:12:32,736 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:32,737 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:12:32,895 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:12:33,848 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:12:33,848 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:12:34,578 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:12:34,580 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:12:34,582 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:12:34,582 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:12:34,582 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4911
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.54
  eval_samples              =       1043
  eval_samples_per_second   =    674.565
  eval_steps_per_second     =      0.647
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.08s/it]
100% 3/3 [00:03<00:00,  1.16s/it]
100% 3/3 [00:03<00:00,  1.17s/it]
[INFO|training_args.py:1902] 2024-11-05 04:12:40,547 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:12:40,548 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:12:40,549 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:12:43,624 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:43,625 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:12:43,677 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:43,678 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:43,680 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:43,680 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:43,680 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:43,680 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:43,680 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:43,680 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:12:43,681 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:43,682 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:12:43,808 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:12:44,776 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:12:44,776 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:12:45,622 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:12:45,624 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:12:45,627 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:12:45,627 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:12:45,627 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:05.90
  eval_samples            =       5463
  eval_samples_per_second =    925.508
  eval_steps_per_second   =      0.508
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.06s/it]
100% 3/3 [00:03<00:00,  1.22s/it]
100% 3/3 [00:03<00:00,  1.20s/it]
[INFO|training_args.py:1902] 2024-11-05 04:12:51,778 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:12:51,779 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:12:51,780 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:12:54,534 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:54,535 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:12:54,604 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:54,605 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:54,607 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:54,607 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:54,607 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:54,607 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:54,607 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:12:54,607 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:12:54,608 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:12:54,609 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:12:54,729 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:12:55,672 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:12:55,673 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:12:56,707 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:12:56,709 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:12:56,712 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:12:56,712 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:12:56,712 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.09
  eval_samples            =       5463
  eval_samples_per_second =    896.513
  eval_steps_per_second   =      0.492
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.56it/s]
[INFO|training_args.py:1902] 2024-11-05 04:12:58,443 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:12:58,444 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:12:58,444 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:13:01,412 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:01,413 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:13:01,462 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:01,463 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:01,467 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:01,467 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:01,467 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:01,467 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:01,467 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:01,467 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:13:01,468 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:01,469 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:13:01,616 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:13:02,599 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:13:02,599 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:13:03,265 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:13:03,267 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:13:03,269 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:13:03,269 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:13:03,269 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6199
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.67
  eval_samples            =       1500
  eval_samples_per_second =    897.649
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.598
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 30.42it/s]
2024-11-05 04:13:05,040 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:13:05,043 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models rte: 0.6642599277978339
2024-11-05 04:13:05,044 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models mrpc: 0.8698752228163994
2024-11-05 04:13:05,046 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models cola: 0.49626557567073315
2024-11-05 04:13:05,047 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:13:05,048 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models stsb: 0.8719209914483586
2024-11-05 04:13:05,049 - INFO - [diffusion][Epoch 11995] average DIFF reconstruction auto_encoder_models accuracy: 0.7923556677588909
2024-11-05 04:13:05,050 - INFO - [diffusion][Epoch 11995] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6642599277978339), ('rte', 0.6570397111913358), ('mrpc', 0.8698752228163994), ('mrpc', 0.8683274021352313), ('cola', 0.49626557567073315), ('cola', 0.49626557567073315), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.8719209914483586), ('stsb', 0.871910148249921)]
2024-11-05 04:13:05,051 - INFO - [diffusion][Epoch 11995] ---------------------------------
2024-11-05 04:13:05,145 - INFO - [diffusion][Epoch 11995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:05,147 - INFO - [diffusion][Epoch 11996] Epoch 11997/12000
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =       0.62
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.72
  eval_samples            =       1500
  eval_samples_per_second =    871.225
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.581
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:13:08,241 - INFO - [diffusion][Epoch 11996] diffusion training Loss: 0.04141160938888788
2024-11-05 04:13:08,243 - INFO - [diffusion][Epoch 11996] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:13:31,763 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:13:31,766 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:13:31,767 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:13:34,631 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:34,633 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:13:34,696 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:34,697 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:34,699 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:34,699 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:34,699 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:34,699 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:34,699 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:34,699 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:13:34,700 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:34,701 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:13:34,839 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:13:35,802 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:13:35,802 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:13:36,593 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:13:36,596 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:13:36,598 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:13:36,598 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:13:36,598 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.11it/s]
[INFO|training_args.py:1902] 2024-11-05 04:13:37,718 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:13:37,718 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:13:37,719 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:13:41,582 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:41,583 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:13:41,648 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:41,649 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:41,651 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:41,651 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:41,651 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:41,651 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:41,651 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:41,651 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:13:41,651 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:41,653 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:13:41,821 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:13:42,838 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:13:42,839 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:13:44,098 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:13:44,101 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:13:44,103 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:13:44,103 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:13:44,103 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.05
  eval_samples            =        872
  eval_samples_per_second =    827.447
  eval_steps_per_second   =      0.949
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.20it/s]
[INFO|training_args.py:1902] 2024-11-05 04:13:45,160 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:13:45,160 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:13:45,161 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:13:48,196 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:48,197 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:13:48,256 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:48,257 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:48,259 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:48,259 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:48,259 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:48,259 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:48,259 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:48,259 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:13:48,260 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:48,260 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:13:48,408 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:13:49,442 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:13:49,442 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:13:50,355 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:13:50,358 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:13:50,360 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:13:50,360 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:13:50,360 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.00
  eval_samples            =        872
  eval_samples_per_second =    871.694
  eval_steps_per_second   =        1.0
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.78it/s]
[INFO|training_args.py:1902] 2024-11-05 04:13:50,881 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:13:50,881 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:13:50,882 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:13:53,926 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:53,927 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:13:53,974 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:53,975 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:53,977 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:53,977 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:53,977 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:53,977 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:53,977 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:53,977 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:13:53,977 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:53,978 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:13:54,107 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:13:55,092 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:13:55,092 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:13:56,216 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:13:56,218 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:13:56,220 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:13:56,220 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:13:56,220 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6606
  eval_loss               =     0.6463
  eval_runtime            = 0:00:00.46
  eval_samples            =        277
  eval_samples_per_second =    598.544
  eval_steps_per_second   =      2.161
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 23.98it/s]
[INFO|training_args.py:1902] 2024-11-05 04:13:56,720 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:13:56,721 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:13:56,722 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:13:59,393 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:59,396 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:13:59,447 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:59,449 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:59,452 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:59,452 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:59,452 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:59,452 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:59,452 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:13:59,452 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:13:59,454 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:13:59,456 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:13:59,604 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:14:00,549 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:14:00,550 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:14:01,059 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:14:01,061 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:14:01,064 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:14:01,064 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:14:01,064 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.657
  eval_loss               =     0.6459
  eval_runtime            = 0:00:00.44
  eval_samples            =        277
  eval_samples_per_second =    626.824
  eval_steps_per_second   =      2.263
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  5.11it/s]
[INFO|training_args.py:1902] 2024-11-05 04:14:02,085 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:14:02,086 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:14:02,087 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:14:05,186 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:05,187 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:14:05,238 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:05,239 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:05,243 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:05,243 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:05,243 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:05,243 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:05,244 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:05,244 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:14:05,244 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:05,245 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:14:05,371 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:14:06,335 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:14:06,335 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:14:07,135 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:14:07,136 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:14:07,138 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:14:07,138 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:14:07,138 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4115
  eval_runtime            = 0:00:00.95
  eval_samples            =        408
  eval_samples_per_second =    425.925
  eval_steps_per_second   =      1.044
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.99it/s]
[INFO|training_args.py:1902] 2024-11-05 04:14:07,762 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:14:07,762 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:14:07,763 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:14:10,890 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:10,892 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:14:10,943 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:10,944 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:10,946 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:10,946 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:10,946 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:10,946 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:10,946 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:10,946 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:14:10,946 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:10,947 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:14:11,077 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:14:12,040 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:14:12,041 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:14:13,801 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:14:13,803 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:14:13,805 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:14:13,805 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:14:13,805 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8455
  eval_f1                 =     0.8699
  eval_loss               =     0.4119
  eval_runtime            = 0:00:00.56
  eval_samples            =        408
  eval_samples_per_second =    726.049
  eval_steps_per_second   =       1.78
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.51it/s]
[INFO|training_args.py:1902] 2024-11-05 04:14:15,103 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:14:15,104 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:14:15,105 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:14:17,949 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:17,950 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:14:18,002 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:18,003 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:18,005 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:18,005 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:18,005 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:18,005 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:18,005 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:18,005 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:14:18,005 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:18,006 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:14:18,129 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:14:19,134 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:14:19,135 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:14:20,145 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:14:20,147 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:14:20,150 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:14:20,150 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:14:20,150 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4913
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.24
  eval_samples              =       1043
  eval_samples_per_second   =    840.454
  eval_steps_per_second     =      0.806
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.68it/s]
[INFO|training_args.py:1902] 2024-11-05 04:14:21,404 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:14:21,404 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:14:21,405 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:14:24,348 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:24,349 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:14:24,398 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:24,399 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:24,401 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:24,401 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:24,401 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:24,401 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:24,401 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:24,401 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:14:24,402 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:24,403 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:14:24,526 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:14:25,482 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:14:25,486 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:14:26,393 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:14:26,395 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:14:26,397 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:14:26,398 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:14:26,398 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4912
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.20
  eval_samples              =       1043
  eval_samples_per_second   =    867.873
  eval_steps_per_second     =      0.832
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.01it/s]
100% 3/3 [00:03<00:00,  1.09s/it]
100% 3/3 [00:03<00:00,  1.18s/it]
[INFO|training_args.py:1902] 2024-11-05 04:14:32,763 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:14:32,763 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:14:32,764 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:14:35,489 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:35,491 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:14:35,545 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:35,546 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:35,548 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:35,548 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:35,548 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:35,548 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:35,548 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:35,548 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:14:35,548 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:35,549 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:14:35,669 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:14:36,640 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:14:36,640 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:14:37,799 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:14:37,801 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:14:37,804 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:14:37,804 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:14:37,804 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.30
  eval_samples            =       5463
  eval_samples_per_second =    866.477
  eval_steps_per_second   =      0.476
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.04s/it]
100% 3/3 [00:03<00:00,  1.15s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
[INFO|training_args.py:1902] 2024-11-05 04:14:44,038 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:14:44,039 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:14:44,039 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:14:47,217 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:47,218 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:14:47,270 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:47,271 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:47,273 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:47,274 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:47,274 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:47,274 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:47,274 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:47,274 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:14:47,274 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:47,275 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:14:47,428 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:14:48,421 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:14:48,421 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:14:49,541 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:14:49,543 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:14:49,546 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:14:49,546 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:14:49,546 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.17
  eval_samples            =       5463
  eval_samples_per_second =    884.998
  eval_steps_per_second   =      0.486
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.15it/s]
[INFO|training_args.py:1902] 2024-11-05 04:14:51,332 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:14:51,333 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:14:51,335 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:14:53,988 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:53,989 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:14:54,051 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:54,052 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:54,054 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:54,054 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:54,054 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:54,054 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:54,054 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:14:54,054 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:14:54,055 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:14:54,056 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:14:54,182 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:14:55,123 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:14:55,123 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:14:55,706 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:14:55,708 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:14:55,710 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:14:55,710 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:14:55,710 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =       0.62
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.72
  eval_samples            =       1500
  eval_samples_per_second =    870.174
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =       0.58
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.24it/s]
2024-11-05 04:14:57,420 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:14:57,422 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models rte: 0.6606498194945848
2024-11-05 04:14:57,423 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models mrpc: 0.8698752228163994
2024-11-05 04:14:57,424 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models cola: 0.49626557567073315
2024-11-05 04:14:57,425 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:14:57,427 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models stsb: 0.8719257161826436
2024-11-05 04:14:57,428 - INFO - [diffusion][Epoch 11996] average DIFF reconstruction auto_encoder_models accuracy: 0.792054004530015
2024-11-05 04:14:57,429 - INFO - [diffusion][Epoch 11996] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6606498194945848), ('rte', 0.6570397111913358), ('mrpc', 0.8683274021352313), ('mrpc', 0.8698752228163994), ('cola', 0.49626557567073315), ('cola', 0.49626557567073315), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.8718955730723762), ('stsb', 0.8719257161826436)]
2024-11-05 04:14:57,430 - INFO - [diffusion][Epoch 11996] ---------------------------------
2024-11-05 04:14:57,525 - INFO - [diffusion][Epoch 11996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:57,529 - INFO - [diffusion][Epoch 11997] Epoch 11998/12000
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =       0.62
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.66
  eval_samples            =       1500
  eval_samples_per_second =    903.563
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.602
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:15:00,722 - INFO - [diffusion][Epoch 11997] diffusion training Loss: 0.04268069006502628
2024-11-05 04:15:00,724 - INFO - [diffusion][Epoch 11997] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:15:23,847 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:15:23,848 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:15:23,851 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:15:26,941 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:26,942 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:15:26,992 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:26,994 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:26,997 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:26,997 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:26,997 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:26,997 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:26,997 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:26,997 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:15:26,998 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:26,999 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:15:27,166 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:15:28,146 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:15:28,146 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:15:29,119 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:15:29,121 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:15:29,124 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:15:29,124 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:15:29,124 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.76it/s]
[INFO|training_args.py:1902] 2024-11-05 04:15:30,281 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:15:30,281 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:15:30,282 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:15:33,123 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:33,124 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:15:33,170 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:33,171 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:33,172 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:33,172 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:33,172 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:33,172 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:33,172 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:33,172 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:15:33,173 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:33,174 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:15:33,324 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:15:34,280 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:15:34,282 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:15:34,938 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:15:34,940 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:15:34,942 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:15:34,943 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:15:34,943 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.08
  eval_samples            =        872
  eval_samples_per_second =    802.279
  eval_steps_per_second   =       0.92
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 47.79it/s]
[INFO|training_args.py:1902] 2024-11-05 04:15:36,343 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:15:36,343 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:15:36,343 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:15:39,382 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:39,383 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:15:39,431 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:39,432 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:39,433 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:39,433 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:39,433 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:39,434 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:39,434 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:39,434 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:15:39,434 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:39,435 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:15:39,577 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:15:40,528 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:15:40,528 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:15:41,391 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:15:41,393 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:15:41,396 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:15:41,396 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:15:41,396 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.35
  eval_samples            =        872
  eval_samples_per_second =    643.099
  eval_steps_per_second   =      0.737
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.33it/s]
[INFO|training_args.py:1902] 2024-11-05 04:15:41,933 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:15:41,933 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:15:41,934 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:15:45,040 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:45,041 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:15:45,148 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:45,149 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:45,151 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:45,151 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:45,151 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:45,151 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:45,151 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:45,151 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:15:45,151 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:45,152 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:15:45,307 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:15:46,246 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:15:46,247 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:15:47,910 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:15:47,913 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:15:47,916 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:15:47,916 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:15:47,916 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6606
  eval_loss               =     0.6464
  eval_runtime            = 0:00:00.47
  eval_samples            =        277
  eval_samples_per_second =    580.732
  eval_steps_per_second   =      2.097
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 48.36it/s]
[INFO|training_args.py:1902] 2024-11-05 04:15:48,448 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:15:48,449 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:15:48,450 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:15:51,430 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:51,431 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:15:51,477 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:51,478 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:51,480 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:51,480 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:51,480 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:51,480 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:51,480 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:51,480 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:15:51,480 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:51,481 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:15:51,603 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:15:52,550 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:15:52,551 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:15:53,194 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:15:53,196 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:15:53,200 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:15:53,200 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:15:53,200 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6643
  eval_loss               =     0.6469
  eval_runtime            = 0:00:00.48
  eval_samples            =        277
  eval_samples_per_second =     574.39
  eval_steps_per_second   =      2.074
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.26it/s]
[INFO|training_args.py:1902] 2024-11-05 04:15:53,882 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:15:53,883 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:15:53,884 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:15:56,588 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:56,589 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:15:56,648 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:56,649 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:56,651 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:56,651 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:56,651 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:56,651 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:56,651 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:15:56,651 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:15:56,652 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:15:56,653 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:15:56,792 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:15:57,793 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:15:57,793 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:15:58,926 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:15:58,928 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:15:58,931 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:15:58,931 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:15:58,931 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4122
  eval_runtime            = 0:00:00.61
  eval_samples            =        408
  eval_samples_per_second =    658.888
  eval_steps_per_second   =      1.615
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.09it/s]
[INFO|training_args.py:1902] 2024-11-05 04:15:59,488 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:15:59,488 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:15:59,489 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:16:02,271 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:02,272 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:16:02,324 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:02,325 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:02,327 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:02,327 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:02,327 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:02,327 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:02,327 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:02,327 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:16:02,328 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:02,329 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:16:02,457 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:16:03,501 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:16:03,502 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:16:04,387 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:16:04,389 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:16:04,391 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:16:04,391 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:16:04,391 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8455
  eval_f1                 =     0.8699
  eval_loss               =     0.4116
  eval_runtime            = 0:00:00.50
  eval_samples            =        408
  eval_samples_per_second =     815.26
  eval_steps_per_second   =      1.998
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.90it/s]
[INFO|training_args.py:1902] 2024-11-05 04:16:05,679 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:16:05,680 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:16:05,680 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:16:08,818 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:08,820 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:16:08,870 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:08,871 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:08,873 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:08,873 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:08,873 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:08,873 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:08,873 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:08,873 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:16:08,873 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:08,874 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:16:09,002 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:16:09,958 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:16:09,963 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:16:10,671 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:16:10,673 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:16:10,676 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:16:10,676 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:16:10,676 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4912
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.25
  eval_samples              =       1043
  eval_samples_per_second   =    831.522
  eval_steps_per_second     =      0.797
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.22it/s]
[INFO|training_args.py:1902] 2024-11-05 04:16:12,002 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:16:12,002 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:16:12,003 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:16:15,160 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:15,161 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:16:15,212 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:15,213 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:15,215 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:15,215 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:15,215 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:15,215 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:15,215 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:15,215 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:16:15,216 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:15,217 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:16:15,350 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:16:16,306 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:16:16,306 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:16:16,911 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:16:16,913 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:16:16,915 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:16:16,915 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:16:16,915 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4909
  eval_matthews_correlation =     0.4987
  eval_runtime              = 0:00:01.24
  eval_samples              =       1043
  eval_samples_per_second   =    839.543
  eval_steps_per_second     =      0.805
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.08s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
100% 3/3 [00:03<00:00,  1.15s/it]
[INFO|training_args.py:1902] 2024-11-05 04:16:23,419 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:16:23,420 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:16:23,420 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:16:26,106 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:26,107 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:16:26,162 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:26,163 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:26,165 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:26,165 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:26,165 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:26,165 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:26,165 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:26,165 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:16:26,165 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:26,166 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:16:26,292 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:16:27,259 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:16:27,260 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:16:27,929 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:16:27,931 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:16:27,934 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:16:27,934 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:16:27,934 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.44
  eval_samples            =       5463
  eval_samples_per_second =    847.381
  eval_steps_per_second   =      0.465
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.02s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
[INFO|training_args.py:1902] 2024-11-05 04:16:33,801 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:16:33,801 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:16:33,802 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:16:36,698 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:36,699 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:16:36,749 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:36,750 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:36,762 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:36,762 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:36,762 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:36,763 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:36,763 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:36,763 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:16:36,763 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:36,764 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:16:36,895 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:16:37,860 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:16:37,860 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:16:39,010 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:16:39,012 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:16:39,014 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:16:39,014 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:16:39,014 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2103
  eval_runtime            = 0:00:05.80
  eval_samples            =       5463
  eval_samples_per_second =    940.893
  eval_steps_per_second   =      0.517
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 30.06it/s]
[INFO|training_args.py:1902] 2024-11-05 04:16:40,580 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:16:40,581 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:16:40,582 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:16:43,319 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:43,320 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:16:43,371 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:43,372 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:43,374 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:43,374 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:43,374 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:43,374 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:43,374 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:16:43,374 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:16:43,375 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:16:43,376 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:16:43,513 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:16:44,524 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:16:44,528 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:16:45,482 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:16:45,484 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:16:45,486 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:16:45,486 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:16:45,486 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6204
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.50
  eval_samples            =       1500
  eval_samples_per_second =    997.054
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.665
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 28.15it/s]
2024-11-05 04:16:47,353 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:16:47,356 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models rte: 0.6642599277978339
2024-11-05 04:16:47,357 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models mrpc: 0.8698752228163994
2024-11-05 04:16:47,358 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models cola: 0.49867676415016876
2024-11-05 04:16:47,359 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:16:47,360 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models stsb: 0.8718813119605813
2024-11-05 04:16:47,361 - INFO - [diffusion][Epoch 11997] average DIFF reconstruction auto_encoder_models accuracy: 0.7928505024435126
2024-11-05 04:16:47,362 - INFO - [diffusion][Epoch 11997] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6606498194945848), ('rte', 0.6642599277978339), ('mrpc', 0.8683274021352313), ('mrpc', 0.8698752228163994), ('cola', 0.49626557567073315), ('cola', 0.49867676415016876), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.8718813119605813), ('stsb', 0.8718665471704747)]
2024-11-05 04:16:47,364 - INFO - [diffusion][Epoch 11997] ---------------------------------
2024-11-05 04:16:47,455 - INFO - [diffusion][Epoch 11997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:47,457 - INFO - [diffusion][Epoch 11998] Epoch 11999/12000
***** eval metrics *****
  eval_combined_score     =     0.8737
  eval_loss               =     0.6204
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.81
  eval_samples            =       1500
  eval_samples_per_second =    828.354
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.552
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:16:51,061 - INFO - [diffusion][Epoch 11998] diffusion training Loss: 0.045032636262476444
2024-11-05 04:16:51,063 - INFO - [diffusion][Epoch 11998] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:17:13,777 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:17:13,778 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:17:13,779 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:17:17,043 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:17,044 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:17:17,094 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:17,095 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:17,097 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:17,097 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:17,097 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:17,097 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:17,097 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:17,097 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:17:17,098 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:17,098 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:17:17,248 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:17:18,222 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:17:18,223 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:17:18,969 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:17:18,971 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:17:18,973 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:17:18,973 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:17:18,973 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.60it/s]
[INFO|training_args.py:1902] 2024-11-05 04:17:20,042 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:17:20,043 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:17:20,044 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:17:22,703 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:22,705 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:17:22,768 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:22,769 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:22,771 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:22,771 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:22,771 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:22,771 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:22,771 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:22,771 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:17:22,771 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:22,772 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:17:22,906 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:17:23,901 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:17:23,902 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:17:24,671 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:17:24,673 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:17:24,675 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:17:24,675 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:17:24,675 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.00
  eval_samples            =        872
  eval_samples_per_second =    866.199
  eval_steps_per_second   =      0.993
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.77it/s]
[INFO|training_args.py:1902] 2024-11-05 04:17:26,027 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:17:26,028 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:17:26,028 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:17:28,985 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:28,986 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:17:29,042 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:29,043 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:29,047 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:29,047 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:29,047 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:29,047 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:29,047 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:29,047 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:17:29,047 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:29,048 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:17:29,199 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:17:30,181 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:17:30,181 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:17:30,821 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:17:30,823 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:17:30,825 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:17:30,826 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:17:30,826 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.29
  eval_samples            =        872
  eval_samples_per_second =    675.977
  eval_steps_per_second   =      0.775
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.09it/s]
[INFO|training_args.py:1902] 2024-11-05 04:17:31,292 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:17:31,292 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:17:31,293 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:17:34,171 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:34,173 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:17:34,220 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:34,221 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:34,223 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:34,224 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:34,224 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:34,224 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:34,224 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:34,224 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:17:34,224 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:34,225 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:17:34,369 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:17:35,295 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:17:35,295 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:17:36,163 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:17:36,165 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:17:36,168 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:17:36,168 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:17:36,168 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.657
  eval_loss               =      0.646
  eval_runtime            = 0:00:00.40
  eval_samples            =        277
  eval_samples_per_second =    678.527
  eval_steps_per_second   =       2.45
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.55it/s]
[INFO|training_args.py:1902] 2024-11-05 04:17:36,682 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:17:36,682 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:17:36,683 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:17:39,804 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:39,805 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:17:39,863 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:39,864 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:39,868 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:39,868 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:39,868 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:39,868 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:39,868 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:39,868 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:17:39,869 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:39,870 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:17:40,014 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:17:40,977 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:17:40,977 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:17:42,055 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:17:42,058 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:17:42,060 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:17:42,060 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:17:42,060 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6498
  eval_loss               =     0.6462
  eval_runtime            = 0:00:00.45
  eval_samples            =        277
  eval_samples_per_second =    606.339
  eval_steps_per_second   =      2.189
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.64it/s]
[INFO|training_args.py:1902] 2024-11-05 04:17:42,719 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:17:42,720 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:17:42,721 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:17:48,167 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:48,168 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:17:48,215 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:48,216 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:48,218 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:48,218 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:48,218 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:48,218 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:48,218 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:48,219 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:17:48,219 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:48,220 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:17:48,616 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:17:49,658 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:17:49,658 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:17:50,459 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:17:50,461 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:17:50,463 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:17:50,464 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:17:50,464 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4121
  eval_runtime            = 0:00:00.60
  eval_samples            =        408
  eval_samples_per_second =    674.125
  eval_steps_per_second   =      1.652
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.07it/s]
[INFO|training_args.py:1902] 2024-11-05 04:17:51,121 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:17:51,121 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:17:51,122 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:17:53,865 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:53,866 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:17:53,917 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:53,918 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:53,920 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:53,920 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:53,920 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:53,920 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:53,920 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:53,920 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:17:53,921 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:53,922 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:17:54,061 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:17:55,014 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:17:55,014 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:17:55,824 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:17:55,826 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:17:55,828 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:17:55,828 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:17:55,828 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4122
  eval_runtime            = 0:00:00.59
  eval_samples            =        408
  eval_samples_per_second =    680.369
  eval_steps_per_second   =      1.668
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.52it/s]
[INFO|training_args.py:1902] 2024-11-05 04:17:57,050 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:17:57,050 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:17:57,051 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:17:59,759 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:59,760 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:17:59,822 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:59,823 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:59,825 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:59,825 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:59,825 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:59,825 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:59,825 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:17:59,825 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:17:59,826 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:17:59,827 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:17:59,954 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:18:00,932 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:18:00,933 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:18:01,681 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:18:01,683 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:18:01,685 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:18:01,685 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:18:01,685 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4913
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.16
  eval_samples              =       1043
  eval_samples_per_second   =    891.771
  eval_steps_per_second     =      0.855
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.29it/s]
[INFO|training_args.py:1902] 2024-11-05 04:18:03,002 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:18:03,003 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:18:03,003 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:18:06,632 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:06,633 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:18:06,680 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:06,681 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:06,683 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:06,683 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:06,683 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:06,683 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:06,683 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:06,683 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:18:06,683 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:06,684 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:18:06,807 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:18:07,797 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:18:07,797 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:18:08,920 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:18:08,921 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:18:08,923 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:18:08,923 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:18:08,923 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4912
  eval_matthews_correlation =     0.4987
  eval_runtime              = 0:00:01.27
  eval_samples              =       1043
  eval_samples_per_second   =    819.514
  eval_steps_per_second     =      0.786
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.07s/it]
100% 3/3 [00:03<00:00,  1.16s/it]
100% 3/3 [00:03<00:00,  1.16s/it]
[INFO|training_args.py:1902] 2024-11-05 04:18:15,150 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:18:15,150 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:18:15,151 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:18:18,068 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:18,069 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:18:18,118 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:18,119 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:18,121 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:18,121 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:18,121 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:18,121 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:18,122 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:18,122 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:18:18,122 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:18,123 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:18:18,284 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:18:19,264 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:18:19,264 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:18:19,899 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:18:19,901 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:18:19,904 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:18:19,904 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:18:19,904 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.16
  eval_samples            =       5463
  eval_samples_per_second =    885.639
  eval_steps_per_second   =      0.486
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.06s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
[INFO|training_args.py:1902] 2024-11-05 04:18:26,127 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:18:26,128 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:18:26,128 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:18:29,180 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:29,181 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:18:29,249 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:29,250 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:29,252 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:29,252 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:29,252 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:29,252 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:29,252 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:29,252 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:18:29,252 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:29,253 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:18:29,386 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:18:30,374 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:18:30,374 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:18:31,156 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:18:31,158 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:18:31,161 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:18:31,161 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:18:31,161 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.16
  eval_samples            =       5463
  eval_samples_per_second =    886.485
  eval_steps_per_second   =      0.487
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.97it/s]
[INFO|training_args.py:1902] 2024-11-05 04:18:32,903 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:18:32,904 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:18:32,904 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:18:35,510 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:35,511 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:18:35,561 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:35,562 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:35,564 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:35,564 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:35,564 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:35,564 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:35,564 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:18:35,564 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:18:35,564 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:18:35,565 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:18:35,684 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:18:36,646 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:18:36,646 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:18:37,451 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:18:37,453 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:18:37,455 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:18:37,455 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:18:37,455 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6191
  eval_pearson            =      0.872
  eval_runtime            = 0:00:01.68
  eval_samples            =       1500
  eval_samples_per_second =    890.492
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.594
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.83it/s]
2024-11-05 04:18:39,220 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:18:39,223 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models rte: 0.6570397111913358
2024-11-05 04:18:39,224 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models mrpc: 0.8683274021352313
2024-11-05 04:18:39,225 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models cola: 0.49867676415016876
2024-11-05 04:18:39,227 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:18:39,228 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models stsb: 0.8719616144746848
2024-11-05 04:18:39,229 - INFO - [diffusion][Epoch 11998] average DIFF reconstruction auto_encoder_models accuracy: 0.7912290398873996
2024-11-05 04:18:39,230 - INFO - [diffusion][Epoch 11998] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6570397111913358), ('rte', 0.6498194945848376), ('mrpc', 0.8683274021352313), ('mrpc', 0.8683274021352313), ('cola', 0.49626557567073315), ('cola', 0.49867676415016876), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.8719616144746848), ('stsb', 0.8719270561804288)]
2024-11-05 04:18:39,231 - INFO - [diffusion][Epoch 11998] ---------------------------------
2024-11-05 04:18:39,318 - INFO - [diffusion][Epoch 11998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:39,319 - INFO - [diffusion][Epoch 11999] Epoch 12000/12000
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6202
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.71
  eval_samples            =       1500
  eval_samples_per_second =    873.726
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.582
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:18:42,667 - INFO - [diffusion][Epoch 11999] diffusion training Loss: 0.04368923511356115
2024-11-05 04:18:42,669 - INFO - [diffusion][Epoch 11999] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:19:05,764 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:19:05,765 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:19:05,766 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:19:08,469 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:08,471 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:19:08,517 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:08,518 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:08,522 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:08,522 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:08,522 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:08,522 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:08,522 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:08,522 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:19:08,522 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:08,523 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:19:08,661 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:19:09,702 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:19:09,702 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:19:10,688 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:19:10,690 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:19:10,693 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:19:10,693 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:19:10,693 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.12it/s]
[INFO|training_args.py:1902] 2024-11-05 04:19:11,698 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:19:11,698 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:19:11,699 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:19:14,884 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:14,885 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:19:14,937 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:14,938 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:14,940 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:14,940 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:14,940 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:14,940 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:14,940 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:14,940 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:19:14,940 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:14,941 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:19:15,074 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:19:16,096 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:19:16,096 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:19:17,394 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:19:17,396 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:19:17,398 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:19:17,398 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:19:17,398 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:00.94
  eval_samples            =        872
  eval_samples_per_second =     926.87
  eval_steps_per_second   =      1.063
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.40it/s]
[INFO|training_args.py:1902] 2024-11-05 04:19:18,530 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:19:18,530 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:19:18,531 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:19:21,146 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:21,147 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:19:21,240 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:21,241 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:21,243 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:21,243 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:21,243 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:21,243 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:21,243 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:21,243 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:19:21,244 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:21,245 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:19:21,366 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:19:22,366 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:19:22,366 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:19:23,119 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:19:23,121 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:19:23,124 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:19:23,124 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:19:23,124 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =      0.201
  eval_runtime            = 0:00:01.07
  eval_samples            =        872
  eval_samples_per_second =    812.316
  eval_steps_per_second   =      0.932
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.20it/s]
[INFO|training_args.py:1902] 2024-11-05 04:19:23,655 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:19:23,655 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:19:23,656 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:19:26,707 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:26,708 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:19:26,769 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:26,770 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:26,772 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:26,772 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:26,772 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:26,772 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:26,772 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:26,772 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:19:26,773 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:26,774 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:19:26,901 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:19:27,896 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:19:27,897 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:19:28,981 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:19:28,983 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:19:28,985 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:19:28,985 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:19:28,985 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6606
  eval_loss               =     0.6464
  eval_runtime            = 0:00:00.46
  eval_samples            =        277
  eval_samples_per_second =    592.228
  eval_steps_per_second   =      2.138
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 45.60it/s]
[INFO|training_args.py:1902] 2024-11-05 04:19:29,449 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:19:29,450 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:19:29,451 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:19:32,670 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:32,671 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:19:32,726 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:32,727 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:32,729 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:32,729 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:32,729 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:32,729 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:32,729 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:32,729 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:19:32,730 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:32,730 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:19:32,861 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:19:33,832 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:19:33,834 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:19:34,847 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:19:34,849 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:19:34,852 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:19:34,852 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:19:34,852 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.657
  eval_loss               =     0.6466
  eval_runtime            = 0:00:00.40
  eval_samples            =        277
  eval_samples_per_second =    687.227
  eval_steps_per_second   =      2.481
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.56it/s]
[INFO|training_args.py:1902] 2024-11-05 04:19:35,742 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:19:35,743 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:19:35,743 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:19:38,648 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:38,649 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:19:38,723 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:38,724 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:38,726 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:38,726 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:38,726 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:38,726 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:38,726 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:38,726 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:19:38,726 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:38,727 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:19:38,876 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:19:39,831 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:19:39,831 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:19:40,927 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:19:40,928 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:19:40,930 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:19:40,930 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:19:40,930 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4119
  eval_runtime            = 0:00:00.83
  eval_samples            =        408
  eval_samples_per_second =    490.783
  eval_steps_per_second   =      1.203
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.03it/s]
[INFO|training_args.py:1902] 2024-11-05 04:19:41,511 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:19:41,511 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:19:41,512 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:19:44,092 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:44,093 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:19:44,141 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:44,142 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:44,144 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:44,144 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:44,144 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:44,144 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:44,144 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:44,144 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:19:44,145 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:44,146 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:19:44,289 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:19:45,298 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:19:45,299 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:19:46,176 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:19:46,178 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:19:46,179 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:19:46,179 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:19:46,179 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8186
  eval_combined_score     =     0.8435
  eval_f1                 =     0.8683
  eval_loss               =     0.4118
  eval_runtime            = 0:00:00.51
  eval_samples            =        408
  eval_samples_per_second =    790.732
  eval_steps_per_second   =      1.938
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 42.12it/s]
[INFO|training_args.py:1902] 2024-11-05 04:19:47,300 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:19:47,300 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:19:47,301 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:19:50,318 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:50,319 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:19:50,367 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:50,368 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:50,370 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:50,370 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:50,370 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:50,370 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:50,370 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:50,370 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:19:50,371 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:50,371 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:19:50,518 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:19:51,527 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:19:51,528 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:19:52,568 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:19:52,570 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:19:52,572 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:19:52,573 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:19:52,573 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4909
  eval_matthews_correlation =     0.4963
  eval_runtime              = 0:00:01.06
  eval_samples              =       1043
  eval_samples_per_second   =    983.187
  eval_steps_per_second     =      0.943
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.82it/s]
[INFO|training_args.py:1902] 2024-11-05 04:19:53,819 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:19:53,819 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:19:53,820 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:19:56,591 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:56,592 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:19:56,645 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:56,646 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:56,648 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:56,648 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:56,648 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:56,648 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:56,648 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:19:56,649 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:19:56,649 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:19:56,650 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:19:56,774 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:19:57,740 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:19:57,740 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:19:58,832 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:19:58,834 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:19:58,837 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:19:58,837 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:19:58,837 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4913
  eval_matthews_correlation =     0.4987
  eval_runtime              = 0:00:01.18
  eval_samples              =       1043
  eval_samples_per_second   =    878.039
  eval_steps_per_second     =      0.842
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.06s/it]
100% 3/3 [00:03<00:00,  1.15s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
[INFO|training_args.py:1902] 2024-11-05 04:20:05,257 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:20:05,257 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:20:05,258 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:20:08,390 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:20:08,392 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:20:08,447 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:20:08,448 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:08,450 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:08,450 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:08,451 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:08,451 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:08,451 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:08,451 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:20:08,451 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:20:08,452 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:20:08,608 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:20:09,585 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:20:09,585 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:20:10,268 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:20:10,270 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence, question, idx. If sentence, question, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:20:10,273 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:20:10,273 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:20:10,273 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:06.36
  eval_samples            =       5463
  eval_samples_per_second =    858.201
  eval_steps_per_second   =      0.471
trainable params: 0 || all params: 124,720,898 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.02it/s]
100% 3/3 [00:03<00:00,  1.10s/it]
100% 3/3 [00:03<00:00,  1.09s/it]
[INFO|training_args.py:1902] 2024-11-05 04:20:16,097 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:20:16,098 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:20:16,099 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:20:19,468 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:20:19,469 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:20:19,514 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:20:19,516 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:19,519 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:19,519 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:19,519 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:19,519 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:19,519 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:19,520 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:20:19,520 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:20:19,521 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:20:19,670 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:20:20,637 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:20:20,637 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:20:21,562 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:20:21,563 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:20:21,565 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:20:21,565 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:20:21,565 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9193
  eval_loss               =     0.2104
  eval_runtime            = 0:00:05.77
  eval_samples            =       5463
  eval_samples_per_second =    946.101
  eval_steps_per_second   =       0.52
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.30it/s]
[INFO|training_args.py:1902] 2024-11-05 04:20:23,440 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:20:23,441 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:20:23,441 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:20:26,280 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:20:26,281 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:20:26,335 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:20:26,336 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:26,338 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:26,339 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:26,339 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:26,339 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:26,339 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:20:26,339 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:20:26,339 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:20:26,340 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:20:26,462 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:20:27,435 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:20:27,436 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:20:28,564 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:20:28,566 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:20:28,569 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:20:28,569 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:20:28,569 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =       0.62
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.81
  eval_samples            =       1500
  eval_samples_per_second =    825.682
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =       0.55
trainable params: 0 || all params: 124,720,129 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.36it/s]
2024-11-05 04:20:30,163 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:20:30,166 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models rte: 0.6606498194945848
2024-11-05 04:20:30,168 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models mrpc: 0.8683274021352313
2024-11-05 04:20:30,169 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models cola: 0.49867676415016876
2024-11-05 04:20:30,170 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models qnli: 0.9192751235584844
2024-11-05 04:20:30,171 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models stsb: 0.8719316714326395
2024-11-05 04:20:30,172 - INFO - [diffusion][Epoch 11999] average DIFF reconstruction auto_encoder_models accuracy: 0.7921243382617639
2024-11-05 04:20:30,173 - INFO - [diffusion][Epoch 11999] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6606498194945848), ('rte', 0.6570397111913358), ('mrpc', 0.8683274021352313), ('mrpc', 0.8683274021352313), ('cola', 0.49626557567073315), ('cola', 0.49867676415016876), ('qnli', 0.9192751235584844), ('qnli', 0.9192751235584844), ('stsb', 0.8719316714326395), ('stsb', 0.8718702548050988)]
2024-11-05 04:20:30,174 - INFO - [diffusion][Epoch 11999] ---------------------------------
2024-11-05 04:20:30,268 - INFO - [diffusion][Epoch 11999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:30,271 - INFO - [diffusion][Epoch 11999] Training complete
***** eval metrics *****
  eval_combined_score     =     0.8738
  eval_loss               =     0.6204
  eval_pearson            =     0.8719
  eval_runtime            = 0:00:01.54
  eval_samples            =       1500
  eval_samples_per_second =    972.447
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.648
=====================================
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb: \ 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb: | 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: diffusion learning rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    diffusion train_loss █▅▄▃▄▆▄▅▇▂▅▄▅▇▅▄▆▂▃▃▃▃▁▂▁▃▃▂▃▄▂▄▂▁▁▃▃▃▁▃
wandb: 
wandb: Run summary:
wandb: diffusion learning rate 0.001
wandb:    diffusion train_loss 0.04369
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/jin509/para_diff/lora-cond-p-diff/wandb/offline-run-20241105_002034-3bqcc928
wandb: Find logs at: ./wandb/offline-run-20241105_002034-3bqcc928/logs
